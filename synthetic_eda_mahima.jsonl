{"purpose": "Analyze monthly transaction patterns and customer demographics for a retail bank.", "raw_table": "TransactionID,CustomerID,TransactionDate,TransactionAmount,AccountType,Branch,CustomerAge\nT1001,C123,2023-01-15,250.75,Checking,Downtown,34\nT1002,c124,15/01/2023,,-Savings,midtown,45\nT1003,C125,2023/01/16,1000.00,Checking,UPTOWN,29\nT1004,C126,2023-01-17,540.2,SAVINGS,Downtown,NA\nT1005,C127,17-01-2023,89.99,Checking,Midtown,38\nT1006,C128,,120.00,Checking,Uptown,40\nT1007,C129,2023-01-18,NA,Checking,downtown,50\nT1008,c130,2023-1-19,310.50,Savings,MidTown,27\nT1009,C131,2023-01-20,-150,Checking,Downtown,33\nT1010,C132,2023-01-21,450.00,Retail,Downtown,41\nT1011,C133,2023-01-22,600.00,Checking,Midtown,NA\nT1012,C134,01-23-2023,200.00,Savings,Midtown,36", "eda_steps": ["Check for missing values in each column", "Standardize 'TransactionDate' format to YYYY-MM-DD", "Identify and count unique AccountType values", "Calculate descriptive statistics for TransactionAmount", "Detect negative or zero values in TransactionAmount", "Count number of transactions per Branch", "Summarize CustomerAge distribution and missing values", "Compute correlation between CustomerAge and TransactionAmount", "List top 3 branches by total transaction amount"], "eda_results": {"missing_values": {"TransactionID": 0, "CustomerID": 0, "TransactionDate": 1, "TransactionAmount": 2, "AccountType": 0, "Branch": 0, "CustomerAge": 2}, "standardized_dates_sample": ["2023-01-15", "2023-01-15", "2023-01-16", "2023-01-17", "2023-01-17", "missing", "2023-01-18", "2023-01-19", "2023-01-20", "2023-01-21", "2023-01-22", "2023-01-23"], "account_type_counts": {"Checking": 7, "Savings": 4, "SAVINGS": 1, "Retail": 1}, "transaction_amount_stats": {"count": 11, "mean": 345.95, "std_dev": 308.9, "min": -150, "25%": 120.0, "50%": 250.75, "75%": 540.2, "max": 1000.0}, "negative_or_zero_amounts": 1, "transactions_per_branch": {"Downtown": 5, "Midtown": 4, "UPTOWN": 1, "Uptown": 1}, "customer_age_summary": {"count": 12, "missing": 2, "mean": 37.6, "min": 27, "max": 50}, "correlation_customerage_transactionamount": 0.3, "top_3_branches_by_transaction_amount": {"Downtown": 1391.95, "Midtown": 1150.49, "UPTOWN": 1000.0}}}
{"purpose": "Analyze customer churn patterns and usage statistics in a telecom dataset to identify potential risk factors.", "raw_table": "CustomerID,SignupDate,LastCallDate,MonthlyCharges,ContractType,Churn,DataUsageGB\n1001,2021-01-15,2022/03/20,45.50,month-to-month,Yes,12.5\n1002,2020-12-05,2022-03-18,,One year,No,8.1\n1003,15-02-2021,2022-03-19,55.75,Month-To-Month,yes,15\n1004,2021/03/01,,60.00,TWO YEAR,No,16.3\n1005,2021-04-20,2022-03-15,39.99,month-to-month,,7.8\n1006,2021-05-25,2022-03-21,44.00,one year,No,NaN\n1007,2021-06-10,2022-03-20,49.95,MONTH-TO-MONTH,Yes,13.2\n1008,2021-07-05,2022-03-22,NaN,Two Year,No,14.0\n1009,2021-08-15,2022-03-16,52.25,month to month,Yes,9.5\n1010,2021-09-30,2022-03-20,47.00,one Year,No,10.1\n1011,,2022-03-18,50.00,Month-to-month,Yes,11.7\n1012,2021-11-12,2022-03-19,48.50,One YEAR,No,8.9\n1013,2021-12-01,2022-03-21,55.00,Month-to-Month,No,12.0\n1014,2022-01-05,2022-03-22,60.00,Two year,No,15.5", "eda_steps": ["Check for missing values in each column", "Standardize the ContractType values to consistent capitalization", "Convert SignupDate and LastCallDate to a uniform date format", "Calculate descriptive statistics for MonthlyCharges and DataUsageGB", "Generate value counts for Churn status", "Identify the number of unique customers and duplicate entries if any", "Analyze correlation between MonthlyCharges and DataUsageGB", "Determine the average MonthlyCharges for each ContractType", "Summarize the distribution skewness for MonthlyCharges"], "eda_results": {"missing_values": {"CustomerID": 0, "SignupDate": 1, "LastCallDate": 1, "MonthlyCharges": 2, "ContractType": 0, "Churn": 1, "DataUsageGB": 1}, "standardized_contract_types": ["month-to-month", "one year", "month-to-month", "two year", "month-to-month", "one year", "month-to-month", "two year", "month-to-month", "one year", "month-to-month", "one year", "month-to-month", "two year"], "date_conversion_success": {"SignupDate": 13, "LastCallDate": 14}, "summary_stats": {"MonthlyCharges": {"count": 12, "mean": 50.74, "std": 6.68, "min": 39.99, "25%": 45.38, "50%": 49.95, "75%": 55.75, "max": 60.0, "skewness": 0.12}, "DataUsageGB": {"count": 13, "mean": 12.68, "std": 2.87, "min": 7.8, "25%": 9.5, "50%": 12.5, "75%": 15.0, "max": 16.3}}, "value_counts": {"Churn": {"Yes": 6, "No": 7, "missing": 1}}, "unique_customers": 14, "correlations": {"MonthlyCharges_vs_DataUsageGB": 0.79}, "average_monthly_charges_by_contract": {"month-to-month": 48.44, "one year": 49.69, "two year": 60.0}}}
{"purpose": "Analyze monthly transactions and customer behavior in a retail banking dataset to identify data quality issues and summarize key metrics.", "raw_table": "TransactionID,CustomerID,TransactionDate,TransactionType,Amount,Branch,AccountType\nT001,C101,2023-01-15,Deposit,5000,New York,Checking\nT002,C102,15/02/2023,withdrawal,,-,Savings\nT003,c103,2023/03/10,Deposit,1500,los angeles,Checking\nT004,C104,2023-04-05,TRANSFER,2000,Chicago,Savings\nT005,C105,2023-05-20,Withdrawal,-300,CHICAGO,Checking\nT006,C106,06-15-2023,deposit,1000,Boston,checking\nT007,C107,,Withdrawal,700,Boston,Savings\nT008,C108,2023-07-01,Deposit,abc,New York,Checking\nT009,C109,2023-08-12,Transfer,2500,LOS ANGELES,Savings\nT010,C110,2023-09-30,Withdrawal,1200,Boston,Savings\nT011,C111,2023-10-15,Deposit,0,New york,Checking", "eda_steps": ["Check and report missing values in each column", "Standardize the capitalization in categorical columns: TransactionType, Branch, AccountType", "Parse and standardize the TransactionDate column to a uniform date format", "Compute descriptive statistics for the Amount column, ignoring non-numeric entries", "Generate value counts for TransactionType and AccountType", "Identify transactions with negative or zero Amount values", "Calculate the number of unique customers and their transaction counts", "Summarize the distribution of transactions per Branch"], "eda_results": {"missing_values": {"TransactionID": 0, "CustomerID": 0, "TransactionDate": 1, "TransactionType": 0, "Amount": 2, "Branch": 1, "AccountType": 0}, "standardized_categories": {"TransactionType": ["Deposit", "Withdrawal", "Transfer"], "Branch": ["New York", "Boston", "Chicago", "Los Angeles"], "AccountType": ["Checking", "Savings"]}, "parsed_dates_summary": {"earliest_date": "2023-01-15", "latest_date": "2023-10-15", "invalid_dates": 1}, "amount_stats": {"count": 11, "mean": 1272.73, "std_dev": 1188.88, "min": -300, "25%": 700, "50%": 1200, "75%": 2000, "max": 5000}, "value_counts": {"TransactionType": {"Deposit": 5, "Withdrawal": 4, "Transfer": 2}, "AccountType": {"Checking": 6, "Savings": 5}}, "negative_or_zero_amounts": {"count": 2, "TransactionIDs": ["T005", "T011"]}, "unique_customers_count": 11, "transactions_per_customer": {"C101": 1, "C102": 1, "C103": 1, "C104": 1, "C105": 1, "C106": 1, "C107": 1, "C108": 1, "C109": 1, "C110": 1, "C111": 1}, "transactions_per_branch": {"New York": 3, "Boston": 3, "Chicago": 2, "Los Angeles": 2}}}
{"purpose": "Analyze viewer ratings and genre popularity trends for recent movies.", "raw_table": "MovieID,Title,Genre,ReleaseDate,ViewerRating,NumReviews,BoxOfficeMillion\n1,The Last Quest,Action,2023-07-15,8.2,1500,120.5\n2,love in paris,romance,15-Aug-2023,7.9,,85.3\n3,Shadow Realm,Thriller,2023/09/02,6.5,840,55.2\n4,The Last Quest,Action,2023-07-15,8.2,1500,120.5\n5,Old Town Mystery,thriller,2023-07-01,,450,40\n6,Comedy Nights,Comedy,2023-06-20,7.1,1120,60.1\n7,Drama Unfolds,Drama,2023-08-10,7.3,980,n/a\n8,The Lost World,Adventure,2023-07-25,6.9,760,72.9\n9,Shadow realm,THRILLER,2023/09/02,6.5,840,55.2\n10,love in Paris,Romance,2023-08-15,7.9,900,85.3\n11,Comedy nights,comedy,06/20/2023,7.1,1120,60.1\n12,New Horizons,Sci-Fi,2023-10-01,8.0,1300,98.7\n13,Old town mystery,Thriller,2023-07-01,5.8,450,40\n14,Drama unfolds,Drama,2023-08-10,7.3,980,62.0", "eda_steps": ["Check missing value percentages for each column", "Standardize and clean Genre and Title columns for consistent capitalization", "Generate value counts for Genre", "Identify duplicate movies by Title and ReleaseDate", "Compute descriptive statistics for ViewerRating and NumReviews", "Summarize BoxOfficeMillion statistics excluding missing or non-numeric values", "Check correlation between ViewerRating, NumReviews, and BoxOfficeMillion", "Identify top 3 movies by BoxOfficeMillion", "Count number of unique release dates"], "eda_results": {"missing_values": {"MovieID": 0, "Title": 0, "Genre": 0, "ReleaseDate": 0, "ViewerRating": 2, "NumReviews": 1, "BoxOfficeMillion": 2}, "cleaned_genre_value_counts": {"Thriller": 5, "Action": 2, "Romance": 2, "Comedy": 2, "Drama": 2, "Adventure": 1, "Sci-Fi": 1}, "duplicate_movies": [{"Title": "The Last Quest", "ReleaseDate": "2023-07-15", "count": 2}, {"Title": "Shadow Realm", "ReleaseDate": "2023-09-02", "count": 2}, {"Title": "Love in Paris", "ReleaseDate": "2023-08-15", "count": 2}, {"Title": "Comedy Nights", "ReleaseDate": "2023-06-20", "count": 2}, {"Title": "Old Town Mystery", "ReleaseDate": "2023-07-01", "count": 2}, {"Title": "Drama Unfolds", "ReleaseDate": "2023-08-10", "count": 2}], "viewer_rating_stats": {"count": 12, "mean": 7.13, "std": 0.76, "min": 5.8, "25%": 6.5, "50%": 7.15, "75%": 7.9, "max": 8.2}, "num_reviews_stats": {"count": 13, "mean": 980, "std": 335.4, "min": 450, "25%": 760, "50%": 900, "75%": 1120, "max": 1500}, "box_office_stats": {"count": 12, "mean": 70.6, "std": 26.4, "min": 40, "25%": 55.2, "50%": 60.1, "75%": 85.3, "max": 120.5}, "correlations": {"ViewerRating_vs_NumReviews": 0.45, "ViewerRating_vs_BoxOfficeMillion": 0.52, "NumReviews_vs_BoxOfficeMillion": 0.78}, "top_3_movies_box_office": [{"Title": "The Last Quest", "BoxOfficeMillion": 120.5}, {"Title": "Love in Paris", "BoxOfficeMillion": 85.3}, {"Title": "New Horizons", "BoxOfficeMillion": 98.7}], "unique_release_dates_count": 9}}
{"purpose": "Analyze user engagement patterns and content types on a social media platform over a brief period.", "raw_table": "post_id,user_id,post_date,content_type,likes,comments,shares\n101,U001,2023-01-01,Photo,120,15,10\n102,u002,01/02/2023,video,95,20,,\n103,U003,2023/01/03,text,45,5,0\n104,U004,2023-1-04,Link,,8,2\n105,U005,2023-01-05,photo,130,,15\n106,U006,2023-01/06,Text,60,7,3\n107,U007,2023-01-07,VIDEO,85,12,5\n108,u008,2023/01/08,,70,10,1\n109,U009,2023-01-09,Photo,110,13,8\n110,U010,2023-01-10,video,NaN,18,6\n111,U011,2023/01/11,text,55,6,2\n112,u012,2023-01-12,link,40,4,0", "eda_steps": ["Check and report missing values for each column", "Standardize content_type values to lowercase and fill missing with 'unknown'", "Convert post_date to a consistent datetime format", "Calculate basic statistics (mean, median) for likes, comments, and shares", "Generate value counts for content_type", "Identify posts with missing or zero engagement metrics", "Summarize date range and number of posts per day", "Examine correlation between likes, comments, and shares"], "eda_results": {"missing_values": {"post_id": 0, "user_id": 0, "post_date": 0, "content_type": 1, "likes": 3, "comments": 2, "shares": 1}, "content_type_value_counts": {"photo": 3, "video": 3, "text": 3, "link": 2, "unknown": 1}, "post_date_range": {"start_date": "2023-01-01", "end_date": "2023-01-12", "total_posts": 12}, "engagement_stats": {"likes": {"mean": 75.5, "median": 70, "missing_count": 3}, "comments": {"mean": 10.1, "median": 8, "missing_count": 2}, "shares": {"mean": 5.1, "median": 3, "missing_count": 1}}, "posts_with_missing_or_zero_engagement": {"post_ids": [102, 104, 105, 108, 110]}, "posts_per_day": {"2023-01-01": 1, "2023-01-02": 1, "2023-01-03": 1, "2023-01-04": 1, "2023-01-05": 1, "2023-01-06": 1, "2023-01-07": 1, "2023-01-08": 1, "2023-01-09": 1, "2023-01-10": 1, "2023-01-11": 1, "2023-01-12": 1}, "correlations": {"likes_comments": 0.88, "likes_shares": 0.79, "comments_shares": 0.85}}}
{"purpose": "Analyze defect rates and production metrics across different manufacturing lines to identify patterns and data quality issues.", "raw_table": "Production_Line,Date,Shift,Defects,Units_Produced,Operator,Machine_ID\nLine A,2024-01-01,Morning,5,500,John Doe,M-100\nline b,01/02/2024,Night,2,450,,M-101\nLine A,2024-01-03,afternoon,,480,Jane Smith,M-100\nLine C,2024/01/04,Morning,1,400,ALex,BadMachine\nLine B,2024-01-05,MORNING,3,470,John Doe,M-101\nLine A,2024-1-06,Night,0,510,jane smith,M-100\nline c,2024-01-07,Afternoon,4,,Alex,M-102\nLine B,,Morning,2,460,Mark,M-101\nLine A,2024-01-09,Night,3,500,John Doe,M-100\nLine C,2024-01-10,Morning,NaN,420,alex,M-102", "eda_steps": ["Check the percentage of missing values in each column", "Standardize the capitalization of categorical columns like Production_Line, Shift, and Operator", "Parse and standardize the date format in the Date column", "Compute descriptive statistics for numeric columns Defects and Units_Produced", "Generate value counts for Production_Line and Shift columns", "Identify unique Machine_ID values and check for unusual entries", "Calculate correlation between Defects and Units_Produced", "List operators with the highest average defect rates"], "eda_results": {"missing_values": {"Production_Line": 0, "Date": 1, "Shift": 0, "Defects": 2, "Units_Produced": 1, "Operator": 1, "Machine_ID": 1}, "standardized_categories": {"Production_Line": ["Line A", "Line B", "Line C"], "Shift": ["Morning", "Night", "Afternoon"], "Operator": ["John Doe", "Jane Smith", "Alex", "Mark"]}, "date_parsing_issues": {"original_formats": ["2024-01-01", "01/02/2024", "2024/01/04", "2024-1-06", null], "standardized_format": "YYYY-MM-DD", "unparsed_rows": 1}, "summary_stats": {"Defects": {"count": 8, "mean": 2.5, "min": 0, "max": 5, "std_dev": 1.66}, "Units_Produced": {"count": 9, "mean": 468.9, "min": 400, "max": 510, "std_dev": 38.2}}, "value_counts": {"Production_Line": {"Line A": 5, "Line B": 3, "Line C": 3}, "Shift": {"Morning": 5, "Night": 3, "Afternoon": 2}}, "machine_id_issues": {"unique_ids": ["M-100", "M-101", "M-102", "BadMachine"], "unusual_entries": ["BadMachine"]}, "correlations": {"Defects_vs_Units_Produced": -0.45}, "top_operators_by_avg_defects": {"John Doe": 3.3, "Jane Smith": 0, "Alex": 2.5, "Mark": 2}}}
{"purpose": "Explore student performance and attendance trends in a high school semester dataset.", "raw_table": "StudentID,Name,Grade,Subject,Score,Attendance,ExamDate\n101,john doe,10,Math,85,Present,2023/03/12\n102,Jane Smith,10,science,78,Absent,03-15-2023\n103,alice jones,11,Math,,Present,2023-03-14\n104,Bob Brown,11,English,88,,March 16 2023\n105,CHARLIE black,10,Science,92,Present,2023/03/15\n106,david white,12,math,73,Absent,\n107,Eva Green,12,English,90,Present,2023/03/13\n108,Frank Moore,11,Science,85,Present,03/14/2023\n109,Grace Lee,10,english,NaN,Absent,2023-3-16\n110,henry Kim,12,Math,95,Present,2023/03/12", "eda_steps": ["Check the data types of each column", "Identify and count missing values in each column", "Standardize and clean the 'Subject' and 'Name' columns for consistent capitalization", "Convert 'ExamDate' to a uniform date format", "Calculate basic descriptive statistics (mean, median, std) for 'Score'", "Count the frequency of each category in the 'Attendance' column", "Group the average 'Score' by 'Grade' and 'Subject'", "Identify the number of students missing scores", "Check correlation between 'Score' and 'Attendance' status encoded as binary"], "eda_results": {"data_types": {"StudentID": "int64", "Name": "object", "Grade": "int64", "Subject": "object", "Score": "float64", "Attendance": "object", "ExamDate": "object"}, "missing_values": {"Score": 2, "Attendance": 1, "ExamDate": 1}, "cleaned_subjects": {"Math": 4, "Science": 3, "English": 3}, "score_statistics": {"mean": 85.1, "median": 86.5, "std_dev": 7.3}, "attendance_counts": {"Present": 6, "Absent": 3, "Missing": 1}, "average_score_by_grade_subject": {"10_Math": 85, "10_Science": 85, "10_English": null, "11_Math": null, "11_Science": 85, "11_English": 88, "12_Math": 84, "12_English": 90}, "students_missing_scores": 2, "score_attendance_correlation": -0.42}}
{"purpose": "Examine the distribution and completeness of public housing applications submitted in 2023 to identify common applicant demographics and data quality issues.", "raw_table": "ApplicationID,ApplicantName,DateSubmitted,IncomeLevel,HouseholdSize,Region,Status\nA001,John Doe,2023-01-15,35000,3,north,Approved\nA002,jane smith,15/02/2023,42000,4,South,Denied\nA003,,2023/03/05,28000,2,NORTH,Pending\nA004,Bob Lee,2023-04-22,NaN,5,East,approved\nA005,Alice,2023.05.18,37000,3,west,Denied\nA006,Mary Ann,Jun 1 2023,45000,,South,Pending\nA007,Tom O'Neil,2023-06-15,32000,2,East,Approved\nA008,NA,2023-07-20,39000,4,West,Denied\nA009,Mike Brown,2023-08-05,,3,North,Pending\nA010,Sara K,08/20/2023,41000,3,south,APPROVED\nA011,Linda,2023-09-15,36000,2,East,Denied\nA012,James,2023-10-01,38000,NaN,West,pending\nA013,NaN,2023-11-12,40000,4,north,Approved", "eda_steps": ["Check for missing values in each column and calculate their percentages", "Standardize the 'Region' and 'Status' columns to uniform capitalization", "Compute basic descriptive statistics for 'IncomeLevel' and 'HouseholdSize'", "Generate value counts for the 'Status' and 'Region' columns", "Identify the number of unique applicants (non-missing ApplicantName)", "Analyze date format consistency and convert 'DateSubmitted' to a standard datetime format", "Calculate the average income level by Region", "Summarize the distribution of HouseholdSize across application statuses"], "eda_results": {"missing_values": {"ApplicationID": "0%", "ApplicantName": "15.4%", "DateSubmitted": "0%", "IncomeLevel": "15.4%", "HouseholdSize": "15.4%", "Region": "0%", "Status": "0%"}, "standardized_categories": {"Region": ["North", "South", "East", "West"], "Status": ["Approved", "Denied", "Pending"]}, "summary_stats": {"IncomeLevel": {"count": 11, "mean": 37545.45, "std": 5605.11, "min": 28000, "max": 45000}, "HouseholdSize": {"count": 11, "mean": 3.09, "std": 1.11, "min": 2, "max": 5}}, "value_counts": {"Status": {"Approved": 4, "Denied": 4, "Pending": 4}, "Region": {"North": 4, "South": 3, "East": 3, "West": 4}}, "unique_applicants": 11, "date_format_issues": {"original_formats_detected": ["YYYY-MM-DD", "DD/MM/YYYY", "YYYY/MM/DD", "YYYY.MM.DD", "MMM D YYYY", "MM/DD/YYYY"], "dates_standardized": true}, "avg_income_by_region": {"North": 34333.33, "South": 43500, "East": 34666.67, "West": 38000}, "household_size_by_status": {"Approved": {"mean": 2.5, "min": 2, "max": 3}, "Denied": {"mean": 3.5, "min": 2, "max": 5}, "Pending": {"mean": 3.67, "min": 2, "max": 4}}}}
{"purpose": "Analyze sales performance and customer purchase behavior across different store locations.", "raw_table": "OrderID,CustomerID,ProductCategory,SaleDate,Quantity,UnitPrice,StoreLocation\n1001,C123,eLEctroniCs,2023-01-15,2,299.99,New York\n1002,C124,Home & kitchen,01/20/2023,1,89.5,Boston\n1003,,electronics,2023/01/22,3,299.99,NEW YORK\n1004,C125,Toys,15-Feb-2023,2,19.99,Los Angeles\n1005,C126,clothing,2023-02-17,1,,san francisco\n1006,C127,Clothing,2023-02-18,2,49.99,San Francisco\n1007,C128,home & Kitchen,2023-02-20,5,22.99,Boston\n1008,C129,ELECtronics,2023-2-21,1,299.99,new york\n1009,C130,Toys,,1,19.99,los angeles\n1010,C131,Clothing,2023-02-25, ,49.99,San Francisco\n1011,C132,home & kitchen,02/27/2023,2,89.5,Boston\n1012,C133,Toys,2023-02-28,3,19.99,Los Angeles", "eda_steps": ["Check and count missing values in each column", "Standardize the formatting of categorical columns ProductCategory and StoreLocation", "Convert SaleDate to a consistent date format and identify missing dates", "Compute descriptive statistics for Quantity and UnitPrice columns", "Calculate total sales per ProductCategory", "Identify top 2 StoreLocations by total sales", "Examine the distribution of Quantity purchased", "Generate value counts for ProductCategory after cleaning"], "eda_results": {"missing_values": {"OrderID": 0, "CustomerID": 1, "ProductCategory": 0, "SaleDate": 1, "Quantity": 1, "UnitPrice": 1, "StoreLocation": 0}, "standardized_categories": {"ProductCategory": ["Electronics", "Home & Kitchen", "Toys", "Clothing"], "StoreLocation": ["New York", "Boston", "Los Angeles", "San Francisco"]}, "date_cleaning": {"valid_dates": 13, "missing_dates": 1}, "summary_stats": {"Quantity": {"count": 14, "mean": 2.21, "std": 1.37, "min": 1, "max": 5}, "UnitPrice": {"count": 13, "mean": 117.14, "std": 118.77, "min": 19.99, "max": 299.99}}, "total_sales_per_category": {"Electronics": 2099.93, "Home & Kitchen": 572.48, "Toys": 159.92, "Clothing": 149.97}, "top_store_locations": {"New York": 1799.94, "Boston": 572.48}, "quantity_distribution": {"1": 5, "2": 4, "3": 2, "5": 1, "missing": 1}, "product_category_value_counts": {"Electronics": 4, "Home & Kitchen": 4, "Toys": 4, "Clothing": 3}}}
{"purpose": "Examine citizen complaints filed at a local government office to identify common complaint types and data quality issues.", "raw_table": "complaint_id,submission_date,complaint_type,department,status,response_time_days\n101,2023/01/15,Noise,Public Works,Resolved,5\n102,15-02-2023,Water Leak,Water Services,Pending,\n103,2023-03-01,noise,public works,Resolved,3\n104,03/05/2023,Trash Collection, Sanitation,resolved,7\n105,,Illegal Parking,Transportation,Closed,2\n106,2023-04-12,Water leak,Water services,Pending,NA\n107,2023.04.18,Noise,Public works,Closed,4\n108,2023-05-01,Trash collection,Sanitation,,6\n109,2023-05-15,Illegal parking,Transportation,Resolved,3\n110,2023-05-20,Noise,Public Works,Closed,three\n111,05/25/2023,Noise,Public Works,Resolved,5\n112,2023-06-01,Water Leak,Water Services,resolved,4", "eda_steps": ["Standardize all complaint_type and department values to consistent capitalization", "Parse submission_date into a uniform date format and identify missing or malformed dates", "Calculate the percentage of missing values in each column", "Generate value counts for complaint_type and status columns", "Compute descriptive statistics (mean, median, min, max) for response_time_days after cleaning", "Identify entries with non-numeric or missing response_time_days values", "Analyze distribution of complaint submissions over time", "Check for duplicate complaint_id values"], "eda_results": {"missing_values": {"complaint_id": 0, "submission_date": 1, "complaint_type": 0, "department": 1, "status": 2, "response_time_days": 3}, "value_counts": {"complaint_type": {"Noise": 5, "Water Leak": 3, "Trash Collection": 2, "Illegal Parking": 2}, "status": {"Resolved": 5, "Pending": 2, "Closed": 3, "": 2}}, "response_time_days_stats": {"count": 9, "mean": 4.44, "median": 4, "min": 2, "max": 7}, "non_numeric_response_time_entries": [110], "standardized_departments": ["Public Works", "Water Services", "Sanitation", "Transportation"], "submission_date_issues": {"missing_or_invalid": [105], "formats_detected": ["YYYY/MM/DD", "DD-MM-YYYY", "YYYY-MM-DD", "MM/DD/YYYY", "YYYY.MM.DD"]}, "duplicate_complaint_ids": []}}
{"purpose": "Analyze customer purchase behavior and product category popularity over the last month.", "raw_table": "Order_ID,Customer_ID,Order_Date,Product_Category,Quantity,Unit_Price,Payment_Method,Customer_Region\n1001,C001,2024-05-01,eLEctronics,2,199.99,Credit Card,North\n1002,C002,5/2/2024,Home & kitchen,1,89.5,Paypal,south\n1003,C003,2024/05/03,Books,5,12.99,Credit card,East\n1004,C001,05-04-2024,electronics,1,,Debit Card,North\n1005,C004,,Clothing,3,45.00,cash,West\n1006,C005,2024-05-06,Gardening,2,34.99,credit Card,South\n1007,C006,2024-05-07,home & Kitchen,1,120.00,Paypal,East\n1008,C007,2024/05/08,Books,NaN,15.99,Debit Card,west\n1009,C008,2024-5-09,Clothing,1,40.00,Cash,North\n1010,C009,May 10 2024,Electronics,1,250,CREDIT CARD,South\n1011,C010,2024-05-11,gardening,4,invalid_price,Cash,East\n1012,C011,2024-05-12,Books,2,13.5,Paypal,North\n1013,C012,2024-05-13,Clothing,2,42.00,Credit Card,West\n1014,C013,2024-05-14,Electronics,,199.99,credit card,South\n1015,C014,2024-05-15,Home & Kitchen,1,85.00,debit card,East", "eda_steps": ["Parse and standardize the Order_Date column into a consistent date format", "Identify and count missing values in each column", "Normalize capitalization inconsistencies in Product_Category and Payment_Method columns", "Calculate descriptive statistics for Quantity and Unit_Price columns", "List unique values and their counts for Product_Category and Customer_Region", "Detect and summarize rows with invalid or missing numeric values", "Identify top 3 most frequent Product_Category values", "Analyze total sales (Quantity * Unit_Price) per Product_Category", "Check distribution skewness for Quantity and Unit_Price", "Summarize payment method usage frequency"], "eda_results": {"missing_values": {"Order_ID": 0, "Customer_ID": 0, "Order_Date": 1, "Product_Category": 0, "Quantity": 2, "Unit_Price": 2, "Payment_Method": 0, "Customer_Region": 0}, "normalized_categories": {"Product_Category": ["Electronics", "Home & Kitchen", "Books", "Clothing", "Gardening"], "Payment_Method": ["Credit Card", "Paypal", "Debit Card", "Cash"]}, "descriptive_stats": {"Quantity": {"count": 13, "mean": 2.0, "std_dev": 1.47, "min": 1, "max": 5}, "Unit_Price": {"count": 13, "mean": 90.61, "std_dev": 74.77, "min": 12.99, "max": 250.0}}, "value_counts": {"Product_Category": {"Electronics": 4, "Home & Kitchen": 3, "Books": 3, "Clothing": 3, "Gardening": 2}, "Customer_Region": {"North": 4, "South": 4, "East": 4, "West": 3}, "Payment_Method": {"Credit Card": 5, "Paypal": 3, "Debit Card": 3, "Cash": 3}}, "invalid_rows_summary": {"Quantity_missing_or_NaN": [8, 14], "Unit_Price_invalid_or_missing": [4, 11]}, "top_categories": ["Electronics", "Home & Kitchen", "Books"], "total_sales_per_category": {"Electronics": 1099.97, "Home & Kitchen": 294.5, "Books": 130.78, "Clothing": 169.0, "Gardening": 69.98}, "distribution_skewness": {"Quantity": 1.02, "Unit_Price": 1.4}}}
{"purpose": "Analyze user engagement patterns and content types on a social media platform over a two-week period.", "raw_table": "user_id,post_date,content_type,likes,comments,shares\nU001,2024-04-01,Video,120,15,8\nU002,4/2/2024,text,45,,2\nU003,2024-04-03,Image,87,9,missing\nU004,,Video,130,20,10\nU005,2024-04-05,Poll,NaN,5,3\nu006,2024-04-06,video,110,12,9\nU007,2024/04/07,Text,50,7,1\nU008,April 8 2024,Image,95,NaN,4\nU009,2024-04-09,Link,30,4,0\nU010,2024-4-10,Poll,60,6,2\nU011,2024-04-11,Video,NaN,13,7\nU012,4-12-2024,image,80,8,5\nU013,2024-04-13,Text,55,10,\nU014,2024-04-14,LINK,25,,1", "eda_steps": ["Check and summarize missing values for each column", "Standardize 'post_date' column to a uniform date format", "Normalize 'content_type' to consistent capitalization", "Calculate descriptive statistics for 'likes', 'comments', and 'shares'", "Generate value counts for 'content_type'", "Identify rows with missing or 'NaN' numeric engagement values", "Compute correlation matrix between 'likes', 'comments', and 'shares'", "Determine the top 2 content types by average likes", "Summarize the distribution skewness of 'comments'"], "eda_results": {"missing_values": {"user_id": 0, "post_date": 1, "content_type": 0, "likes": 3, "comments": 3, "shares": 2}, "post_date_standardized": {"all_dates_valid": true, "missing_count": 1}, "content_type_normalized_counts": {"Video": 4, "Text": 3, "Image": 3, "Poll": 2, "Link": 2}, "descriptive_statistics": {"likes": {"count": 12, "mean": 71.25, "std": 34.89, "min": 25, "25%": 46.25, "50%": 60, "75%": 95, "max": 130}, "comments": {"count": 11, "mean": 9.55, "std": 5.06, "min": 4, "25%": 6, "50%": 9, "75%": 13, "max": 20}, "shares": {"count": 12, "mean": 4.58, "std": 3.26, "min": 0, "25%": 2, "50%": 4.5, "75%": 7, "max": 10}}, "missing_numeric_rows": ["Row 2 (likes missing)", "Row 5 (likes NaN)", "Row 3 (shares 'missing')", "Row 8 (comments NaN)", "Row 13 (shares missing)"], "correlations": {"likes_comments": 0.91, "likes_shares": 0.85, "comments_shares": 0.78}, "top_content_types_by_avg_likes": {"Video": 120, "Image": 87}, "comments_skewness": 0.45}}
{"purpose": "Analyze daily ridership patterns and service types for a city's public transportation system.", "raw_table": "Date,Route,Service_Type,Ridership,On_Time_Percentage\n2024-01-01,Blue Line,express,1200,95.5\n01/02/2024,Green Line,Local,850,88\n2024/01/03,blue line,Express,1300,94.2\n2024-01-04,Red line,local,,90.1\n2024-01-05,Yellow Line,EXPRESS,950,missing\n2024-01-06,green line,Local,870,89.5\nJan 07 2024,Red Line,Local,760,91.3\n2024-01-08,Yellow line,express,980,92.0\n2024-01-09,Blue Line,local,1100,87.7\n2024-01-10,Green Line,Express,1050,93.8\n2024-01-11,red Line,Local,800,85.4", "eda_steps": ["Standardize the Date column to a uniform date format", "Normalize capitalization in the Route and Service_Type columns", "Check for missing values in all columns", "Compute descriptive statistics for Ridership and On_Time_Percentage", "Generate value counts for Route and Service_Type", "Calculate average ridership by Route and Service_Type", "Identify dates with missing Ridership or On_Time_Percentage values"], "eda_results": {"missing_values": {"Date": 0, "Route": 0, "Service_Type": 0, "Ridership": 1, "On_Time_Percentage": 1}, "value_counts": {"Route": {"Blue Line": 3, "Green Line": 3, "Red Line": 3, "Yellow Line": 2}, "Service_Type": {"Express": 5, "Local": 6}}, "summary_stats": {"Ridership": {"count": 11, "mean": 975.45, "std_dev": 178.2, "min": 760, "max": 1300}, "On_Time_Percentage": {"count": 10, "mean": 90.95, "std_dev": 3.24, "min": 85.4, "max": 95.5}}, "average_ridership_by_route_and_service": {"Blue Line": {"Express": 1250, "Local": 1100}, "Green Line": {"Express": 1050, "Local": 860}, "Red Line": {"Local": 780}, "Yellow Line": {"Express": 965}}, "dates_with_missing_values": ["2024-01-04", "2024-01-05"]}}
{"purpose": "Explore housing listing data to understand price distribution and identify data quality issues.", "raw_table": "ListingID,DateListed,Price,Location,PropertyType,Bedrooms,Bathrooms,SquareFeet,Agent\n101,2023-01-15,350000,New york,Condo,2,1,950,John Doe\n102,15/02/2023,450000,los Angeles,house,3,2,1200,Jane Smith\n103,2023/03/01,NaN,Chicago,Apartment,1,1,,Alex Johnson\n104,2023-04-05,500000,new york,Condo,3,2,1100,jane smith\n105,04-20-2023,425000,Houston,Townhouse,3,,1300,John Doe\n106,2023-05-15,390000,Los angeles,house,2,2,1000,Unknown\n107,2023-06-01,480000,Chicago,Apartment,2,2,1050,Alex Johnson\n108,2023/06/15,NaN,NEW YORK,Condo,2,1,950,john doe\n109,07/01/2023,470000,Houston,Townhouse,3,2,1350,Jane Smith\n110,2023-07-10,520000,Los Angeles,House,4,3,1600,Alex Johnson\n111,2023-08-05,410000,Chicago,apartment,2,1,980,John Doe\n112,2023-08-15,450000,New York,Condo,2,2,1000,jane smith", "eda_steps": ["Check for missing values and their percentages per column", "Standardize capitalization in Location and PropertyType columns", "Convert Price and SquareFeet columns to numeric and summarize descriptive statistics", "Generate value counts for Agent and PropertyType columns", "Analyze distribution of Bedrooms and Bathrooms with summary statistics", "Examine unique date formats and standardize DateListed to yyyy-mm-dd", "Calculate correlations between Price, Bedrooms, Bathrooms, and SquareFeet", "Identify listings with missing Price or SquareFeet and count them"], "eda_results": {"missing_values": {"Price": 2, "SquareFeet": 1, "Bathrooms": 1, "Agent": 0, "Bedrooms": 0, "Location": 0, "PropertyType": 0, "DateListed": 0}, "standardized_categories": {"Location": ["New York", "Los Angeles", "Chicago", "Houston"], "PropertyType": ["Condo", "House", "Apartment", "Townhouse"], "Agent": ["John Doe", "Jane Smith", "Alex Johnson", "Unknown"]}, "price_summary_stats": {"count": 10, "mean": 444000, "median": 445000, "min": 350000, "max": 520000, "std_dev": 54000}, "squarefeet_summary_stats": {"count": 11, "mean": 1137, "median": 1100, "min": 950, "max": 1600, "std_dev": 210}, "value_counts": {"Agent": {"John Doe": 4, "Jane Smith": 4, "Alex Johnson": 3, "Unknown": 1}, "PropertyType": {"Condo": 4, "House": 3, "Apartment": 3, "Townhouse": 2}}, "bedrooms_bathrooms_stats": {"Bedrooms": {"mean": 2.5, "median": 2, "min": 1, "max": 4}, "Bathrooms": {"mean": 1.7, "median": 2, "min": 1, "max": 3}}, "date_formats_found": ["yyyy-mm-dd", "dd/mm/yyyy", "yyyy/mm/dd", "mm-dd-yyyy", "dd/mm/yy"], "listings_with_missing_price": 2, "listings_with_missing_squarefeet": 1, "correlations": {"Price_vs_Bedrooms": 0.87, "Price_vs_Bathrooms": 0.79, "Price_vs_SquareFeet": 0.92, "Bedrooms_vs_Bathrooms": 0.76}}}
{"purpose": "Analyze crime report patterns and data quality within a municipal police department.", "raw_table": "Report_ID,Date,Crime_Type,Location,Officer_ID,Status,Reported_By,Severity\n101,2023/01/15,BURGLARY,Downtown,off123,Closed,john doe,High\n102,15-02-2023,Assault,uptown,Off124,open,,medium\n103,03/10/2023,THEFT,Midtown,OFF125,Closed,Jane Smith,low\n104,2023-04-05,Robbery,downtown,off123,Closed,john doe ,High\n105,2023/04/20,,Suburb,Off126,Open,alice,Medium\n106,2023-05-01,Assault,Midtown,OFF127,closed,,Low\n107,06-05-2023,Vandalism,Suburb,,Open,Bob,Medium\n108,2023-06-10,THEFT,Uptown,off124,Open,Jane Smith,Low\n109,2023/06/15,Burglary,Downtown,Off123,Closed,John Doe,high\n110,,Robbery,Midtown,OFF125,Closed,,Medium\n111,2023-07-01,Assault,Downtown,off128,Open,Alice,Medium\n112,07/15/2023,THEFT,uptown,off124,Open,jane smith,Low", "eda_steps": ["Check for missing values in each column and calculate their percentages", "Standardize the 'Date' column to a uniform date format", "Convert all 'Crime_Type' entries to lowercase and identify unique crime categories", "Generate value counts for the 'Status' and 'Severity' columns", "Identify duplicated 'Report_ID' or inconsistent 'Officer_ID' formats", "Summarize the number of reports by 'Location'", "Calculate descriptive statistics for 'Severity' by encoding High=3, Medium=2, Low=1", "Explore correlation between 'Severity' and 'Status' encoded as numeric", "List reports where 'Reported_By' is missing"], "eda_results": {"missing_values": {"Report_ID": "0%", "Date": "9.09%", "Crime_Type": "9.09%", "Location": "0%", "Officer_ID": "9.09%", "Status": "0%", "Reported_By": "27.27%", "Severity": "0%"}, "standardized_date_format": ["2023-01-15", "2023-02-15", "2023-03-10", "2023-04-05", "2023-04-20", "2023-05-01", "2023-05-06", "2023-06-10", "2023-06-15", null, "2023-07-01", "2023-07-15"], "unique_crime_types": ["burglary", "assault", "theft", "robbery", "vandalism", null], "value_counts_status": {"Closed": 5, "Open": 6, "closed": 1}, "value_counts_severity": {"High": 3, "Medium": 1, "Low": 4, "high": 1, "medium": 0, "low": 0}, "officer_id_issues": {"missing_officer_id": [107], "inconsistent_casing_officer_ids": ["off123", "Off124", "OFF125", "off128"]}, "reports_by_location": {"Downtown": 4, "Uptown": 3, "Midtown": 3, "Suburb": 2}, "severity_stats_numeric_encoding": {"mean_severity": 2.09, "median_severity": 2, "std_dev_severity": 0.79, "encoding": {"High": 3, "Medium": 2, "Low": 1}}, "correlation_severity_status": 0.32, "reports_missing_reported_by": [102, 106, 110]}}
{"purpose": "Analyze monthly sales and customer purchase patterns for retail products.", "raw_table": "OrderID,CustomerID,ProductCategory,Quantity,Price,OrderDate,PaymentMethod\n1001,C001,electronics,2,399.99,2023/01/15,Credit Card\n1002,C002,Home Appliances,1,89.5,15-01-2023,credit card\n1003,C003,electronics,NaN,199.99,2023-01-17,PayPal\n1004,C001,Toys,3,15.99,2023/01/20,Cash\n1005,C004,Electronics,1,abc,2023/1/22,Credit Card\n1006,C005,home appliances,2,129.99,01/23/2023,Paypal\n1007,C006,Books,5,9.99,2023-01-25,CASH\n1008,C007,books,3,12.5,2023-01-26,Credit Card\n1009,C008,Toys,NaN,15.99,,Credit Card\n1010,C009,Clothing,4,29.99,2023/01/28,Debit Card\n1011,C002,clothing,2,30.0,28-01-2023,Debit Card\n1012,C010,Books,1,9.99,2023/01/29,Credit card\n1013,,Electronics,1,399.99,2023/01/30,PayPal\n1014,C011,Home Appliances,1,85,2023/01/31,Credit Card", "eda_steps": ["Check for missing values in all columns", "Standardize capitalization in ProductCategory", "Convert OrderDate to consistent date format", "Identify and handle non-numeric values in Quantity and Price", "Compute descriptive statistics for Quantity and Price", "Generate value counts for ProductCategory and PaymentMethod", "Calculate total sales (Quantity * Price) per order", "Find top 3 ProductCategories by total sales", "Summarize missing OrderDate entries"], "eda_results": {"missing_values": {"OrderID": 0, "CustomerID": 1, "ProductCategory": 0, "Quantity": 2, "Price": 1, "OrderDate": 1, "PaymentMethod": 0}, "standardized_product_categories": {"Electronics": 5, "Home Appliances": 3, "Toys": 2, "Books": 3, "Clothing": 2}, "order_date_format": "All dates converted to YYYY-MM-DD format except 1 missing", "non_numeric_handling": {"Quantity": "Rows with NaN Quantity set to median value 2.5", "Price": "One non-numeric price ('abc') treated as missing and dropped from total sales calculation"}, "summary_stats": {"Quantity": {"count": 14, "mean": 2.64, "std": 1.45, "min": 1, "max": 5}, "Price": {"count": 14, "mean": 112.13, "std": 141.58, "min": 9.99, "max": 399.99}}, "value_counts": {"PaymentMethod": {"Credit Card": 6, "PayPal": 3, "Cash": 2, "Debit Card": 2, "Paypal": 1}}, "total_sales_per_order": {"1001": 799.98, "1002": 89.5, "1003": 499.975, "1004": 47.97, "1005": null, "1006": 259.98, "1007": 49.95, "1008": 37.5, "1009": null, "1010": 119.96, "1011": 60.0, "1012": 9.99, "1013": 399.99, "1014": 85}, "top_3_product_categories_by_sales": {"Electronics": 1706.95, "Home Appliances": 434.48, "Clothing": 179.96}, "missing_order_dates": 1}}
{"purpose": "Analyze customer call and data usage patterns to identify potential churn risks.", "raw_table": "CustomerID,CallDurationMinutes,DataUsageMB,PlanType,LastPaymentDate,ChurnStatus\n1001,35,1500,Premium,2023/05/15,No\n1002,45,,basic,2023-04-20,Yes\n1003,12,300,Standard,15-May-2023,No\n1004,,500,STANDARD,2023/05/01,No\n1005,60,2500,premium,,Yes\n1006,20,700,BasiC,2023-05-10,no\n1007,5,150,Standard,2023/05/12,No\n1008,50,2000,premium,2023/04/30,Yes\n1009,25,,Basic,,No\n1010,40,1800,STANDARD,2023-05-03,Yes", "eda_steps": ["Check and count missing values in each column", "Standardize the 'PlanType' column to consistent capitalization", "Convert 'LastPaymentDate' to a uniform date format", "Compute descriptive statistics for 'CallDurationMinutes' and 'DataUsageMB'", "Generate value counts for the 'ChurnStatus' and 'PlanType' columns", "Calculate the correlation between 'CallDurationMinutes' and 'DataUsageMB'", "Identify customers with missing 'LastPaymentDate' and analyze churn status", "Summarize average call duration and data usage by 'PlanType'"], "eda_results": {"missing_values": {"CustomerID": 0, "CallDurationMinutes": 1, "DataUsageMB": 3, "PlanType": 0, "LastPaymentDate": 2, "ChurnStatus": 0}, "standardized_PlanType_counts": {"Basic": 4, "Standard": 4, "Premium": 3}, "LastPaymentDate_conversion": {"dates_parsed": 8, "dates_missing": 2}, "summary_stats": {"CallDurationMinutes": {"count": 9, "mean": 31.78, "std": 17.68, "min": 5, "25%": 20, "50%": 35, "75%": 45, "max": 60}, "DataUsageMB": {"count": 7, "mean": 1350, "std": 806.23, "min": 150, "25%": 500, "50%": 1500, "75%": 2000, "max": 2500}}, "value_counts": {"ChurnStatus": {"No": 6, "Yes": 4}, "PlanType": {"Basic": 4, "Standard": 4, "Premium": 3}}, "correlations": {"CallDurationMinutes_vs_DataUsageMB": 0.92}, "missing_LastPaymentDate_churn": {"count": 2, "churn_status_distribution": {"No": 1, "Yes": 1}}, "average_usage_by_PlanType": {"Basic": {"avg_call_duration": 32.5, "avg_data_usage": 475}, "Standard": {"avg_call_duration": 20.5, "avg_data_usage": 1062.5}, "Premium": {"avg_call_duration": 48.3, "avg_data_usage": 2000}}}}
{"purpose": "Analyze public transportation usage patterns and identify data quality issues in city bus ridership records.", "raw_table": "Route,Date,Passengers,Revenue,Driver Name,Weather\n12A,2023-03-01,120,240.50,john SMITH,Clear\n7B,03/02/2023,85,170.00,Anne Lee,cloudy\n12a,2023/03/03,,245.75,JONATHAN DOE,Rain\n7B,2023-3-04,95,190.00,anne lee,Sunny\n15C,2023-03-05,110,220.00,Mary-Jane,clear\n12A,March 6 2023,130,260.00,John smith,Partly Cloudy\n15c,2023-03-07,105,210.00,MARY-JANE,clear\n7b,2023-03-08,NaN,180.00,Anne Lee,Cloudy\n12A,2023-03-09,125,NaN,jonathan doe,Clear\n15C,2023-03-10,115,230.00,Mary-jane,Sunny\n", "eda_steps": ["Check missing values percentages in each column", "Standardize the 'Route' column capitalization", "Parse 'Date' column into a consistent date format", "Generate descriptive statistics for 'Passengers' and 'Revenue'", "Compute value counts for 'Route' and 'Weather' columns", "Identify unique driver names and count their records", "Check correlation between 'Passengers' and 'Revenue'", "Summarize distribution skewness for 'Passengers'"], "eda_results": {"missing_values": {"Route": 0, "Date": 0, "Passengers": 2, "Revenue": 1, "Driver Name": 0, "Weather": 0}, "standardized_routes": ["12A", "7B", "12A", "7B", "15C", "12A", "15C", "7B", "12A", "15C"], "date_parsing_issues": 0, "summary_stats": {"Passengers": {"count": 8, "mean": 111.875, "std": 16.2, "min": 85, "25%": 105, "50%": 112.5, "75%": 125, "max": 130}, "Revenue": {"count": 9, "mean": 213.94, "std": 30.88, "min": 170, "25%": 190, "50%": 220, "75%": 240.5, "max": 260}}, "value_counts": {"Route": {"12A": 4, "7B": 3, "15C": 3}, "Weather": {"Clear": 3, "Cloudy": 2, "Sunny": 2, "Rain": 1, "Partly Cloudy": 1}}, "driver_name_counts": {"John Smith": 3, "Anne Lee": 3, "Jonathan Doe": 2, "Mary-Jane": 3}, "correlations": {"Passengers_Revenue": 0.97}, "skewness": {"Passengers": 0.12}}}
{"purpose": "Analyze customer call patterns and dropped call rates in a telecom network.", "raw_table": "CustomerID,CallDuration,CallType,CallDate,Dropped,Region\n1001, 300, voice, 2023-01-15, yes, north\n1002,,DATA,01/18/2023,no, South\n1003,120,Voice,2023/01/17,YES,East\n1004,45,text,17-01-2023,No,west\n1005,200,voice,2023-1-19, yEs, North\n1006,phone,voice,2023-01-20,no,South\n1007,150,DATA,2023.01.21,No,EAST\n1008, ,text,2023-01-22,No,West\n1009,90,voice,01/23/2023,Yes,north\n1010,60,voice,2023-01-24,No,SouTh\n1011,30,VoicE,2023-01-25,no,east\n1012,250,Voice,2023/01/26,yes,West", "eda_steps": ["Standardize capitalization in categorical columns CallType, Dropped, and Region", "Parse CallDate column into consistent date format", "Identify and count missing values in CallDuration column", "Compute descriptive statistics for CallDuration", "Generate value counts for CallType and Region columns", "Calculate percentage of dropped calls overall and by Region", "Check for any inconsistent data types in CallDuration", "Summarize call duration distribution skewness", "Identify top 2 Regions with the highest dropped call rates"], "eda_results": {"missing_values": {"CallDuration": 2, "CallType": 0, "Dropped": 0, "Region": 0, "CallDate": 0}, "data_type_issues": {"CallDuration": ["row 6 contains non-numeric value 'phone'"]}, "standardized_categories": {"CallType": {"voice": 7, "data": 2, "text": 2, "phone": 0}, "Dropped": {"yes": 4, "no": 8}, "Region": {"north": 3, "south": 3, "east": 3, "west": 3}}, "call_duration_stats": {"count": 11, "mean": 129.09, "std": 87.12, "min": 30, "25%": 60, "50%": 120, "75%": 200, "max": 300, "skewness": 0.67}, "call_type_counts": {"voice": 7, "data": 2, "text": 2}, "region_counts": {"north": 3, "south": 3, "east": 3, "west": 3}, "dropped_call_percentage": {"overall": 33.33, "north": 66.67, "south": 0.0, "east": 33.33, "west": 0.0}, "top_regions_dropped_calls": ["north", "east"]}}
{"purpose": "Analyze customer purchase behavior and product category distribution in an online store.", "raw_table": "OrderID,CustomerID,ProductCategory,Quantity,Price,OrderDate,PaymentMethod,DeliveryStatus\n1001,C001,eLectronics,2,399.99,2024-01-15,Credit Card,Delivered\n1002,C002,Clothing,1,49.99,15/01/2024,paypal,Delivered\n1003,C003,Home & garden,3,,2024-01-17,Credit Card,delivered\n1004,,electronics,1,299.99,01-18-2024,Credit card,Pending\n1005,C005,Clothing,2,79.98,2024/01/19,Crypto,canceled\n1006,C006,Toys,5,15.99,2024-01-20,Credit Card,Delivered\n1007,C007,,1,9.99,2024-01-20,Paypal,Delivered\n1008,C001,HOME & Garden,2,59.99,2024-01-21,Credit Card,Pending\n1009,C008,Clothing,,39.99,2024/01/22,Cash on Delivery,Delivered\n1010,C009,Electronics,1,199.99,22-01-2024,credit card,Delivered\n1011,C010,Toys,4,12.99,2024-01-23,Credit Card,Pending\n1012,C011,books,3,29.97,2024-01-24,Paypal,Delivered\n1013,C012,Books,1,9.99,,paypal,Delivered\n1014,C013,Clothing,2,59.98,2024-01-25,Credit Card,Delivered", "eda_steps": ["Check for missing values in each column", "Standardize the capitalization in ProductCategory and PaymentMethod columns", "Parse and unify OrderDate into a consistent date format", "Compute descriptive statistics for Quantity and Price", "Generate value counts for ProductCategory and DeliveryStatus", "Identify rows with missing CustomerID or ProductCategory", "Calculate total revenue per ProductCategory", "Summarize PaymentMethod usage frequencies"], "eda_results": {"missing_values": {"OrderID": 0, "CustomerID": 1, "ProductCategory": 1, "Quantity": 2, "Price": 1, "OrderDate": 1, "PaymentMethod": 0, "DeliveryStatus": 0}, "standardized_categories": {"ProductCategory": ["Electronics", "Clothing", "Home & Garden", "Electronics", "Clothing", "Toys", "", "Home & Garden", "Clothing", "Electronics", "Toys", "Books", "Books", "Clothing"], "PaymentMethod": ["Credit Card", "Paypal", "Credit Card", "Credit Card", "Crypto", "Credit Card", "Paypal", "Credit Card", "Cash on Delivery", "Credit Card", "Credit Card", "Paypal", "Paypal", "Credit Card"]}, "parsed_order_dates": {"dates": ["2024-01-15", "2024-01-15", "2024-01-17", "2024-01-18", "2024-01-19", "2024-01-20", "2024-01-20", "2024-01-21", "2024-01-22", "2024-01-22", "2024-01-23", "2024-01-24", null, "2024-01-25"]}, "summary_stats": {"Quantity": {"count": 12, "mean": 2.17, "median": 2, "min": 1, "max": 5}, "Price": {"count": 13, "mean": 100.85, "median": 49.99, "min": 9.99, "max": 399.99}}, "value_counts": {"ProductCategory": {"Clothing": 4, "Electronics": 3, "Home & Garden": 2, "Toys": 2, "Books": 2, "": 1}, "DeliveryStatus": {"Delivered": 9, "Pending": 3, "Canceled": 1}}, "rows_with_missing_critical_info": {"missing_CustomerID": [1004], "missing_ProductCategory": [1007]}, "total_revenue_per_category": {"Electronics": 899.97, "Clothing": 229.94, "Home & Garden": 119.98, "Toys": 107.92, "Books": 39.96, "": 9.99}, "payment_method_usage": {"Credit Card": 7, "Paypal": 4, "Crypto": 1, "Cash on Delivery": 1}}}
{"purpose": "Analyze recent temperature and precipitation patterns across different climate zones to identify missing data and summary statistics.", "raw_table": "Date,Location,Climate_Zone,Temperature_C,Precipitation_mm\n2024-01-01,New York,Temperate,3.5,5.2\n01/02/2024,Miami,Tropical,22.1,0\n2024-01-03,Denver,AlpIne,-5.0,1.0\n2024-1-04,Seattle,temperate,6.2,\n2024-01-05,Anchorage,Polar,-12.3,0.0\n2024/01/06,Mexico City,Subtropical,18.5,3.1\n2024-01-07,Sydney,Subtropical,25.0,0\n,Tokyo,Temperate,7.8,10.5\n2024-01-09,Cape Town,Mediterranean,20.3,2.3\n2024-01-10,Moscow,temperate,-8.4,0.0\n2024-01-11,Cairo,Desert,15.5,0\n2024-01-12,Buenos Aires,temperate,22.0,NaN\n2024-01-13,London,Temperate,5.0,7.8", "eda_steps": ["Check and summarize missing values per column", "Standardize Climate_Zone capitalization", "Convert Date column to consistent date format", "Compute descriptive statistics for Temperature_C and Precipitation_mm", "Generate value counts for Climate_Zone", "Identify rows with missing Temperature_C or Precipitation_mm values", "Calculate correlation between Temperature_C and Precipitation_mm", "Find top 3 locations with highest average temperature"], "eda_results": {"missing_values": {"Date": 1, "Location": 0, "Climate_Zone": 0, "Temperature_C": 0, "Precipitation_mm": 2}, "cleaned_climate_zone_counts": {"Temperate": 5, "Tropical": 1, "Alpine": 1, "Polar": 1, "Subtropical": 2, "Mediterranean": 1, "Desert": 1}, "date_format_consistency": "All dates converted to YYYY-MM-DD format; 1 missing date filled with null", "summary_stats": {"Temperature_C": {"count": 13, "mean": 8.35, "std": 11.43, "min": -12.3, "25%": -3.5, "50%": 6.2, "75%": 20.3, "max": 25.0}, "Precipitation_mm": {"count": 12, "mean": 3.17, "std": 3.44, "min": 0.0, "25%": 0.0, "50%": 1.65, "75%": 5.15, "max": 10.5}}, "rows_missing_temp_or_precip": [{"Date": "2024-01-04", "Location": "Seattle", "Temperature_C": 6.2, "Precipitation_mm": null}, {"Date": "2024-01-12", "Location": "Buenos Aires", "Temperature_C": 22.0, "Precipitation_mm": null}], "correlations": {"Temperature_C_vs_Precipitation_mm": -0.28}, "top_3_locations_by_avg_temp": [{"Location": "Sydney", "Avg_Temperature_C": 25.0}, {"Location": "Buenos Aires", "Avg_Temperature_C": 22.0}, {"Location": "Miami", "Avg_Temperature_C": 22.1}]}}
{"purpose": "Examine sales performance and customer demographics in a retail store over a two-week period.", "raw_table": "TransactionID,CustomerID,PurchaseDate,ProductCategory,UnitsSold,UnitPrice,PaymentMethod\n1001,C001,2024/04/01,Electronics,2,299.99,Credit Card\n1002,c002,2024-04-01,clothing,1,49.99,Cash\n1003,C003,04-02-2024,HOME appliances, ,199.5,credit card\n1004,C004,2024/4/03,Electronics,1,299.99,Debit\n1005,C005,2024-04-03,Clothing,3,49.99,Cash\n1006,C006,2024-04-04,Accessories,2,,Credit card\n1007,C007,April 05 2024,home Appliances,1,199.50,Cash\n1008,,2024-04-05,Clothing,2,49.99,credit card\n1009,C009,2024/04/06,electronics,1,299.99,Cash\n1010,C010,2024-04-07,accessories,1,19.99,Debit\n1011,C011,2024-04-07,clothing,NaN,49.99,Credit card\n1012,C012,2024-04-08,Electronics,3,299.99,Credit Card\n1013,C013,2024-04-08,Home Appliances,2,199.50,Cash\n1014,C014,2024-04-09,clothing,1,49.99,credit card", "eda_steps": ["Check for missing values and data inconsistencies.", "Standardize capitalization in categorical columns like ProductCategory and PaymentMethod.", "Convert PurchaseDate to a consistent date format.", "Compute descriptive statistics for numeric columns UnitsSold and UnitPrice.", "Generate value counts for ProductCategory and PaymentMethod.", "Identify transactions with missing UnitsSold or UnitPrice.", "Calculate total sales amount per transaction (UnitsSold * UnitPrice).", "Summarize total sales by ProductCategory.", "Analyze the distribution of PaymentMethods."], "eda_results": {"missing_values": {"CustomerID": 1, "UnitsSold": 2, "UnitPrice": 1}, "standardized_product_categories": ["Electronics", "Clothing", "Home Appliances", "Accessories"], "standardized_payment_methods": ["Credit Card", "Cash", "Debit"], "purchase_date_format": "All dates converted to YYYY-MM-DD", "summary_stats": {"UnitsSold": {"count": 13, "mean": 1.62, "std": 0.81, "min": 1, "max": 3}, "UnitPrice": {"count": 14, "mean": 133.86, "std": 128.59, "min": 19.99, "max": 299.99}}, "value_counts": {"ProductCategory": {"Clothing": 5, "Electronics": 4, "Home Appliances": 3, "Accessories": 2}, "PaymentMethod": {"Credit Card": 6, "Cash": 5, "Debit": 2}}, "transactions_missing_units_sold": [1003, 1011], "transactions_missing_unit_price": [1006], "total_sales_per_transaction": {"1001": 599.98, "1002": 49.99, "1003": null, "1004": 299.99, "1005": 149.97, "1006": null, "1007": 199.5, "1008": 99.98, "1009": 299.99, "1010": 19.99, "1011": null, "1012": 899.97, "1013": 399.0, "1014": 49.99}, "total_sales_by_category": {"Electronics": 2099.93, "Clothing": 349.93, "Home Appliances": 598.5, "Accessories": 19.99}, "payment_method_distribution_percent": {"Credit Card": 42.8, "Cash": 35.7, "Debit": 14.3}}}
{"purpose": "Examine manufacturing batch data to understand defect rates and production variability.", "raw_table": "BatchID,ProductionDate,Shift,Machine,DefectCount,Operator,RunTimeMinutes,MaterialGrade\nB001,2024-01-15,morning,M01,5,alice,120,Grade A\nB002,15/01/2024,Morning,m02,2,Bob,115,grade a\nB003,2024/01/16,Evening,M01,,Charlie,130,Grade B\nB004,01-17-2024,Night,m03,7,,125,grade C\nb005,2024-01-18,night,M02,3,alice,110,Grade B\nB006,2024-01-18,Evening,M01,1,Bob,,Grade A\nB007,2024.01.19,Morning,m03,4,Charlie,140,Grade C\nB008,19-01-2024,Morning,M02,0,Alice,100,Grade A\nB009,2024-01-20,Night,M03,6,bob,115,Grade a\nB010,,Evening,m01,2,Charlie,130,grade b", "eda_steps": ["Check and summarize missing values in each column", "Standardize the date format in ProductionDate", "Normalize capitalization in Shift, Machine, Operator, and MaterialGrade columns", "Compute descriptive statistics for numeric columns DefectCount and RunTimeMinutes", "Generate value counts for categorical columns Shift, Machine, Operator, and MaterialGrade", "Calculate the correlation between DefectCount and RunTimeMinutes", "Identify batches with missing critical data", "Determine the average DefectCount per Machine"], "eda_results": {"missing_values": {"BatchID": 0, "ProductionDate": 1, "Shift": 0, "Machine": 0, "DefectCount": 1, "Operator": 1, "RunTimeMinutes": 1, "MaterialGrade": 0}, "standardized_dates": ["2024-01-15", "2024-01-15", "2024-01-16", "2024-01-17", "2024-01-18", "2024-01-18", "2024-01-19", "2024-01-19", "2024-01-20", null], "normalized_categories": {"Shift": {"morning": 4, "evening": 3, "night": 3}, "Machine": {"M01": 4, "M02": 3, "M03": 3}, "Operator": {"alice": 3, "bob": 3, "charlie": 3, "missing": 1}, "MaterialGrade": {"Grade A": 4, "Grade B": 3, "Grade C": 3}}, "summary_stats": {"DefectCount": {"count": 9, "mean": 3.33, "std": 2.18, "min": 0, "25%": 1.5, "50%": 3, "75%": 5, "max": 7}, "RunTimeMinutes": {"count": 9, "mean": 122.78, "std": 13.94, "min": 100, "25%": 115, "50%": 120, "75%": 130, "max": 140}}, "value_counts": {"Shift": {"morning": 4, "evening": 3, "night": 3}, "Machine": {"M01": 4, "M02": 3, "M03": 3}, "Operator": {"alice": 3, "bob": 3, "charlie": 3, "missing": 1}, "MaterialGrade": {"Grade A": 4, "Grade B": 3, "Grade C": 3}}, "correlations": {"DefectCount_RunTimeMinutes": -0.11}, "missing_critical_data": ["B003 (DefectCount)", "B004 (Operator)", "B006 (RunTimeMinutes)", "B010 (ProductionDate)"], "average_defects_per_machine": {"M01": 2.67, "M02": 1.67, "M03": 5.67}}}
{"purpose": "Explore student performance and attendance patterns to identify potential factors affecting grades.", "raw_table": "Student_ID,Name,Grade,Subject,Score,Attendance_Date,Present\n101,alice,10,Math,88,2023-03-01,Yes\n102,Bob,10,Math,92,03/02/2023,yes\n103,charlie,11,Science,,2023-03-01,No\n104,Diana,11,science,85,2023/03/02,Y\n105,Ed,10,History,78,2023-3-01,No\n106,Fiona,10,history,81,03-02-2023,Yes\n107,George,12,Math,95,2023-03-01,YES\n108,Hanna,12,Math,NaN,03/02/2023,no\n109,Ian,11,Science,89,2023-03-01,Yes\n110,jane,10,Math,90,2023-03-03,y\n111,Kevin,,History,74,2023-03-03,No\n112,Linda,12,science,88,2023-03-03,Yes\n113,Mike,11,History,82,,Yes\n114,Nick,12,Science,91,03/03/2023,n\n115,Olivia,10,Math,85,2023-03-03,Yes", "eda_steps": ["Check for missing values across all columns", "Standardize capitalization for categorical columns: Name, Subject, Present", "Convert Attendance_Date to a consistent date format", "Calculate summary statistics for Score by Subject", "Generate value counts for Present status", "Identify students missing Grade values", "Analyze average Score by Grade level", "Determine correlation between Score and presence status", "List top 3 subjects by average Score"], "eda_results": {"missing_values": {"Student_ID": 0, "Name": 0, "Grade": 1, "Subject": 0, "Score": 3, "Attendance_Date": 1, "Present": 0}, "standardized_categories": {"Subject": ["Math", "Science", "History"], "Present": ["Yes", "No"]}, "attendance_dates_parsed": {"invalid_dates": 0, "unique_dates": 3}, "summary_stats_by_subject": {"Math": {"count": 6, "mean": 88.33, "min": 85, "max": 95}, "Science": {"count": 5, "mean": 88.6, "min": 85, "max": 91}, "History": {"count": 4, "mean": 78.75, "min": 74, "max": 82}}, "value_counts_present": {"Yes": 8, "No": 6}, "students_missing_grade": ["Kevin"], "average_score_by_grade": {"10": 86.4, "11": 85.5, "12": 91.33}, "correlation_score_present": 0.68, "top_subjects_by_average_score": ["Math", "Science", "History"]}}
{"purpose": "Analyze patterns in daily public transportation usage and identify missing data issues.", "raw_table": "Date,Route,Passenger_Count,Fare_Type,Bus_Delay_Minutes\n2024-01-01,Route A,120,Regular,5\n1/2/2024,route b,95,student,NA\n2024-01-03,ROUTE A,110,Regular,7\n2024-01-04,Route C,,Regular,3\n01-05-2024,route B,85,student,4\n2024-01-06,Route c,100,Regular,2\n2024/01/07,Route A,130,Regular,NA\n2024-01-08,route B,90,Student,6\n2024-1-09,Route C,105,regular,1\n2024-01-10,Route A,115,Regular,0\n2024-01-11,ROUTE B,88,STUDENT,3\n2024-01-12,Route C,102,Regular,NA\n2024-01-13,Route D,50,Discount,4", "eda_steps": ["Check the percentage of missing values in each column", "Standardize the Route and Fare_Type columns to consistent capitalization", "Compute descriptive statistics for Passenger_Count and Bus_Delay_Minutes", "Generate value counts for the Route column", "Generate value counts for the Fare_Type column", "Identify rows with missing Passenger_Count values", "Compute the average Bus_Delay_Minutes by Route", "Check correlation between Passenger_Count and Bus_Delay_Minutes"], "eda_results": {"missing_values": {"Date": 0, "Route": 0, "Passenger_Count": 1, "Fare_Type": 0, "Bus_Delay_Minutes": 3}, "standardized_categories": {"Route": ["Route A", "Route B", "Route C", "Route D"], "Fare_Type": ["Regular", "Student", "Discount"]}, "summary_stats": {"Passenger_Count": {"count": 13, "mean": 100.62, "std": 22.96, "min": 50, "max": 130}, "Bus_Delay_Minutes": {"count": 10, "mean": 3.6, "std": 2.2, "min": 0, "max": 7}}, "value_counts": {"Route": {"Route A": 4, "Route B": 4, "Route C": 4, "Route D": 1}, "Fare_Type": {"Regular": 8, "Student": 4, "Discount": 1}}, "rows_with_missing_passenger_count": [4], "average_delay_by_route": {"Route A": 4.0, "Route B": 4.33, "Route C": 2.0, "Route D": 4.0}, "correlations": {"Passenger_Count_and_Bus_Delay_Minutes": 0.15}}}
{"purpose": "Analyze citizen complaints submitted to the city government to identify common issues and data quality problems.", "raw_table": "Complaint_ID,Date_Submitted,Department,Issue_Description,Status,Priority,Resolution_Time_Days\nC1001,2023-01-15,PUBLIC WORKS,Pothole on Main Street,Closed,High,5\nC1002,15/02/2023,health,Noise complaint at night,Open,Medium,\nC1003,03-03-2023,Public Works,Streetlight not working,closed,LOW,3\nC1004,,Sanitation,Overflowing trash bins,Open,,7\nC1005,2023-04-12,HEALTH,Food safety violation,CLOSED,HIGH,10\nC1006,2023/04/25,Transportation,Bus stop shelter damaged,In Progress,medium,8\nC1007,2023-05-01,Public works,Road signage missing,Closed,High,2\nC1008,May 10 2023,Sanitation,Illegal dumping,Open,LOW,\nC1009,2023-06-01,Health,Water contamination report,Closed,Medium,12\nC1010,2023-06-15,transportation,Traffic light malfunction,Open,High,4\nC1011,2023-07-01,Sanitation,Missed garbage pickup,Closed,Medium,6\nC1012,2023-07-05,Health,Unlicensed vendor complaint,Open,Medium,", "eda_steps": ["Standardize the Department names to consistent capitalization", "Parse Date_Submitted into a uniform date format and identify missing dates", "Calculate missing value percentages for each column", "Generate value counts for the Status and Priority columns", "Compute basic statistics (mean, median) for Resolution_Time_Days", "Identify complaints with missing Resolution_Time_Days and their Status", "Find top 3 Departments by number of complaints", "Check correlation between Priority level and Resolution_Time_Days", "Summarize distribution of complaint submission dates over months"], "eda_results": {"missing_values": {"Complaint_ID": 0, "Date_Submitted": 1, "Department": 0, "Issue_Description": 0, "Status": 0, "Priority": 2, "Resolution_Time_Days": 3}, "value_counts": {"Status": {"Closed": 6, "Open": 5, "In Progress": 1}, "Priority": {"High": 4, "Medium": 5, "Low": 3}}, "department_standardization": {"public works": 3, "health": 4, "sanitation": 3, "transportation": 2}, "date_parsing_issues": {"Missing or invalid dates": 1, "Date range": "2023-01-15 to 2023-07-05"}, "resolution_time_stats": {"mean_days": 6.43, "median_days": 6, "min_days": 2, "max_days": 12, "count": 9}, "missing_resolution_time_by_status": {"Open": 3, "Closed": 0, "In Progress": 0}, "top_departments_by_complaints": {"health": 4, "public works": 3, "sanitation": 3}, "priority_correlation_resolution_time": {"High": 5.25, "Medium": 8.0, "Low": 5.0}, "complaints_per_month": {"January": 1, "February": 1, "March": 1, "April": 2, "May": 2, "June": 2, "July": 2}}}
{"purpose": "Analyze customer call behavior and service usage patterns to identify potential churn risks.", "raw_table": "CustomerID,CallDuration,CallType,ServicePlan,RegistrationDate,Churned\n1001,35,Local,Premium,2023/01/15,No\n1002, ,International,basic,15-02-2023,Yes\n1003,20,Local,Standard,2023-03-05,No\n1004,45,INTERNATIONAL,Basic,,No\n1005,NaN,Local,PREMIUM,2023/02/28,Yes\n1006,30,Local,standard,03/03/2023,No\n1007,10,local,Premium,2023-04-01,\n1008,50,International,Basic,2023/03-15,No\n1009,25,Local,Standard,2023-01-30,No\n1010,40,INTERNATIONAL,premium,2023/02/18,Yes", "eda_steps": ["Check the percentage of missing values in each column", "Standardize the CallType and ServicePlan columns to consistent capitalization", "Convert RegistrationDate to a uniform date format and identify missing or invalid dates", "Compute descriptive statistics for CallDuration", "Generate value counts for CallType and ServicePlan", "Identify number of churned vs non-churned customers", "Analyze average CallDuration grouped by ServicePlan", "Check correlation between CallDuration and Churned status encoded as binary"], "eda_results": {"missing_values": {"CustomerID": "0%", "CallDuration": "20%", "CallType": "0%", "ServicePlan": "0%", "RegistrationDate": "10%", "Churned": "10%"}, "standardized_categories": {"CallType": {"Local": 6, "International": 4}, "ServicePlan": {"Basic": 4, "Premium": 4, "Standard": 3}}, "invalid_dates": {"RegistrationDate": 0}, "call_duration_stats": {"count": 8, "mean": 32.88, "std": 13.46, "min": 10, "25%": 25, "50%": 30, "75%": 40, "max": 50}, "churn_counts": {"Yes": 3, "No": 6}, "avg_call_duration_by_serviceplan": {"Basic": 45, "Premium": 28.33, "Standard": 22.5}, "correlation_call_duration_churned": -0.45}}
{"purpose": "Examine monthly transaction patterns and customer segments in retail banking data.", "raw_table": "Customer_ID,Transaction_Date,Transaction_Amount,Account_Type,Branch,Transaction_Type\nC001,2023-01-15,120.50,Savings,New York,Deposit\nc002,15/02/2023,250.0,checking,los angeles,Withdrawal\nC003,2023/03/10,,-,Chicago,Deposit\nC004,April 5 2023,500.75,Savings,New york,withdrawal\nc005,2023-05-12,1300.00,Checking,Boston,Deposit\nC006,,300.25,Savings,Houston,Withdrawal\nc007,2023-07-01,Not Available,Checking,Chicago,Deposit\nc008,2023-08-16,450.00,savings,Los Angeles,Deposit\nC009,2023-09-20,700.00,Checking,Boston,Withdrawal\nC010,2023-10-05,150,checking,houston,deposit\nC011,11/11/2023,900.00,Savings,Chicago,Withdrawal\nC012,2023-12-25,850,Checking,New York,Deposit", "eda_steps": ["Check for missing values in all columns", "Standardize capitalization in categorical columns: Account_Type, Branch, Transaction_Type", "Parse and unify Transaction_Date into a consistent date format", "Compute descriptive statistics for Transaction_Amount", "Generate value counts for Account_Type and Transaction_Type", "Calculate the number of transactions per Branch", "Identify transactions with missing or invalid amounts", "Analyze monthly transaction volumes based on Transaction_Date"], "eda_results": {"missing_values": {"Customer_ID": 0, "Transaction_Date": 1, "Transaction_Amount": 2, "Account_Type": 1, "Branch": 0, "Transaction_Type": 0}, "standardized_categories": {"Account_Type": ["Savings", "Checking", "Unknown"], "Branch": ["New York", "Los Angeles", "Chicago", "Boston", "Houston"], "Transaction_Type": ["Deposit", "Withdrawal"]}, "transaction_dates_parsed": {"earliest": "2023-01-15", "latest": "2023-12-25", "invalid_dates": 1}, "summary_stats": {"Transaction_Amount": {"count": 11, "mean": 546.13, "std": 415.42, "min": 120.5, "25%": 150, "50%": 450, "75%": 700, "max": 1300}}, "value_counts": {"Account_Type": {"Savings": 5, "Checking": 6, "Unknown": 1}, "Transaction_Type": {"Deposit": 6, "Withdrawal": 6}}, "transactions_per_branch": {"New York": 3, "Los Angeles": 2, "Chicago": 3, "Boston": 2, "Houston": 2}, "invalid_amounts": ["", "Not Available"], "monthly_transaction_counts": {"January": 1, "February": 1, "March": 1, "April": 1, "May": 1, "June": 0, "July": 1, "August": 1, "September": 1, "October": 1, "November": 1, "December": 1}}}
{"purpose": "Analyze customer churn patterns and usage behavior in a telecom dataset.", "raw_table": "CustomerID,SignupDate,PlanType,MonthlyCharge,DataUsageGB,Churn,LastContact\nC001,2023-01-15,Premium,75.5,12.4,No,2024/04/10\nc002,15-02-2023,standard,45.0,7.8,yes,04-15-2024\nC003,2023/03/05,PREMIUM,na,15.6,No,2024-03-22\nC004,,Basic,30,5.3,No,2024-04-01\nc005,2023-01-20,Standard,50,NA,Yes,2024-04-09\nC006,2023-02-28,basic,29.5,4.7,No,\nC007,2023.03.15,Standard,48.0,8.0,no,2024-04-07\nc008,2023-01-10,PREMIUM,80.0,14.2,No,2024-04-12\nC009,2023-02-25,standard,47.5,8.5,Yes,2024-04-10\nc010,03/10/2023,BASIC,32,5.0,No,2024-04-06", "eda_steps": ["Check and report the percentage of missing values per column", "Standardize the 'PlanType' column to consistent capitalization", "Convert 'SignupDate' and 'LastContact' to datetime format", "Calculate descriptive statistics for 'MonthlyCharge' and 'DataUsageGB'", "Generate value counts for the 'Churn' column", "Identify correlation between 'MonthlyCharge' and 'DataUsageGB'", "Find the average 'MonthlyCharge' grouped by 'PlanType'", "Count number of customers per 'PlanType'", "Check for duplicate 'CustomerID' entries"], "eda_results": {"missing_values": {"CustomerID": "0%", "SignupDate": "10%", "PlanType": "0%", "MonthlyCharge": "10%", "DataUsageGB": "20%", "Churn": "0%", "LastContact": "10%"}, "standardized_plan_types": ["Premium", "Standard", "Basic"], "date_conversion": {"SignupDate_invalid_or_missing": 1, "LastContact_invalid_or_missing": 1}, "summary_stats": {"MonthlyCharge": {"count": 14, "mean": 49.07, "std": 17.42, "min": 29.5, "max": 80.0}, "DataUsageGB": {"count": 12, "mean": 9.88, "std": 3.64, "min": 4.7, "max": 15.6}}, "value_counts_churn": {"No": 7, "Yes": 3}, "correlations": {"MonthlyCharge_vs_DataUsageGB": 0.97}, "avg_monthly_charge_by_plan": {"Basic": 30.5, "Standard": 47.6, "Premium": 77.75}, "customer_count_by_plan": {"Basic": 3, "Standard": 4, "Premium": 2}, "duplicate_customer_ids": 0}}
{"purpose": "Analyze patterns in housing sales data to understand price distributions and missing information.", "raw_table": "ListingID,City,Price,Area_sqft,Bedrooms,Bathrooms,Sale_Date,Property_Type\n1001,New york,750000,1200,3,2,2023-05-10,Condo\n1002,los angeles,NaN,1500,4,3,05/15/2023,Single Family\n1003,Chicago,450000,1100,,2,2023/05/20,condo\n1004,Houston,380000,NaN,3,2,2023-05-25,TownHouse\n1005,Phoenix,320000,900,2,1,2023-13-05,Single family\n1006,Philadelphia,NaN,1300,3,,2023-05-30,apartment\n1007,San Antonio,290000,950,2,1,2023-05-28,Apartment\n1008,San Diego,680000,1400,3,2,May 29, 2023,Condo\n1009,Dallas,550000,1250,3,2,2023-05-22,Single Family\n1010,San Jose,NaN,,4,3,2023-05-31,Single family\n1011,Austin,480000,1150,3,2,2023-05-27,Townhouse\n1012,Jacksonville,360000,1000,2,1,,TownHouse\n1013,fort worth,410000,1050,3,2,2023-05-21,Condo", "eda_steps": ["Check missing value percentages for each column", "Standardize capitalization in City and Property_Type columns", "Convert Sale_Date to a uniform date format and identify invalid dates", "Compute descriptive statistics for numeric columns (Price, Area_sqft, Bedrooms, Bathrooms)", "Generate value counts for Property_Type", "Identify top 3 cities by number of listings", "Calculate correlation matrix for numeric variables", "Summarize distribution skewness for Price and Area_sqft"], "eda_results": {"missing_values": {"ListingID": 0, "City": 0, "Price": 4, "Area_sqft": 2, "Bedrooms": 1, "Bathrooms": 2, "Sale_Date": 1, "Property_Type": 0}, "standardized_categories": {"City": ["New York", "Los Angeles", "Chicago", "Houston", "Phoenix", "Philadelphia", "San Antonio", "San Diego", "Dallas", "San Jose", "Austin", "Jacksonville", "Fort Worth"], "Property_Type": ["Condo", "Single Family", "Condo", "Townhouse", "Single Family", "Apartment", "Apartment", "Condo", "Single Family", "Single Family", "Townhouse", "Townhouse", "Condo"]}, "invalid_dates": ["2023-13-05"], "descriptive_stats": {"Price": {"count": 9, "mean": 460000, "std": 153893.6, "min": 290000, "25%": 360000, "50%": 450000, "75%": 550000, "max": 750000}, "Area_sqft": {"count": 11, "mean": 1145, "std": 171.2, "min": 900, "25%": 1000, "50%": 1150, "75%": 1250, "max": 1500}, "Bedrooms": {"count": 12, "mean": 3, "std": 0.78, "min": 2, "25%": 2, "50%": 3, "75%": 3, "max": 4}, "Bathrooms": {"count": 11, "mean": 1.91, "std": 0.57, "min": 1, "25%": 1, "50%": 2, "75%": 2, "max": 3}}, "property_type_counts": {"Condo": 4, "Single Family": 4, "Townhouse": 3, "Apartment": 2}, "top_cities": {"New York": 1, "Los Angeles": 1, "Chicago": 1, "Houston": 1, "Phoenix": 1, "Philadelphia": 1, "San Antonio": 1, "San Diego": 1, "Dallas": 1, "San Jose": 1, "Austin": 1, "Jacksonville": 1, "Fort Worth": 1}, "correlations": {"Price_Area_sqft": 0.87, "Price_Bedrooms": 0.65, "Price_Bathrooms": 0.59, "Area_sqft_Bedrooms": 0.72}, "distribution_skewness": {"Price": 0.61, "Area_sqft": 0.37}}}
{"purpose": "Analyze monthly stock trading data to identify missing values, distribution patterns, and correlations between volume and price change.", "raw_table": "Date,Stock_Symbol,Open,Close,Volume,Market_Segment\n2023-01-02,AAPL,135.9,137.2,12000000,Tech\n2023/01/03,GOOGL,90.5,,8500000,tech\n01-04-2023,MSFT,240.0,243.5,15000000,TECH\n2023-01-05,amzn,3200.0,3215.6,NaN,Retail\n2023-01-06,FB,205.1,208.3,11000000,Social Media\n2023/01/07,aapl,137.3,138.0,12500000,Tech\n2023-1-08,GOOGL,92.0,91.5,9000000,Tech\n01-09-2023,MSFT,,245.1,15200000,Tech\n2023-01-10,AMZN,3210.0,3225.0,14000000,Retail\n2023-01-11,FB,209.0,211.2,,Social-media\n2023-01-12,AAPL,138.1,139.0,13000000,TECH\n2023/01/13,GOOGL,91.7,92.5,8800000,TECH\n2023-01-14,msft,244.5,246.0,15300000,Tech\n", "eda_steps": ["Parse and standardize the Date column to a consistent date format", "Check for missing values in all columns and calculate their percentages", "Standardize the Stock_Symbol and Market_Segment columns to uppercase", "Compute descriptive statistics (mean, median, std) for Open, Close, and Volume columns", "Calculate daily price change as Close minus Open and summarize its distribution", "Generate value counts for Market_Segment to identify dominant segments", "Compute correlation coefficients between Open, Close, Volume, and price change", "Identify rows with missing Volume or Close values", "Visualize distribution skewness of Volume"], "eda_results": {"missing_values": {"Date": "0%", "Stock_Symbol": "0%", "Open": "8.3%", "Close": "8.3%", "Volume": "16.7%", "Market_Segment": "0%"}, "value_counts": {"Market_Segment": {"TECH": 9, "RETAIL": 2, "SOCIAL MEDIA": 2, "SOCIAL-MEDIA": 1}}, "summary_stats": {"Open": {"mean": 782.54, "median": 138.1, "std": 1134.62}, "Close": {"mean": 782.85, "median": 139.0, "std": 1134.04}, "Volume": {"mean": 12085714, "median": 12500000, "std": 2287905}, "Price_Change": {"mean": 2.31, "median": 1.6, "std": 4.75}}, "correlations": {"Open_Close": 0.998, "Open_Volume": -0.12, "Close_Volume": -0.11, "Price_Change_Volume": -0.05}, "top_categories": {"Stock_Symbol": {"AAPL": 3, "GOOGL": 3, "MSFT": 3, "AMZN": 2, "FB": 2}}, "rows_with_missing": {"Volume": [4, 10], "Close": [1, 7], "Open": [8]}, "volume_skewness": "Moderately right-skewed distribution"}}
{"purpose": "Analyze streaming platform show popularity and user ratings to identify trends and data quality issues.", "raw_table": "Show_ID,Title,Genre,Release_Date,Avg_Rating,Num_Reviews,Platform\ns001,The Last Kingdom,Drama,2015-03-10,8.4,1500,netflix\ns002,STRANGER Things,SCI-FI,2016/07/15,8.8,2300,Netflix\ns003,Friends,Comedy,1994-09-22,8.9,,netflix\ns004,Breaking Bad,drama,2008-01-20,9.5,2100,AMAZON Prime\ns005,The Witcher,Fantasy,2019-12-20,7.8,1700,netFlix\ns006,Dark,SCI-FI,2017-12-01,8.8,900,netflix\ns007,Game of Thrones,Fantasy,2011-04-17,9.3,2500,hbo\ns008,The Office,comedy,2005-03-24,8.9,1400,nbc\ns009,Mandalorian, SCI-FI,2019-11-12,8.7,1800,Disney+\ns010,Ozark,Drama,2017-07-21,8.4,1100,netflix\ns011,Lost,,2004-09-22,8.3,1250,abc\ns012,Westworld,SCI-FI,2016-10-02,N/A,1350,HBO\n", "eda_steps": ["Check for missing values in all columns", "Standardize Genre and Platform columns to consistent capitalization", "Convert Release_Date to a uniform date format", "Calculate descriptive statistics for Avg_Rating and Num_Reviews", "Generate value counts for Genre and Platform columns", "Identify shows with missing Avg_Rating or Num_Reviews", "Find top 3 most reviewed shows", "Analyze correlation between Avg_Rating and Num_Reviews"], "eda_results": {"missing_values": {"Show_ID": 0, "Title": 0, "Genre": 1, "Release_Date": 0, "Avg_Rating": 1, "Num_Reviews": 1, "Platform": 0}, "value_counts": {"Genre": {"Drama": 3, "Sci-Fi": 4, "Comedy": 2, "Fantasy": 2, "": 1}, "Platform": {"Netflix": 6, "Amazon Prime": 1, "HBO": 2, "NBC": 1, "Disney+": 1, "ABC": 1}}, "descriptive_stats": {"Avg_Rating": {"count": 11, "mean": 8.65, "min": 7.8, "max": 9.5, "std_dev": 0.53}, "Num_Reviews": {"count": 13, "mean": 1630.77, "min": 900, "max": 2500, "std_dev": 517.56}}, "shows_missing_ratings_or_reviews": [{"Show_ID": "s003", "Title": "Friends", "Missing": "Num_Reviews"}, {"Show_ID": "s012", "Title": "Westworld", "Missing": "Avg_Rating"}], "top_3_most_reviewed_shows": [{"Title": "Game of Thrones", "Num_Reviews": 2500}, {"Title": "Stranger Things", "Num_Reviews": 2300}, {"Title": "Breaking Bad", "Num_Reviews": 2100}], "correlation_avg_rating_num_reviews": 0.46}}
{"purpose": "Analyze monthly temperature and precipitation patterns across different climate zones to identify data quality issues and basic trends.", "raw_table": "Date,Location,Climate_Zone,Avg_Temperature_C,Precipitation_mm\n2023/01/15,las vegas,Desert,12.3,5.1\n2023-02-20,New York,TEMPERATE,1.2,60.3\n03-18-2023,Seattle,temperate,7.8,45.0\n2023-04-22,,Temperate,13.2,NaN\n2023-05-10,Mexico City,Tropical,18.5,120.7\n2023-06-05,Helsinki,Subarctic,-0.5,30\n2023-07-15,las Vegas,desert,35.0,0\n2023/08/20,New York,TEMPERATE,28.3,NaN\n2023-09-25,Seattle,temperate,16.1,80.2\n2023-10-30,Mexico City,Tropical,21.4,85.0\n11/15/2023,Helsinki,SUBARCTIC,3.2,20.5\n2023-12-01,Las Vegas,Desert,10.0,2.0\n,Seattle,Temperate,9.5,50.0", "eda_steps": ["Check the percentage of missing values in each column", "Standardize date formats to ISO (YYYY-MM-DD)", "Normalize capitalization in categorical columns 'Location' and 'Climate_Zone'", "Compute descriptive statistics for 'Avg_Temperature_C' and 'Precipitation_mm'", "Generate value counts for 'Climate_Zone'", "Identify rows with inconsistent or missing 'Date' or 'Location' entries", "Calculate the correlation between 'Avg_Temperature_C' and 'Precipitation_mm'", "List top 2 locations by average temperature", "Summarize distribution skewness for temperature and precipitation"], "eda_results": {"missing_values": {"Date": "8.33%", "Location": "8.33%", "Climate_Zone": "0%", "Avg_Temperature_C": "0%", "Precipitation_mm": "16.67%"}, "standardized_dates": ["2023-01-15", "2023-02-20", "2023-03-18", "2023-04-22", "2023-05-10", "2023-06-05", "2023-07-15", "2023-08-20", "2023-09-25", "2023-10-30", "2023-11-15", "2023-12-01", null], "normalized_categories": {"Location": ["Las Vegas", "New York", "Seattle", "Seattle", "Mexico City", "Helsinki", "Las Vegas", "New York", "Seattle", "Mexico City", "Helsinki", "Las Vegas", "Seattle"], "Climate_Zone": {"Desert": 3, "Temperate": 6, "Tropical": 2, "Subarctic": 2}}, "summary_stats": {"Avg_Temperature_C": {"count": 13, "mean": 14.34, "std_dev": 11.05, "min": -0.5, "25%": 7.8, "50%": 13.2, "75%": 21.4, "max": 35.0}, "Precipitation_mm": {"count": 11, "mean": 44.09, "std_dev": 38.78, "min": 0.0, "25%": 5.1, "50%": 30.0, "75%": 60.3, "max": 120.7}}, "value_counts_climate_zone": {"Temperate": 6, "Desert": 3, "Tropical": 2, "Subarctic": 2}, "inconsistent_rows": {"missing_date_rows": [12], "missing_location_rows": [3]}, "correlations": {"Avg_Temperature_C_vs_Precipitation_mm": -0.42}, "top_locations_by_avg_temp": {"Las Vegas": 19.1, "Mexico City": 20.0}, "distribution_skewness": {"Avg_Temperature_C": 0.98, "Precipitation_mm": 1.25}}}
{"purpose": "Analyze social media user engagement and posting behavior during March 2024.", "raw_table": "user_id,post_date,post_type,likes,comments,shares,content_length\nu001,03/01/2024,Image,120,15,5,250\nu002,03-02-2024,video,300,40,20,480\nU003,2024/03/03,text,45,,2,100\nu004,03/04/24,Image,NA,10,NA,200\nu005,3/5/2024,link,60,5,1,150\nu006,03-06-2024,text,25,3,0,NA\nu007,2024-03-07,Video,500,60,30,600\nu008,03/08/2024,image,80,8,4,220\nu009,March 9 2024,Text,70,7,3,180\nu010,03/10/2024,Link,55,4,1,140\nu011,03/11/2024,IMAGE,90,,6,230\nu012,03-12-2024,text,40,2,0,110\nu013,2024/03/13,Video,330,50,25,550\nu014,,image,85,9,5,210", "eda_steps": ["Standardize date formats in the 'post_date' column", "Normalize 'post_type' values to lowercase", "Check for missing values in each column", "Compute descriptive statistics for numeric columns: likes, comments, shares, content_length", "Generate value counts for the 'post_type' column", "Identify posts with missing 'post_date' or 'content_length'", "Calculate average likes, comments, and shares per post_type", "Summarize distribution skewness for likes and shares"], "eda_results": {"missing_values": {"user_id": 0, "post_date": 1, "post_type": 0, "likes": 1, "comments": 3, "shares": 2, "content_length": 2}, "value_counts_post_type": {"image": 5, "video": 3, "text": 4, "link": 2}, "descriptive_stats": {"likes": {"count": 13, "mean": 133.85, "std": 149.5, "min": 25, "25%": 55, "50%": 80, "75%": 300, "max": 500}, "comments": {"count": 11, "mean": 15.18, "std": 19.37, "min": 2, "25%": 5, "50%": 8, "75%": 40, "max": 60}, "shares": {"count": 12, "mean": 9.17, "std": 11.33, "min": 0, "25%": 1, "50%": 4.5, "75%": 20, "max": 30}, "content_length": {"count": 12, "mean": 275.83, "std": 161.26, "min": 100, "25%": 150, "50%": 210, "75%": 480, "max": 600}}, "posts_missing_fields": {"missing_post_date": ["u014"], "missing_content_length": ["u006", "u004"]}, "average_engagement_per_post_type": {"image": {"likes": 95, "comments": 10.4, "shares": 4}, "video": {"likes": 376.66, "comments": 50, "shares": 25}, "text": {"likes": 45, "comments": 4, "shares": 1.25}, "link": {"likes": 57.5, "comments": 4.5, "shares": 1}}, "distribution_skewness": {"likes": 1.75, "shares": 1.65}}}
{"purpose": "Analyze monthly transactions and customer segments for a retail bank to identify data quality issues and gain insights into transaction patterns.", "raw_table": "TransactionID,CustomerID,TransactionDate,TransactionType,Amount,CustomerSegment\nT001,C123,2023-01-15,Deposit,500.00,Premium\nT002,c124,15/02/2023,withdrawal,250.5,standard\nT003,C125,2023-03-05,DEPOSIT,,Standard\nT004,C126,2023/04/10,Transfer,1000.00,VIP\nT005,C127,2023-04-15,Deposit,abc,Premium\nT006,C128,,Withdrawal,300.00,Basic\nT007,C129,2023-06-01,TRANSFER,1500,VIP\nT008,C130,2023-06-15,Deposit,700.00,standard\nT009,C131,2023-07-20,withdrawal,,Unknown\nT010,C132,2023-07-25,Deposit,450.25,BASIC\nT011,C133,07-30-2023,Deposit,600,Premium\nT012,C134,2023-08-01,Transfer,850.00,Standard\nT013,C135,2023-08-05,Withdrawal,400.00,Standard", "eda_steps": ["Check for missing values in each column", "Standardize capitalization in TransactionType and CustomerSegment columns", "Convert TransactionDate to a standard date format", "Identify and handle non-numeric Amount values", "Compute descriptive statistics for Amount", "Generate value counts for TransactionType and CustomerSegment", "Calculate the correlation between Amount and TransactionType encoded numerically", "Identify the top 2 most frequent customer segments"], "eda_results": {"missing_values": {"TransactionID": 0, "CustomerID": 0, "TransactionDate": 1, "TransactionType": 0, "Amount": 2, "CustomerSegment": 1}, "standardized_values": {"TransactionType": ["deposit", "withdrawal", "transfer"], "CustomerSegment": ["premium", "standard", "vip", "basic", "unknown"]}, "date_conversion_errors": 1, "amount_conversion_errors": 1, "summary_stats": {"Amount": {"count": 11, "mean": 668.66, "std": 426.54, "min": 250.5, "25%": 450.25, "50%": 600, "75%": 850, "max": 1500}}, "value_counts": {"TransactionType": {"deposit": 6, "withdrawal": 4, "transfer": 3}, "CustomerSegment": {"standard": 4, "premium": 3, "vip": 2, "basic": 2, "unknown": 1}}, "correlations": {"Amount_vs_TransactionType_encoded": 0.27}, "top_categories": {"CustomerSegment": ["standard", "premium"]}}}
{"purpose": "Analyze real estate listings to understand property characteristics and identify data quality issues.", "raw_table": "PropertyID,ListingDate,Location,Price,Area_sqft,Bedrooms,Bathrooms,PropertyType\n101,2023-01-15,New York,850000,1200,3,2,Apartment\n102,15/02/2023,Boston,650000,,2,1,Condo\n103,2023-Mar-10,new york,920000,1350,4,,Apartment\n104,2023-04-01,San Francisco,1250000,900,2,2,house\n105,,Los Angeles,780000,1100,3,2,House\n106,2023/05/20,Boston,abc,1000,3,2,Condo\n107,2023-06-15,los angeles,700000,950,2,1,Townhouse\n108,2023/07/01,San Francisco,1100000,1050,3,2,House\n109,2023-07-10,New york,870000,1150,3,2,Apartment\n110,2023-08-05,Boston,690000,980,2,1,condo\n111,2023-08-20,,920000,1300,3,2,Apartment\n112,2023-09-01,Los Angeles,800000,1200,3,2,House\n113,2023-09-15,San Francisco,NaN,1000,2,1,House\n114,2023-10-01,New York,940000,1400,4,3,Apartment", "eda_steps": ["Check for missing values in each column", "Standardize capitalization in categorical columns like Location and PropertyType", "Convert ListingDate to a consistent date format", "Compute descriptive statistics for numeric columns Price and Area_sqft", "Generate value counts for PropertyType and Location", "Identify rows with non-numeric Price values and handle them", "Calculate correlation between Price, Area_sqft, Bedrooms, and Bathrooms", "Summarize distribution skewness for Price", "Identify the top 3 most common PropertyTypes"], "eda_results": {"missing_values": {"PropertyID": 0, "ListingDate": 1, "Location": 1, "Price": 2, "Area_sqft": 1, "Bedrooms": 0, "Bathrooms": 1, "PropertyType": 0}, "standardized_categories": {"Locations": ["New York", "Boston", "San Francisco", "Los Angeles", "Townhouse"], "PropertyTypes": ["Apartment", "Condo", "House", "Townhouse"]}, "listing_date_format": "Converted all to ISO 8601 format (YYYY-MM-DD), with one missing value", "summary_stats": {"Price": {"count": 12, "mean": 864583.33, "std": 168040.56, "min": 650000, "25%": 700000, "50%": 850000, "75%": 920000, "max": 1250000}, "Area_sqft": {"count": 13, "mean": 1133.85, "std": 143.37, "min": 900, "25%": 980, "50%": 1150, "75%": 1300, "max": 1400}}, "value_counts": {"PropertyType": {"Apartment": 5, "Condo": 3, "House": 5, "Townhouse": 1}, "Location": {"New York": 4, "Boston": 3, "San Francisco": 4, "Los Angeles": 3, "": 1}}, "non_numeric_price_rows": [6], "correlations": {"Price_Area_sqft": 0.87, "Price_Bedrooms": 0.78, "Price_Bathrooms": 0.65, "Area_sqft_Bedrooms": 0.72}, "price_skewness": 0.45, "top_property_types": ["Apartment", "House", "Condo"]}}
{"purpose": "Examine student performance and attendance trends across different subjects to identify potential areas for academic support.", "raw_table": "StudentID,Name,Subject,Score,Attendance,TestDate\n101,john doe,Math,85,Present,2023-03-15\n102,Jane Smith,Science, ,Absent,15/03/2023\n103,ALICE Johnson,math,78,Present,2023/03/16\n104,Bob lee,History,92,present,2023-03-17\n105,carol O'connor,Science,88,Absent,2023-3-18\n106,Daniel King,Math,NaN,Present,03-19-2023\n107,eva M.,history,81,Present,2023.03.20\n108,Frank Wright,science,74,Absent,2023-03-21\n109,Gina Li,Math,90,Present,March 22, 2023\n110,Henry Ford,History,,Present,2023-03-23\n111,Ivy Chen,SCIENCE,85,present,\n112,Jackie Chan,Math,77,Absent,2023-03-24", "eda_steps": ["Check and standardize capitalization in categorical columns (Name, Subject, Attendance).", "Identify and count missing values in all columns.", "Standardize date formats in TestDate column.", "Compute descriptive statistics for the Score column, ignoring missing values.", "Generate value counts for the Subject and Attendance columns.", "Calculate average Score grouped by Subject.", "Analyze attendance distribution by Subject.", "Identify students with missing Scores and Attendance records."], "eda_results": {"missing_values": {"StudentID": 0, "Name": 0, "Subject": 0, "Score": 3, "Attendance": 0, "TestDate": 1}, "value_counts": {"Subject": {"Math": 5, "Science": 4, "History": 3}, "Attendance": {"Present": 7, "Absent": 4}}, "standardized_Attendance_counts": {"Present": 7, "Absent": 4}, "score_stats": {"count": 9, "mean": 83.33, "std_dev": 6.02, "min": 74, "25%": 78, "50%": 85, "75%": 88, "max": 92}, "average_score_by_subject": {"Math": 82.5, "Science": 82.33, "History": 86.5}, "attendance_by_subject": {"Math": {"Present": 4, "Absent": 1}, "Science": {"Present": 1, "Absent": 3}, "History": {"Present": 2, "Absent": 1}}, "students_missing_scores": [{"StudentID": 102, "Name": "Jane Smith", "Subject": "Science"}, {"StudentID": 106, "Name": "Daniel King", "Subject": "Math"}, {"StudentID": 110, "Name": "Henry Ford", "Subject": "History"}], "students_missing_testdate": [{"StudentID": 111, "Name": "Ivy Chen", "Subject": "Science"}]}}
{"purpose": "Analyze monthly sales performance and customer demographics for a retail store.", "raw_table": "InvoiceID,CustomerID,PurchaseDate,ProductCategory,Quantity,UnitPrice,PaymentMethod,CustomerAge\n1001,C001,2023-01-15,Electronics,2,299.99,Credit Card,34\n1002,C002,15/02/2023,home appliances,1,150.50,Cash,28\n1003,C003,2023/03/10,clothing,,49.99,credit card,NaN\n1004,C004,2023-04-05,Electronics,3,299.99,Paypal,45\n1005,,2023-05-20,Clothing,2,19.99,Credit card,22\n1006,C006,06-15-2023,Furniture,1,399.00,CASH,40\n1007,C007,2023-07-02,Toys,5,15.00,Debit Card,35\n1008,C008,,electronics,1,299.99,Credit Card,38\n1009,C009,2023-08-18,Home Appliances,2,NaN,Paypal,27\n1010,C010,2023-09-10,Clothing,3,25.00,Cash,31\n1011,C011,2023-09-15,Toys,NaN,15.00,Debit card,29\n1012,C012,2023-10-01,Furniture,1,399.00,Credit Card,50\n1013,C013,2023-10-10,Electronics,4,299.99,CASH,42\n1014,C014,2023-11-25,Clothing,2,20.00,Credit card,NaN\n", "eda_steps": ["Check the format consistency of the PurchaseDate column and parse dates", "Identify and count missing values per column", "Standardize capitalization in ProductCategory and PaymentMethod columns", "Compute descriptive statistics for Quantity, UnitPrice, and CustomerAge", "Generate value counts for ProductCategory and PaymentMethod", "Identify top-3 most purchased product categories by total Quantity", "Calculate total sales amount per InvoiceID (Quantity * UnitPrice) and summarize", "Analyze distribution of CustomerAge including missing age counts"], "eda_results": {"missing_values": {"InvoiceID": 0, "CustomerID": 1, "PurchaseDate": 1, "ProductCategory": 0, "Quantity": 2, "UnitPrice": 1, "PaymentMethod": 0, "CustomerAge": 2}, "date_parsing_issues": {"inconsistent_formats_detected": true, "missing_dates": 1}, "standardized_categories": {"ProductCategory": {"electronics": 4, "home appliances": 2, "clothing": 4, "furniture": 2, "toys": 2}, "PaymentMethod": {"credit card": 5, "cash": 3, "paypal": 2, "debit card": 2}}, "summary_stats": {"Quantity": {"count": 12, "mean": 2.42, "std": 1.34, "min": 1, "25%": 1.0, "50%": 2.0, "75%": 3.0, "max": 5}, "UnitPrice": {"count": 13, "mean": 160.69, "std": 139.87, "min": 15.0, "25%": 20.0, "50%": 49.99, "75%": 299.99, "max": 399.0}, "CustomerAge": {"count": 12, "mean": 34.92, "std": 7.76, "min": 22, "25%": 28.0, "50%": 34.5, "75%": 40.5, "max": 50}}, "top_categories_by_quantity": {"electronics": 10, "clothing": 11, "toys": 7}, "total_sales_per_invoice": {"1001": 599.98, "1002": 150.5, "1003": null, "1004": 899.97, "1005": 39.98, "1006": 399.0, "1007": 75.0, "1008": 299.99, "1009": null, "1010": 75.0, "1011": null, "1012": 399.0, "1013": 1199.96, "1014": 40.0}, "total_sales_summary": {"total_sales": 4177.38, "average_sales": 369.76, "sales_with_missing_values": 3}, "customer_age_distribution": {"ages": [34, 28, null, 45, 22, 40, 35, 38, 27, 31, 29, 50, 42, null], "missing_ages": 2}}}
{"purpose": "Analyze customer transaction patterns and product category performance to inform inventory decisions.", "raw_table": "TransactionID,CustomerID,TransactionDate,ProductCategory,Quantity,UnitPrice,PaymentMethod\nT001,Cust01,2023/01/15,Electronics,2,299.99,Credit Card\nT002,Cust02,15-01-2023,Home & Garden,1,85.50,Cash\nT003,Cust03,2023-01-16,clothing,3,49.95,credit card\nT004,Cust04,2023/01/17,Electronics,,199.99,Debit Card\nT005,Cust05,01/18/2023,SPORTS,4,15.00,PayPal\nT006,Cust02,2023-01-19,home & garden,2,,Cash\nT007,Cust06,2023/01/20,Clothing,1,59.99,Credit Card\nT008,Cust07,2023/01/21,Toys,5,9.99,Cash\nT009,Cust08,,Electronics,1,299.99,Credit Card\nT010,Cust09,2023/01/22,ToYs,2,10.50,Paypal\nT011,Cust10,2023-01-23,Sports,3,20.00,Debit Card\nT012,Cust11,2023-01-23,Clothing,1,49.95,\nT013,Cust12,2023/01/24,Electronics,2,299.99,Credit Card\nT014,Cust13,2023/01/25,Home & garden,1,80.00,Credit Card", "eda_steps": ["Check for missing values in each column", "Standardize capitalization for ProductCategory and PaymentMethod columns", "Convert TransactionDate to a consistent date format", "Calculate descriptive statistics for Quantity and UnitPrice", "Generate value counts for ProductCategory and PaymentMethod", "Identify top 3 product categories by total sales revenue", "Calculate total revenue per transaction", "Check for duplicates in TransactionID", "Summarize number of transactions per customer"], "eda_results": {"missing_values": {"TransactionID": 0, "CustomerID": 0, "TransactionDate": 1, "ProductCategory": 0, "Quantity": 1, "UnitPrice": 1, "PaymentMethod": 1}, "standardized_categories": {"ProductCategory": ["Electronics", "Home & Garden", "Clothing", "Sports", "Toys"], "PaymentMethod": ["Credit Card", "Cash", "Debit Card", "PayPal"]}, "date_conversion": {"ConvertedDates": 14, "FailedConversions": 1}, "summary_stats": {"Quantity": {"count": 14, "mean": 2.36, "std": 1.42, "min": 1, "25%": 1, "50%": 2, "75%": 3, "max": 5}, "UnitPrice": {"count": 13, "mean": 110.98, "std": 125.55, "min": 9.99, "25%": 15.0, "50%": 59.99, "75%": 299.99, "max": 299.99}}, "value_counts": {"ProductCategory": {"Electronics": 4, "Home & Garden": 3, "Clothing": 3, "Sports": 2, "Toys": 2}, "PaymentMethod": {"Credit Card": 6, "Cash": 3, "Debit Card": 2, "PayPal": 2}}, "top_categories_by_revenue": {"Electronics": 1799.94, "Clothing": 209.79, "Home & Garden": 245.5}, "total_revenue_per_transaction": {"T001": 599.98, "T002": 85.5, "T003": 149.85, "T004": null, "T005": 60.0, "T006": null, "T007": 59.99, "T008": 49.95, "T009": 299.99, "T010": 21.0, "T011": 60.0, "T012": 49.95, "T013": 599.98, "T014": 80.0}, "duplicate_transaction_ids": 0, "transactions_per_customer": {"Cust01": 1, "Cust02": 2, "Cust03": 1, "Cust04": 1, "Cust05": 1, "Cust06": 1, "Cust07": 1, "Cust08": 1, "Cust09": 1, "Cust10": 1, "Cust11": 1, "Cust12": 1, "Cust13": 1}}}
{"purpose": "Analyze monthly temperature and precipitation patterns across different climate zones to identify data quality issues and basic statistical summaries.", "raw_table": "Date,Location,Climate_Zone,Avg_Temperature_C,Precipitation_mm\n2023-01-15,Sahara,Arid,23.5,0\n2023/02/15,amazon,Rainforest,28.4,310\n2023-03-15,Siberia,Tundra,-15.2,5\n15-04-2023,Alps,Alpine,5.1,45\n2023-05-15,amazon,Rainforest,27.8,N/A\n2023-6-15,Sahara,arid,30.2,1\n2023-07-15,Siberia,Tundra,-10.0,7\n2023-08-15,,Alpine,6.3,50\n2023-09-15,Alps,alpine,8,,\n2023-10-15,amazon,Rainforest,29.1,280\n2023-11-15,Sahara,Arid,MISSING,0\n2023-12-15,Siberia,Tundra,-20.5,2", "eda_steps": ["Standardize date formats to YYYY-MM-DD", "Identify and count missing values in each column", "Standardize capitalization in categorical columns 'Location' and 'Climate_Zone'", "Replace non-numeric and missing temperature values with NaN", "Compute descriptive statistics for 'Avg_Temperature_C' and 'Precipitation_mm'", "Generate value counts for 'Climate_Zone'", "Check correlation between 'Avg_Temperature_C' and 'Precipitation_mm'", "Identify rows with inconsistent or missing 'Location' values"], "eda_results": {"missing_values": {"Date": 0, "Location": 1, "Climate_Zone": 0, "Avg_Temperature_C": 1, "Precipitation_mm": 2}, "standardized_dates": ["2023-01-15", "2023-02-15", "2023-03-15", "2023-04-15", "2023-05-15", "2023-06-15", "2023-07-15", "2023-08-15", "2023-09-15", "2023-10-15", "2023-11-15", "2023-12-15"], "standardized_categories": {"Location": ["Sahara", "Amazon", "Siberia", "Alps", "Amazon", "Sahara", "Siberia", "MISSING", "Alps", "Amazon", "Sahara", "Siberia"], "Climate_Zone": ["Arid", "Rainforest", "Tundra", "Alpine", "Rainforest", "Arid", "Tundra", "Alpine", "Alpine", "Rainforest", "Arid", "Tundra"]}, "descriptive_statistics": {"Avg_Temperature_C": {"count": 11, "mean": 7.04, "std": 18.01, "min": -20.5, "25%": -10.0, "50%": 6.3, "75%": 27.85, "max": 30.2}, "Precipitation_mm": {"count": 10, "mean": 69.0, "std": 117.0, "min": 0, "25%": 0, "50%": 5, "75%": 45, "max": 310}}, "value_counts_climate_zone": {"Arid": 3, "Rainforest": 3, "Tundra": 3, "Alpine": 3}, "correlations": {"Avg_Temperature_C_vs_Precipitation_mm": -0.42}, "inconsistent_location_rows": [{"Date": "2023-08-15", "Location": "", "Climate_Zone": "Alpine", "Avg_Temperature_C": 6.3, "Precipitation_mm": 50}]}}
{"purpose": "Analyze student performance and attendance patterns in a high school to identify factors affecting grades.", "raw_table": "StudentID,Name,Grade,Subject,TestDate,Score,Attendance\n1,alice,10,Math,2023-03-15,88,Present\n2,Bob,10,english,15/03/2023,92,Present\n3,CHARLIE,11,Math,2023/03/16, ,Absent\n4,David,11,Science,03-16-2023,85,Present\n5,Eva,10,Math,2023-03-15,91,Present\n6,Frank,11,English,2023-03-17,78,Present\n7,Grace,10,science,17 Mar 2023,87,Absent\n8,henry,11,Math,2023-03-16,82,Present\n9,Ivy,10,English,2023/03/15,90,\n10,Jack,11,Science,16-03-2023,88,Present\n11,kate,10,Math,2023-03-15,85,Present\n12,Liam,11,english,2023-03-17,80,Present\n", "eda_steps": ["Check and count missing values in each column", "Standardize the Subject and Attendance columns capitalization", "Parse and unify TestDate formats into ISO format YYYY-MM-DD", "Compute descriptive statistics (mean, median, std) for Score grouped by Grade and Subject", "Generate value counts for Attendance statuses", "Identify students with missing scores or attendance records", "Calculate average Score by Attendance status", "Create distribution summary of Scores across all students", "Determine correlation between Grade level and Score"], "eda_results": {"missing_values": {"StudentID": 0, "Name": 0, "Grade": 0, "Subject": 0, "TestDate": 0, "Score": 1, "Attendance": 1}, "standardized_values": {"Subjects": ["Math", "English", "Science"], "Attendance": ["Present", "Absent", "Missing"]}, "unified_dates": ["2023-03-15", "2023-03-15", "2023-03-16", "2023-03-16", "2023-03-15", "2023-03-17", "2023-03-17", "2023-03-16", "2023-03-15", "2023-03-16", "2023-03-15", "2023-03-17"], "score_stats_by_grade_subject": {"10": {"Math": {"mean": 88.0, "median": 88.0, "std": 3.0}, "English": {"mean": 91.0, "median": 91.0, "std": 1.414}, "Science": {"mean": 87.0, "median": 87.0, "std": 0.0}}, "11": {"Math": {"mean": 82.0, "median": 82.0, "std": 0.0}, "English": {"mean": 79.0, "median": 79.0, "std": 1.414}, "Science": {"mean": 86.5, "median": 86.5, "std": 2.121}}}, "attendance_value_counts": {"Present": 9, "Absent": 2, "Missing": 1}, "students_missing_records": {"MissingScore": ["CHARLIE"], "MissingAttendance": ["Ivy"]}, "avg_score_by_attendance": {"Present": 85.9, "Absent": 87.0, "Missing": null}, "score_distribution_summary": {"min": 78, "max": 92, "mean": 85.9, "median": 86.0, "std": 4.8}, "correlation_grade_score": 0.12}}
{"purpose": "Analyze household electricity consumption patterns and identify missing data issues.", "raw_table": "household_id,date,energy_kwh,region,type\nH001,2024-01-01,12.5,Northern,Residential\nh002,01/02/2024,15.3,Southern,commercial\nH003,2024-1-03,,Eastern,Residential\nH004,2024-01-04,20.1,Western,Industrial\nH005,2024/01/05,18.7,northern,Residential\nH006,06-01-2024,22.2,Southern,Commercial\nH007,2024-01-07,17.8,Eastern,RESIDENTIAL\nH008,2024-01-08,nan,Western,Industrial\nh009,2024-01-09,19.0,Northern,Commercial\nH010,,21.4,Southern,Industrial", "eda_steps": ["Check for missing values in each column", "Standardize the date format to YYYY-MM-DD", "Standardize the capitalization of 'region' and 'type' columns", "Compute descriptive statistics for the energy_kwh column", "Generate value counts for region and type columns", "Identify rows with missing or invalid energy_kwh values", "Calculate the average energy consumption per region", "Check for duplicate household_id entries"], "eda_results": {"missing_values": {"household_id": 0, "date": 1, "energy_kwh": 2, "region": 0, "type": 0}, "standardized_date": ["2024-01-01", "2024-01-02", "2024-01-03", "2024-01-04", "2024-01-05", "2024-01-06", "2024-01-07", "2024-01-08", "2024-01-09", null], "standardized_region_counts": {"Northern": 3, "Southern": 3, "Eastern": 2, "Western": 2}, "standardized_type_counts": {"Residential": 4, "Commercial": 3, "Industrial": 3}, "descriptive_stats_energy_kwh": {"count": 8, "mean": 18.43, "std": 3.28, "min": 12.5, "25%": 17.3, "50%": 18.9, "75%": 20.1, "max": 22.2}, "rows_with_missing_or_invalid_energy_kwh": [2, 7], "average_energy_per_region": {"Northern": 15.33, "Southern": 19.63, "Eastern": 17.8, "Western": 20.1}, "duplicate_household_id_count": 0}}
{"purpose": "Analyze viewership patterns and genre popularity of streaming platform shows over recent months.", "raw_table": "ShowID,ShowName,Genre,Release_Date,Avg_Viewers,Rating,Platform\n1,The Last Quest,Fantasy,2022-11-05,1500000,8.7,netflix\n2,SPACE Wars,Sci-Fi,11/12/2023,1200000,8.1,Netflix\n3,The cook-off,Reality,,750000,7.3,hulu\n4,Dark nights,HORROR,2023-02-15,NaN,6.9,Hulu\n5,Historic Tales,Drama,2022/09/20,980000,8.0,amazon prime\n6,The last quest,fantasy,2022-11-05,1450000,8.7,Netflix\n7,Sing it Out!,Music,2023-01-10,430000,7.4,Amazon Prime\n8,space wars,Sci-fi,2023-11-12,1150000,8.1,netFlix\n9,Unknown,Unknown,2023-05-05,,5.0,disney+\n10,THE COOK-OFF,Reality,2023-03-15,800000,7.5,Hulu\n11,Dark Nights,Horror,2023-02-15,670000,,Hulu\n12,Historic tales,Drama,2022/09/20,1000000,8.2,Amazon Prime", "eda_steps": ["Check the number of missing values in each column", "Standardize genre and platform names to lowercase", "Identify duplicate shows based on ShowName ignoring case", "Calculate average viewers and rating statistics", "Count the number of shows per genre", "Find shows with missing Release_Date and Rating", "Analyze correlation between Avg_Viewers and Rating", "List top 3 shows by Avg_Viewers"], "eda_results": {"missing_values": {"ShowID": 0, "ShowName": 0, "Genre": 0, "Release_Date": 1, "Avg_Viewers": 2, "Rating": 2, "Platform": 0}, "standardized_genres": ["fantasy", "sci-fi", "reality", "horror", "drama", "music", "unknown"], "standardized_platforms": ["netflix", "hulu", "amazon prime", "disney+"], "duplicate_shows": ["the last quest", "space wars", "the cook-off", "dark nights", "historic tales"], "avg_viewers_stats": {"mean": 963333, "median": 875000, "min": 430000, "max": 1500000}, "rating_stats": {"mean": 7.64, "median": 7.75, "min": 5.0, "max": 8.7}, "shows_per_genre": {"fantasy": 2, "sci-fi": 2, "reality": 2, "horror": 2, "drama": 2, "music": 1, "unknown": 1}, "shows_missing_release_date": ["The cook-off"], "shows_missing_rating": ["Dark Nights", "Unknown"], "correlation_avg_viewers_rating": 0.78, "top_3_shows_by_viewers": [{"ShowName": "The Last Quest", "Avg_Viewers": 1500000}, {"ShowName": "The last quest", "Avg_Viewers": 1450000}, {"ShowName": "Space Wars", "Avg_Viewers": 1200000}]}}
{"purpose": "Analyze customer call and data usage patterns to identify inconsistencies and usage trends.", "raw_table": "CustomerID,CallDurationMinutes,DataUsageMB,PlanType,SignupDate,Region\n1001,15,500,prepaid,2023-01-15,North\n1002,NA,1500,Postpaid,15/02/2023,East\n1003,35,NaN,PrePaid,2023/03/05,west\n1004,45,2000,postpaid,2023-03-18,East\n1005,20,850,PREPAID,March 20 2023,South\n1006,,400,postpaid,2023-04-10,North\n1007,10,300,PrePaid,2023-04-25,north\n1008,25,NaN,postpaid,2023-05-01,East\n1009,NA,1000,PREPAID,2023-05-10,South\n1010,30,700,postpaid,2023-05-15,West\n1011,15,650,Prepaid,,East\n1012,40,1200,postpaid,2023-06-01,South\n1013,50,,PostPaid,2023-06-05,East", "eda_steps": ["Check for missing values in all columns", "Standardize PlanType values to lowercase", "Standardize Region values to title case", "Parse SignupDate into a consistent date format", "Compute descriptive statistics for CallDurationMinutes and DataUsageMB", "Generate value counts for PlanType and Region", "Identify rows with missing CallDurationMinutes or DataUsageMB", "Calculate correlation between CallDurationMinutes and DataUsageMB", "Identify the earliest and latest SignupDate"], "eda_results": {"missing_values": {"CustomerID": 0, "CallDurationMinutes": 3, "DataUsageMB": 4, "PlanType": 0, "SignupDate": 1, "Region": 0}, "standardized_values": {"PlanType": {"prepaid": 6, "postpaid": 7}, "Region": {"North": 3, "East": 5, "West": 2, "South": 3}}, "signup_dates_parsed": {"earliest": "2023-01-15", "latest": "2023-06-05"}, "summary_stats": {"CallDurationMinutes": {"count": 10, "mean": 29.5, "std": 12.8, "min": 10, "max": 50}, "DataUsageMB": {"count": 9, "mean": 979, "std": 581, "min": 300, "max": 2000}}, "value_counts": {"PlanType": {"prepaid": 6, "postpaid": 7}, "Region": {"North": 3, "East": 5, "West": 2, "South": 3}}, "missing_data_rows": {"CallDurationMinutes": [1002, 1009, 1006], "DataUsageMB": [1003, 1008, 1013, 1011], "SignupDate": [1011]}, "correlations": {"CallDurationMinutes_DataUsageMB": 0.82}}}
{"purpose": "Analyze monthly sales performance across product categories to identify trends and data quality issues.", "raw_table": "OrderID,OrderDate,ProductCategory,UnitsSold,UnitPrice,TotalRevenue,Region\n1001,2023/01/15,Electronics,5,299.99,1499.95,North\n1002,01-20-2023,Clothing,10,49.99,499.9,South\n1003,2023-1-23,books,2,,39.98,East\n1004,,Electronics,3,299.99,899.97,west\n1005,2023-02-02,Toys,8,19.99,159.92,North\n1006,2023/02/05,Clothing,,45.00,225.00,South\n1007,02-10-2023,Clothing,7,49.99,349.93,South\n1008,2023-02-15,TOYS,3,20,60,East\n1009,2023-02-18,Books,5,19.99,,West\n1010,2023-02-21,Electronics,1,299.99,299.99,North\n1011,2023-02-25,,4,15.00,60.00,East\n1012,2023-03-01,Toys,6,19.99,119.94,South\n1013,03/05/2023,Clothing,9,50,450,South\n1014,2023/03/07,Books,3,20,60,North", "eda_steps": ["Inspect column data types and convert OrderDate to a consistent date format", "Check and report missing values in each column", "Standardize capitalization in ProductCategory and Region columns", "Calculate total missing UnitsSold and UnitPrice records", "Compute summary statistics for UnitsSold, UnitPrice, and TotalRevenue", "Generate value counts for ProductCategory and Region", "Identify rows with inconsistent or missing ProductCategory or OrderDate", "Calculate total revenue by ProductCategory", "Identify top 2 product categories by total units sold"], "eda_results": {"missing_values": {"OrderID": 0, "OrderDate": 1, "ProductCategory": 1, "UnitsSold": 1, "UnitPrice": 1, "TotalRevenue": 2, "Region": 0}, "data_types": {"OrderID": "int", "OrderDate": "date", "ProductCategory": "string", "UnitsSold": "float", "UnitPrice": "float", "TotalRevenue": "float", "Region": "string"}, "standardized_categories": {"ProductCategory": ["Electronics", "Clothing", "Books", "Toys", "Unknown"], "Region": ["North", "South", "East", "West"]}, "summary_stats": {"UnitsSold": {"count": 13, "mean": 5.31, "std": 2.57, "min": 1, "max": 10}, "UnitPrice": {"count": 13, "mean": 92.14, "std": 122.28, "min": 15, "max": 299.99}, "TotalRevenue": {"count": 12, "mean": 325.15, "std": 446.64, "min": 39.98, "max": 1499.95}}, "value_counts": {"ProductCategory": {"Clothing": 4, "Electronics": 3, "Books": 3, "Toys": 3, "Unknown": 1}, "Region": {"South": 5, "North": 4, "East": 4, "West": 2}}, "rows_with_issues": {"missing_orderdate": [1004], "missing_productcategory": [1011], "missing_units_sold": [1006], "missing_unit_price": [1003]}, "total_revenue_by_category": {"Electronics": 2699.91, "Clothing": 1524.83, "Books": 159.98, "Toys": 339.86, "Unknown": 60.0}, "top_categories_by_units_sold": {"Clothing": 26, "Toys": 17}}}
{"purpose": "Analyze recent monthly temperature and precipitation patterns across selected climate zones to detect inconsistencies and missing data.", "raw_table": "Date,Region,Temperature_C,Precipitation_mm,Weather_Condition\n2024-01-15,Tropical,29.5,120,Sunny\n2024/02/15,arctic,-15.2,,Snowy\n15-03-2024,Temperate,12.1,45,Rain\n2024-04-15,Tropical,32.0,80,Sunny\n2024-05-15,temperate,16.5,55,Cloudy\n2024-06-15,ARCTIC,-10.8,5,Snow\n2024-07-15,Tropical,30.2,130,Sunny\n2024-08-15,Temperate,,60,rain\n2024-09-15,TROPICAL,28.7,85,Sunny\n2024-10-15,Temperate,14.9,,cloudy\n2024-11-15,Arctic,-18.0,2,Snowy\n2024-12-15,Temperate,10.5,50,Rain\n", "eda_steps": ["Standardize the 'Date' column to YYYY-MM-DD format", "Normalize the 'Region' column capitalization", "Check and report missing values in all columns", "Compute descriptive statistics for Temperature_C and Precipitation_mm", "Generate value counts for 'Region' and 'Weather_Condition'", "Identify records with inconsistent or unusual date formats", "Calculate correlation between Temperature_C and Precipitation_mm", "Summarize frequency of missing data by month"], "eda_results": {"missing_values": {"Date": 0, "Region": 0, "Temperature_C": 1, "Precipitation_mm": 2, "Weather_Condition": 0}, "value_counts": {"Region": {"Tropical": 4, "Arctic": 3, "Temperate": 5}, "Weather_Condition": {"Sunny": 4, "Snowy": 2, "Rain": 3, "Cloudy": 2}}, "summary_stats": {"Temperature_C": {"count": 11, "mean": 13.29, "std": 15.46, "min": -18.0, "25%": -10.8, "50%": 14.9, "75%": 29.85, "max": 32.0}, "Precipitation_mm": {"count": 10, "mean": 59.2, "std": 44.9, "min": 2, "25%": 5, "50%": 55, "75%": 85, "max": 130}}, "correlations": {"Temperature_C_vs_Precipitation_mm": -0.32}, "inconsistent_dates": ["2024/02/15", "15-03-2024"], "missing_by_month": {"February": {"Precipitation_mm": 1}, "August": {"Temperature_C": 1}, "October": {"Precipitation_mm": 1}}}}
{"purpose": "Analyze housing market trends and identify key property features affecting price.", "raw_table": "Property_ID,City,Listing_Date,Price,Area_sqft,Bedrooms,Bathrooms,Property_Type,Year_Built\n101,New york,2023-01-15,850000,1200,3,2,Apartment,1999\n102,los angeles,15/02/2023,1250000,2000,4,3,house,2005\n103,Chicago,2023/03/01,,1500,3,,Condo,2010\n104,Houston,2023-03-15,600000,850,2,1,townhouse,2000\n105,Phoenix,04-04-2023,450000,900,2,1,apartment,1995\n106,New york,2023-04-10,920000,1300,3,2,Apartment,2002\n107,los angeles,2023-04-20,1350000,2100,4,3,House,2008\n108,Chicago,2023-05-05,700000,1600,3,2,condo,2012\n109,Houston,2023/05/10,580000,800,2,1,townHouse,2001\n110,Phoenix,,430000,850,2,1,Apartment,1997\n111,New york,2023-06-01,880000,1250,3,2,Apartment,2001\n112,los angeles,2023-06-15,1400000,2200,4,3,House,2011\n113,Chicago,06/20/2023,710000,1650,3,2,Condo,2015\n114,Houston,2023-06-25,590000,900,,1,Townhouse,2003\n115,Phoenix,2023-06-30,440000,880,2,1,Apartment,\n", "eda_steps": ["Check for missing values in each column and calculate their percentage", "Standardize city names capitalization", "Parse and unify date formats in Listing_Date", "Generate descriptive statistics for numeric columns: Price, Area_sqft, Bedrooms, Bathrooms, Year_Built", "Calculate value counts for Property_Type", "Identify correlation between Price and Area_sqft", "Identify the most common number of bedrooms and bathrooms", "Summarize Year_Built distribution and check for any missing or unusual values"], "eda_results": {"missing_values": {"Property_ID": "0%", "City": "0%", "Listing_Date": "7%", "Price": "7%", "Area_sqft": "0%", "Bedrooms": "7%", "Bathrooms": "7%", "Property_Type": "0%", "Year_Built": "7%"}, "standardized_cities": {"New York": 3, "Los Angeles": 4, "Chicago": 4, "Houston": 4, "Phoenix": 4}, "listing_date_parsing": {"all_dates_parsed": true, "formats": ["YYYY-MM-DD", "DD/MM/YYYY", "YYYY/MM/DD", "MM-DD-YYYY", "MM/DD/YYYY"]}, "summary_stats": {"Price": {"count": 14, "mean": 789642.86, "std": 337455.52, "min": 430000, "25%": 575000, "50%": 700000, "75%": 920000, "max": 1400000}, "Area_sqft": {"count": 15, "mean": 1373.33, "std": 484.65, "min": 800, "25%": 900, "50%": 1250, "75%": 1650, "max": 2200}, "Bedrooms": {"count": 14, "mean": 3.0, "std": 0.75, "min": 2, "25%": 2, "50%": 3, "75%": 4, "max": 4}, "Bathrooms": {"count": 14, "mean": 1.79, "std": 0.6, "min": 1, "25%": 1, "50%": 2, "75%": 2.75, "max": 3}, "Year_Built": {"count": 14, "mean": 2004.36, "std": 6.6, "min": 1995, "25%": 2000, "50%": 2005, "75%": 2010, "max": 2015}}, "property_type_counts": {"Apartment": 5, "House": 4, "Condo": 4, "Townhouse": 3}, "correlations": {"Price_Area_sqft": 0.95}, "common_bedrooms": {"3_bedrooms": 8, "2_bedrooms": 5, "4_bedrooms": 4}, "common_bathrooms": {"2_bathrooms": 7, "1_bathroom": 6, "3_bathrooms": 3}, "year_built_distribution": {"1990s": 2, "2000-2005": 6, "2006-2010": 3, "2011-2015": 3, "missing": 1}}}
{"purpose": "Explore recent temperature and precipitation patterns to understand climate variability in different regions.", "raw_table": "Date,Region,Temperature_C,Precipitation_mm,Weather_Condition\n2024-01-01,north,5,12.3,Sunny\n01/02/2024,South,7,,rainy\n2024-01-03,East,NA,0.0,Cloudy\n2024-1-04,west,-2,5.1,snow\n2024/01/05,NORTH,3,8.7,SUNNY\n2024-01-06,south,8,missing,Rainy\n2024-01-07,East,6,10.0,cloudy\n2024-01-08,West,-1,,Snow\n2024-1-09,north,4,7.2,sunny\n2024-01-10,South,NA,0.0,Cloudy\n2024-01-11,East,5,NA,cloudy\n2024-01-12,west,0,2.3,Snow\n", "eda_steps": ["Standardize region names to consistent capitalization", "Convert Date column to uniform date format", "Identify and count missing values in each column", "Compute descriptive statistics for Temperature_C and Precipitation_mm", "Generate value counts for Weather_Condition", "Analyze average temperature and precipitation by Region", "Check for correlation between Temperature_C and Precipitation_mm", "Summarize frequency of missing data patterns"], "eda_results": {"missing_values": {"Date": 0, "Region": 0, "Temperature_C": 2, "Precipitation_mm": 3, "Weather_Condition": 0}, "value_counts_Weather_Condition": {"Sunny": 3, "Rainy": 2, "Cloudy": 4, "Snow": 3}, "descriptive_stats": {"Temperature_C": {"count": 10, "mean": 4.5, "std_dev": 3.4, "min": -2, "max": 8}, "Precipitation_mm": {"count": 9, "mean": 6.96, "std_dev": 4.14, "min": 0.0, "max": 12.3}}, "average_by_region": {"North": {"Temperature_C": 4.0, "Precipitation_mm": 9.4}, "South": {"Temperature_C": 7.5, "Precipitation_mm": 0.0}, "East": {"Temperature_C": 5.67, "Precipitation_mm": 5.0}, "West": {"Temperature_C": -1.0, "Precipitation_mm": 3.8}}, "correlations": {"Temperature_C_vs_Precipitation_mm": -0.45}, "missing_data_patterns": {"Rows with missing Temperature_C only": 2, "Rows with missing Precipitation_mm only": 1, "Rows with missing both Temperature_C and Precipitation_mm": 1}}}
{"purpose": "Examine patient demographics and lab test results to identify data quality issues and patient distribution patterns in a hospital dataset.", "raw_table": "Patient_ID,Age,Gender,Diagnosis,Admission_Date,Cholesterol_mg_dl,Blood_Pressure\n001,45,Male,Hypertension,2023/1/15,210,130/85\n002,58,Female,diabetes,15-02-2023,NA,140/90\n003,,Male,Hypertension,2023-03-01,180,120/80\n004,50,male,Asthma,3/10/2023,190,125/78\n005,37,Female,,2023-04-05,205,NA\n006,29,Other,COPD,2023-04-25,198,115/75\n007,NaN,FEMALE,diabetes,2023-05-01,220,140/90\n008,65,Female,Hypertension,2023/06/12,210,135/88\n009,52,Male,Diabetes,06-15-2023,230,145/92\n010,48,Male,Asthma,2023-06-20,205,130/85\n011,40,,Hypertension,2023/07/01,195,128/82", "eda_steps": ["Check for missing values in each column", "Standardize the casing of categorical columns such as Gender and Diagnosis", "Parse and unify the date format in Admission_Date to YYYY-MM-DD", "Compute descriptive statistics for Age and Cholesterol_mg_dl columns", "Generate value counts for Gender and Diagnosis columns", "Extract systolic and diastolic values from Blood_Pressure and compute their descriptive statistics", "Identify rows with inconsistent or missing Blood_Pressure values", "Analyze correlation between Age and Cholesterol_mg_dl", "List patients with missing Age or Diagnosis information"], "eda_results": {"missing_values": {"Patient_ID": 0, "Age": 2, "Gender": 1, "Diagnosis": 1, "Admission_Date": 0, "Cholesterol_mg_dl": 1, "Blood_Pressure": 1}, "value_counts": {"Gender": {"Male": 4, "Female": 4, "Other": 1, "": 1}, "Diagnosis": {"Hypertension": 4, "Diabetes": 3, "Asthma": 2, "COPD": 1, "": 1}}, "standardized_diagnosis": ["Hypertension", "Diabetes", "Hypertension", "Asthma", "", "COPD", "Diabetes", "Hypertension", "Diabetes", "Asthma", "Hypertension"], "standardized_gender": ["Male", "Female", "Male", "Male", "Female", "Other", "Female", "Female", "Male", "Male", ""], "admission_date_parsed": ["2023-01-15", "2023-02-15", "2023-03-01", "2023-03-10", "2023-04-05", "2023-04-25", "2023-05-01", "2023-06-12", "2023-06-15", "2023-06-20", "2023-07-01"], "age_stats": {"count": 9, "mean": 48.44, "std": 10.62, "min": 29, "25%": 40, "50%": 48, "75%": 52, "max": 65}, "cholesterol_stats": {"count": 10, "mean": 204.3, "std": 15.86, "min": 180, "25%": 195, "50%": 205, "75%": 210, "max": 230}, "blood_pressure_extracted_stats": {"systolic": {"count": 10, "mean": 130.8, "std": 8.95, "min": 115, "max": 145}, "diastolic": {"count": 10, "mean": 84.3, "std": 6.02, "min": 75, "max": 92}}, "rows_with_invalid_bp": [5], "age_cholesterol_correlation": 0.42, "patients_missing_age_or_diagnosis": ["003", "005", "007", "011"]}}
{"purpose": "Analyze recent stock trading data to identify patterns in trade volumes and price changes.", "raw_table": "Trade_ID,Stock_Symbol,Trade_Date,Trade_Time,Trade_Volume,Trade_Price,Trade_Type\nT001,aapl,2024-03-01,09:30,100,145.3,Buy\nT002,GOOGL,03/01/2024,09:31,250,,SELL\nT003,msft,2024-03-01,09:32,150,280.75,Buy\nT004,AAPL,2024-03-01,09:33,MISSING,146.1,BUY\nT005,googl,2024/03/01,09:34,300,2750.5,sell\nT006,MSFT,2024-03-01,09:35,200,281.25,Buy\nT007,aapl,2024-3-01,09:36,120,145.9,BUY\nT008,GOOGL,2024-03-01,09:37,NaN,2748.0,SELL\nT009,msft,2024-03-01,09:38,180,282.0,Buy\nT010,AAPL,2024-03-1,09:39,130,146.0,buy", "eda_steps": ["Standardize the casing of Stock_Symbol and Trade_Type columns", "Convert Trade_Date and Trade_Time to a single datetime column", "Identify and report missing values per column", "Compute descriptive statistics for Trade_Volume and Trade_Price", "Generate value counts for Trade_Type", "Calculate average Trade_Price per Stock_Symbol", "Summarize total Trade_Volume per Stock_Symbol", "Check for any duplicate Trade_ID entries"], "eda_results": {"missing_values": {"Trade_Volume": 2, "Trade_Price": 1}, "value_counts": {"Trade_Type": {"buy": 6, "sell": 3}}, "summary_stats": {"Trade_Volume": {"count": 8, "mean": 173.75, "min": 100, "max": 300, "std": 68.53}, "Trade_Price": {"count": 9, "mean": 730.71, "min": 145.3, "max": 2750.5, "std": 1145.47}}, "average_trade_price_per_stock": {"aapl": 145.825, "msft": 281.333, "googl": 2749.25}, "total_trade_volume_per_stock": {"aapl": 350, "msft": 530, "googl": 550}, "duplicate_trade_id_count": 0}}
{"purpose": "Analyze recent real estate listings to understand price distribution and property characteristics.", "raw_table": "ListingID,Location,Price,Area_sqft,Bedrooms,Bathrooms,ListingDate,PropertyType\n1,downtown,350000,850,2,1.5,2023-05-10,Apartment\n2,Suburb,450000,1200,3,2,05/12/2023,House\n3,Downtown,NaN,900,2,2,2023-05-11,apartment\n4,suburb,475000,1300,4,2.5,2023/05/13,House\n5,Midtown,400000,NaN,3,,2023-05-12,Townhouse\n6,midTown,420000,1100,3,2,May 14 2023,Townhouse\n7,downtown,360000,870,2,1,2023-05-10,Apartment\n8,SUBURB,460000,1250,3,2,13-05-2023,house\n9,Midtown,,1150,3,2,2023-05-15,Townhouse\n10,Downtown,355000,880,2,1.5,2023-05-11,Apartment\n11,suburb,470000,1280,3,2,2023/05/16,House\n12,Midtown,415000,1120,3,2,2023-5-17,Townhouse", "eda_steps": ["Check for missing values in all columns", "Standardize capitalization for Location and PropertyType columns", "Convert ListingDate to consistent date format", "Compute descriptive statistics for Price and Area_sqft", "Generate value counts for PropertyType", "Analyze distribution of Bedrooms and Bathrooms", "Identify listings missing Price or Area_sqft", "Calculate average Price per Location", "Check correlation between Price and Area_sqft"], "eda_results": {"missing_values": {"Price": 2, "Area_sqft": 1, "Bathrooms": 1}, "standardized_categories": {"Location": {"downtown": 4, "suburb": 4, "midtown": 4}, "PropertyType": {"Apartment": 4, "House": 4, "Townhouse": 4}}, "listing_date_format": "All dates converted to ISO format YYYY-MM-DD", "summary_stats": {"Price": {"count": 10, "mean": 417500, "std": 42820, "min": 350000, "25%": 360000, "50%": 415000, "75%": 460000, "max": 475000}, "Area_sqft": {"count": 11, "mean": 1091, "std": 152, "min": 850, "25%": 880, "50%": 1120, "75%": 1250, "max": 1300}}, "value_counts": {"PropertyType": {"Apartment": 4, "House": 4, "Townhouse": 4}}, "bedrooms_distribution": {"2": 4, "3": 6, "4": 1}, "bathrooms_distribution": {"1": 1, "1.5": 2, "2": 6, "2.5": 1, "missing": 1}, "missing_price_or_area_listings": [3, 5, 9], "average_price_per_location": {"downtown": 353750, "suburb": 463750, "midtown": 411250}, "correlations": {"Price_vs_Area_sqft": 0.89}}}
{"purpose": "Analyze daily electricity consumption patterns and identify data quality issues in residential energy usage.", "raw_table": "Date,Region,Consumption_kWh,Customer_Type,Peak_Hour_Consumption_kWh\n2024-01-01,NORTH,35.7,residential,12.5\n2024/01/02,south,42.3,Residential,14.1\n01-03-2024,East,,Residential,15.0\n2024-01-04,West,28.4,commercial,7.8\n2024-01-05,North,39.0,RESIDENTIAL,13.2\n2024-1-06,SOUTH,45.1,residential,16.4\n2024-01-07,East,41.2,Residential,\n2024-01-08,West,30.5,Commercial,8.1\n2024/01/09,North,38.7,Residential,14.0\n01-10-2024,south,,residential,15.5\n2024-01-11,East,43.8,Residential,16.2\n2024-01-12,west,29.9,commercial,7.9\n2024-01-13,North,37.2,Residential,13.7\n2024-01-14,SOUTH,44.0,Residential,15.8", "eda_steps": ["Parse the Date column into a consistent date format", "Convert Region and Customer_Type columns to consistent capitalization", "Check for missing values in all columns", "Compute descriptive statistics for Consumption_kWh and Peak_Hour_Consumption_kWh", "Generate value counts for Region and Customer_Type", "Identify rows with missing Consumption_kWh values", "Calculate correlation between Consumption_kWh and Peak_Hour_Consumption_kWh", "Summarize peak hour consumption distribution", "Highlight any inconsistent or unusual Region entries"], "eda_results": {"missing_values": {"Date": 0, "Region": 0, "Consumption_kWh": 2, "Customer_Type": 0, "Peak_Hour_Consumption_kWh": 1}, "value_counts": {"Region": {"North": 4, "South": 4, "East": 3, "West": 3}, "Customer_Type": {"Residential": 11, "Commercial": 3}}, "summary_stats": {"Consumption_kWh": {"count": 12, "mean": 38.7, "std": 5.0, "min": 28.4, "25%": 35.275, "50%": 38.95, "75%": 42.825, "max": 45.1}, "Peak_Hour_Consumption_kWh": {"count": 13, "mean": 13.77, "std": 2.68, "min": 7.8, "25%": 12.5, "50%": 14.0, "75%": 15.75, "max": 16.4}}, "correlations": {"Consumption_kWh_vs_Peak_Hour_Consumption_kWh": 0.96}, "inconsistent_entries": {"Region_variations": ["NORTH", "south", "SOUTH", "west", "West", "North"], "Customer_Type_variations": ["residential", "Residential", "RESIDENTIAL", "commercial", "Commercial"]}, "rows_with_missing_consumption": [3, 10]}}
{"purpose": "Analyze public transportation usage data to identify trends and data quality issues.", "raw_table": "Date,Route,Passengers,Revenue,Weather,DelayMinutes\n2024-01-05,Route A,120,300.50,sunny,5\n2024/01/06,route b,85,215.75,Cloudy,0\n01-07-2024,Route A,,310.00,rain,12\n2024-01-08,Route C,45,110.00,sunny,\n2024-01-09,Route b,90,220.00,fog,3\n2024-1-10,route C,50,115.25,Sunny,2\n2024-01-11,Route A,nan,305.00,Rain,7\n2024-01-12,Route B,95,225.50,cloudy,4\n2024-01-13,Route C,55,125.75,rain,8\n2024-01-14,Route A,130,320.00,Sunny,NaN\n2024-01-15,Route D,40,100.00,cloudy,1\n2024-01-16,Route D,38,98.50,Sunny,0", "eda_steps": ["Check for missing values in all columns", "Standardize the Route column capitalization", "Parse and standardize the Date column to a consistent format", "Compute descriptive statistics for numeric columns Passengers, Revenue, and DelayMinutes", "Generate value counts for the Weather and Route columns", "Identify percentage of missing values per column", "Calculate average DelayMinutes by Route", "Summarize total Revenue by Date", "Visualize distribution skewness for Passengers and DelayMinutes"], "eda_results": {"missing_values": {"Date": 0, "Route": 0, "Passengers": 2, "Revenue": 0, "Weather": 0, "DelayMinutes": 2}, "value_counts": {"Route": {"Route A": 4, "Route B": 3, "Route C": 3, "Route D": 2}, "Weather": {"sunny": 3, "Cloudy": 2, "rain": 3, "fog": 1, "cloudy": 2, "Sunny": 2}}, "summary_stats": {"Passengers": {"count": 10, "mean": 74.8, "std": 31.4, "min": 38, "25%": 45, "50%": 52.5, "75%": 90, "max": 130}, "Revenue": {"count": 12, "mean": 205.5, "std": 89.4, "min": 98.5, "25%": 110, "50%": 217.75, "75%": 310, "max": 320}, "DelayMinutes": {"count": 10, "mean": 4.0, "std": 4.1, "min": 0, "25%": 0.75, "50%": 4, "75%": 7.5, "max": 12}}, "average_delay_by_route": {"Route A": 8.0, "Route B": 2.33, "Route C": 6.0, "Route D": 0.5}, "total_revenue_by_date": {"2024-01-05": 300.5, "2024-01-06": 215.75, "2024-01-07": 310.0, "2024-01-08": 110.0, "2024-01-09": 220.0, "2024-01-10": 115.25, "2024-01-11": 305.0, "2024-01-12": 225.5, "2024-01-13": 125.75, "2024-01-14": 320.0, "2024-01-15": 100.0, "2024-01-16": 98.5}, "missing_percentage": {"Passengers": 16.7, "DelayMinutes": 16.7}, "distribution_skewness": {"Passengers": 0.9, "DelayMinutes": 1.2}}}
{"purpose": "Analyze public library usage patterns and identify key demographics and visit trends.", "raw_table": "Library_ID,User_Age,Visit_Date,Membership_Type,Books_Borrowed,Favorite_Section\n101,25,2023/01/15,standard,3,Fiction\n102,Thirty,15-02-2023,Premium,5,History\n103,42,2023-03-01,STANDARD,2,Science\n104,,2023/03/15,Premium,7,fiction\n105,29,03/25/2023,standard,,Biography\n106,34,2023-04-10,premium,4,Science\n107,22,2023-04-12,Standard,1,Children's\n108,27,04/20/2023,standard,2,History\n109,31,2023/04/22,PREMIUM,6,Fiction\n110,NaN,2023-04-25,standard,3,History\n111,38,2023-04-30,standard,NaN,Science\n112,45,2023-05-01,premium,5,History", "eda_steps": ["Check and summarize missing values in each column", "Standardize the capitalization and formatting of categorical columns", "Convert Visit_Date to a consistent date format and extract month information", "Compute descriptive statistics for numeric columns User_Age and Books_Borrowed", "Generate value counts for Membership_Type and Favorite_Section", "Identify top 3 Favorite_Section categories by frequency", "Analyze correlation between User_Age and Books_Borrowed", "Summarize visit counts by month"], "eda_results": {"missing_values": {"Library_ID": 0, "User_Age": 2, "Visit_Date": 0, "Membership_Type": 0, "Books_Borrowed": 2, "Favorite_Section": 0}, "standardized_categories": {"Membership_Type": {"standard": 7, "premium": 5}, "Favorite_Section": {"fiction": 3, "history": 4, "science": 3, "biography": 1, "children's": 1}}, "visit_month_counts": {"January": 1, "February": 1, "March": 3, "April": 5, "May": 1}, "summary_stats": {"User_Age": {"count": 10, "mean": 32.6, "std_dev": 6.9, "min": 22, "max": 45}, "Books_Borrowed": {"count": 11, "mean": 3.91, "std_dev": 2.02, "min": 1, "max": 7}}, "top_favorite_sections": ["history", "fiction", "science"], "correlation_UserAge_BooksBorrowed": 0.65}}
{"purpose": "Analyze machine downtime and defect rates in manufacturing shifts to identify key issues affecting production efficiency.", "raw_table": "MachineID,Shift,Date,OperatingHours,DowntimeMinutes,DefectRate,Operator\nM01,morning,2023-04-01,8,30,0.05,john smith\nM02,Morning,04/01/2023,7.5,45,0.07,Alice\nM01,Afternoon,2023/04/01,8,,0.04,BOB\nM03,NIGHT,01-04-2023,7,60,missing,Eve\nM02,afternoon,2023-04-02,8,20,0.03,Alice\nM01,morning,2023-04-02,,50,0.06,John Smith\nM03,Night,2023-04-02,8,55,0.1,Eve\nM02,MORNING,2023-04-03,8,40,0.08,alice\nM01,Afternoon,2023-04-03,8,35,0.05,BOB\nM03,Night,2023-04-03,7.5,70,0.09,EVE\n", "eda_steps": ["Check and summarize missing values for each column", "Standardize Shift and Operator column values to consistent capitalization", "Convert Date column to a uniform date format", "Compute descriptive statistics for numeric columns (OperatingHours, DowntimeMinutes, DefectRate)", "Generate value counts for Shift and MachineID columns", "Identify average DefectRate and DowntimeMinutes per MachineID", "Find correlation between OperatingHours, DowntimeMinutes, and DefectRate", "Visualize distribution skewness for DefectRate"], "eda_results": {"missing_values": {"MachineID": 0, "Shift": 0, "Date": 0, "OperatingHours": 1, "DowntimeMinutes": 1, "DefectRate": 1, "Operator": 0}, "standardized_values": {"Shift_unique_values": ["Morning", "Afternoon", "Night"], "Operator_unique_values": ["John Smith", "Alice", "Bob", "Eve"]}, "date_format": "All dates converted to YYYY-MM-DD", "summary_stats": {"OperatingHours": {"count": 14, "mean": 7.71, "std": 0.31, "min": 7, "25%": 7.5, "50%": 8, "75%": 8, "max": 8}, "DowntimeMinutes": {"count": 14, "mean": 45.71, "std": 14.82, "min": 20, "25%": 35, "50%": 45, "75%": 55, "max": 70}, "DefectRate": {"count": 14, "mean": 0.064, "std": 0.018, "min": 0.03, "25%": 0.05, "50%": 0.06, "75%": 0.08, "max": 0.1}}, "value_counts": {"Shift": {"Morning": 4, "Afternoon": 3, "Night": 4}, "MachineID": {"M01": 4, "M02": 4, "M03": 4}}, "average_metrics_per_machine": {"M01": {"DefectRate": 0.05, "DowntimeMinutes": 42.5}, "M02": {"DefectRate": 0.06, "DowntimeMinutes": 35}, "M03": {"DefectRate": 0.096, "DowntimeMinutes": 61.67}}, "correlations": {"OperatingHours_vs_DowntimeMinutes": -0.58, "OperatingHours_vs_DefectRate": -0.45, "DowntimeMinutes_vs_DefectRate": 0.82}, "skewness": {"DefectRate": 0.84}}}
{"purpose": "Analyze user engagement patterns across different social media platforms and post types.", "raw_table": "user_id,post_id,platform,post_type,likes,comments,shares,post_date\nU001,P1001,Facebook,Photo,150,20,5,2023-01-15\nU002,P1002,twitter,Text,,15,3,15-January-2023\nU003,P1003,Instagram,video,200,,10,2023/01/16\nu004,P1004,facebook,Link,50,5,,16-01-2023\nU005,P1005,Instagram,Photo,300,30,25,01-17-2023\nU006,P1006,LinkedIn,text,75,8,2,2023-01-18\nU007,P1007,Twitter,Video,180,25,12,2023-01-19\nU008,P1008,FACEBOOK,Photo,NaN,10,4,2023-01-20\nU009,P1009,Instagram,Poll,90,,1,2023/01/21\nU010,P1010,linkedin,photo,60,7,0,21-01-2023", "eda_steps": ["Standardize platform and post_type column capitalization", "Convert post_date to a consistent date format", "Check for missing values in likes, comments, and shares", "Calculate basic statistics (mean, median) for numeric engagement metrics", "Generate value counts for platform and post_type", "Identify the post with the highest likes", "Analyze correlation between likes, comments, and shares", "Summarize number of posts per user", "Detect any unusual or unexpected post_type categories"], "eda_results": {"missing_values": {"likes": 2, "comments": 2, "shares": 1}, "value_counts": {"platform": {"Facebook": 3, "Twitter": 2, "Instagram": 3, "LinkedIn": 2}, "post_type": {"Photo": 4, "Text": 2, "Video": 2, "Link": 1, "Poll": 1}}, "summary_stats": {"likes": {"mean": 130.56, "median": 90, "min": 50, "max": 300}, "comments": {"mean": 14.44, "median": 10, "min": 5, "max": 30}, "shares": {"mean": 8.67, "median": 5, "min": 0, "max": 25}}, "top_post_by_likes": {"post_id": "P1005", "likes": 300, "platform": "Instagram", "post_type": "Photo"}, "correlations": {"likes_comments": 0.89, "likes_shares": 0.85, "comments_shares": 0.82}, "posts_per_user": {"U001": 1, "U002": 1, "U003": 1, "U004": 1, "U005": 1, "U006": 1, "U007": 1, "U008": 1, "U009": 1, "U010": 1}, "unusual_post_types": ["Poll"]}}
{"purpose": "Analyze movie box office performance and audience ratings to identify trends and data quality issues.", "raw_table": "Movie_Title,Release_Date,Genre,Box_Office_Million,Audience_Rating,Director\nInception,2010-07-16,Sci-Fi,829.89,8.8,Christopher Nolan\navatar,12/18/2009,Sci-fi, 2.79,7.8,James Cameron\nThe Godfather,1972-03-24,Crime,246.12,9.2,Francis Ford Coppola\nparasite,2019/05/30,Thriller,258.8,,Bong Joon Ho\nJoker,10-04-2019,crime,1074.1,8.5,Todd Phillips\nTitanic,1997-12-19,Romance,2187,7.8,James cameron\nAvengers: Endgame,2019-04-26,Action,2797.8,8.4,Anthony Russo\nFrozen II,2019-11-22,Animation,1450.0,7.0,Chris Buck\ninterstellar,2014-11-07,Sci-Fi,677.5,8.6,Christopher Nolan\nThe Dark Knight,2008-07-18,Action,1004.9,9.0,christopher nolan\nSoul,2020-12-25,animation,121.5,8.1,Pete Docter\n\n", "eda_steps": ["Check for missing values in each column", "Standardize Genre and Director capitalization", "Convert Release_Date to a consistent date format", "Compute descriptive statistics for Box_Office_Million and Audience_Rating", "Generate value counts for Genre", "Identify top 3 directors by number of movies", "Calculate correlation between Box_Office_Million and Audience_Rating", "Highlight rows with missing Audience_Rating", "Summarize box office distribution skewness"], "eda_results": {"missing_values": {"Movie_Title": 0, "Release_Date": 0, "Genre": 0, "Box_Office_Million": 0, "Audience_Rating": 1, "Director": 0}, "standardized_genres": {"sci-fi": 3, "crime": 2, "thriller": 1, "romance": 1, "action": 2, "animation": 2}, "standardized_directors_top": {"Christopher Nolan": 3, "James Cameron": 2, "Others": 6}, "release_dates_consistent_format": true, "descriptive_stats": {"Box_Office_Million": {"count": 12, "mean": 1023.59, "std": 877.21, "min": 2.79, "25%": 228.48, "50%": 741.5, "75%": 1450.0, "max": 2797.8}, "Audience_Rating": {"count": 11, "mean": 8.24, "std": 0.66, "min": 7.0, "25%": 7.8, "50%": 8.4, "75%": 8.8, "max": 9.2}}, "correlation_BoxOffice_AudienceRating": 0.56, "rows_with_missing_Audience_Rating": [{"Movie_Title": "parasite", "Audience_Rating": null}], "box_office_skewness": 1.34}}
{"purpose": "Analyze rental property listings to understand pricing distribution and common amenities.", "raw_table": "ListingID,Neighborhood,PriceUSD,Bedrooms,Bathrooms,SquareFeet,AvailabilityDate,Amenities\n1,Downtown,2500,2,1,850,2024-05-01,Pool;Gym\n2,Uptown,1800,1,1,600,05/10/2024,Gym\n3,Midtown,,3,2,1200,2024/06/15,Pool;Parking\n4,downtown,2200,2,1,900,2024-05-20,parking;gym\n5,Suburb,1500,1,1,,2024-07-01,Garden\n6,UPTOWN,1900,2,1,700,2024-05-15,Pool\n7,MidTown,2100,2,,850,2024-06-01,Gym;Parking\n8,suburb,1600,1,1,650,7/5/2024,Garden;Parking\n9,Downtown,2400,2,2,900,2024-05-10,Pool;GYM\n10,Midtown,2000,2,1,800,,Pool\n11,Uptown,1750,,1,620,2024-05-25,gYm\n12,Suburb,1550,1,1,700,2024-07-10,Garden\n13,Downtown,2300,2,1,880,2024/05/15,Pool;parking\n14,Midtown,2050,2,2,850,2024-06-05,Gym\n15,uptown,1950,2,1,720,2024-05-18,Pool;Gym", "eda_steps": ["Check for missing values in all columns", "Standardize the capitalization of the 'Neighborhood' and 'Amenities' columns", "Parse and standardize the 'AvailabilityDate' to YYYY-MM-DD format", "Compute descriptive statistics for numeric columns: PriceUSD, Bedrooms, Bathrooms, SquareFeet", "Generate value counts for the 'Neighborhood' column", "Identify the most common amenities across all listings", "Calculate correlation matrix for numeric features", "Summarize the distribution skewness for PriceUSD", "Identify listings with missing PriceUSD or Bedrooms"], "eda_results": {"missing_values": {"ListingID": 0, "Neighborhood": 0, "PriceUSD": 1, "Bedrooms": 2, "Bathrooms": 1, "SquareFeet": 1, "AvailabilityDate": 1, "Amenities": 0}, "standardized_neighborhood_counts": {"Downtown": 5, "Uptown": 4, "Midtown": 5, "Suburb": 3}, "amenities_frequency": {"Pool": 8, "Gym": 9, "Parking": 5, "Garden": 3}, "priceUSD_descriptive_stats": {"count": 14, "mean": 1989.29, "std": 278.56, "min": 1500, "25%": 1800, "50%": 1950, "75%": 2300, "max": 2500, "skewness": 0.21}, "numeric_correlation_matrix": {"PriceUSD_Bedrooms": 0.85, "PriceUSD_Bathrooms": 0.78, "PriceUSD_SquareFeet": 0.9, "Bedrooms_Bathrooms": 0.75, "Bedrooms_SquareFeet": 0.88, "Bathrooms_SquareFeet": 0.8}, "listings_missing_price_or_bedrooms": [3, 11]}}
{"purpose": "Analyze monthly temperature and precipitation patterns across different climate zones to identify data quality issues and basic trends.", "raw_table": "Date,Zone,Avg_Temperature_C,Precipitation_mm\n2023-01-15,Tropical,29.4,120\n2023/02/15,temperate,15.2,85\n15-Mar-2023,Arid, 22.1 ,\n2023-04-15,tropical,31.0,130\n2023-05-15,Temperate,16.8,NA\n2023-06-15,Arid,24.3,15\n2023-07-15,tropical,,140\n2023-08-15,Temperate,18.0,95\n2023-09-15,arid,23.7,10\n2023-10-15,TROPICAL,30.5,135\n2023-11-15,Temperate,14.1,90\n2023-12-15,Arid,21.9,\n", "eda_steps": ["Standardize date formats to YYYY-MM-DD", "Normalize capitalization in 'Zone' column", "Check for missing values in all columns", "Compute descriptive statistics for Avg_Temperature_C and Precipitation_mm", "Generate value counts for the 'Zone' categorical column", "Identify rows with missing temperature or precipitation data", "Calculate correlation between Avg_Temperature_C and Precipitation_mm", "Summarize average temperature and precipitation by Zone"], "eda_results": {"missing_values": {"Date": 0, "Zone": 0, "Avg_Temperature_C": 1, "Precipitation_mm": 3}, "value_counts": {"Zone": {"Tropical": 4, "Temperate": 4, "Arid": 4}}, "summary_stats": {"Avg_Temperature_C": {"count": 11, "mean": 22.4, "std_dev": 5.6, "min": 14.1, "max": 31.0}, "Precipitation_mm": {"count": 9, "mean": 87.8, "std_dev": 50.2, "min": 10, "max": 140}}, "rows_with_missing_values": [{"Date": "15-Mar-2023", "Zone": "Arid", "Avg_Temperature_C": "22.1", "Precipitation_mm": null}, {"Date": "2023-05-15", "Zone": "Temperate", "Avg_Temperature_C": "16.8", "Precipitation_mm": "NA"}, {"Date": "2023-07-15", "Zone": "Tropical", "Avg_Temperature_C": null, "Precipitation_mm": "140"}, {"Date": "2023-12-15", "Zone": "Arid", "Avg_Temperature_C": "21.9", "Precipitation_mm": null}], "correlations": {"Avg_Temperature_C vs Precipitation_mm": -0.43}, "average_by_zone": {"Tropical": {"Avg_Temperature_C": 30.3, "Precipitation_mm": 131.3}, "Temperate": {"Avg_Temperature_C": 15.0, "Precipitation_mm": 90.0}, "Arid": {"Avg_Temperature_C": 22.7, "Precipitation_mm": 12.5}}}}
{"purpose": "Examine patient vital signs and demographics to identify data quality issues and summarize key statistics.", "raw_table": "Patient_ID,Age,Gender,Admission_Date,BP_Systolic,BP_Diastolic,Heart_Rate,Diagnosis\nP001,34,Male,2023-01-15,120,80,72,Hypertension\nP002,58,female,15-Feb-2023,135,,85,diabetes\nP003,,Male,2023/03/01,110,70,NaN,Asthma\nP004,45,FEMALE,2023-03-15,140,90,88,\nP005,29,M,2023-04-10,130,85,79,Hypertension\nP006,NaN,F,2023-04-12,125,,80,Diabetes\nP007,62,Male,04/20/2023,NaN,95,90,Hypertension\nP008,48,Female,2023-05-01,138,88,87,asthma\nP009,50,Male,2023-05-03,142,92,89,Hypertension\nP010,53,Female,,128,82,85,Diabetes", "eda_steps": ["Check for missing values in each column", "Standardize the Gender column values", "Parse and unify Admission_Date formats", "Compute descriptive statistics for Age, BP_Systolic, BP_Diastolic, and Heart_Rate", "Generate value counts for Diagnosis", "Identify rows with inconsistent or missing vital sign measurements", "Calculate correlation matrix for numeric vital signs", "Summarize distribution skewness for Age and Heart_Rate"], "eda_results": {"missing_values": {"Patient_ID": 0, "Age": 2, "Gender": 0, "Admission_Date": 1, "BP_Systolic": 2, "BP_Diastolic": 3, "Heart_Rate": 1, "Diagnosis": 1}, "standardized_gender_counts": {"Male": 5, "Female": 5}, "admission_date_formats": {"YYYY-MM-DD": 6, "DD-MMM-YYYY": 1, "YYYY/MM/DD": 1, "MM/DD/YYYY": 1, "Missing": 1}, "summary_stats": {"Age": {"count": 8, "mean": 47.375, "std": 11.84, "min": 29, "25%": 38.25, "50%": 49, "75%": 54.75, "max": 62}, "BP_Systolic": {"count": 8, "mean": 131.25, "std": 10.81, "min": 110, "25%": 125, "50%": 130, "75%": 138, "max": 142}, "BP_Diastolic": {"count": 5, "mean": 85, "std": 8.94, "min": 70, "25%": 82, "50%": 85, "75%": 90, "max": 95}, "Heart_Rate": {"count": 9, "mean": 84.44, "std": 6.47, "min": 72, "25%": 80, "50%": 85, "75%": 88, "max": 90}}, "diagnosis_value_counts": {"Hypertension": 4, "Diabetes": 3, "Asthma": 2, "Missing": 1}, "rows_with_missing_vital_signs": ["P002", "P003", "P006", "P007", "P004"], "correlations": {"BP_Systolic_vs_BP_Diastolic": 0.95, "BP_Systolic_vs_Heart_Rate": 0.45, "BP_Diastolic_vs_Heart_Rate": 0.39, "Age_vs_Heart_Rate": -0.12, "Age_vs_BP_Systolic": 0.56}, "distribution_skewness": {"Age": 0.01, "Heart_Rate": 0.35}}}
{"purpose": "Analyze urban public transportation trip patterns and identify data quality issues.", "raw_table": "Trip_ID,Start_Time,End_Time,Route,Passenger_Count,Fare,Payment_Type\n1,2023/01/15 08:05,2023-01-15 08:45,Route A,12,2.50,Cash\n2,15-01-2023 09:00 AM,2023-01-15 09:40,route b,8,2.50,CARD\n3,2023-01-15 09:15,2023/01/15 09:55,Route A,,3.00,Cash\n4,2023-01-15 10:00,2023-01-15 10:40,Route C,15,3.00,card\n5,2023/01/15 10:30,2023-01-15 11:10,ROUTE B,7,,Cash\n6,2023-01-15 11:00,2023-01-15 11:45,Route A,10,2.50,cash\n7,2023-01-15 11:20,2023/01/15 12:00,route c,13,3.00,CARD\n8,2023-01-15 12:00,,Route A,11,2.50,CASH\n9,2023-01-15 12:15,2023-01-15 12:55,Route B,9,2.50,Cash\n10,2023-01-15 12:30,2023/01/15 13:10,route C,14,3.00,card\n", "eda_steps": ["Check and summarize missing values for each column", "Standardize 'Route' and 'Payment_Type' columns to consistent capitalization", "Parse and unify date formats in 'Start_Time' and 'End_Time' columns", "Calculate trip duration in minutes by subtracting Start_Time from End_Time", "Compute descriptive statistics for numeric columns: Passenger_Count, Fare, Trip Duration", "Generate value counts for 'Route' and 'Payment_Type' columns", "Identify rows with missing or invalid 'Passenger_Count' or 'Fare' values", "Analyze distribution skewness for numeric columns", "Check correlation between Passenger_Count and Fare", "Identify top-2 most frequent routes by trip count"], "eda_results": {"missing_values": {"Trip_ID": 0, "Start_Time": 0, "End_Time": 1, "Route": 0, "Passenger_Count": 1, "Fare": 1, "Payment_Type": 0}, "value_counts": {"Route": {"route a": 4, "route b": 3, "route c": 3}, "Payment_Type": {"cash": 4, "card": 4}}, "standardization_notes": {"Route": "All route names converted to lowercase and trimmed", "Payment_Type": "All payment types converted to lowercase"}, "trip_duration_minutes": {"min": 40, "max": 45, "mean": 42.8, "median": 42.5}, "summary_stats": {"Passenger_Count": {"count": 9, "mean": 11.0, "std": 2.66, "min": 7, "max": 15, "missing": 1}, "Fare": {"count": 9, "mean": 2.78, "std": 0.22, "min": 2.5, "max": 3.0, "missing": 1}}, "skewness": {"Passenger_Count": 0.32, "Fare": -1.2, "Trip_Duration": 0.1}, "correlations": {"Passenger_Count_Fare": 0.65}, "top_routes": {"route a": 4, "route b": 3}, "data_quality_issues": ["One missing End_Time at Trip_ID 8", "One missing Passenger_Count at Trip_ID 3", "One missing Fare at Trip_ID 5", "Inconsistent date formats in Start_Time and End_Time", "Inconsistent capitalization in Route and Payment_Type columns"]}}
{"purpose": "Analyze monthly transaction patterns and customer segmentation in retail banking data.", "raw_table": "TransactionID,CustomerID,TransactionDate,TransactionType,Amount,Currency,Branch\nT001,C101,2023-01-12,deposit,5000,usd,New york\nT002,C102,01/15/2023,Withdrawal, -2000,USD,new york\nT003,C103,2023/01/18,TRANSFER,1500,eur,Boston\nT004,C101,2023-02-03,Deposit, ,USD,Boston\nT005,C104,2023-02-15,withdrawal,-500,Usd,Chicago\nT006,C105,,deposit,2500,usd,Chicago\nT007,C106,2023-03-01,transfer,1000,EUR,los angeles\nT008,C103,03-05-2023,Deposit,3000,usd,Los Angeles\nT009,C107,2023-03-10,Withdrawal,,usd,Chicago\nT010,C108,2023-03-12,Transfer,-1500,eur,Chicago\nT011,C109,2023/03/15,Deposit,4500,USD,New york\nT012,C110,2023-03-20,Withdrawal,-700,USD,New York", "eda_steps": ["Check for missing values in all columns", "Standardize date formats in TransactionDate column", "Convert all Currency values to uppercase", "Calculate descriptive statistics for Amount column", "Generate value counts for TransactionType and Branch columns", "Identify number of unique customers", "Calculate total transaction amount by TransactionType", "Check for negative amounts and their associated transaction types"], "eda_results": {"missing_values": {"TransactionID": 0, "CustomerID": 0, "TransactionDate": 1, "TransactionType": 0, "Amount": 2, "Currency": 0, "Branch": 0}, "date_format_standardized": true, "currency_standardized": ["USD", "USD", "EUR", "USD", "USD", "USD", "EUR", "USD", "USD", "EUR", "USD", "USD"], "summary_stats": {"Amount": {"count": 10, "mean": 1600, "std": 2559.7, "min": -2000, "25%": -575, "50%": 1250, "75%": 3500, "max": 5000}}, "value_counts": {"TransactionType": {"deposit": 4, "withdrawal": 4, "transfer": 4}, "Branch": {"New york": 4, "Boston": 2, "Chicago": 4, "Los Angeles": 2}}, "unique_customers": 10, "total_amount_by_type": {"deposit": 10500, "withdrawal": -3200, "transfer": 1000}, "negative_amounts": {"count": 5, "transaction_types": ["withdrawal", "withdrawal", "withdrawal", "transfer", "withdrawal"]}}}
{"purpose": "Analyze crop yield performance and planting patterns across different farms.", "raw_table": "FarmID,Crop,PlantingDate,Yield_kg,Soil_Type,Irrigation,Harvested\nF001,Wheat,2023-03-15,1500,Loam,Drip,Yes\nf002,Corn,03/20/2023,2000,Sandy,None,Yes\nF003,Rice,2023/03/25,1800,Clay,SPRINKLER,No\nF004,Wheat,15-03-2023,NaN,Loam,Drip,Yes\nF005,Corn,2023-03-18,1950,Sandy,none,Yes\nF006,Barley,,1700,Loam,Drip,No\nF007,wheat,2023-3-17,1600,LoAM,Drip,Yes\nF008,Rice,03-22-2023,1850,clay,SPRINKLER,Yes\nF009,Corn,2023-03-19,NaN,Sandy,Drip,Yes\nF010,Barley,2023-03-21,1650,loam,drip,No", "eda_steps": ["Check for missing values in all columns", "Standardize capitalization in Crop, Soil_Type, and Irrigation columns", "Parse and unify PlantingDate format to YYYY-MM-DD", "Compute descriptive statistics for Yield_kg", "Generate value counts for Crop and Soil_Type columns", "Identify percentage of farms that harvested their crops", "Compute correlation between Yield_kg and Irrigation type (encoded)", "Summarize number of unique farms per crop type"], "eda_results": {"missing_values": {"PlantingDate": 1, "Yield_kg": 2}, "standardization_examples": {"Crop_unique_after": ["Wheat", "Corn", "Rice", "Barley"], "Soil_Type_unique_after": ["Loam", "Sandy", "Clay"], "Irrigation_unique_after": ["Drip", "None", "Sprinkler"]}, "planting_dates_parsed": ["2023-03-15", "2023-03-20", "2023-03-25", "2023-03-15", "2023-03-18", null, "2023-03-17", "2023-03-22", "2023-03-19", "2023-03-21"], "summary_stats_yield_kg": {"count": 8, "mean": 1743.75, "std_dev": 171.33, "min": 1500, "25%": 1650, "50%": 1750, "75%": 1900, "max": 2000}, "value_counts": {"Crop": {"Wheat": 3, "Corn": 3, "Rice": 2, "Barley": 2}, "Soil_Type": {"Loam": 4, "Sandy": 3, "Clay": 2}}, "harvested_percentage": 80, "correlations": {"Yield_kg_and_Irrigation": 0.45}, "unique_farms_per_crop": {"Wheat": 3, "Corn": 3, "Rice": 2, "Barley": 2}}}
{"purpose": "Assess crop yield patterns and field conditions to improve agricultural productivity.", "raw_table": "FieldID,Crop,PlantingDate,HarvestDate,SoilType,Yield(tons),Moisture%,PesticideUsed\nF01,Wheat,2023-03-12,2023/08/25,Loam,3.5,12.5,Yes\nF02,corn,03/15/2023,2023-09-01,Clay,,15.0,no\nF03,Soybean,2023-03-10,2023-08-28,SAND,2.8,NaN,Yes\nf04,Wheat,2023-03-14,2023-08-27,loam,3.7,13.0,YES\nF05,Barley,2023/03/16,2023-08-30,Peat,3.0,14,yes\nF06,Corn,2023-03-13,08/29/2023,clay,3.2,15.5,No\nF07,Soybean,2023-3-15,2023-08-31,Sand,2.9,13.2,YES\nF08,Wheat,,2023-08-26,LoAm,3.6,12.8,Yes\nF09,barley,2023-03-17,2023-08-29,peat,NaN,14.1,no\nF10,Corn,2023-03-12,2023/08/28,Clay,3.3,15.3,No", "eda_steps": ["Check for missing values in each column", "Standardize capitalization in categorical columns 'Crop', 'SoilType', and 'PesticideUsed'", "Parse and unify date formats in 'PlantingDate' and 'HarvestDate'", "Compute descriptive statistics for numeric columns 'Yield(tons)' and 'Moisture%'", "Generate value counts for the 'Crop' and 'SoilType' columns", "Calculate the average yield per crop type", "Identify records with missing 'Yield(tons)' values", "Compute correlation between 'Yield(tons)' and 'Moisture%'"], "eda_results": {"missing_values": {"FieldID": 0, "Crop": 0, "PlantingDate": 1, "HarvestDate": 0, "SoilType": 0, "Yield(tons)": 2, "Moisture%": 1, "PesticideUsed": 0}, "standardized_categories": {"Crop": ["Wheat", "Corn", "Soybean", "Barley"], "SoilType": ["Loam", "Clay", "Sand", "Peat"], "PesticideUsed": ["Yes", "No"]}, "date_format_summary": {"PlantingDate": "Unified to YYYY-MM-DD with 1 missing value", "HarvestDate": "Unified to YYYY-MM-DD"}, "summary_stats": {"Yield(tons)": {"count": 8, "mean": 3.25, "std_dev": 0.32, "min": 2.8, "max": 3.7}, "Moisture%": {"count": 9, "mean": 13.88, "std_dev": 1.08, "min": 12.5, "max": 15.5}}, "value_counts": {"Crop": {"Wheat": 3, "Corn": 3, "Soybean": 2, "Barley": 2}, "SoilType": {"Loam": 3, "Clay": 3, "Sand": 2, "Peat": 2}}, "average_yield_per_crop": {"Wheat": 3.6, "Corn": 3.25, "Soybean": 2.85, "Barley": 3.0}, "records_missing_yield": ["F02", "F09"], "correlations": {"Yield(tons)_vs_Moisture%": 0.67}}}
{"purpose": "Examine housing property listings to understand price distribution, identify missing data patterns, and explore relationships between features.", "raw_table": "PropertyID,Location,Price,Size_sqft,Bedrooms,Bathrooms,Year_Built,List_Date\n101,Downtown,550000,850,2,1,1998,2023/01/15\n102,suburbs,450000,950,,2,2005,15-02-2023\n103,Uptown,NaN,1200,3,2,2010,2023.03.01\n104,Downtown,600000,900,2,,1995,2023-01-20\n105,Suburbs,480000,1100,3,2,NaN,2023/02/28\n106,midtown,520000,NaN,2,1,2000,03/10/2023\n107,Uptown,620000,1300,3,2,2015,2023-03-05\n108,Downtown,580000,880,2,1,1999,\n109,Midtown,530000,970,2,1,2003,2023/01/25\n110,suburbs,470000,1000,3,2,2008,2023-02-15", "eda_steps": ["Check missing value percentages in each column", "Standardize capitalization in the Location column", "Convert List_Date values to a consistent date format", "Compute descriptive statistics for Price and Size_sqft columns", "Generate value counts for the Bedrooms and Bathrooms columns", "Identify the oldest and newest Year_Built values", "Calculate the correlation between Price and Size_sqft", "Summarize unique locations and their counts"], "eda_results": {"missing_values": {"PropertyID": "0%", "Location": "0%", "Price": "9.1%", "Size_sqft": "9.1%", "Bedrooms": "9.1%", "Bathrooms": "9.1%", "Year_Built": "9.1%", "List_Date": "9.1%"}, "standardized_locations": {"downtown": 3, "suburbs": 3, "uptown": 2, "midtown": 2}, "date_conversion_sample": {"Original": ["2023/01/15", "15-02-2023", "2023.03.01", "03/10/2023"], "Converted": ["2023-01-15", "2023-02-15", "2023-03-01", "2023-03-10"]}, "summary_stats": {"Price": {"count": 10, "mean": 537000, "std": 55901, "min": 450000, "25%": 475000, "50%": 525000, "75%": 580000, "max": 620000}, "Size_sqft": {"count": 11, "mean": 1023, "std": 151, "min": 850, "25%": 900, "50%": 970, "75%": 1100, "max": 1300}}, "value_counts": {"Bedrooms": {"2": 6, "3": 4}, "Bathrooms": {"1": 5, "2": 4, "missing": 1}}, "Year_Built_extremes": {"oldest": 1995, "newest": 2015}, "correlations": {"Price_Size_sqft": 0.85}, "unique_locations": {"downtown": 3, "suburbs": 3, "uptown": 2, "midtown": 2}}}
{"purpose": "Analyze student performance and attendance patterns in a high school semester.", "raw_table": "StudentID,Name,Grade,Subject,Score,Attendance,ExamDate\n101,alice,10,Math,88,Present,2023-03-15\n102,Bob,11,english,92,Absent,15/03/2023\n103,CHARLIE,10,Science,85,,2023/03/16\n104,Diana,11,Math,NaN,Present,2023-03-15\n105,edward,10,English,78,Present,03-17-2023\n106,Fiona,12,Science,90,Present,2023-3-18\n107,George,11,Math,75,Absent,March 19, 2023\n108,harry,10,science,88,Present,2023-03-20\n109,Ivy,12,English,NaN,Present,03/21/2023\n110,jack,11,Math,82,Present,2023/03/22\n111,Kate,12,SCIENCE,95,Absent,2023-3-23\n112,Luke,,Math,80,Present,2023-03-24\n113,Mary,10,English,85,Absent,2023-03-25\n114,Nick,11,Science,77,Present,2023-03-26", "eda_steps": ["Check the data types of each column and convert dates to a uniform format", "Identify and count missing values in each column", "Standardize capitalization in categorical columns like Name, Grade, and Subject", "Compute summary statistics (mean, median, std) for the Score column", "Generate value counts for the Attendance column", "Analyze the distribution of scores by Subject", "Identify students with missing or invalid scores", "Calculate the number of unique students per Grade", "Check correlation between Score and Attendance status"], "eda_results": {"data_types": {"StudentID": "int", "Name": "string", "Grade": "string (with missing and inconsistent capitalization)", "Subject": "string (mixed capitalization)", "Score": "float (contains NaN and missing values)", "Attendance": "string (Present/Absent with no missing values except one)", "ExamDate": "date (inconsistent formats normalized)"}, "missing_values": {"Grade": 1, "Score": 2, "Attendance": 1}, "standardized_categories": {"Name": ["Alice", "Bob", "Charlie", "Diana", "Edward", "Fiona", "George", "Harry", "Ivy", "Jack", "Kate", "Luke", "Mary", "Nick"], "Grade": ["10", "11", "12", "missing"], "Subject": ["Math", "English", "Science"]}, "score_summary_stats": {"mean": 84.36, "median": 85.0, "std_dev": 6.12, "count": 12}, "attendance_value_counts": {"Present": 9, "Absent": 4, "missing": 1}, "score_distribution_by_subject": {"Math": {"count": 5, "mean_score": 80.0}, "English": {"count": 4, "mean_score": 85.0}, "Science": {"count": 5, "mean_score": 87.0}}, "students_missing_scores": ["104", "109"], "unique_students_per_grade": {"10": 5, "11": 5, "12": 4, "missing": 1}, "correlation_score_attendance": {"Present_mean_score": 85.7, "Absent_mean_score": 81.0, "correlation": 0.23}}}
{"purpose": "Evaluate production line performance and identify quality issues in manufactured parts.", "raw_table": "PartID,ProductionDate,Line,Operator,Defects,ProductionTime(min),Shift\n1001,2024-01-05,Line A,john doe,2,45,Day\n1002,01/06/2024,line a,Jane Smith,0,50,day\n1003,2024-1-07,Line B,Mary O'Neill,,55,Night\n1004,2024/01/08,LINE B,michael brown,1,48,Night\n1005,2024-01-09,Line C,Anna lee,3,60,\n1006,Jan 10 2024,line c,,0,62,Night\n1007,2024-01-11,Line A,John Doe,1,49,Day\n1008,2024-01-12,line a,Mary O'neill,NaN,47,Day\n1009,2024-01-13,Line B,Michael Brown,2,53,Night\n1010,2024/01/14,Line C,Anna Lee,1,58,Day\n1011,2024-01-15,Line A,Jane smith,0,46,Day\n1012,,Line B,Mary O'Neill,1,54,Night", "eda_steps": ["Standardize capitalization in 'Line' and 'Operator' columns", "Parse and unify 'ProductionDate' format", "Identify and count missing values in all columns", "Compute descriptive statistics for 'Defects' and 'ProductionTime(min)'", "Generate value counts for 'Line' and 'Shift' columns", "Calculate average defects per production line", "Find correlation between defects and production time", "Identify operators with the highest defect counts"], "eda_results": {"missing_values": {"PartID": 0, "ProductionDate": 1, "Line": 0, "Operator": 1, "Defects": 2, "ProductionTime(min)": 0, "Shift": 1}, "value_counts": {"Line": {"Line A": 4, "Line B": 4, "Line C": 3}, "Shift": {"Day": 6, "Night": 5, "": 1}}, "summary_stats": {"Defects": {"count": 13, "mean": 1.08, "std": 1.07, "min": 0, "25%": 0, "50%": 1, "75%": 2, "max": 3}, "ProductionTime(min)": {"count": 14, "mean": 52.79, "std": 5.1, "min": 45, "25%": 47, "50%": 53, "75%": 58, "max": 62}}, "avg_defects_per_line": {"Line A": 0.75, "Line B": 1.5, "Line C": 1.33}, "correlations": {"Defects_vs_ProductionTime": 0.43}, "top_operators_by_defects": {"Anna Lee": 4, "Michael Brown": 3, "Mary O'Neill": 2, "John Doe": 3, "Jane Smith": 0}}}
{"purpose": "Analyze customer call patterns and identify data quality issues in telecom usage records.", "raw_table": "CustomerID,CallDate,CallDuration,CallType,ServiceRegion,Charge\nC001,2023-04-01,300,Voice,North,15.5\nc002,04/02/2023,NA,VOICE,South,12.0\nC003,2023/04/03,180,Sms,East,5.0\nC004,2023-4-04,240,voice,west,13.0\nc005,2023-04-05,,SMS,North,NA\nC006,2023-4-06,300,Data,South,20.0\nC007, 2023-04-07 ,200,Voice,East,14.0\nC008,2023-04-08,abc,voice,West,18.5\nC009,2023-04-09,150,Data,North,17.0\nC010,,210,Voice,East,16.0\nC011,2023-04-11,190,VOICE,Unknown,15.0\nC012,2023-04-12,220,Voice,South,invalid\n", "eda_steps": ["Check for missing values in all columns", "Standardize CallType and ServiceRegion capitalization", "Convert CallDate to datetime format handling inconsistent formats", "Convert CallDuration and Charge to numeric, coercing errors to NaN", "Compute descriptive statistics for CallDuration and Charge", "Generate value counts for CallType and ServiceRegion", "Identify rows with missing or invalid CallDuration or Charge", "Calculate correlation between CallDuration and Charge"], "eda_results": {"missing_values": {"CustomerID": 0, "CallDate": 1, "CallDuration": 3, "CallType": 0, "ServiceRegion": 1, "Charge": 2}, "standardized_categories": {"CallType": {"voice": 7, "sms": 2, "data": 2}, "ServiceRegion": {"north": 3, "south": 3, "east": 3, "west": 2, "unknown": 1}}, "descriptive_stats": {"CallDuration": {"count": 9, "mean": 215.56, "std": 48.33, "min": 150, "25%": 190, "50%": 210, "75%": 240, "max": 300}, "Charge": {"count": 10, "mean": 14.55, "std": 4.11, "min": 5.0, "25%": 13.0, "50%": 15.25, "75%": 17.75, "max": 20.0}}, "invalid_entries": {"CallDuration": ["C008"], "Charge": ["C005", "C012"]}, "correlations": {"CallDuration_Charge": 0.89}}}
{"purpose": "Analyze crop yield variations across different regions and identify data quality issues in the agricultural dataset.", "raw_table": "Region,Crop,Planting Date,Harvest Date,Yield (tons/ha),Soil Type,Fertilizer Used\nNorth,Maize,2023-03-15,2023/09/20,6.5,Clay,Yes\nsouth,rice,15-04-2023,2023-10-10,5.8,Sandy,No\nEAST,Wheat,2023/03/20,,7.1,Loamy,yes\nWest,maize,2023-03-18,2023-09-25,6.8,CLAY,Yes\nnorth,Rice,2023-04-16,2023-10-12,missing,Sandy,NO\nEast,Wheat,,2023-08-30,7.3,Loamy,YES\nsouth,Maize,2023-03-25,2023-09-22,6.3,Sandy,\nWest,Rice,2023-04-10,2023-10-15,5.9,Clay,No\nnorth,Wheat,2023-03-22,2023-08-28,7.0,loamy,yes\nEast,Maize,2023-03-17,2023-09-24,6.6,Loamy,YES\nSouth,Wheat,2023-03-21,2023-08-29,7.2,loamy,no", "eda_steps": ["Check for missing values in each column", "Standardize the capitalization of categorical columns: Region, Crop, Soil Type, Fertilizer Used", "Parse and standardize date columns to a uniform format", "Compute descriptive statistics for the Yield column, excluding non-numeric entries", "Generate value counts for the Crop and Region columns", "Identify rows with inconsistent or missing Yield values", "Calculate the average Yield per Crop type", "Summarize missing value percentages per column"], "eda_results": {"missing_values": {"Region": 0, "Crop": 0, "Planting Date": 1, "Harvest Date": 1, "Yield (tons/ha)": 1, "Soil Type": 0, "Fertilizer Used": 1}, "standardized_categories": {"Region": ["North", "South", "East", "West"], "Crop": ["Maize", "Rice", "Wheat"], "Soil Type": ["Clay", "Sandy", "Loamy"], "Fertilizer Used": ["Yes", "No"]}, "date_format_consistency": "All dates standardized to ISO format YYYY-MM-DD", "yield_descriptive_stats": {"count": 12, "mean": 6.75, "std_dev": 0.58, "min": 5.8, "25%": 6.3, "50%": 6.65, "75%": 7.05, "max": 7.3}, "value_counts": {"Crop": {"Maize": 4, "Rice": 3, "Wheat": 5}, "Region": {"North": 3, "South": 3, "East": 3, "West": 2}}, "rows_with_problematic_yield": [4], "average_yield_per_crop": {"Maize": 6.55, "Rice": 5.85, "Wheat": 7.12}, "missing_value_percentages": {"Planting Date": 8.33, "Harvest Date": 8.33, "Yield (tons/ha)": 8.33, "Fertilizer Used": 8.33, "Other Columns": 0}}}
{"purpose": "Analyze daily ridership patterns and vehicle types in city public transportation.", "raw_table": "Date,Route,Vehicle_Type,Passengers,Revenue,Driver_ID\n2024-01-01,Route 5,Bus,120,300.50,DR1001\n01/02/2024,route 5,Bus,,350.75,DR1002\n2024/01/03,Route 7,TrAm,85,220.00,DR1003\n2024-01-04,route 7,tram,90,,DR1003\n2024-01-05,Route 12,Bus,150,400.00,dr1004\n2024-01-06,Route 5,Bus,130,320.00,DR1001\n2024-01-07,Route 12,Bus,,410.00,DR1004\n2024/01/08,route 7,TRAM,95,230.00,dr1003\n2024-1-9,Route 5,Bus,125,310.00,DR1002\n2024-01-10,Route 7,Bus,80,200,DR1003\n2024-01-11,,bus,100,250.50,DR1001\n2024-01-12,Route 12,Bus,140,,DR1004", "eda_steps": ["Check the data types and format consistency of each column", "Identify and count missing values per column", "Standardize the Vehicle_Type and Route columns to uniform capitalization", "Compute descriptive statistics for numeric columns Passengers and Revenue", "Generate value counts for Vehicle_Type and Route columns", "Calculate total and average passengers by Vehicle_Type", "Identify rows with missing Route or Passengers data", "Analyze revenue distribution and check for outliers", "Summarize counts of unique Driver_IDs and their assignments"], "eda_results": {"missing_values": {"Date": 0, "Route": 1, "Vehicle_Type": 0, "Passengers": 2, "Revenue": 3, "Driver_ID": 0}, "standardized_columns_sample": {"Vehicle_Type": ["Bus", "Bus", "Tram", "Tram", "Bus", "Bus", "Bus", "Tram", "Bus", "Bus", "Bus", "Bus"], "Route": ["Route 5", "Route 5", "Route 7", "Route 7", "Route 12", "Route 5", "Route 12", "Route 7", "Route 5", "Route 7", null, "Route 12"]}, "summary_stats": {"Passengers": {"count": 10, "mean": 111.5, "std": 23.94, "min": 80, "25%": 90, "50%": 122.5, "75%": 140, "max": 150}, "Revenue": {"count": 9, "mean": 291.75, "std": 74.69, "min": 200, "25%": 220, "50%": 310, "75%": 350.75, "max": 410}}, "value_counts": {"Vehicle_Type": {"Bus": 8, "Tram": 4}, "Route": {"Route 5": 4, "Route 7": 4, "Route 12": 3, "null": 1}}, "passengers_by_vehicle_type": {"Bus": {"total_passengers": 865, "average_passengers": 108.13}, "Tram": {"total_passengers": 270, "average_passengers": 90}}, "missing_data_rows": {"missing_route": [10], "missing_passengers": [1, 6]}, "revenue_outliers": {"max_value": 410, "min_value": 200, "possible_outlier_threshold": 400, "outliers": ["2024-01-07,Route 12,Bus,,410.00,DR1004"]}, "driver_assignments": {"DR1001": 3, "DR1002": 2, "DR1003": 4, "DR1004": 3}}}
{"purpose": "Explore patient demographics and clinical measurements to identify data quality issues and initial trends in a small medical dataset.", "raw_table": "PatientID,Age,Gender,AdmissionDate,BloodPressure_systolic,BloodPressure_diastolic,Diagnosis,Weight_kg\n001,45,M,2023/01/15,120,80,hypertension,78\n002,60,F,15-02-2023,135,,Diabetes,82\n003,38,male,2023-03-10,110,70,Hypertension,NaN\n004,,F,2023-4-1,140,90,diabetes,85.5\n005,50,Female,03/22/2023,130,85,HTN,79\n006,29,M,2023-03-30,115,75,none,70\n007,55,Unknown,2023-03-28,145,88,Diabetes,missing\n008,62,F,2023/02/20,138,92,Diabetes,90\n009,47,,2023-03-05,125,80,hypertension,76\n010,51,f,2023/03/15,NaN,85,HTN,83\n011,44,M,2023/03/18,128,82,Hypertension,80\n012,33,F,20-03-2023,118,78,none,68", "eda_steps": ["Check missing value counts per column", "Standardize gender values to lowercase and unify categories", "Parse AdmissionDate into a consistent date format", "Calculate descriptive statistics for numeric columns: Age, BloodPressure_systolic, BloodPressure_diastolic, Weight_kg", "Generate value counts for Diagnosis column with unified categories", "Identify rows with inconsistent or missing BloodPressure_systolic values", "Count unique PatientIDs to verify data consistency"], "eda_results": {"missing_values": {"PatientID": 0, "Age": 1, "Gender": 1, "AdmissionDate": 0, "BloodPressure_systolic": 1, "BloodPressure_diastolic": 0, "Diagnosis": 0, "Weight_kg": 2}, "gender_value_counts": {"m": 4, "f": 5, "unknown": 1, "": 1}, "diagnosis_value_counts": {"hypertension": 5, "diabetes": 4, "none": 2, "htn": 2}, "standardized_diagnosis_counts": {"hypertension": 7, "diabetes": 4, "none": 2}, "age_stats": {"count": 11, "mean": 46.5, "min": 29, "max": 62, "std_dev": 10.3}, "blood_pressure_systolic_stats": {"count": 11, "mean": 127.3, "min": 110, "max": 145, "std_dev": 10.7}, "blood_pressure_diastolic_stats": {"count": 12, "mean": 82.3, "min": 70, "max": 92, "std_dev": 7.1}, "weight_kg_stats": {"count": 10, "mean": 79.3, "min": 68, "max": 90, "std_dev": 6.9}, "inconsistent_blood_pressure_systolic_rows": [{"PatientID": "010", "BloodPressure_systolic": "NaN"}], "unique_patient_ids_count": 12}}
{"purpose": "Analyze customer purchase patterns and identify common product categories and missing data issues.", "raw_table": "OrderID,CustomerName,ProductCategory,PurchaseDate,Quantity,UnitPrice,PaymentMethod\n1001,alice,Electronics,2023-01-05,2,299.99,Credit Card\n1002,Bob,home appliances,1/15/2023,1,89.5,Paypal\n1003,CHARLIE,Electronics,2023/01/20,,199.99,Credit Card\n1004,Diana,clothing,2023-01-20,3,,Debit Card\n1005,,GARDEN,01-25-2023,5,15.99,Credit Card\n1006,Elena,Electronics,2023-02-01,1,299.99,CASH\n1007,Frank,clothing,2023-02-03,2,49.99,credit card\n1008,Grace,Home Appliances,Feb 05 2023,1,89.5,Paypal\n1009,Hannah,Gardening,2023-02-10,4,15.99,Credit Card\n1010,Ian,Electronics,2023-02-12,1,299.99,Debit Card\n1011,Jack,clothing,,2,39.99,Cash\n1012,Kate,Toys,2023-02-15,3,19.99,Paypal\n1013,Liam,toys,2023-02-16,1,19.99,credit card", "eda_steps": ["Check missing value counts per column", "Standardize ProductCategory capitalization and unify similar categories", "Parse and standardize PurchaseDate format", "Compute descriptive statistics for Quantity and UnitPrice", "Generate value counts for PaymentMethod", "Identify top 3 ProductCategories by total quantity sold", "Calculate average UnitPrice by ProductCategory"], "eda_results": {"missing_values": {"OrderID": 0, "CustomerName": 1, "ProductCategory": 0, "PurchaseDate": 1, "Quantity": 1, "UnitPrice": 1, "PaymentMethod": 0}, "standardized_product_categories": {"Electronics": 4, "Home Appliances": 2, "Clothing": 3, "Garden": 2, "Toys": 2}, "purchase_dates": {"earliest": "2023-01-05", "latest": "2023-02-16", "missing_dates": 1}, "summary_stats": {"Quantity": {"count": 13, "mean": 2.31, "min": 1, "max": 5}, "UnitPrice": {"count": 13, "mean": 116.56, "min": 15.99, "max": 299.99}}, "payment_method_counts": {"Credit Card": 4, "Paypal": 3, "Debit Card": 2, "Cash": 2, "credit card": 2}, "top_categories_by_quantity": {"Electronics": 8, "Clothing": 7, "Garden": 9}, "average_unitprice_by_category": {"Electronics": 274.99, "Home Appliances": 89.5, "Clothing": 44.99, "Garden": 15.99, "Toys": 19.99}}}
{"purpose": "Analyze monthly temperature and precipitation patterns across different climate zones to identify data quality issues and preliminary trends.", "raw_table": "Date,Climate_Zone,Temperature_C,Precipitation_mm,Weather_Condition\n2024-01-15,Tropical,30.5,5.2,Sunny\n15/02/2024,temperate,12, ,rainy\n2024-03-10,Arctic,-15.2,0,Snow\n2024/04/05,TROPICAL,28.1,12.4,Cloudy\n2024-05-20,Temperate,18.3,7.8,Rain\n06-06-2024,arctic,-10.6,,snow\n2024-07-14,Tropical,31.2,3.1,Sunny\n2024-08-01,Temperate,21,,RAINY\n2024-09-10,Arctic,-5.0,0,Cloudy\n2024-10-05,Tropical,29.0,4.5,sunny\n2024-11-12,temperate,15.2,8.3,Rain\n2024-12-20,Arctic,,1.2,Snowy", "eda_steps": ["Standardize the Climate_Zone column capitalization", "Parse and unify the Date column format to YYYY-MM-DD", "Check for missing values in all columns", "Compute descriptive statistics for Temperature_C and Precipitation_mm", "Generate value counts for Weather_Condition", "Calculate correlation between Temperature_C and Precipitation_mm", "Identify unique Climate_Zone values after standardization", "Summarize missing data count per Climate_Zone"], "eda_results": {"standardized_climate_zones": ["Tropical", "Temperate", "Arctic"], "date_formats_unified": true, "missing_values": {"Temperature_C": 1, "Precipitation_mm": 3, "Weather_Condition": 0, "Date": 0, "Climate_Zone": 0}, "summary_stats": {"Temperature_C": {"count": 11, "mean": 14.4, "std": 15.4, "min": -15.2, "max": 31.2}, "Precipitation_mm": {"count": 9, "mean": 5.67, "std": 4.17, "min": 0, "max": 12.4}}, "weather_condition_counts": {"Sunny": 2, "Rainy": 2, "Snow": 1, "Cloudy": 2, "Rain": 1, "snow": 1, "Snowy": 1}, "temperature_precipitation_correlation": -0.36, "missing_data_per_climate_zone": {"Tropical": {"Temperature_C": 0, "Precipitation_mm": 0}, "Temperate": {"Temperature_C": 0, "Precipitation_mm": 2}, "Arctic": {"Temperature_C": 1, "Precipitation_mm": 1}}}}
{"purpose": "Analyze user engagement patterns and content types on a social media platform to identify trends and data quality issues.", "raw_table": "user_id,post_date,content_type,likes,comments,shares,is_promoted\nU001,2023-01-05,Image,120,15,5,True\nU002,05/01/2023,video,200,,10,false\nU003,,Text,50,5,2,True\nU004,2023/01/07,IMage,NaN,0,0,FALSE\nU005,2023-1-08,Link,80,10,,TRUE\nU006,2023-01-08,Video,150,20,8,False\nU007,2023-01-09,text,45,,1,TRUE\nU008,01-10-2023,Link,70,7,3,false\nU009,2023-01-11,Image,130,12,5,TRUE\nU010,2023-01-12,VIDEO,180,25,9,false\nU011,2023-01-12,,90,8,4,True\nU012,2023-1-13,text,40,4,,false", "eda_steps": ["Check missing value percentages for all columns", "Standardize content_type values to consistent capitalization", "Parse and standardize post_date to a uniform date format", "Compute descriptive statistics for numeric columns likes, comments, and shares", "Generate value counts for content_type", "Identify number of promoted vs non-promoted posts", "Calculate correlation matrix between likes, comments, and shares", "Identify top 3 posts with highest likes", "Summarize distribution skewness for likes"], "eda_results": {"missing_values": {"post_date": 1, "content_type": 1, "likes": 1, "comments": 3, "shares": 3, "is_promoted": 0}, "content_type_standardized": {"Image": 3, "Video": 3, "Text": 3, "Link": 2, "Missing": 1}, "post_date_standardized_sample": ["2023-01-05", "2023-01-05", "Missing", "2023-01-07", "2023-01-08", "2023-01-08", "2023-01-09", "2023-01-10", "2023-01-11", "2023-01-12", "2023-01-12", "2023-01-13"], "summary_stats": {"likes": {"count": 11, "mean": 105.45, "median": 90, "std": 54.29, "min": 40, "max": 200}, "comments": {"count": 9, "mean": 11.11, "median": 10, "std": 7.57, "min": 0, "max": 25}, "shares": {"count": 9, "mean": 4.78, "median": 4, "std": 2.99, "min": 0, "max": 10}}, "value_counts_is_promoted": {"True": 6, "False": 5}, "correlations": {"likes_comments": 0.85, "likes_shares": 0.88, "comments_shares": 0.92}, "top_3_likes_posts": [{"user_id": "U002", "likes": 200}, {"user_id": "U010", "likes": 180}, {"user_id": "U006", "likes": 150}], "likes_skewness": 0.65}}
{"purpose": "Analyze patient demographics and lab test results to identify data quality issues and summary statistics.", "raw_table": "Patient_ID,Age,Gender,Diagnosis,Test_Date,Cholesterol,Glucose,Smoker\n001,45,M,Hypertension,2023-01-15,220,110,Yes\n002,38,female,diabetes,15/02/2023,180,missing,no\n003,,Male,Hypertension,2023/03/20,NaN,105,YES\n004,50,Male,HyperTension,2023-04-10,210,115,No\n005,29,Female,healthy,2023-05-01,190,100,No\n006,NaN,F,Diabetes,May 12 2023,200,NaN,yes\n007,55,m,diabetes,2023-06-07,230,130,No\n008,40,Female,Healthy,2023-07-20,195,108,MIXED\n009,47,M,Hypertension,,215,112,Yes\n010,33,Female,Diabetes,2023-08-15,185,99,no", "eda_steps": ["Check missing value percentages for each column", "Standardize the Gender column values", "Standardize the Diagnosis column values", "Convert Test_Date to a consistent date format and identify missing dates", "Compute descriptive statistics for Age, Cholesterol, and Glucose", "Generate value counts for Diagnosis and Smoker columns", "Identify unique values in the Smoker column and correct inconsistencies"], "eda_results": {"missing_values": {"Patient_ID": 0, "Age": 2, "Gender": 0, "Diagnosis": 0, "Test_Date": 1, "Cholesterol": 1, "Glucose": 2, "Smoker": 0}, "standardized_gender": {"Male": 5, "Female": 4, "F": 0, "M": 0, "female": 0, "m": 0}, "standardized_diagnosis": {"Hypertension": 4, "Diabetes": 4, "Healthy": 2}, "test_date_issues": {"missing_dates": 1, "date_formats_found": ["YYYY-MM-DD", "DD/MM/YYYY", "YYYY/MM/DD", "Month DD YYYY"]}, "summary_stats": {"Age": {"count": 8, "mean": 41.375, "min": 29, "max": 55}, "Cholesterol": {"count": 9, "mean": 204.44, "min": 180, "max": 230}, "Glucose": {"count": 8, "mean": 109.875, "min": 99, "max": 130}}, "value_counts": {"Diagnosis": {"Hypertension": 4, "Diabetes": 4, "Healthy": 2}, "Smoker": {"Yes": 3, "No": 4, "no": 2, "YES": 1, "MIXED": 1, "missing": 1}}, "smoker_unique_values": ["Yes", "No", "YES", "no", "MIXED", "missing", "yes"]}}
{"purpose": "Analyze customer call patterns and service usage to identify potential areas for improving network reliability and customer satisfaction.", "raw_table": "CustomerID,CallStart,CallDuration,ServiceType,CallQuality,DataUsageMB,Region\nC001,2023-04-01 08:15,12,voice,good, ,north\nc002,04/02/2023 14:30,8,Voice,poor, ,South\nC003,2023/04/03 16:45, ,VIDEO,good,150,East\nc004,2023-04-03T18:00,5,video,average,100,West\nC005,,20,voice,Good, ,north\nC006,2023-04-05 09:20,15,VOICE,poor, ,south\nC007,2023-04-05 22:10,10,video,,200,East\nc008,2023-4-6 07:05,7,voice,average, ,west\nc009,2023-04-06 12:00, ,voice,Good, ,north\nC010,2023-04-07 20:00,25,VIDEO,poor,300,SouTH", "eda_steps": ["Check for missing values in all columns", "Standardize capitalization in ServiceType and CallQuality columns", "Compute descriptive statistics for CallDuration and DataUsageMB", "Generate value counts for ServiceType and CallQuality", "Identify the number of calls per Region", "Analyze the distribution of CallDuration grouped by CallQuality", "Summarize missing values percentages for each column"], "eda_results": {"missing_values": {"CustomerID": 0, "CallStart": 1, "CallDuration": 2, "ServiceType": 0, "CallQuality": 1, "DataUsageMB": 5, "Region": 0}, "standardized_categories": {"ServiceType": {"voice": 6, "video": 4}, "CallQuality": {"good": 4, "poor": 3, "average": 2, "missing": 1}}, "summary_stats": {"CallDuration": {"count": 8, "mean": 12.75, "min": 5, "max": 25, "std": 6.53}, "DataUsageMB": {"count": 5, "mean": 150, "min": 100, "max": 300, "std": 81.65}}, "value_counts": {"ServiceType": {"voice": 6, "video": 4}, "CallQuality": {"good": 4, "poor": 3, "average": 2, "missing": 1}}, "calls_per_region": {"north": 3, "south": 3, "east": 2, "west": 2}, "call_duration_by_quality": {"good": {"count": 4, "mean": 12.33, "min": 5, "max": 20}, "poor": {"count": 3, "mean": 16, "min": 8, "max": 25}, "average": {"count": 2, "mean": 6, "min": 5, "max": 7}, "missing": {"count": 1, "mean": null}}, "missing_value_percentages": {"CallStart": 10.0, "CallDuration": 20.0, "DataUsageMB": 50.0, "CallQuality": 10.0}}}
{"purpose": "Analyze machine downtime causes and durations to identify key factors affecting production efficiency.", "raw_table": "MachineID,Date,DowntimeHours,Cause,OperatorShift\nM01,2023/01/05,2.5,maintenance,morning\nM02,01-06-2023,1.0,Power Failure,NIGHT\nM03,2023-01-07,,Mechanical Failure,morning\nm04,2023/1/08,3.0,Operator Error,Afternoon\nM05,2023-01-09,0.5,maintenance,Morning\nM01,2023-01-10,1.5,Operator Error,,\nM02,2023-01-11,2.0,mechanical failure,Night\nM03,2023/01/12,NaN,Power failure,Afternoon\nM04,2023-01-13,2.0,Operator error,MORNING\nM05,2023-01-14,1.2,MAINTENANCE,afternoon\nM01,2023/01/15,1.0,unknown,night", "eda_steps": ["Check missing values for all columns", "Standardize the Cause column capitalization", "Convert Date column to consistent date format", "Compute descriptive statistics for DowntimeHours", "Generate value counts for Cause and OperatorShift columns", "Identify rows with missing or NaN DowntimeHours", "Compute average downtime by Cause", "Summarize downtime distribution by OperatorShift"], "eda_results": {"missing_values": {"MachineID": 0, "Date": 0, "DowntimeHours": 3, "Cause": 0, "OperatorShift": 2}, "standardized_cause_value_counts": {"maintenance": 3, "power failure": 2, "mechanical failure": 2, "operator error": 3, "unknown": 1}, "operator_shift_value_counts": {"morning": 3, "night": 3, "afternoon": 3, "": 2}, "downtime_hours_stats": {"count": 11, "mean": 1.68, "std_dev": 0.87, "min": 0.5, "25%": 1.0, "50%": 1.5, "75%": 2.25, "max": 3.0}, "rows_with_missing_downtime": [2, 7, 8], "average_downtime_by_cause": {"maintenance": 1.07, "power failure": 1.0, "mechanical failure": 1.75, "operator error": 2.17, "unknown": 1.0}, "downtime_distribution_by_operator_shift": {"morning": {"count": 3, "mean": 1.33}, "night": {"count": 3, "mean": 1.17}, "afternoon": {"count": 3, "mean": 2.07}, "unknown": {"count": 2, "mean": null}}}}
{"purpose": "Analyze production batch quality and identify factors affecting defect rates.", "raw_table": "Batch_ID,Production_Date,Machine_ID,Operator,Product_Type,Units_Produced,Defects,Shift\nB001,2024/01/15,MACH-01,alice,WidgetA,1000,5,Morning\nB002,15-01-2024,MACH-02,Bob,widgetA,950,,Afternoon\nB003,2024-01-16,Mach-01,Charlie,WidgetB,NaN,2,Morning\nB004,01/17/2024,MACH-03,Alice,WidgetB,1100,7,Night\nb005,2024/01/18,MACH-02,bob,widgeta,1050,3,Afternoon\nB006,18-01-2024,MACH-01,Charlie,WidgetC,980,1,Morning\nB007,2024/1/19,Mach-03,Alice,WidgetC,NaN,NaN,Night\nB008,2024/01/20,MACH-02,bob,WidgetB,1020,8,Afternoon\nB009,20/01/2024,MACH-01,Charlie,widgetB,970,NaN,Morning\nB010,2024-01-21,MACH-03,Alice,WidgetA,1005,4,Night", "eda_steps": ["Check for missing values in each column", "Standardize capitalization in categorical columns: Machine_ID, Operator, Product_Type, Shift", "Parse and unify Production_Date into a consistent date format", "Compute descriptive statistics for numeric columns: Units_Produced, Defects", "Generate value counts for categorical columns: Product_Type, Shift", "Calculate defect rate as Defects divided by Units_Produced for each batch", "Identify correlation between Units_Produced and Defects", "Summarize average defect rate by Machine_ID", "Check distribution skewness for Defects and Units_Produced"], "eda_results": {"missing_values": {"Batch_ID": 0, "Production_Date": 0, "Machine_ID": 0, "Operator": 0, "Product_Type": 0, "Units_Produced": 2, "Defects": 3, "Shift": 0}, "standardized_unique_values": {"Machine_ID": ["MACH-01", "MACH-02", "MACH-03"], "Operator": ["Alice", "Bob", "Charlie"], "Product_Type": ["WidgetA", "WidgetB", "WidgetC"], "Shift": ["Morning", "Afternoon", "Night"]}, "date_range": {"min": "2024-01-15", "max": "2024-01-21"}, "summary_stats": {"Units_Produced": {"count": 13, "mean": 1019.14, "std": 52.64, "min": 950, "25%": 980, "50%": 1005, "75%": 1050, "max": 1100}, "Defects": {"count": 10, "mean": 4.1, "std": 2.51, "min": 1, "25%": 3, "50%": 4.5, "75%": 5, "max": 8}}, "value_counts": {"Product_Type": {"WidgetA": 4, "WidgetB": 4, "WidgetC": 2}, "Shift": {"Morning": 4, "Afternoon": 3, "Night": 3}}, "defect_rates": {"B001": 0.005, "B002": null, "B003": null, "B004": 0.00636, "B005": 0.00286, "B006": 0.00102, "B007": null, "B008": 0.00784, "B009": null, "B010": 0.00398}, "correlations": {"Units_Produced_vs_Defects": 0.56}, "average_defect_rate_by_machine": {"MACH-01": 0.0037, "MACH-02": 0.00535, "MACH-03": 0.00417}, "skewness": {"Units_Produced": 0.12, "Defects": 0.85}}}
{"purpose": "Analyze production line efficiency and defect rates in manufactured batches.", "raw_table": "BatchID,ProductionDate,Line,UnitsProduced,DefectCount,Operator,Shift\nB001,2024-01-15,line1,1000,5,John Doe,Morning\nB002,01/16/2024,line2,950,,Jane Smith,Afternoon\nb003,2024-01-17,Line1,980,7,john doe,Morning\nB004,,LINE3,1050,12,Mary-Jane,Morning\nB005,2024/01/19,line2,NaN,3,Jane Smith,Night\nB006,2024-01-20,Line2,970,NaN,Mark,TwiLight\nB007,2024-01-21,Line1,1020,4,Mary-Jane,Morning\nB008,2024-1-22,line3,990,6,mary-jane,Night\nB009,2024-01-23,line1,1005,NaN,John Doe,Afternoon\nb010,01-24-2024,Line2,985,8,MARK,Night", "eda_steps": ["Check for missing values in each column", "Standardize the capitalization of categorical columns: Line, Operator, Shift", "Convert ProductionDate to a consistent date format and identify missing dates", "Calculate descriptive statistics for UnitsProduced and DefectCount", "Generate value counts for each unique 'Line' and 'Shift'", "Compute the defect rate as DefectCount divided by UnitsProduced for each batch", "Identify batches with unusually high defect rates", "Check correlation between UnitsProduced and DefectCount", "Summarize unique operators and their total units produced"], "eda_results": {"missing_values": {"BatchID": 0, "ProductionDate": 1, "Line": 0, "UnitsProduced": 1, "DefectCount": 3, "Operator": 0, "Shift": 0}, "standardized_categories": {"Line": ["Line1", "Line2", "Line3"], "Shift": ["Morning", "Afternoon", "Night", "Twilight"], "Operator": ["John Doe", "Jane Smith", "Mary-Jane", "Mark"]}, "date_format_consistency": {"ParsedDates": 9, "MissingDates": 1, "InconsistentFormats": ["01/16/2024", "2024/01/19", "2024-1-22", "01-24-2024"]}, "summary_stats": {"UnitsProduced": {"count": 9, "mean": 997.22, "std": 33.34, "min": 950, "25%": 980, "50%": 995, "75%": 1020, "max": 1050}, "DefectCount": {"count": 7, "mean": 6.43, "std": 2.97, "min": 3, "25%": 4, "50%": 5, "75%": 7, "max": 12}}, "value_counts": {"Line": {"Line1": 4, "Line2": 4, "Line3": 2}, "Shift": {"Morning": 4, "Afternoon": 2, "Night": 3, "Twilight": 1}}, "defect_rate": {"B001": 0.005, "B002": null, "B003": 0.00714, "B004": 0.01143, "B005": 0.00303, "B006": null, "B007": 0.00392, "B008": 0.00606, "B009": null, "B010": 0.00812}, "high_defect_batches": ["B004"], "correlation_units_defects": 0.65, "operators_summary": {"John Doe": {"batches": 3, "total_units": 2985}, "Jane Smith": {"batches": 2, "total_units": 950}, "Mary-Jane": {"batches": 3, "total_units": 3050}, "Mark": {"batches": 2, "total_units": 1955}}}}
{"purpose": "Analyze student performance and attendance patterns to identify factors impacting grades.", "raw_table": "Student_ID,Name,Enrollment_Date,Grade,Attendance_Percentage,Major,Extracurricular_Activity\nS001,Alice smith,2022-09-01,A,95,Computer Science,football\ns002,Bob Johnson,09/03/2022,B+,Ninety,engineering,Basketball\nS003,CHARLIE davis,2022/09/05,C,85,CompSci, \nS004,denise Lee,2022-9-7,A-,78,,Drama Club\nS005,Edward King,2022-09-10,B,88,Engineering,Soccer\nS006,frank moore,2022-09-15,,92,computer science,football\nS007,Grace Liu,2022-0918,B-,60,Engineering,Basketball\nS008,Hannah O'Neil,20220920,C+,70,comp sci,Soccer\ns009,Ian Wright,2022/09/22,A,missing,Drama, \nS010,julia Kim,2022-09-25,B+,80,Engineering,\n", "eda_steps": ["Check for and count missing values in each column", "Standardize the 'Major' column to consistent categories", "Convert 'Enrollment_Date' to uniform date format", "Calculate descriptive statistics for 'Attendance_Percentage' and map grades to numeric scores", "Generate value counts for 'Grade' and 'Extracurricular_Activity'", "Analyze correlation between numeric grade scores and attendance percentage", "Identify students with missing grades or attendance data", "Summarize distribution skewness for attendance percentages"], "eda_results": {"missing_values": {"Student_ID": 0, "Name": 0, "Enrollment_Date": 0, "Grade": 1, "Attendance_Percentage": 2, "Major": 1, "Extracurricular_Activity": 3}, "standardized_majors": {"Computer Science": 4, "Engineering": 4, "Drama": 2}, "grade_numeric_mapping": {"A": 4.0, "A-": 3.7, "B+": 3.3, "B": 3.0, "B-": 2.7, "C+": 2.3, "C": 2.0, "Missing": null}, "attendance_descriptive_stats": {"count": 8, "mean": 80.0, "std_dev": 12.5, "min": 60, "max": 95, "skewness": -0.35}, "grade_value_counts": {"A": 2, "A-": 1, "B+": 2, "B": 1, "B-": 1, "C+": 1, "C": 1, "Missing": 1}, "extracurricular_value_counts": {"Football": 2, "Basketball": 2, "Soccer": 2, "Drama Club": 1, "": 3}, "correlation_grade_attendance": 0.68, "students_missing_data": {"missing_grade": ["S006"], "missing_attendance": ["S002", "S009"]}}}
{"purpose": "Analyze student performance and attendance patterns to identify factors affecting exam scores.", "raw_table": "Student_ID,Name,Age,Grade,Exam_Date,Exam_Score,Attendance,Extra_Curricular\n101,alice,15,10th,2023-05-12,88,95%,Basketball\n102,Bob,16,10th,05/13/2023,92,90%,Drama\n103,CHARLIE,,11th,2023/05/14,85,85%,sports\n104,Daisy,15,10TH,14-05-2023,,80%,Chess\n105,eva,16,11th,2023-05-15,90,NaN,Music\n106,Frank,15,10th,2023-15-05,78,88%,\n107,george,16,11th,2023-05-16,82,92%,Drama\n108,Hannah,,10Th,2023-05-17,95,97%,Basketball\n109,Ian,15,10th,2023-5-18,88,89%,music\n110,Julia,16,11TH,May 19, 2023,91,93%,Sports\n", "eda_steps": ["Check for missing values in each column", "Standardize date formats in Exam_Date column", "Convert Age and Exam_Score to numeric types and compute descriptive statistics", "Calculate value counts for Grade and Extra_Curricular columns", "Analyze attendance rates by Grade", "Identify students with missing Exam_Score", "Compute correlation between Attendance and Exam_Score", "Summarize unique count of students and average scores per Grade"], "eda_results": {"missing_values": {"Student_ID": 0, "Name": 0, "Age": 2, "Grade": 0, "Exam_Date": 0, "Exam_Score": 1, "Attendance": 1, "Extra_Curricular": 1}, "standardized_exam_dates": ["2023-05-12", "2023-05-13", "2023-05-14", "2023-05-14", "2023-05-15", null, "2023-05-16", "2023-05-17", "2023-05-18", "2023-05-19"], "descriptive_stats": {"Age": {"count": 8, "mean": 15.5, "min": 15, "max": 16}, "Exam_Score": {"count": 9, "mean": 87.9, "min": 78, "max": 95}}, "value_counts": {"Grade": {"10th": 6, "11th": 4}, "Extra_Curricular": {"Basketball": 2, "Drama": 2, "Sports": 2, "Chess": 1, "Music": 1, "sports": 1, "music": 1, "": 1}}, "attendance_by_grade": {"10th": "89.0%", "11th": "90.0%"}, "students_missing_exam_score": [{"Student_ID": "104", "Name": "Daisy"}], "correlation_attendance_exam_score": 0.68, "summary_per_grade": {"10th": {"students": 6, "average_exam_score": 86.5}, "11th": {"students": 4, "average_exam_score": 90.75}}}}
{"purpose": "Analyze user engagement patterns and content types on a social media platform.", "raw_table": "user_id,post_date,post_type,likes,comments,shares\nA123,2024-01-15,Photo,120,15,5\nb456,01/20/2024,video,200,,10\nC789,2024/01/22,text,45,5,0\nD012,2024-01-23,Photo,NaN,8,2\nE345,2024-01-25,Link,30,3,\nF678,01-27-2024,video,NaN,NaN,NaN\ng901,2024-01-28,photo,85,10,3\nH234,2024-01-30,text,50,7,1\nI567,2024/02/01,Video,150,20,15\nj890,02/03/2024,link,40,,4\nK123,,Photo,100,12,6\nL456,2024-02-05,Text,NaN,9,0\nM789,2024-02-07,video,180,25,20", "eda_steps": ["Check the data types of each column and identify inconsistencies", "Inspect and calculate the percentage of missing values per column", "Standardize the date format in the post_date column", "Normalize the post_type values to consistent capitalization", "Compute descriptive statistics for likes, comments, and shares columns", "Generate value counts for the post_type column", "Identify top 2 posts by number of likes", "Calculate correlation matrix between likes, comments, and shares"], "eda_results": {"data_types": {"user_id": "string", "post_date": "string with inconsistent formatting and missing values", "post_type": "string with inconsistent capitalization", "likes": "numeric with missing values", "comments": "numeric with missing values", "shares": "numeric with missing values"}, "missing_values_percent": {"user_id": 0, "post_date": 7.69, "post_type": 0, "likes": 23.07, "comments": 23.07, "shares": 23.07}, "standardized_post_date_sample": ["2024-01-15", "2024-01-20", "2024-01-22", "2024-01-23", "2024-01-25", "2024-01-27", "2024-01-28", "2024-01-30", "2024-02-01", "2024-02-03", "missing", "2024-02-05", "2024-02-07"], "normalized_post_type_counts": {"photo": 4, "video": 4, "text": 3, "link": 2}, "likes_summary_stats": {"count": 10, "mean": 100.0, "std": 58.17, "min": 30, "25%": 45, "50%": 90, "75%": 180, "max": 200}, "top_2_posts_by_likes": [{"user_id": "b456", "likes": 200}, {"user_id": "M789", "likes": 180}], "correlations": {"likes_comments": 0.92, "likes_shares": 0.88, "comments_shares": 0.85}}}
{"purpose": "Analyze daily ridership patterns and vehicle types for a city bus service over two weeks.", "raw_table": "Date,Route,Vehicle_Type,Passengers,Revenue\n2024-04-01,10A,Bus,120,240.5\n2024/04/02,10a,bus,135,270\n04-03-2024,10B,Bus,NaN,310\n2024-04-04,10B,Bus,150,NaN\n2024-04-05,10A,MiniBus,85,170\n2024-04-06,10a,Minibus,90,180\n2024-04-07,10C,bus,105,210\n2024-04-08,10c,Bus,NaN,200\n2024-04-09,10C,Bus,95,190\n04/10/2024,10B,Bus,130,260\n2024-04-11,10A,Bus,145,290\n20240412,10B,Bus,140,280\n2024-04-13,10A,Bus,NaN,275\n2024-04-14,10C,MINIBUS,88,176", "eda_steps": ["Standardize the 'Date' column to a single date format", "Normalize 'Route' and 'Vehicle_Type' columns to consistent capitalization", "Check and report missing values in each column", "Compute descriptive statistics for 'Passengers' and 'Revenue'", "Generate value counts for 'Route' and 'Vehicle_Type'", "Calculate total passengers and total revenue per route", "Identify days with missing passenger counts and estimate missing values", "Examine correlation between 'Passengers' and 'Revenue'", "Summarize distribution skewness for numeric columns"], "eda_results": {"missing_values": {"Date": 0, "Route": 0, "Vehicle_Type": 0, "Passengers": 3, "Revenue": 1}, "value_counts": {"Route": {"10A": 5, "10B": 5, "10C": 4}, "Vehicle_Type": {"Bus": 11, "Minibus": 3}}, "summary_stats": {"Passengers": {"count": 11, "mean": 118.64, "std": 22.96, "min": 85, "25%": 95, "50%": 130, "75%": 140, "max": 150, "skewness": -0.12}, "Revenue": {"count": 13, "mean": 242.65, "std": 44.53, "min": 170, "25%": 190, "50%": 260, "75%": 275, "max": 310, "skewness": 0.05}}, "totals_per_route": {"10A": {"total_passengers": 545, "total_revenue": 1245.5}, "10B": {"total_passengers": 540, "total_revenue": 1050}, "10C": {"total_passengers": 288, "total_revenue": 776}}, "correlations": {"Passengers_Revenue": 0.99}, "missing_passengers_estimate": {"2024-04-03": 125, "2024-04-08": 100, "2024-04-13": 130}}}
{"purpose": "Analyze customer purchasing behavior and identify key product categories driving sales.", "raw_table": "OrderID,CustomerID,OrderDate,ProductCategory,Quantity,UnitPrice,PaymentMethod\n1001,C123,2023/01/15,Electronics,2,299.99,Credit Card\n1002,C124,15-01-2023,home Appliances,1,159.5,PayPal\n1003,C125,2023-01-16,Fashion,3,49.99,CREDIT card\n1004,C123,01/17/2023,Electronics,,299.99,Credit Card\n1005,C126,2023-01-18,beauty,5,15,Debit card\n1006,C127,18/01/2023,Home appliances,2,160.0,Paypal\n1007,C128,,Fashion,1,55.0,Cash\n1008,C129,2023-01-20,Sports,4,75.0,Credit Card\n1009,C130,2023/01/21,beauty,2,not available,Debit Card\n1010,C131,2023-1-22,SPORTS,3,70,CASH\n1011,C132,2023-01-23,Toys,1,25.0,Credit card\n1012,C133,2023-01-23,Fashion,2,50,PayPal", "eda_steps": ["Check for missing values in each column", "Standardize and clean the ProductCategory and PaymentMethod columns for consistent capitalization", "Convert OrderDate to a uniform date format", "Identify and handle invalid or missing numeric entries in Quantity and UnitPrice", "Compute total sales (Quantity * UnitPrice) for each row", "Generate descriptive statistics for Quantity, UnitPrice, and total sales", "Calculate value counts for ProductCategory and PaymentMethod", "Identify top 3 product categories by total sales", "Examine correlation between Quantity and UnitPrice"], "eda_results": {"missing_values": {"OrderID": 0, "CustomerID": 0, "OrderDate": 1, "ProductCategory": 0, "Quantity": 1, "UnitPrice": 1, "PaymentMethod": 0}, "cleaned_categories": {"ProductCategory": ["Electronics", "Home Appliances", "Fashion", "Beauty", "Sports", "Toys"], "PaymentMethod": ["Credit Card", "PayPal", "Debit Card", "Cash"]}, "invalid_numeric": {"Quantity": [4], "UnitPrice": [8]}, "descriptive_statistics": {"Quantity": {"count": 14, "mean": 2.43, "std": 1.38, "min": 1, "max": 5}, "UnitPrice": {"count": 13, "mean": 123.85, "std": 116.52, "min": 15, "max": 299.99}, "TotalSales": {"count": 13, "mean": 303.35, "std": 350.22, "min": 25, "max": 599.98}}, "value_counts": {"ProductCategory": {"Fashion": 3, "Electronics": 2, "Home Appliances": 2, "Beauty": 2, "Sports": 2, "Toys": 1}, "PaymentMethod": {"Credit Card": 4, "PayPal": 3, "Debit Card": 2, "Cash": 2}}, "top_categories_by_sales": {"Electronics": 899.97, "Home Appliances": 479.0, "Sports": 430.0}, "correlations": {"Quantity_UnitPrice_correlation": -0.12}}}
{"purpose": "Analyze user engagement patterns and post characteristics on a social media platform.", "raw_table": "post_id,user_id,post_date,post_type,likes,comments,shares\n101,u123,2023-03-15,Photo,120,15,8\n102,U124,15/03/2023,video,200,30,12\n103,u125,2023/03/17,Text,50,,5\n104,u126,03-18-2023,PHOTO,180,25,10\n105,u127,,Link,90,10,3\n106,u128,2023-03-19,video,NaN,20,7\n107,U129,2023/3/20,text,70,5,2\n108,u130,2023-03-21,Photo,130,18,NaN\n109,u131,2023-03-22,Video,210,35,15\n110,U132,2023-3-23,link,40,4,1\n111,u133,2023-03-24,photo,110,12,6\n112,u134,03/25/2023,Text,60,NaN,4\n113,U135,2023-03-26,Video,NaN,28,9\n114,u136,2023-03-27,Link,55,7,3", "eda_steps": ["Standardize the date format in the post_date column to YYYY-MM-DD", "Convert post_type values to consistent capitalization", "Calculate the percentage of missing values per column", "Compute descriptive statistics for numeric columns: likes, comments, shares", "Generate value counts for post_type", "Identify posts with missing engagement metrics (likes, comments, shares)", "Find correlation coefficients between likes, comments, and shares", "Determine top 3 post types by average likes", "Summarize the distribution skewness for likes"], "eda_results": {"missing_values": {"post_id": 0, "user_id": 0, "post_date": 1, "post_type": 0, "likes": 2, "comments": 2, "shares": 1}, "value_counts": {"post_type": {"Photo": 4, "Video": 4, "Text": 3, "Link": 3}}, "summary_stats": {"likes": {"count": 12, "mean": 110.83, "std": 60.15, "min": 40, "25%": 57.5, "50%": 110, "75%": 180, "max": 210}, "comments": {"count": 12, "mean": 16.42, "std": 10.07, "min": 4, "25%": 7, "50%": 15, "75%": 25, "max": 35}, "shares": {"count": 13, "mean": 7.23, "std": 4.17, "min": 1, "25%": 4, "50%": 7, "75%": 10, "max": 15}}, "posts_with_missing_engagement": [103, 105, 106, 108, 112, 113], "correlations": {"likes_comments": 0.94, "likes_shares": 0.89, "comments_shares": 0.91}, "top_post_types_by_avg_likes": {"Video": 155, "Photo": 135, "Text": 60, "Link": 61.67}, "likes_skewness": 0.45}}
{"purpose": "Analyze user engagement patterns and content types on a social media platform to identify activity trends and data quality issues.", "raw_table": "post_id,user_id,post_date,content_type,likes,comments,shares\n101,U001,2023-01-15,Image,120,15,5\n102,u002,15/01/2023,video,200,,10\n103,U003,2023/01/16,text,85,5,2\n104,U004,,IMAGE,90,3,1\n105,u005,2023-01-17,Text,NaN,8,0\n106,U006,2023-1-18,video,150,12,7\n107,U007,2023-01-19,Link,20,0,0\n108,U008,2023-01-19,text,70,NaN,3\n109,u009,01-20-2023,Video,180,20,\n110,U010,2023-01-21,image,130,10,4\n111,U011,2023-01-22,video,NaN,NaN,NaN\n112,u012,2023-01-23,text,95,,6\n113,U013,2023-01-24,unknown,50,2,1", "eda_steps": ["Check for missing values in each column and calculate their percentages", "Standardize the content_type column to lowercase", "Convert post_date to a consistent datetime format and identify invalid or missing dates", "Calculate descriptive statistics (mean, median, std) for likes, comments, and shares", "Generate value counts for the content_type column", "Identify posts with missing metrics (likes, comments, shares)", "Analyze correlation between likes, comments, and shares", "Find top 3 users by total likes accumulated", "Summarize the distribution skewness of likes"], "eda_results": {"missing_values": {"post_id": "0%", "user_id": "0%", "post_date": "7.7%", "content_type": "0%", "likes": "15.4%", "comments": "23.1%", "shares": "15.4%"}, "content_type_value_counts": {"image": 3, "video": 4, "text": 4, "link": 1, "unknown": 1}, "post_date_issues": {"invalid_or_missing": [104]}, "likes_statistics": {"mean": 106.25, "median": 95, "std_dev": 58.5, "skewness": 0.85}, "comments_statistics": {"mean": 8, "median": 8, "std_dev": 6.1}, "shares_statistics": {"mean": 3.7, "median": 3, "std_dev": 3.0}, "missing_metrics_rows": {"likes": [105, 111], "comments": [102, 108, 111, 112], "shares": [109, 111]}, "correlations": {"likes_comments": 0.89, "likes_shares": 0.82, "comments_shares": 0.76}, "top_users_by_likes": {"u002": 200, "u009": 180, "u006": 150}}}
{"purpose": "Analyze trip durations and patterns for a city's bike-sharing system to identify usage trends and data quality issues.", "raw_table": "Trip_ID,Start_Date,End_Date,User_Type,Start_Station,End_Station,Trip_Duration,Payment_Method\n1001,2023-03-01 08:15,2023-03-01 08:45,Subscriber,Central Park,Harbor View,30,Credit Card\n1002,03/02/2023 09:00 AM,03/02/2023 09:25 AM,casual,central park,Harbor view,25,Cash\n1003,2023-03-03 10:10,,Subscriber,Downtown,,35,Credit Card\n1004,2023-3-04 07:50,2023-03-04 08:20,Subscriber,downtown,Central Park,30,credit card\n1005,03-05-2023 18:15,03-05-2023 18:45,casual,Harbor View,Downtown,30,Cash\n1006,2023/03/06 20:00,2023/03/06 20:45,Subscriber,Harbor view,central park,45,Credit Card\n1007,2023-03-07 14:30,2023-03-07 15:05,casual,Central Park,Downtown,35,\n1008,2023-03-08 08:00,2023-03-08 08:30,Subscriber,Harbor View,Harbor View,30,Credit card\n1009,2023-03-09 11:15,2023-03-09 11:45,SUBSCRIBER,Central park,Harbor view,30,Cash\n1010,2023-03-10 09:45 AM,2023-03-10 10:10 AM,casual,,Downtown,25,Credit Card\n1011,2023-03-11 13:05,2023-03-11 13:40,Subscriber,Central Park,Downtown,35,Credit Card\n1012,2023-03-12 07:55,2023-03-12 08:25,Subscriber,Harbor View,Central Park,30,Credit Card\n1013,2023-03-13 16:20,2023-03-13 16:55,Casual,Downtown,Central Park,35,Cash\n1014,2023-03-14 18:00,,Subscriber,Central Park,Harbor View,,Credit Card\n", "eda_steps": ["Check missing value counts and percentages for all columns", "Standardize date formats in Start_Date and End_Date columns", "Calculate trip durations where missing using Start_Date and End_Date timestamps", "Compute descriptive statistics for Trip_Duration", "Generate value counts for User_Type, Start_Station, End_Station, and Payment_Method", "Identify inconsistent capitalization in categorical columns and normalize them", "Detect trips with zero or implausible durations", "Analyze correlation between User_Type and Trip_Duration", "Summarize frequency of payment methods"], "eda_results": {"missing_values": {"Trip_ID": 0, "Start_Date": 0, "End_Date": 2, "User_Type": 0, "Start_Station": 1, "End_Station": 1, "Trip_Duration": 2, "Payment_Method": 1}, "standardized_dates": {"Start_Date": "All converted to YYYY-MM-DD HH:mm format", "End_Date": "All converted to YYYY-MM-DD HH:mm format or marked missing"}, "imputed_trip_durations": {"Trip_ID_1003": 35, "Trip_ID_1014": null}, "trip_duration_stats": {"count": 14, "mean": 32.14, "std_dev": 6.5, "min": 25, "max": 45}, "value_counts": {"User_Type": {"Subscriber": 9, "Casual": 5}, "Start_Station": {"Central Park": 6, "Harbor View": 5, "Downtown": 3, "": 1}, "End_Station": {"Harbor View": 5, "Downtown": 5, "Central Park": 3, "": 1}, "Payment_Method": {"Credit Card": 9, "Cash": 4, "": 1}}, "capitalization_issues": {"User_Type": ["casual", "Casual", "SUBSCRIBER", "Subscriber"], "Start_Station": ["central park", "Central Park", "downtown", "Downtown", "harbor view", "Harbor View"], "End_Station": ["harbor view", "Harbor View", "central park", "Central Park", "downtown", "Downtown"], "Payment_Method": ["Credit card", "credit card", "Credit Card"]}, "implausible_durations": {"zero_or_negative": 0, "missing_trip_duration_with_missing_end_date": 1}, "correlation_UserType_TripDuration": {"Subscriber_mean_duration": 32.78, "Casual_mean_duration": 31.2}, "payment_method_summary": {"Credit Card": 9, "Cash": 4, "Missing": 1}}}
{"purpose": "Analyze streaming platform movie performance and viewer ratings trends.", "raw_table": "MovieID,Title,Genre,ReleaseDate,Duration,AvgRating,TotalViews,Language\n101,The Last Quest,Action,2020-07-15,120,4.5,15000,english\n102,Love & War,Romance,2019/12/01,95,4.2,NaN,English\n103,space odyssey,SCi-Fi,15-Aug-2021,130,4.8,18000,ENGLISH\n104,Haunted Nights,Horror,2021-10-31,100,3.9,8000,english\n105,The Pianist,Drama,2018-05-20,140,4.7,12500,\n106,Crazy Laughs,Comedy,2020-11-25,NaN,4.1,11000,English\n107,Unknown File,Thriller,2020-01-15,110,3.5,7000,english\n108,The last quest,action,07-15-2020,120,4.6,15500,English\n109,Midnight Run,Action,2021-2-30,115,4.3,14000,english\n110,Silent Whisper,Drama,2019-03-10,130,NaN,9000,English\n111,Funky Town,Comedy,2019-07-20,105,3.8,9500,enGLISH\n112,Alien World,SCI-Fi,2021/08/15,135,4.9,20000,English\n", "eda_steps": ["Check for missing values in each column", "Standardize Genre and Language capitalization", "Convert ReleaseDate to a consistent date format and identify invalid dates", "Compute descriptive statistics for Duration, AvgRating, and TotalViews", "Generate value counts for Genre", "Identify duplicate movie titles considering case insensitivity", "Calculate correlation between AvgRating and TotalViews", "List movies with missing Duration or AvgRating"], "eda_results": {"missing_values": {"MovieID": 0, "Title": 0, "Genre": 0, "ReleaseDate": 0, "Duration": 2, "AvgRating": 2, "TotalViews": 1, "Language": 1}, "standardized_genres": {"Action": 3, "Romance": 1, "Sci-Fi": 3, "Horror": 1, "Drama": 2, "Comedy": 2, "Thriller": 1}, "standardized_languages": {"English": 12, "": 1}, "invalid_dates": ["2021-2-30"], "descriptive_stats": {"Duration": {"count": 13, "mean": 117.92, "min": 95, "max": 140}, "AvgRating": {"count": 12, "mean": 4.27, "min": 3.5, "max": 4.9}, "TotalViews": {"count": 14, "mean": 12635.71, "min": 7000, "max": 20000}}, "duplicate_titles": ["The Last Quest"], "genre_value_counts": {"Action": 3, "Sci-Fi": 3, "Drama": 2, "Comedy": 2, "Romance": 1, "Horror": 1, "Thriller": 1}, "correlation_AvgRating_TotalViews": 0.78, "movies_missing_duration_or_rating": [{"MovieID": 106, "Title": "Crazy Laughs", "Missing": ["Duration"]}, {"MovieID": 110, "Title": "Silent Whisper", "Missing": ["AvgRating"]}]}}
{"purpose": "Assess public transportation usage patterns and identify data quality issues in ridership records.", "raw_table": "Route_ID,Date,Passenger_Count,Day_Type,Weather_Condition,On_Time_Percentage\n101,2023-04-01,230,Weekday,CLEAR,95\n102,04/02/2023,,Weekend,Rainy,88\n103,2023/04/03,150,Weekday,cloudy,90\n101,April 04 2023,245,weekday,CLEAR,92\n104,2023-4-5,NaN,Weekday,Snow,85\n102,2023-04-06,180,weekend,RAINY,89\n103,2023-04-07,160,,Cloudy,91\n105,2023-04-08,100,Holiday,CLEAR,NaN\n101,2023-4-09,230,WeekDay,Clear,93\n104,2023-04-10,NaN,Weekend,snow,87\n105,,90,holiday,CLEAR,90\n102,2023-04-12,200,Weekend,rainy,88\n103,2023-04-13,155,Weekday,Cloudy,92\n", "eda_steps": ["Check the percentage of missing values in each column", "Standardize date formats to YYYY-MM-DD", "Convert all categorical columns to consistent capitalization", "Compute descriptive statistics for numeric columns: Passenger_Count and On_Time_Percentage", "Generate value counts for Day_Type and Weather_Condition columns", "Identify rows with missing or NaN Passenger_Count", "Analyze on-time performance averages by Route_ID", "Identify any inconsistent or unusual categories in Day_Type", "Summarize distribution skewness for Passenger_Count"], "eda_results": {"missing_values": {"Route_ID": 0, "Date": 1, "Passenger_Count": 3, "Day_Type": 1, "Weather_Condition": 0, "On_Time_Percentage": 1}, "date_standardization": "All dates converted to YYYY-MM-DD format, missing date remains as NaN", "categorical_standardization": {"Day_Type": ["Weekday", "Weekend", "Holiday", null], "Weather_Condition": ["Clear", "Rainy", "Cloudy", "Snow"]}, "summary_stats": {"Passenger_Count": {"count": 11, "mean": 181.36, "std": 50.78, "min": 90, "25%": 150, "50%": 180, "75%": 230, "max": 245, "skewness": 0.11}, "On_Time_Percentage": {"count": 12, "mean": 90.83, "std": 3.31, "min": 85, "25%": 88, "50%": 90.5, "75%": 92.75, "max": 95}}, "value_counts": {"Day_Type": {"Weekday": 6, "Weekend": 4, "Holiday": 2, "null": 1}, "Weather_Condition": {"Clear": 5, "Rainy": 3, "Cloudy": 3, "Snow": 2}}, "missing_passenger_rows": [2, 4, 9], "on_time_performance_by_route": {"101": 93.33, "102": 88.33, "103": 91.0, "104": 86.0, "105": 90.0}, "unusual_day_type_categories": ["null"], "notes": "Passenger_Count data is missing in three records; Day_Type has inconsistent capitalization and one missing value; dates were inconsistently formatted but standardized successfully."}}
{"purpose": "Analyze customer call and data usage patterns to identify trends and data quality issues in telecom service records.", "raw_table": "Customer_ID,Call_Duration_Minutes,Data_Usage_MB,Plan_Type,Last_Active_Date,Customer_Satisfaction\nc001, 34,  1200, premium, 2023/01/15, 4\nC002, 45,  1500, Basic, 15-02-2023, 5\nc003, ,  700, PREMIUM, 2023-03-01, 3\nC004, 20, fifteen hundred, standard, 2023/04/10, 2\nc005, 55, 1100, basic, , 4\nc006, 40, 1300, Standard, 2023/04/05, NaN\nc007, 25, 900, Premium, 2023/02/28, 5\nc008, 30, , Basic, 2023-02-15, 4\nC009, 60, 1600, standard, 03/20/2023, 3\nc010, twenty, 1400, Basic, 2023-01-25, 5\nc011, 35, 1000, premium, 2023-03-10, 4\nc012, 28, 1100, Standard, 2023-04-01, 3", "eda_steps": ["Check for missing values in each column", "Standardize the 'Plan_Type' column capitalization", "Convert 'Call_Duration_Minutes' and 'Data_Usage_MB' to numeric, handling non-numeric entries", "Parse and standardize 'Last_Active_Date' to ISO format yyyy-mm-dd", "Compute descriptive statistics for numeric columns: Call_Duration_Minutes, Data_Usage_MB, Customer_Satisfaction", "Generate value counts for the 'Plan_Type' column", "Calculate the correlation matrix for numeric columns", "Identify rows with inconsistent or missing 'Customer_Satisfaction' values"], "eda_results": {"missing_values": {"Customer_ID": 0, "Call_Duration_Minutes": 1, "Data_Usage_MB": 2, "Plan_Type": 0, "Last_Active_Date": 1, "Customer_Satisfaction": 1}, "plan_type_value_counts": {"premium": 4, "basic": 4, "standard": 4}, "numeric_conversion_issues": {"Call_Duration_Minutes": ["c010"], "Data_Usage_MB": ["c004"]}, "standardized_dates": ["2023-01-15", "2023-02-15", "2023-03-01", "2023-04-10", null, "2023-04-05", "2023-02-28", "2023-02-15", "2023-03-20", "2023-01-25", "2023-03-10", "2023-04-01"], "summary_stats": {"Call_Duration_Minutes": {"count": 11, "mean": 36.0, "std": 11.6, "min": 20, "25%": 27.0, "50%": 34.0, "75%": 44.0, "max": 60}, "Data_Usage_MB": {"count": 10, "mean": 1210.0, "std": 296.4, "min": 700, "25%": 1050, "50%": 1150, "75%": 1400, "max": 1600}, "Customer_Satisfaction": {"count": 11, "mean": 3.9, "std": 1.0, "min": 2, "25%": 3, "50%": 4, "75%": 5, "max": 5}}, "correlations": {"Call_Duration_Minutes": {"Data_Usage_MB": 0.87, "Customer_Satisfaction": -0.25}, "Data_Usage_MB": {"Customer_Satisfaction": -0.15}}, "rows_with_missing_satisfaction": ["c006"]}}
{"purpose": "Investigate crop yield patterns and soil quality factors influencing wheat production.", "raw_table": "Farm_ID,Date_Planted,Crop_Type,Soil_pH,Soil_Type,Yield_kg,irrigated\n001,2023/03/15,Wheat,6.5,Loamy,1200,Yes\n002,15-Mar-2023,wheat,6.8,Clay,1150,yes\n003,2023-03-17,,Sandy,6.2,980,No\n004,03/18/23,Barley,7.0,Loamy,1100,NO\n005,Mar 19 2023,WHEAT,6.7,,1300,Yes\n006,2023.03.20,Wheat,6.4,Sandy,NaN,Yes\n007,2023-03-21,Barley,6.9,Clay,1080,No\n008,19/03/2023,wheat,6.6,Loamy,1250,YEs\n009,2023-03-20,Barley,7.1,Sandy,1025,No\n010,2023/03/22,Wheat,6.3,Loamy,1190,yes", "eda_steps": ["Standardize Crop_Type values to consistent capitalization", "Parse and unify Date_Planted into ISO date format YYYY-MM-DD", "Check and report missing values across columns", "Calculate descriptive statistics for Yield_kg and Soil_pH", "Generate value counts for Crop_Type and Soil_Type", "Analyze Yield_kg differences between irrigated and non-irrigated farms", "Compute correlation between Soil_pH and Yield_kg", "Identify any duplicate Farm_ID entries"], "eda_results": {"missing_values": {"Crop_Type": 1, "Soil_Type": 1, "Yield_kg": 1}, "value_counts": {"Crop_Type": {"Wheat": 6, "Barley": 3}, "Soil_Type": {"Loamy": 4, "Clay": 2, "Sandy": 3, "Missing": 1}}, "summary_stats": {"Yield_kg": {"count": 9, "mean": 1136.67, "std_dev": 110.56, "min": 980, "max": 1300}, "Soil_pH": {"count": 10, "mean": 6.65, "std_dev": 0.28, "min": 6.2, "max": 7.1}}, "yield_irrigation_comparison": {"irrigated_mean_yield": 1216.67, "non_irrigated_mean_yield": 1035}, "correlations": {"Soil_pH_vs_Yield_kg": 0.42}, "duplicates": {"Farm_ID_duplicates": 0}, "date_format_standardization": "All Date_Planted values converted to YYYY-MM-DD"}}
{"purpose": "Analyze crop yield patterns and environmental factors in different farm regions.", "raw_table": "Farm_ID,Crop_Type,Planting_Date,Harvest_Date,Yield_Tons,Rainfall_mm,Soil_Quality\nF001,wheat,2023-03-15,2023-09-20,25.4,120.5,Good\nf002,Corn,03/20/2023,09/25/2023,30.1,110.2,Moderate\nF003,Rice,,10-01-2023,28.7,150.0,Excellent\nf004,Wheat,2023/03/18,2023-09-22,24.9,,Good\nF005,CORN,March 22 2023,09-27-2023,29.5,105.3,Poor\nf006,Rice,2023-03-25,Sep 30 2023,,160.1,excellent\nF007,barley,2023-03-17,2023-09-24,15.2,95.7,Moderate\nF008,Barley,2023-03-19,09/26/2023,16.0,98.3,moderate\nf009,Wheat,2023-03-16,2023-09-23,26.0,115.0,good\nF010,Corn,2023-03-21,2023-09-28,31.0,108.7,Poor", "eda_steps": ["Check for missing values in each column", "Standardize Crop_Type and Soil_Quality capitalization", "Convert Planting_Date and Harvest_Date to datetime format", "Calculate planting to harvest duration in days", "Compute descriptive statistics for Yield_Tons and Rainfall_mm", "Generate value counts for Crop_Type and Soil_Quality", "Identify correlation between Yield_Tons and Rainfall_mm", "Find average Yield_Tons per Crop_Type", "Summarize missing data patterns"], "eda_results": {"missing_values": {"Farm_ID": 0, "Crop_Type": 0, "Planting_Date": 1, "Harvest_Date": 0, "Yield_Tons": 1, "Rainfall_mm": 1, "Soil_Quality": 0}, "standardized_categories": {"Crop_Type": {"Wheat": 3, "Corn": 3, "Rice": 2, "Barley": 2}, "Soil_Quality": {"Good": 3, "Moderate": 3, "Excellent": 2, "Poor": 2}}, "date_conversion_issues": {"Planting_Date": ["F003"], "Harvest_Date": []}, "plant_to_harvest_duration_days": {"mean": 190, "min": 187, "max": 193, "missing": 1}, "summary_stats": {"Yield_Tons": {"count": 9, "mean": 24.9, "std": 5.5, "min": 15.2, "max": 31.0}, "Rainfall_mm": {"count": 9, "mean": 116.2, "std": 20.1, "min": 95.7, "max": 160.1}}, "value_counts": {"Crop_Type": {"Wheat": 3, "Corn": 3, "Rice": 2, "Barley": 2}, "Soil_Quality": {"Good": 3, "Moderate": 3, "Excellent": 2, "Poor": 2}}, "correlations": {"Yield_Tons_Rainfall_mm": 0.57}, "average_yield_per_crop": {"Wheat": 25.43, "Corn": 30.2, "Rice": 28.7, "Barley": 15.6}, "missing_data_summary": "Planting_Date is missing for 1 farm (F003), Yield_Tons missing for 1 farm (F006), Rainfall_mm missing for 1 farm (F004)."}}
{"purpose": "Examine residential electricity consumption patterns and data quality issues over time.", "raw_table": "CustomerID,Date,Consumption_kWh,Region,PaymentStatus\n001,2023-01-10,350,North,PAID\n002,10/01/2023,420,South,Paid\n003,2023-01-12,,east,unpaid\n004,2023-1-13,390,West,PAID\n005,2023/01/14,480,north,Pending\n006,2023-01-15,NaN,SOUTH,paid\n007,15-01-2023,510,East,UnPaid\n008,2023-01-16,470,west,PAID\n009,2023-01-17,445,,paid\n010,01-18-2023,430,South,PAID\n011,2023-01-19,NaN,North,paid\n012,2023/01/20,460,East,Pending\n013,2023-01-21,495,South,PAID\n014,2023-01-22,480,west,PAID", "eda_steps": ["Convert all date entries to a consistent date format", "Standardize region names to title case and correct spelling inconsistencies", "Identify and count missing values in each column", "Compute descriptive statistics for Consumption_kWh, ignoring missing values", "Generate value counts for the PaymentStatus column after standardizing case", "Find correlation between Consumption_kWh and Region encoded as categories", "Identify top 2 regions by average electricity consumption", "Check for duplicate CustomerID entries", "Visualize the distribution skewness of Consumption_kWh"], "eda_results": {"missing_values": {"CustomerID": 0, "Date": 0, "Consumption_kWh": 3, "Region": 1, "PaymentStatus": 0}, "summary_stats": {"Consumption_kWh": {"count": 11, "mean": 455.45, "std": 52.35, "min": 350, "25%": 430, "50%": 460, "75%": 480, "max": 510, "skewness": 0.45}}, "value_counts": {"PaymentStatus": {"PAID": 7, "PAID (standardized)": 7, "UNPAID": 2, "PENDING": 2}}, "region_value_counts": {"North": 3, "South": 4, "East": 3, "West": 3, "Missing": 1}, "top_regions_by_avg_consumption": {"East": 486.67, "South": 461.25}, "duplicate_customer_ids": 0, "correlations": {"Consumption_kWh_vs_RegionCode": 0.12}}}
{"purpose": "Analyze patient demographic and clinical characteristics in a small outpatient clinic dataset.", "raw_table": "Patient_ID,Age,Gender,Visit_Date,Diagnosis,Blood_Pressure,Cholesterol_Level,Smoker\n001,34,Male,2023/01/15,Hypertension,130/85,200,Yes\n002,48,Female,15-02-2023,Diabetes,140/90,NA,No\n003,,female,2023.03.10,hyperTension,135/88,180,YES\n004,52,M,03/20/23,Asthma,,220,No\n005,41,F,2023/04/01,Diabetes,145/95,210,yes\n006,37,Male,2023-04-15,Hypertension,128/82,,No\n007,29,female,2023/05/10,Asthma,120/80,195,\n008,45,Female,2023-05-25,Other,130/85,205,NO\n009,50,Male,2023-06-01,Hypertension,138/90,215,Yes\n010,38,F,06/15/2023,diabetes,142/92,198,No", "eda_steps": ["Check and summarize missing values for each column", "Standardize and count unique values in the Gender column", "Count number of patients per Diagnosis category", "Calculate descriptive statistics for Age and Cholesterol_Level columns", "Extract systolic and diastolic values from Blood_Pressure and compute their mean", "Calculate proportion of smokers vs non-smokers", "Verify and standardize Visit_Date formats"], "eda_results": {"missing_values": {"Patient_ID": 0, "Age": 1, "Gender": 0, "Visit_Date": 0, "Diagnosis": 0, "Blood_Pressure": 1, "Cholesterol_Level": 2, "Smoker": 1}, "gender_counts": {"Male": 4, "Female": 5, "M": 1, "F": 2, "female": 2, "f": 1}, "diagnosis_counts": {"Hypertension": 4, "Diabetes": 3, "Asthma": 2, "Other": 1, "hyperTension": 1, "diabetes": 1}, "age_stats": {"count": 14, "mean": 41.43, "min": 29, "max": 52}, "cholesterol_level_stats": {"count": 11, "mean": 204.18, "min": 180, "max": 220}, "blood_pressure_means": {"systolic_mean": 132.44, "diastolic_mean": 87.22, "count": 13}, "smoker_counts": {"Yes": 4, "No": 5, "Missing": 1}, "visit_date_formats": {"YYYY/MM/DD": 4, "DD-MM-YYYY": 1, "YYYY.MM.DD": 1, "MM/DD/YY": 1, "YYYY-MM-DD": 3, "MM/DD/YYYY": 2}}}
{"purpose": "Analyze customer purchase behavior and product category popularity in a small ecommerce dataset.", "raw_table": "OrderID,CustomerID,OrderDate,ProductCategory,Quantity,Price,PaymentMethod\n1001,cust01,2023/01/15,Electronics,2,299.99,Credit Card\n1002,CUST02,15-01-2023,Fashion,1,49.5,CASH\n1003,cust03,2023-01-16,Home & Garden,3,,paypal\n1004,cust04,,electronics,1,199.99,Credit card\n1005,CUST05,2023/01/17,fashion,2,75.0,Cash\n1006,cust06,2023-01-18,Books,1,15.99,Paypal\n1007,cust07,18/01/2023,Toys,5,9.99,CREDIT CARD\n1008,cust08,2023/01/19,Home & garden,2,45,Cash\n1009,CUST09,2023-01-20,ELECTRONICS,1,99.99,credit card\n1010,cust10,2023/01/21,Books,2,20.00,paypal\n1011,cust11,2023-01-22,Toys,,12,Credit Card\n1012,cust12,01-23-2023,Electronics,1,250,Cash\n1013,cust13,2023/01/24,Fashion,3,60,Paypal\n1014,cust14,2023-01-25,,1,15.5,Credit card", "eda_steps": ["Standardize the ProductCategory values to have consistent capitalization", "Check and report the percentage of missing values for each column", "Parse and unify the OrderDate format to YYYY-MM-DD", "Generate descriptive statistics for Quantity and Price columns", "Calculate total sales (Quantity * Price) per ProductCategory", "Identify the most frequent PaymentMethod used", "Find the top 3 ProductCategories by number of orders"], "eda_results": {"missing_values": {"OrderID": "0%", "CustomerID": "0%", "OrderDate": "7%", "ProductCategory": "7%", "Quantity": "7%", "Price": "7%", "PaymentMethod": "0%"}, "standardized_product_categories": ["Electronics", "Fashion", "Home & Garden", "Electronics", "Fashion", "Books", "Toys", "Home & Garden", "Electronics", "Books", "Toys", "Electronics", "Fashion", null], "parsed_order_dates": ["2023-01-15", "2023-01-15", "2023-01-16", null, "2023-01-17", "2023-01-18", "2023-01-18", "2023-01-19", "2023-01-20", "2023-01-21", "2023-01-22", "2023-01-23", "2023-01-24", "2023-01-25"], "summary_stats": {"Quantity": {"count": 13, "mean": 1.92, "std": 1.34, "min": 1, "max": 5}, "Price": {"count": 13, "mean": 92.48, "std": 103.84, "min": 9.99, "max": 299.99}}, "total_sales_per_category": {"Electronics": 1249.96, "Fashion": 334.5, "Home & Garden": 135.0, "Books": 55.99, "Toys": 69.93, "null": 15.5}, "most_frequent_payment_method": "Credit Card", "top_3_product_categories_by_orders": ["Electronics", "Fashion", "Home & Garden"]}}
{"purpose": "Analyze student performance and attendance patterns to identify factors affecting grades.", "raw_table": "StudentID,Name,Gender,Age,EnrollmentDate,MathScore,EnglishScore,AttendanceRate,Extracurricular\n101,alice,F,15,2021/09/01,78,85,0.95,Basketball\n102,bob,M,16,09-05-2021,88,NaN,0.89,Football\n103,CHARLIE,M,,2021-09-10,92,80,0.92,\n104,Deena,F,15,2021/9/07,NaN,75,0.85,Drama\n105,elijah,M,17,2021/09/01,70,68,missing,football\n106,Fiona,F,16,2021/09/12,85,95,0.97,BASKETBALL\n107,George,M,15,2021-09-03,60,55,0.78,\n108,Hannah,F,16,09/15/2021,93,88,0.99,Drama\n109,Ismail,M,16,20210901,85,82,0.90,Soccer\n110,Jenny,F,15,,65,70,0.80,Basketball\n111,kyle,M,17,2021/09/05,NA,78,0.88,Soccer", "eda_steps": ["Check for missing values in each column and calculate their percentages", "Standardize the EnrollmentDate column to a uniform date format", "Compute descriptive statistics for numeric columns: Age, MathScore, EnglishScore, AttendanceRate", "Generate value counts for Gender and Extracurricular activity columns", "Identify rows with inconsistent or missing scores", "Calculate correlation matrix for numeric score columns and AttendanceRate", "Find top 3 most common extracurricular activities", "Summarize distribution skewness for MathScore and EnglishScore"], "eda_results": {"missing_values": {"StudentID": "0%", "Name": "0%", "Gender": "0%", "Age": "9.1%", "EnrollmentDate": "9.1%", "MathScore": "18.2%", "EnglishScore": "9.1%", "AttendanceRate": "9.1%", "Extracurricular": "27.3%"}, "standardized_dates_sample": {"101": "2021-09-01", "102": "2021-09-05", "103": "2021-09-10", "104": "2021-09-07", "105": "2021-09-01", "106": "2021-09-12", "107": "2021-09-03", "108": "2021-09-15", "109": "2021-09-01", "110": null, "111": "2021-09-05"}, "summary_stats": {"Age": {"count": 11, "mean": 15.82, "std": 0.83, "min": 15, "max": 17}, "MathScore": {"count": 9, "mean": 78.11, "std": 11.02, "min": 60, "max": 93}, "EnglishScore": {"count": 10, "mean": 77.6, "std": 11.06, "min": 55, "max": 95}, "AttendanceRate": {"count": 10, "mean": 0.88, "std": 0.07, "min": 0.78, "max": 0.99}}, "value_counts": {"Gender": {"F": 5, "M": 6}, "Extracurricular": {"Basketball": 3, "Football": 2, "Drama": 2, "Soccer": 2, "": 2}}, "inconsistent_scores_rows": [102, 104, 105, 110, 111], "correlations": {"MathScore_vs_EnglishScore": 0.82, "MathScore_vs_AttendanceRate": 0.45, "EnglishScore_vs_AttendanceRate": 0.5}, "top_categories": {"Extracurricular": [{"Basketball": 3}, {"Football": 2}, {"Drama": 2}]}, "skewness": {"MathScore": -0.35, "EnglishScore": -0.4}}}
{"purpose": "Explore the characteristics and data quality of recent real estate property listings to understand pricing patterns and data inconsistencies.", "raw_table": "ListingID,City,Neighborhood,Price,SquareFeet,Bedrooms,Bathrooms,DateListed,PropertyType\n101,New York,Manhattan,950000,850,2,1.5,2023-01-15,Condo\n102,new york,brooklyn,750000,900,3,2,15/02/2023,Apartment\n103,Los Angeles,Downtown,NaN,1100,2,2,2023/03/05,Loft\n104,los angeles,Hollywood,850000,1050,,2,03-20-2023,House\n105,Chicago,Lincoln Park,620000,950,3,2,2023-04-10,Apartment\n106,Chicago,lincoln park,580000,900,2,1.5,2023-04-15,Apartment\n107,Miami,South Beach,780000,,3,2,2023-05-01,Condo\n108,Miami,South Beach,790000,980,3,Two,05/10/2023,condo\n109,New York,Brooklyn,695000,870,2,1,2023-02-20,Apartment\n110,Chicago,Wicker Park,NaN,800,1,1,2023-04-25,Apartment\n111,Los Angeles,Downtown,890000,1150,3,2,2023-03-10,Loft\n112,miami,South Beach,805000,1000,3,2,2023/05/15,Condo", "eda_steps": ["Check for missing values in each column", "Standardize city and neighborhood names to consistent capitalization", "Convert date formats to a uniform standard", "Calculate descriptive statistics for numeric columns: Price, SquareFeet, Bedrooms, Bathrooms", "Generate value counts for categorical columns: City, Neighborhood, PropertyType", "Identify rows with inconsistent data types, especially in Bathrooms", "Compute correlation matrix between numeric features", "Find top 3 neighborhoods by average price", "Summarize the percentage of missing data by column"], "eda_results": {"missing_values": {"ListingID": 0, "City": 0, "Neighborhood": 0, "Price": 2, "SquareFeet": 1, "Bedrooms": 1, "Bathrooms": 1, "DateListed": 0, "PropertyType": 0}, "standardized_categories": {"City": {"New York": 3, "Los Angeles": 3, "Chicago": 3, "Miami": 3}, "Neighborhood": {"Manhattan": 1, "Brooklyn": 2, "Downtown": 2, "Hollywood": 1, "Lincoln Park": 2, "South Beach": 3, "Wicker Park": 1}, "PropertyType": {"Condo": 4, "Apartment": 5, "Loft": 2, "House": 1}}, "date_standardization": {"formats_detected": ["YYYY-MM-DD", "DD/MM/YYYY", "YYYY/MM/DD", "MM-DD-YYYY", "DD/MM/YYYY"], "all_dates_converted": true}, "descriptive_statistics": {"Price": {"count": 13, "mean": 774615.38, "std": 120021.39, "min": 580000, "25%": 695000, "50%": 780000, "75%": 850000, "max": 950000}, "SquareFeet": {"count": 12, "mean": 953.33, "std": 114.73, "min": 800, "25%": 870, "50%": 950, "75%": 1050, "max": 1150}, "Bedrooms": {"count": 13, "mean": 2.38, "std": 0.67, "min": 1, "25%": 2, "50%": 3, "75%": 3, "max": 3}, "Bathrooms": {"count": 12, "mean": 1.69, "std": 0.38, "min": 1, "25%": 1.5, "50%": 2, "75%": 2, "max": 2}}, "inconsistent_data_types": {"Bathrooms": ["Two at ListingID 108"]}, "correlations": {"Price-SquareFeet": 0.82, "Price-Bedrooms": 0.65, "Price-Bathrooms": 0.6, "SquareFeet-Bedrooms": 0.75, "SquareFeet-Bathrooms": 0.7}, "top_neighborhoods_by_avg_price": {"Manhattan": 950000, "Hollywood": 850000, "Downtown": 890000}, "missing_data_percentage": {"Price": 14.29, "SquareFeet": 7.14, "Bedrooms": 7.14, "Bathrooms": 7.14}}}
{"purpose": "Examine public transportation usage and on-time performance for city bus routes to identify delays and ridership patterns.", "raw_table": "Route_ID,Route_Name,Date,Boardings,On_Time_Percentage,Driver_Name\n101,Downtown Express,2023/01/15,350,89.5,John smith\n102,Uptown Local,15-01-2023,275,82.0,mary jones\n103,Eastside Loop,2023-01-15,NA,78.3,ALICE Johnson\n104,West End,01/15/2023,150,,bob davis\n105,Suburban Connector,2023/1/15,200,85.0,Chris O'Conner\n106,Central Line,2023-01-16,400,92.4,Kate brown\n101,downtown express,2023/01/16,360,90.1,John Smith\n102,Uptown Local,2023-01-16,280,81.5,Mary Jones\n103,Eastside Loop,16-01-2023,190,77.0,Alice Johnson\n104,west end,2023/01/16,155,80.0,Bob Davis\n105,Suburban Connector,2023/01/16,205,83.5,Chris O'conner\n106,Central Line,01/16/2023,410,91.0,Kate Brown\n", "eda_steps": ["Standardize date formats to YYYY-MM-DD", "Normalize Route_Name capitalization", "Calculate missing value counts per column", "Compute descriptive statistics for numeric columns Boardings and On_Time_Percentage", "Generate value counts for categorical columns Route_ID and Driver_Name", "Identify top 3 routes by average boardings", "Check correlation between Boardings and On_Time_Percentage"], "eda_results": {"missing_values": {"Route_ID": 0, "Route_Name": 0, "Date": 0, "Boardings": 1, "On_Time_Percentage": 1, "Driver_Name": 0}, "value_counts": {"Route_ID": {"101": 2, "102": 2, "103": 2, "104": 2, "105": 2, "106": 2}, "Driver_Name": {"John Smith": 2, "Mary Jones": 2, "Alice Johnson": 2, "Bob Davis": 2, "Chris O'Conner": 2, "Kate Brown": 2}}, "summary_stats": {"Boardings": {"count": 13, "mean": 281.5, "std": 92.9, "min": 150, "25%": 195, "50%": 275, "75%": 360, "max": 410}, "On_Time_Percentage": {"count": 13, "mean": 84.4, "std": 5.1, "min": 77.0, "25%": 81.3, "50%": 83.5, "75%": 89.8, "max": 92.4}}, "top_categories": {"Top_3_Routes_by_Avg_Boardings": {"Central Line": 405.0, "Downtown Express": 355.0, "Eastside Loop": 190.0}}, "correlations": {"Boardings_vs_On_Time_Percentage": -0.28}}}
{"purpose": "Analyze monthly temperature and precipitation patterns across different climate zones to identify missing data and basic statistical trends.", "raw_table": "Date,Climate_Zone,Avg_Temp_C,Precip_mm\n2023-01-15, Tropical, 27.4, 210\n2023/02/15, tropical, 28.1, 180\n2023-03-15, Temperate, 15.8, 95\n2023-04-15, temperate, 18.2, 80\n2023/05/15, Arid, 30.5, \n2023-06-15, Arid, thirty-two, 10\n2023-07-15,polar, -5.6, 50\n2023-08/15, Polar, -3.2, 45\n2023-09-15, TROPICAL, 26.9, 220\n2023-10-15, Temperate, 14.7, 105\n2023-11-15,Arid,, 5\n2023-12-15,Polar, -7.1, \n", "eda_steps": ["Standardize the Climate_Zone entries to have consistent capitalization.", "Convert Date column to a uniform date format.", "Identify and count missing values in each column.", "Convert Avg_Temp_C to numeric, handling non-numeric entries appropriately.", "Compute descriptive statistics (mean, median, min, max) for Avg_Temp_C and Precip_mm.", "Generate value counts for Climate_Zone after standardization.", "Check correlation between Avg_Temp_C and Precip_mm.", "Identify months with missing temperature or precipitation data."], "eda_results": {"missing_values": {"Date": 0, "Climate_Zone": 0, "Avg_Temp_C": 2, "Precip_mm": 2}, "value_counts": {"Climate_Zone": {"Tropical": 3, "Temperate": 3, "Arid": 3, "Polar": 3}}, "summary_stats": {"Avg_Temp_C": {"count": 10, "mean": 15.01, "median": 15.3, "min": -7.1, "max": 30.5}, "Precip_mm": {"count": 10, "mean": 100, "median": 92.5, "min": 5, "max": 220}}, "correlations": {"Avg_Temp_C_vs_Precip_mm": 0.48}, "months_with_missing_data": ["May 2023 (Precip_mm missing)", "June 2023 (Avg_Temp_C non-numeric)", "November 2023 (Avg_Temp_C missing)", "December 2023 (Precip_mm missing)"]}}
{"purpose": "Analyze customer purchase patterns and product category distribution in retail transactions.", "raw_table": "TransactionID,CustomerID,ProductCategory,Quantity,UnitPrice,TransactionDate,StoreRegion\nT001,C123,Electronics,2,299.99,2023-01-15,north\nT002,C124,home appliances,1,,15/01/2023,South\nT003,C125,Electronics,NaN,199.99,01-16-2023,NORTH\nT004,C126,Toys,3,15.50,2023/01/17,west\nT005,C127,HOME appliances,2,120.00,2023-01-17,West\nT006,C128,clothing,1,45.00,2023-1-18,South\nT007,C129,Clothing,2,50.00,18-01-2023,SOUTH\nT008,C130,Toys,,15.00,2023-01-19,East\nT009,C131,Electronics,1,299.99,2023-01-20,east\nT010,C132,Garden,1,85.00,20/01/2023,North\nT011,C133,garden,2,80,2023-01-21,NORTH\nT012,C134,clothing,3,NaN,2023-01-22,South\nT013,C135,Electronics,1,299.99,2023-01-23,north\nT014,C136,Toys,2,14.99,2023-01-24,West", "eda_steps": ["Check for missing values in each column", "Standardize capitalization in ProductCategory and StoreRegion columns", "Convert TransactionDate to a consistent date format", "Compute descriptive statistics for Quantity and UnitPrice", "Calculate total sales per ProductCategory (Quantity * UnitPrice)", "Generate value counts for ProductCategory", "Identify the top 2 StoreRegions by number of transactions", "Summarize average UnitPrice by ProductCategory"], "eda_results": {"missing_values": {"TransactionID": 0, "CustomerID": 0, "ProductCategory": 0, "Quantity": 3, "UnitPrice": 2, "TransactionDate": 0, "StoreRegion": 0}, "standardized_categories": {"ProductCategory": ["Electronics", "Home Appliances", "Toys", "Clothing", "Garden"], "StoreRegion": ["North", "South", "West", "East"]}, "date_format_consistency": "All dates converted to YYYY-MM-DD format", "descriptive_statistics": {"Quantity": {"count": 11, "mean": 1.64, "std": 0.85, "min": 1, "25%": 1, "50%": 2, "75%": 2, "max": 3}, "UnitPrice": {"count": 12, "mean": 132.49, "std": 121.86, "min": 14.99, "25%": 45.0, "50%": 80.0, "75%": 199.99, "max": 299.99}}, "total_sales_per_category": {"Electronics": 1799.94, "Home Appliances": 240.0, "Toys": 104.98, "Clothing": 235.0, "Garden": 245.0}, "value_counts_ProductCategory": {"Electronics": 4, "Home Appliances": 2, "Toys": 3, "Clothing": 3, "Garden": 2}, "top_2_StoreRegions": {"North": 5, "South": 4}, "average_UnitPrice_by_ProductCategory": {"Electronics": 274.99, "Home Appliances": 120.0, "Toys": 15.16, "Clothing": 47.5, "Garden": 82.5}}}
{"purpose": "Analyze daily electricity consumption patterns and identify missing data issues in household energy usage.", "raw_table": "Date,Household_ID,Energy_Consumed_kWh,Energy_Source,Peak_Usage_Time\n2024-04-01,HH001,12.5,solar,morning\n2024/04/02,HH002,15.8,Grid,Afternoon\n04-03-2024,HH003,,wind,evening\n2024-04-04,hh004,11.2,Solar,morning\n2024-04-05,HH005,9.7,grid,NA\n2024-4-06,HH006,13.3,Coal,afternoon\n2024-04-07,HH007,14.0,solar,Evening\n2024-04-08,HH008,NA,Grid,morning\n2024-04-09,HH009,10.1,nuclear,MORNING\n2024-04-10,HH010,8.9,Wind,afternoon\n2024-04-11,hh011,12.0,Coal,evening\n2024-04-12,HH012,14.5,grid,MORNING\n2024-04-13,HH013,13.1,Solar,morning", "eda_steps": ["Check and summarize missing values in each column", "Standardize the 'Date' format to YYYY-MM-DD", "Normalize capitalization in 'Household_ID' and 'Energy_Source' columns", "Compute descriptive statistics for 'Energy_Consumed_kWh'", "Generate value counts for 'Energy_Source' and 'Peak_Usage_Time'", "Identify households with missing energy consumption data", "Analyze distribution skewness of energy consumption", "Check correlation between 'Energy_Consumed_kWh' and 'Peak_Usage_Time' (encoded numerically)", "Identify top 2 energy sources by average consumption"], "eda_results": {"missing_values": {"Date": 0, "Household_ID": 0, "Energy_Consumed_kWh": 2, "Energy_Source": 0, "Peak_Usage_Time": 1}, "standardized_dates": ["2024-04-01", "2024-04-02", "2024-04-03", "2024-04-04", "2024-04-05", "2024-04-06", "2024-04-07", "2024-04-08", "2024-04-09", "2024-04-10", "2024-04-11", "2024-04-12", "2024-04-13"], "normalized_household_ids": ["HH001", "HH002", "HH003", "HH004", "HH005", "HH006", "HH007", "HH008", "HH009", "HH010", "HH011", "HH012", "HH013"], "normalized_energy_sources": ["Solar", "Grid", "Wind", "Solar", "Grid", "Coal", "Solar", "Grid", "Nuclear", "Wind", "Coal", "Grid", "Solar"], "energy_consumed_stats": {"count": 11, "mean": 12.0, "std_dev": 1.92, "min": 8.9, "25%": 10.1, "50%": 12.0, "75%": 13.3, "max": 15.8}, "value_counts_energy_source": {"Solar": 4, "Grid": 4, "Wind": 2, "Coal": 2, "Nuclear": 1}, "value_counts_peak_usage_time": {"Morning": 5, "Afternoon": 3, "Evening": 3, "NA": 1}, "households_missing_consumption": ["HH003", "HH008"], "energy_consumed_skewness": 0.14, "correlation_energy_peak_time": 0.05, "top_energy_sources_by_avg_consumption": {"Grid": 13.3, "Solar": 12.7}}}
{"purpose": "Analyze public transportation usage patterns and identify missing data issues.", "raw_table": "Date,Route,Passenger_Count,Fare_Collected,Day_Type\n2024-01-01,Route 5,120,240.50,weekday\n01/02/2024,route 7,NA,300,Weekday\n2024-01-03,Route 5,135,270.75,Weekday\n2024-1-04,route 9,110,220,weekend\n2024-01-05,Route 5,,260,Weekday\n2024-01-06,Route 7,95,190.25,WEEKDAY\n01-07-2024,ROUTE 9,100,200,Weekend\n2024/01/08,Route 5,130,260,weekday\n2024-01-09,route 7,105,NA,Weekday\n2024-01-10,Route 5,115,230,Weekend", "eda_steps": ["Check for missing values in all columns", "Standardize capitalization in 'Route' and 'Day_Type' columns", "Convert 'Date' column to a consistent date format", "Calculate descriptive statistics for 'Passenger_Count' and 'Fare_Collected'", "Generate value counts for 'Route' and 'Day_Type'", "Identify rows with inconsistent or missing data", "Compute correlation between 'Passenger_Count' and 'Fare_Collected'"], "eda_results": {"missing_values": {"Date": 0, "Route": 0, "Passenger_Count": 2, "Fare_Collected": 2, "Day_Type": 0}, "standardized_categories": {"Route": ["Route 5", "Route 7", "Route 9"], "Day_Type": ["Weekday", "Weekend"]}, "date_format_conversion": "All dates converted to YYYY-MM-DD", "summary_stats": {"Passenger_Count": {"count": 13, "mean": 111.43, "std": 12.31, "min": 95, "25%": 100, "50%": 115, "75%": 130, "max": 135}, "Fare_Collected": {"count": 11, "mean": 240.23, "std": 27.45, "min": 190.25, "25%": 220, "50%": 240.5, "75%": 260, "max": 300}}, "value_counts": {"Route": {"Route 5": 5, "Route 7": 3, "Route 9": 3}, "Day_Type": {"Weekday": 7, "Weekend": 4}}, "inconsistent_rows": [2, 5, 8, 9], "correlations": {"Passenger_Count_Fare_Collected": 0.98}}}
{"purpose": "Analyze user engagement patterns and content type distribution on a social media platform.", "raw_table": "user_id,post_date,content_type,likes,comments,shares\n1001,2023-01-05,Video,230,15,5\n1002,01/07/2023,photo,89,,2\n1003,2023/01/08,Text,45,2,0\n1004,2023-1-09,video,NaN,7,1\n1005,2023-01-10,Link,12,0,0\n1006,2023-01-10,PHOTO,78,4,NaN\n1007,,text,34,1,0\n1008,2023-01-12,video,290,20,8\n1009,2023-01-13,Poll,5,,1\n1010,13-01-2023,Photo,NaN,3,0\n1011,2023-01-14,Video,210,NaN,6\n1012,2023-01-15,text,33,1,0\n1013,2023-01-16,link,22,1,1\n1014,2023/01/17,Video,198,9,4", "eda_steps": ["Check for missing values in each column", "Standardize date formats in the post_date column", "Normalize content_type values to lowercase", "Compute descriptive statistics for numeric columns likes, comments, and shares", "Generate value counts for content_type", "Identify posts with missing likes or shares", "Calculate the average likes per content_type", "Find correlation between likes, comments, and shares", "Identify top 3 posts with highest likes"], "eda_results": {"missing_values": {"user_id": 0, "post_date": 1, "content_type": 0, "likes": 3, "comments": 3, "shares": 2}, "standardized_date_samples": ["2023-01-05", "2023-01-07", "2023-01-08", "2023-01-09", "2023-01-10"], "value_counts_content_type": {"video": 5, "photo": 3, "text": 3, "link": 2, "poll": 1}, "descriptive_stats": {"likes": {"count": 11, "mean": 108.45, "std": 92.44, "min": 5, "25%": 33, "50%": 78, "75%": 210, "max": 290}, "comments": {"count": 11, "mean": 6.09, "std": 6.83, "min": 0, "25%": 1, "50%": 3, "75%": 9, "max": 20}, "shares": {"count": 12, "mean": 2.25, "std": 2.48, "min": 0, "25%": 0, "50%": 1, "75%": 4, "max": 8}}, "posts_missing_likes_or_shares": [{"user_id": 1004, "likes": "NaN", "shares": 1}, {"user_id": 1006, "likes": 78, "shares": "NaN"}, {"user_id": 1010, "likes": "NaN", "shares": 0}], "average_likes_per_content_type": {"video": 225.6, "photo": 89.5, "text": 37.3, "link": 17, "poll": 5}, "correlations": {"likes_comments": 0.92, "likes_shares": 0.85, "comments_shares": 0.78}, "top_3_posts_by_likes": [{"user_id": 1008, "likes": 290}, {"user_id": 1001, "likes": 230}, {"user_id": 1011, "likes": 210}]}}
{"purpose": "Explore real estate listing data to understand property characteristics and identify data quality issues.", "raw_table": "ListingID,Price,Location,PropertyType,Area_sqft,Bedrooms,Bathrooms,ListingDate\n001,350000,Downtown,Apartment,850,2,1.5,2023-01-15\n002,450000,suburbs,House,1400,3,2,15/02/2023\n003,na,Midtown,Condo,950,2,,2023-03-10\n004,500000,Downtown,apartment,900,2,2,2023/04/05\n005,620000,Suburbs,House,1600,4,3,04-20-2023\n006,480000,,Condo,1100,3,2,2023-05-01\n007,530000,Midtown,Townhouse,1200,3,2,May 10 2023\n008,470000,Downtown,Apartment,850,two,1.5,2023-06-15\n009,390000,Suburbs,house,1300,3,2,2023-06-20\n010,NaN,Midtown,Condo,,2,2,2023-07-01\n011,510000,Downtown,Apartment,900,2,2,2023-07-10\n012,445000,Suburbs,house,1350,3,2,2023-07-15\n013,600000,Midtown,Condo,1050,3,2,07/20/2023", "eda_steps": ["Check for missing values in each column", "Standardize and clean the 'Location' column capitalization", "Convert 'Price' to numeric, handling missing or invalid entries", "Compute descriptive statistics for numeric columns: Price, Area_sqft, Bedrooms, Bathrooms", "Generate value counts for 'PropertyType' and 'Location'", "Identify inconsistent or unusual categories in 'PropertyType' and 'Bedrooms'", "Parse and standardize 'ListingDate' into a consistent date format", "Calculate correlation matrix between numeric features", "Identify top 3 most common property types", "Summarize count of listings per month"], "eda_results": {"missing_values": {"ListingID": 0, "Price": 2, "Location": 1, "PropertyType": 0, "Area_sqft": 1, "Bedrooms": 1, "Bathrooms": 1, "ListingDate": 0}, "cleaned_location_counts": {"Downtown": 4, "Suburbs": 4, "Midtown": 4}, "price_conversion_issues": 2, "summary_stats": {"Price": {"count": 11, "mean": 495000, "std": 83000, "min": 350000, "25%": 445000, "50%": 480000, "75%": 530000, "max": 620000}, "Area_sqft": {"count": 12, "mean": 1150, "std": 260, "min": 850, "25%": 900, "50%": 1100, "75%": 1350, "max": 1600}, "Bedrooms": {"count": 12, "unique": 4, "most_common": 3, "frequency": 6}, "Bathrooms": {"count": 11, "mean": 1.91, "std": 0.38, "min": 1.5, "max": 3}}, "property_type_counts": {"Apartment": 5, "House": 4, "Condo": 4, "Townhouse": 1}, "unusual_entries": {"Bedrooms": ["two"], "PropertyType": []}, "listing_date_range": {"earliest": "2023-01-15", "latest": "2023-07-20"}, "correlations": {"Price_Area_sqft": 0.85, "Price_Bedrooms": 0.75, "Price_Bathrooms": 0.65, "Area_sqft_Bedrooms": 0.7}, "top_property_types": ["Apartment", "Condo", "House"], "listings_per_month": {"2023-01": 1, "2023-02": 1, "2023-03": 1, "2023-04": 2, "2023-05": 1, "2023-06": 3, "2023-07": 4}}}
{"purpose": "Analyze customer purchase behavior and product category performance in an ecommerce store.", "raw_table": "OrderID,CustomerID,OrderDate,ProductCategory,Quantity,Price,PaymentMethod\n1001,cust_01,2023-01-15,Electronics,2,199.99,Credit Card\n1002,CUST_02,15/01/2023,Clothing,1,49.99,credit card\n1003,cust_03,2023/01/16,Home & Kitchen,3,,Paypal\n1004,cust_04,01-17-2023,Electronics,,299.99,CREDIT CARD\n1005,cust_05,2023-01-18,Toys,5,15.00,\n1006,cust_06,,clothing,2,39.99,Debit Card\n1007,Cust_07,2023-01-20,Books,1,9.99,Debit card\n1008,cust_08,2023-01-21,Home & kitchen,2,89.99,PayPal\n1009,cust_09,2023-01-22,Electronics,1,199.99,Credit Card\n1010,cust_10,2023-01-23,Toys,3,15.00,Paypal\n1011,cust_11,Jan 24 2023,Books,2,8.99,Credit card\n1012,CUST_12,2023-01-25,clothing,1,49.99,Debit Card\n1013,cust_13,2023-01-26,Home & Kitchen,4,85.00,credit Card\n1014,cust_14,2023-01-27,Toys,,15.00,Paypal", "eda_steps": ["Check the percentage of missing values for each column", "Standardize the capitalization and spelling for the ProductCategory and PaymentMethod columns", "Convert OrderDate to a consistent date format and identify any missing dates", "Calculate descriptive statistics for Quantity and Price columns", "Generate value counts for ProductCategory and PaymentMethod", "Identify orders with missing Quantity or Price and count them", "Calculate total sales per ProductCategory", "Find the top 3 most frequent customers by number of orders"], "eda_results": {"missing_values": {"OrderID": "0%", "CustomerID": "0%", "OrderDate": "7%", "ProductCategory": "0%", "Quantity": "21%", "Price": "7%", "PaymentMethod": "7%"}, "standardized_categories": {"ProductCategory": ["Electronics", "Clothing", "Home & Kitchen", "Toys", "Books"], "PaymentMethod": ["Credit Card", "Paypal", "Debit Card"]}, "missing_dates_count": 1, "descriptive_stats": {"Quantity": {"count": 11, "mean": 2.27, "std": 1.35, "min": 1, "max": 5}, "Price": {"count": 13, "mean": 94.99, "std": 99.08, "min": 8.99, "max": 299.99}}, "value_counts": {"ProductCategory": {"Electronics": 3, "Clothing": 3, "Home & Kitchen": 3, "Toys": 3, "Books": 2}, "PaymentMethod": {"Credit Card": 5, "Paypal": 4, "Debit Card": 3}}, "missing_quantity_orders": 3, "missing_price_orders": 1, "total_sales_per_category": {"Electronics": 699.97, "Clothing": 139.97, "Home & Kitchen": 428.97, "Toys": 90.0, "Books": 26.97}, "top_customers_by_orders": {"cust_01": 1, "cust_02": 1, "cust_03": 1, "cust_04": 1, "cust_05": 1, "cust_06": 1, "cust_07": 1, "cust_08": 1, "cust_09": 1, "cust_10": 1, "cust_11": 1, "cust_12": 1, "cust_13": 1, "cust_14": 1}}}
{"purpose": "Analyze patient demographic and clinical characteristics admitted for hypertension treatment.", "raw_table": "Patient_ID,Age,Gender,Blood_Pressure,Admission_Date,Diagnosis,Medication\nP001,54,Male,140/90,2023/03/15,Hypertension,Lisinopril\nP002,47,Female,130/85,15-04-2023,hypertension,amlodipine\nP003,63,M,150/95,2023-05-01,Hypertension,\nP004,NA,Female,135/88,2023/04/20,Hypertension,Losartan\nP005,55,male,142/92,04/25/2023,Hypertensive Crisis,amlodipine\nP006,60,F,NA,2023-05-03,hypertension,Lisinopril\nP007,58,female,138/89,May 5 2023,UNKNOWN,losartan\nP008,49,Male,145/93,2023/03/28,Hypertension,Lisinopril\nP009,52,F,140/90,2023-03-30,Hypertension,amlodipine\nP010,61,Male,NA,2023-04-15,hypertension,losartan\n", "eda_steps": ["Check and count missing values in each column", "Standardize Gender values and compute counts per category", "Convert Admission_Date to consistent date format and identify earliest and latest dates", "Extract systolic and diastolic values from Blood_Pressure and compute their summary statistics", "Calculate the distribution of Diagnosis categories", "Identify patients without recorded Medication", "Analyze Age column: check missing values, compute mean and median", "Cross-tabulate Gender against Medication to find common prescriptions"], "eda_results": {"missing_values": {"Age": 1, "Blood_Pressure": 2, "Medication": 1}, "gender_counts": {"Male": 4, "Female": 5}, "admission_date_range": {"earliest": "2023-03-15", "latest": "2023-05-05"}, "blood_pressure_stats": {"systolic": {"count": 8, "mean": 140.0, "min": 130, "max": 150}, "diastolic": {"count": 8, "mean": 90.1, "min": 85, "max": 95}}, "diagnosis_distribution": {"Hypertension": 6, "hypertension": 3, "Hypertensive Crisis": 1, "UNKNOWN": 1}, "patients_missing_medication": ["P003"], "age_stats": {"count": 9, "mean": 54.9, "median": 55}, "gender_medication_crosstab": {"Male": {"Lisinopril": 2, "amlodipine": 1, "losartan": 1}, "Female": {"amlodipine": 2, "Losartan": 1, "losartan": 1, "": 1}}}}
{"purpose": "Analyze customer order patterns and product popularity in an ecommerce store.", "raw_table": "OrderID,CustomerID,OrderDate,ProductCategory,Quantity,UnitPrice,OrderStatus\n1001,cust_01,2023/01/15,Electronics,2,199.99,completed\n1002,CUST_02,01-17-2023,home appliances,1,89.5,Completed\n1003,cust_03,2023-01-19,electronics,,149.99,Pending\n1004,cust_04,2023/01/20,Fashion,3,29.99,completed\n1005,Cust_05,01/22/2023,Fashion,2,,Shipped\n1006,cust_06,2023-01-23,Books,5,15.0,completed\n1007,cust_07,2023-01-23,Home Appliances,2,95,Cancelled\n1008,cust_08,2023/01/24,Books,one,12.5,Completed\n1009,CUST_09,2023-01-25,electronics,1,199.99,Completed\n1010,cust_10,2023/01/26,Fashion,2,25.0,Pending\n1011,cust_11,,Books,3,15.0,Completed\n1012,cust_12,2023-01-28,Toys,4,20.99,completed\n1013,cust_13,2023/01/29,Toys,2,21.0,completed\n1014,cust_14,01-30-2023,Fashion,3,27.5,shipped\n1015,cust_15,2023-01-31,Electronics,1,199.99,Completed", "eda_steps": ["Inspect and standardize capitalization in ProductCategory and OrderStatus columns", "Convert OrderDate to a consistent date format and identify missing dates", "Identify and count missing values in Quantity and UnitPrice columns", "Convert Quantity to numeric, handling non-numeric entries", "Compute descriptive statistics for Quantity and UnitPrice", "Generate value counts for ProductCategory and OrderStatus", "Calculate total sales (Quantity * UnitPrice) per order and summarize", "Identify the top 3 most ordered product categories", "Check distribution of orders over the date range", "Assess correlation between Quantity and UnitPrice"], "eda_results": {"standardized_categories": {"ProductCategory": ["Electronics", "Home Appliances", "Fashion", "Books", "Toys"], "OrderStatus": ["Completed", "Pending", "Shipped", "Cancelled"]}, "missing_values": {"OrderDate": 1, "Quantity": 2, "UnitPrice": 1}, "quantity_conversion_issues": {"non_numeric_quantities": ["one"]}, "summary_stats": {"Quantity": {"count": 13, "mean": 2.31, "std": 1.23, "min": 1, "max": 5}, "UnitPrice": {"count": 14, "mean": 89.78, "std": 73.16, "min": 12.5, "max": 199.99}}, "value_counts": {"ProductCategory": {"Electronics": 4, "Fashion": 4, "Books": 3, "Home Appliances": 2, "Toys": 2}, "OrderStatus": {"Completed": 7, "Pending": 2, "Shipped": 2, "Cancelled": 1}}, "total_sales_summary": {"count": 13, "mean": 207.05, "min": 12.5, "max": 399.98}, "top_3_product_categories": ["Electronics", "Fashion", "Books"], "orders_over_time": {"start_date": "2023-01-15", "end_date": "2023-01-31", "total_orders": 15, "orders_missing_date": 1}, "correlations": {"Quantity_vs_UnitPrice": -0.12}}}
{"purpose": "Analyze viewership patterns and ratings of popular TV shows over recent months.", "raw_table": "ShowID,ShowName,Genre,ReleaseDate,AvgRating,TotalViews,LastAired\n101,Stranger Things,drama,2016-07-15,8.7,200000000,2023-04-15\n102,Breaking bad,Crime,2008/01/20,9.5,120000000,2023-03-10\n103,The Office,Comedy,2005-03-24,8.9,150000000,2023-04-01\n104,The crown,Drama,2016-11-04,8.6,,2023-02-28\n105,Friends,comedy,1994-09-22,8.9,175000000,2023-04-10\n106,The Mandalorian,Sci-Fi,2019-11-12,8.8,90000000,2023-04-20\n107,Game of Thrones,Fantasy,2011-04-17,9.3,230000000,2022-12-19\n108,Money Heist,crime,2017-05-02,8.3,110000000,2023-04-05\n109,Stranger things,Drama,2016-07-15,8.7,195000000,2023-04-15\n110,Ozark,Crime,,8.4,85000000,2023-03-30", "eda_steps": ["Check for missing values in each column", "Standardize genre capitalization and list unique genres", "Calculate descriptive statistics for AvgRating and TotalViews", "Identify duplicate shows based on ShowName ignoring case", "Analyze distribution of release years extracted from ReleaseDate", "Count how many shows aired their last episode in 2023", "Find top 3 shows by TotalViews", "Summarize average rating per genre"], "eda_results": {"missing_values": {"ShowID": 0, "ShowName": 0, "Genre": 0, "ReleaseDate": 1, "AvgRating": 0, "TotalViews": 1, "LastAired": 0}, "unique_genres": ["drama", "crime", "comedy", "sci-fi", "fantasy"], "descriptive_stats": {"AvgRating": {"count": 10, "mean": 8.71, "std": 0.37, "min": 8.3, "max": 9.5}, "TotalViews": {"count": 9, "mean": 147222222, "std": 52184354, "min": 85000000, "max": 230000000}}, "duplicate_shows": ["Stranger Things"], "release_year_distribution": {"1994": 1, "2005": 1, "2008": 1, "2011": 1, "2016": 2, "2017": 1, "2019": 1, "": 1}, "shows_last_aired_2023": 9, "top_3_shows_by_views": [{"ShowName": "Game of Thrones", "TotalViews": 230000000}, {"ShowName": "Stranger Things", "TotalViews": 200000000}, {"ShowName": "Friends", "TotalViews": 175000000}], "average_rating_per_genre": {"drama": 8.67, "crime": 8.73, "comedy": 8.9, "sci-fi": 8.8, "fantasy": 9.3}}}
{"purpose": "Examine machine maintenance records to identify common issues and data quality problems.", "raw_table": "Machine_ID,Maintenance_Date,Issue_Reported,Technician,Hours_Spent,Cost,Status\nM001,2024-02-15,Overheat,John D,2,150,Completed\nM002,15/03/2024,Leakage,Mary S,3,,Pending\nm003,2024/04/01,,alex k,1.5,75,completed\nM004,2024-04-10,Overheat,John D,,200,Completed\nM005,2024-04-12,Noise,,2,90,Completed\nM006,04-15-2024,Leakage,Mary S,2.5,130,Pending\nM007,2024-04-18,overheat,Alex K,3,180,Completed\nM008,2024-04-20,Noise,Alice P,2,,Completed\nM009,2024-04-21,Leakage,John D,1.5,100,Cancelled\nM010,,Noise,Mary S,2,85,Completed", "eda_steps": ["Check for missing values in each column", "Standardize capitalization in categorical columns", "Parse and unify the date format in Maintenance_Date", "Compute descriptive statistics for numeric columns Hours_Spent and Cost", "Generate value counts for Issue_Reported and Status", "Identify rows with missing critical information", "Calculate average cost and hours spent per Issue_Reported category"], "eda_results": {"missing_values": {"Machine_ID": 0, "Maintenance_Date": 1, "Issue_Reported": 1, "Technician": 1, "Hours_Spent": 1, "Cost": 3, "Status": 0}, "standardized_issue_reported": {"Overheat": 3, "Leakage": 3, "Noise": 3, "Missing": 1}, "standardized_status": {"Completed": 7, "Pending": 2, "Cancelled": 1}, "date_range": {"earliest": "2024-02-15", "latest": "2024-04-21"}, "hours_spent_stats": {"count": 14, "mean": 2.07, "min": 1.5, "max": 3.0}, "cost_stats": {"count": 12, "mean": 120.83, "min": 75, "max": 200}, "average_cost_and_hours_per_issue": {"Overheat": {"average_hours": 2.33, "average_cost": 176.67}, "Leakage": {"average_hours": 2.33, "average_cost": 101.67}, "Noise": {"average_hours": 2.0, "average_cost": 87.5}, "Missing": {"average_hours": 1.5, "average_cost": 75}}, "rows_with_missing_critical_info": ["M003 (missing Issue_Reported)", "M005 (missing Technician)", "M010 (missing Maintenance_Date)", "M004 (missing Hours_Spent)"]}}
{"purpose": "Explore movie ratings and box office performance to identify trends and data quality issues.", "raw_table": "MovieID,Title,Genre,ReleaseDate,BoxOfficeMillions,Rating,NumReviews\n1,The First Dawn,Action,2021-07-15,120.5,7.8,3500\n2,Lost Horizons,Drama,15/08/2020,85.2,8.3,2900\n3,space Odyssey,SCI-FI,2021/12/01,,9.1,4100\n4,Whispering Shadows,thriller,2020-11-30,43.7,6.4,1800\n5,Sunset Reverie,Drama,2020-09-10,67.1,,2100\n6,THE LAST QUEST,Action,2021-03-22,110.3,7.5,3300\n7,Midnight Run,Comedy,2021-06-12,54.0,6.8,NaN\n8,Echoes of Time,Sci-fi,2020-12-25,98.4,8.0,2800\n9,The Unknown,,2021-01-18,35.6,5.9,1500\n10,Hidden Paths,Thriller,2021/08/09,47,7.2,2000\n11,Nightfall,Comedy,2020-07-07,38.9,6.5,1700\n12,Galactic Voyage,Sci-Fi,2021-11-05,130.8,9.3,4500\n13,Silent Waters,Drama,07-15-2020,72.3,7.1,2600\n14,Crimson Peak,Horror,2020-10-31,40.0,6.7,1900", "eda_steps": ["Check the percentage of missing values in each column", "Standardize the Genre column capitalization and identify unique genres", "Convert ReleaseDate to a consistent date format", "Compute summary statistics for numeric columns: BoxOfficeMillions, Rating, NumReviews", "Generate value counts for the Genre column", "Identify movies with missing BoxOfficeMillions or Rating values", "Find the correlation between BoxOfficeMillions, Rating, and NumReviews", "Identify top 3 movies by BoxOfficeMillions", "Summarize rating distribution skewness"], "eda_results": {"missing_values": {"BoxOfficeMillions": "7.14%", "Rating": "7.14%", "NumReviews": "7.14%", "Genre": "7.14%"}, "unique_genres": ["Action", "Drama", "Sci-Fi", "Thriller", "Comedy", "Horror", "Unknown"], "summary_stats": {"BoxOfficeMillions": {"count": 14, "mean": 74.64, "std": 31.75, "min": 35.6, "max": 130.8}, "Rating": {"count": 13, "mean": 7.29, "std": 1.01, "min": 5.9, "max": 9.3}, "NumReviews": {"count": 13, "mean": 2850, "std": 950, "min": 1500, "max": 4500}}, "genre_value_counts": {"Drama": 3, "Action": 2, "Sci-Fi": 3, "Thriller": 2, "Comedy": 2, "Horror": 1, "Unknown": 1}, "missing_boxoffice_or_rating_movies": ["space Odyssey", "Sunset Reverie"], "correlations": {"BoxOfficeMillions_Rating": 0.72, "BoxOfficeMillions_NumReviews": 0.85, "Rating_NumReviews": 0.67}, "top_3_boxoffice_movies": [{"Title": "Galactic Voyage", "BoxOfficeMillions": 130.8}, {"Title": "The First Dawn", "BoxOfficeMillions": 120.5}, {"Title": "THE LAST QUEST", "BoxOfficeMillions": 110.3}], "rating_skewness": -0.45}}
{"purpose": "Examine student performance and attendance patterns in a high school to identify data quality issues and key trends.", "raw_table": "Student_ID,Name,Grade,Subject,Score,Attendance,Test_Date\n101,alice,10,Math,85,Present,2023-03-15\n102,Bob,10,math,92,Absent,15/03/2023\n103,CHARLIE,11,Science,78,,2023/03/16\n104,Diana,11,English,88,present,16-03-2023\n105,Edward,10,science,NaN,Present,2023-03-15\n106,Fiona,12,Math,95,Present,03-17-2023\n107,George,12,english,82,Absent,17/03/2023\n108,Helen,11,Math,88,Present,2023/03/15\n109,Ian,10,Science,72,Present,15-03-2023\n110,Jane,11,English,91,absent,16/03/2023", "eda_steps": ["Check for missing values in each column", "Standardize capitalization in 'Name', 'Grade', and 'Subject' columns", "Convert 'Score' column to numeric and handle missing or invalid values", "Standardize 'Attendance' entries to lowercase and unify categories to 'present' or 'absent'", "Normalize 'Test_Date' to a single consistent date format", "Calculate descriptive statistics for 'Score'", "Generate value counts for 'Grade' and 'Subject' columns", "Calculate attendance rate overall and by grade", "Identify any duplicate Student_ID entries"], "eda_results": {"missing_values": {"Student_ID": 0, "Name": 0, "Grade": 0, "Subject": 0, "Score": 1, "Attendance": 1, "Test_Date": 0}, "standardized_subject_counts": {"Math": 4, "Science": 3, "English": 3}, "attendance_counts": {"present": 7, "absent": 3}, "score_summary_stats": {"count": 9, "mean": 85.1, "std_dev": 7.48, "min": 72, "max": 95, "median": 87}, "grade_counts": {"10": 4, "11": 4, "12": 2}, "attendance_rate_by_grade": {"10": 0.75, "11": 0.75, "12": 0.5}, "duplicate_student_ids": 0, "standardized_test_dates": ["2023-03-15", "2023-03-15", "2023-03-16", "2023-03-16", "2023-03-15", "2023-03-17", "2023-03-17", "2023-03-15", "2023-03-15", "2023-03-16"]}}
{"purpose": "Analyze monthly sales performance and product category distribution for a retail store.", "raw_table": "OrderID,ProductName,Category,Quantity,UnitPrice,OrderDate,CustomerRegion\n1001,bluetooth speaker,Electronics,2,49.99,2023/01/15,North\n1002,Wireless Mouse,electronics,1,25.5,15-01-2023,west\n1003,Office Chair,Furniture,1,,2023-01-17,East\n1004,Desk Lamp,Furniture,3,18.75,01/18/2023,South\n1005,Cotton T-shirt,Clothing,5,9.99,2023-01-20,North\n1006,,Clothing,2,12.5,2023-01-22,West\n1007,Running Shoes,clothing,1,59.9,2023-01-25,East\n1008,USB-C Cable,Electronics,,7.99,2023-01-27,South\n1009,Bookshelf,Furniture,1,120,27-01-2023,east\n1010,Denim Jeans,clothing,3,35.0,2023/01/30,North\n1011,Wireless headphones,Electronics,2,85.5,2023-02-01,West\n1012,Cotton T-shirt,Clothing,4,9.99,2023/02/03,south\n1013,Desk Lamp,Furniture,1,18.75,02-04-2023,North\n1014,Leather Wallet,Accessories,1,45.0,2023-02-05,West\n1015,,Electronics,1,99.99,02/06/2023,South", "eda_steps": ["Check and report missing values in each column", "Standardize the capitalization of the Category column", "Convert OrderDate values to a consistent date format", "Compute descriptive statistics for Quantity and UnitPrice columns", "Generate value counts for the Category column", "Identify top 3 products by total sales amount (Quantity * UnitPrice)", "Summarize the number of orders per CustomerRegion", "Check for any inconsistent or missing ProductName entries"], "eda_results": {"missing_values": {"OrderID": 0, "ProductName": 2, "Category": 0, "Quantity": 2, "UnitPrice": 1, "OrderDate": 0, "CustomerRegion": 0}, "category_standardization": {"Electronics": 6, "Furniture": 4, "Clothing": 5, "Accessories": 1}, "orderdate_format_conversion": {"dates_correctly_parsed": 15, "unique_months": ["2023-01", "2023-02"]}, "quantity_stats": {"count": 13, "mean": 2.23, "median": 2, "min": 1, "max": 5, "missing": 2}, "unitprice_stats": {"count": 14, "mean": 38.92, "median": 25.5, "min": 7.99, "max": 120, "missing": 1}, "category_value_counts": {"Electronics": 6, "Furniture": 4, "Clothing": 5, "Accessories": 1}, "top_3_products_by_sales": {"Bookshelf": 120, "Wireless headphones": 171, "Running Shoes": 59.9}, "orders_per_region": {"North": 4, "West": 4, "East": 3, "South": 4}, "productname_issues": {"missing_product_names": [1006, 1015]}}}
{"purpose": "Analyze urban transportation ride data to identify usage patterns and data quality issues.", "raw_table": "Ride_ID,Date,Start_Location,End_Location,Duration_Minutes,Fare_USD,Payment_Method\n101,2023-05-01,Central Park,Downtown,15,7.5,credit card\n102,5/2/2023,central park,Airport,35,20,CASH\n103,2023/05/03,Downtown,Central park,,15,credit card\n104,2023-05-04,Suburb,Downtown,45,,Credit Card\n105,May 5 2023,Airport,Suburb,30,18,cash\n106,2023-05-06,Downtown,,20,10,Credit\n107,2023-05-07,Central Park,Airport,40,22,CASH\n108,2023-05-08,Suburb,Central park,50,25,credit card\n109,2023-13-09,Airport,Downtown,25,12,Credit card\n110,2023-05-10,Suburb,airport,55,30,Cash\n111,2023-05-11,Central park,Downtown,NA,16,Credit card", "eda_steps": ["Check for missing values in each column", "Standardize the date format to YYYY-MM-DD", "Normalize capitalization in categorical columns: Start_Location, End_Location, Payment_Method", "Compute descriptive statistics for Duration_Minutes and Fare_USD", "Generate value counts for Payment_Method", "Identify rows with inconsistent or invalid dates", "Calculate the correlation between Duration_Minutes and Fare_USD", "Identify top 3 most frequent Start_Location values"], "eda_results": {"missing_values": {"Ride_ID": 0, "Date": 0, "Start_Location": 0, "End_Location": 2, "Duration_Minutes": 2, "Fare_USD": 1, "Payment_Method": 0}, "date_issues": ["Row 9 has invalid date '2023-13-09'", "Row 5 has date in 'May 5 2023' format", "Row 2 has date in '5/2/2023' format", "Row 3 has date in '2023/05/03' format"], "normalized_payment_method_counts": {"Credit Card": 6, "Cash": 4, "Credit": 1}, "start_location_value_counts": {"Central Park": 4, "Downtown": 3, "Suburb": 3, "Airport": 3}, "descriptive_statistics": {"Duration_Minutes": {"count": 9, "mean": 34.44, "std": 12.93, "min": 15, "25%": 25, "50%": 30, "75%": 45, "max": 55}, "Fare_USD": {"count": 10, "mean": 17.1, "std": 6.39, "min": 7.5, "25%": 12, "50%": 16, "75%": 20, "max": 30}}, "correlation_duration_fare": 0.87, "top_3_start_locations": ["Central Park", "Downtown", "Suburb"]}}
{"purpose": "Analyze movie ratings and genre popularity from a sample entertainment dataset.", "raw_table": "movie_id,title,genre,rating,release_date,duration_minutes\n1,Inception,Sci-Fi,8.8,2010-07-16,148\n2,Titanic,Romance,7.8,1997/12/19,195\n3,The GODFATHER,crime,9.2,1972-03-24,175\n4,Avengers: Endgame,Action,8.4,26-04-2019,181\n5,parasite,thriller,,2019-05-30,132\n6,La La Land,Romantic Comedy, 8.0,2016-12-09,128\n7,Spirited Away,Animation,8.6,2001/07/20,125\n8,The Dark Knight,action,9.0,2008-07-18,152\n9,Forrest Gump,Drama,abc,1994-07-06,142\n10,Interstellar,Sci-fi,8.6,2014-11-07,169\n11,The Matrix,SciFi,8.7,1999-03-31,136", "eda_steps": ["Check for missing values in all columns", "Standardize genre column capitalization", "Convert rating column to numeric and identify invalid entries", "Parse release_date to consistent datetime format", "Compute summary statistics for rating and duration_minutes", "Generate value counts for genre", "Identify movies with missing or invalid ratings", "Find correlation between rating and duration_minutes"], "eda_results": {"missing_values": {"movie_id": 0, "title": 0, "genre": 0, "rating": 2, "release_date": 0, "duration_minutes": 0}, "genre_value_counts": {"Sci-Fi": 3, "Romance": 1, "Crime": 1, "Action": 2, "Thriller": 1, "Romantic Comedy": 1, "Animation": 1, "Drama": 1}, "rating_invalid_entries": 2, "rating_summary_stats": {"count": 9, "mean": 8.51, "std": 0.53, "min": 7.8, "25%": 8.4, "50%": 8.6, "75%": 8.8, "max": 9.2}, "duration_summary_stats": {"count": 11, "mean": 150.27, "std": 22.77, "min": 125, "25%": 136, "50%": 148, "75%": 169, "max": 195}, "release_date_parsing_issues": 0, "rating_duration_correlation": 0.58, "movies_missing_or_invalid_ratings": [{"movie_id": 5, "title": "parasite", "rating": null}, {"movie_id": 9, "title": "Forrest Gump", "rating": "abc"}]}}
{"purpose": "Analyze urban bike-sharing trip patterns including user types and trip durations.", "raw_table": "Trip_ID,Start_Time,End_Time,User_Type,Bike_Type,Trip_Duration_mins,Start_Station,End_Station\n1,2023-05-01 08:15,2023-05-01 08:45,Subscriber,Standard,30,Central Park,central park\n2,2023/05/02 09:00,2023-05-02 09:25,Casual,Electric,25,Main St,Main St\n3,May 3 2023 10:20 AM,May 3 2023 10:50 AM,subscriber,standard,30,Downtown,Union Square\n4,2023-05-04 07:50,2023-05-04 08:05,Subscriber,STANDARD,15,,Union Square\n5,2023-05-05 18:10,,Casual,electric,20,Central Park,Downtown\n6,2023-05-06 20:00,2023-05-06 20:35,Subscriber,Standard,35,Main St,Main St\n7,2023-05-07 08:30,2023-05-07 08:50,Casual,Standard,20,Union Square,Downtown\n8,,2023-05-08 09:15,Subscriber,Electric,40,Central Park,Main St\n9,2023-05-09 14:00,2023-05-09 14:20,casual,Electric,20,Main St,Downtown\n10,2023-05-10 12:30,2023-05-10 12:55,Subscriber,Standard,25,Downtown,central park\n", "eda_steps": ["Check for missing values in each column", "Standardize the User_Type and Bike_Type categorical variables to consistent capitalization", "Parse Start_Time and End_Time into datetime objects and compute missing timestamps", "Compute descriptive statistics for Trip_Duration_mins", "Generate value counts for User_Type and Bike_Type", "Identify top 3 most frequent Start_Station and End_Station values", "Calculate average trip duration by User_Type", "Summarize number of trips with missing End_Time or Start_Time"], "eda_results": {"missing_values": {"Trip_ID": 0, "Start_Time": 1, "End_Time": 1, "User_Type": 0, "Bike_Type": 0, "Trip_Duration_mins": 0, "Start_Station": 1, "End_Station": 0}, "standardized_categories": {"User_Type": ["Subscriber", "Casual"], "Bike_Type": ["Standard", "Electric"]}, "descriptive_stats_trip_duration": {"count": 10, "mean": 26, "std": 7.9, "min": 15, "25%": 20, "50%": 25, "75%": 30, "max": 40}, "value_counts_user_type": {"Subscriber": 6, "Casual": 4}, "value_counts_bike_type": {"Standard": 6, "Electric": 4}, "top_start_stations": {"Central Park": 3, "Main St": 3, "Downtown": 2}, "top_end_stations": {"Downtown": 3, "Main St": 2, "Central Park": 2}, "avg_trip_duration_by_user_type": {"Subscriber": 27.5, "Casual": 22.5}, "trips_with_missing_timestamps": {"missing_Start_Time": 1, "missing_End_Time": 1}}}
{"purpose": "Explore patient demographics and lab results to identify data quality issues and summarize key statistics.", "raw_table": "Patient_ID,Age,Gender,Visit_Date,Diagnosis,Blood_Pressure,Cholesterol_mg_dL\n001,45,Male,2023-03-01,Hypertension,130/85,200\n002,38,Female,03/15/2023,diabetes,120/80,180\n003,,female,2023/04/02,Hypertension,135/90,NaN\n004,50,M,2023-04-15,Asthma,140/95,190\n005,29,F,15-05-2023,Healthy,110/70,170\n006,NaN,Male,2023-05-20,Diabetes,,210\n007,41,Female,May 25 2023,hypertension,128/82,195\n008,35,Female,,Asthma,125/78,185\n009,60,M,,DM,145/92,220\n010,47,Male,2023-06-01,Healthy,118/75,NaN", "eda_steps": ["Check for missing values in each column", "Standardize 'Gender' column values", "Parse 'Visit_Date' into a consistent date format", "Compute descriptive statistics for 'Age' and 'Cholesterol_mg_dL'", "Generate value counts for 'Diagnosis'", "Extract systolic and diastolic values from 'Blood_Pressure' and summarize", "Identify patients with missing 'Age' or 'Blood_Pressure'", "Calculate correlation between Age and Cholesterol levels"], "eda_results": {"missing_values": {"Patient_ID": 0, "Age": 2, "Gender": 0, "Visit_Date": 1, "Diagnosis": 0, "Blood_Pressure": 1, "Cholesterol_mg_dL": 2}, "standardized_gender_counts": {"Male": 5, "Female": 5}, "diagnosis_value_counts": {"Hypertension": 3, "Diabetes": 2, "Asthma": 2, "Healthy": 2, "DM": 1}, "age_descriptive_stats": {"count": 8, "mean": 44.4, "std": 9.79, "min": 29, "25%": 35, "50%": 44, "75%": 50, "max": 60}, "cholesterol_descriptive_stats": {"count": 8, "mean": 191.25, "std": 17.75, "min": 170, "25%": 185, "50%": 190, "75%": 200, "max": 220}, "blood_pressure_summary_systolic": {"mean": 131.25, "min": 110, "max": 145}, "blood_pressure_summary_diastolic": {"mean": 83.375, "min": 70, "max": 95}, "patients_missing_age_or_bp": ["003", "006", "009"], "correlation_age_cholesterol": 0.67}}
{"purpose": "Analyze patient demographics and basic health metrics to identify data quality issues and distribution patterns in a small clinical dataset.", "raw_table": "patient_id,age,gender,blood_pressure,cholesterol,test_date,diagnosis\nP001,34,Male,120/80,200,2023-01-15,Hypertension\nP002,29,Female,130/85,,15-02-2023,Diabetes\nP003,,female,115/75,180,2023/03/10,hypertension\nP004,45,M,140/90,220,2023-04-01,Diabetes\nP005,38,F,125/82,195,,None\nP006,52,male,135/88,210,2023-05-20,Hypertension\nP007,47,Female,,205,2023-06-15,Diabetes\nP008,41,Male,128/84,NaN,2023-06-18,diabetes\nP009,36,Female,118/79,190,2023-07-01,Hypertension\nP010,30,M,122/80,185,07/15/2023,None", "eda_steps": ["Check for missing values in each column", "Standardize gender values to 'Male' or 'Female'", "Parse and standardize test_date into YYYY-MM-DD format", "Compute descriptive statistics for age and cholesterol", "Extract systolic and diastolic values from blood_pressure and compute their summary statistics", "Generate value counts for diagnosis", "Identify rows with inconsistent or missing blood_pressure values", "Calculate percentage of missing cholesterol values"], "eda_results": {"missing_values": {"patient_id": 0, "age": 1, "gender": 0, "blood_pressure": 1, "cholesterol": 2, "test_date": 1, "diagnosis": 0}, "gender_standardized_counts": {"Male": 5, "Female": 5}, "test_date_standardized": ["2023-01-15", "2023-02-15", "2023-03-10", "2023-04-01", null, "2023-05-20", "2023-06-15", "2023-06-18", "2023-07-01", "2023-07-15"], "age_stats": {"count": 9, "mean": 39.1, "min": 29, "max": 52, "median": 38}, "cholesterol_stats": {"count": 8, "mean": 197.1, "min": 180, "max": 220, "median": 195}, "blood_pressure_parsed_stats": {"systolic": {"count": 9, "mean": 126.8, "min": 115, "max": 140}, "diastolic": {"count": 9, "mean": 83.3, "min": 75, "max": 90}}, "diagnosis_value_counts": {"Hypertension": 4, "Diabetes": 4, "None": 2}, "rows_with_missing_or_invalid_bp": ["P007"], "cholesterol_missing_percentage": 20.0}}
{"purpose": "Analyze monthly sales performance and product category distribution in a retail store.", "raw_table": "OrderID,Product,Category,Quantity,UnitPrice,OrderDate,CustomerSegment\n1001,Wireless Mouse,Electronics,2,25.5,2023/01/15,Corporate\n1002,office chair,Furniture,1,89.99,15-01-2023,consumer\n1003,Desk Lamp,electronics,3,,2023-01-17,Consumer\n1004,Notebook,Stationery,5,2.5,2023/1/18,Consumer\n1005,Pen,stationery,10,0.99,01/19/2023,Consumer\n1006,Bookshelf,Furniture,1,120.0,2023/01/20,corporate\n1007,USB Cable,Electronics,4,5.75,2023/01/20,Consumer\n1008,Coffee Mug,Kitchenware,,7.5,2023-01-21,consumer\n1009,Desk Organizer,Stationery,2,12.0,2023/01/22,Consumer\n1010,Monitor Stand,electronics,1,30,01-23-2023,corporate\n1011,Stapler,Stationery,3,8.25,2023/01/24,Consumer\n1012,Office Desk,Furniture,1,250.0,2023/01/25,Consumer\n", "eda_steps": ["Check the percentage of missing values in each column", "Standardize the capitalization of categorical columns: Product, Category, and CustomerSegment", "Parse and unify the OrderDate column to a consistent date format", "Compute descriptive statistics for Quantity and UnitPrice columns", "Calculate total sales amount per order (Quantity * UnitPrice) and summarize", "Generate value counts for the Category and CustomerSegment columns", "Identify top 3 best-selling products by total quantity sold", "Analyze the distribution of sales over the order dates"], "eda_results": {"missing_values": {"Quantity": 1, "UnitPrice": 1}, "standardized_categories": {"Category": {"Electronics": 4, "Furniture": 3, "Stationery": 4, "Kitchenware": 1}, "CustomerSegment": {"Consumer": 8, "Corporate": 3}}, "order_date_range": {"min_date": "2023-01-15", "max_date": "2023-01-25"}, "quantity_stats": {"count": 14, "mean": 2.71, "median": 2, "min": 1, "max": 10}, "unitprice_stats": {"count": 14, "mean": 54.37, "median": 12.0, "min": 0.99, "max": 250.0}, "total_sales_summary": {"total_sales_mean": 79.1, "total_sales_median": 23.0, "total_sales_min": 0.99, "total_sales_max": 250.0}, "category_value_counts": {"Electronics": 4, "Furniture": 3, "Stationery": 4, "Kitchenware": 1}, "customer_segment_value_counts": {"Consumer": 8, "Corporate": 3}, "top_3_products_by_quantity": {"Pen": 10, "Desk Lamp": 3, "Stapler": 3}, "sales_by_date_distribution": {"2023-01-15": 2, "2023-01-17": 3, "2023-01-18": 5, "2023-01-19": 10, "2023-01-20": 5, "2023-01-21": 0, "2023-01-22": 2, "2023-01-23": 1, "2023-01-24": 3, "2023-01-25": 1}}}
{"purpose": "Analyze machine downtime patterns and identify common failure causes in the manufacturing plant.", "raw_table": "Machine_ID,Date,Shift,Operator,Status,Downtime_Minutes,Failure_Code,Comments\nM01,2023/01/05,morning,John D,Running,,,\nM02,01-06-2023,Night,susan,killed,45,F02,overheated\nm03,2023-01-07,Afternoon, mike,Stopped,30,F01,Unexpected shutdown\nM01,2023/1/08,Morning,John D,Running,,,\nM02,2023/01/09,Night,susan,Running,,,\nM03,,Afternoon,Mike,Stopped,25,F01,Power failure\nM04,2023-01-10,Morning,,,,,\nm01,2023/01/11,Afternoon,John D,Stopped,15,F03,\nM02,2023-01-12,Night,Susan,killed,,F02,Maintenance needed\nM04,2023-01-13,Morning,Karen,Running,,,\nM03,2023-01-14,afternoon,mike,stopped,20,F01,Unexpected shutdown\nM01,2023/01/15,Morning,John D,Running,,,\nM04,2023-01-16,Morning,Karen,Stopped,10,F04,Sensor error\nM02,2023/01/17,Night,SUSAN,Stopped,35,F02,Overheated\nM03,2023-01-18,Afternoon,Mike,Running,,,\n", "eda_steps": ["Standardize and clean the 'Shift' and 'Status' categorical columns (fix capitalization and inconsistent values).", "Parse the 'Date' column into a consistent date format and identify missing dates.", "Check and summarize the percentage of missing values for each column.", "Compute descriptive statistics (mean, median, std) for 'Downtime_Minutes'.", "Generate value counts for 'Failure_Code' and 'Status'.", "Identify top 2 most frequent failure codes causing downtime.", "Analyze downtime minutes distribution grouped by 'Machine_ID'.", "Check number of unique operators and their frequency of operation.", "Examine correlation between downtime minutes and shift timing (morning, afternoon, night)."], "eda_results": {"missing_values": {"Machine_ID": 0, "Date": 1, "Shift": 0, "Operator": 0, "Status": 1, "Downtime_Minutes": 5, "Failure_Code": 2, "Comments": 6}, "value_counts": {"Status": {"Running": 7, "Stopped": 6, "Killed": 2}, "Failure_Code": {"F01": 3, "F02": 4, "F03": 1, "F04": 1, "": 5}, "Shift": {"Morning": 7, "Night": 5, "Afternoon": 5}, "Operator": {"John D": 4, "Susan": 4, "Mike": 4, "Karen": 2}}, "summary_stats": {"Downtime_Minutes": {"count": 10, "mean": 26.5, "median": 27.5, "std": 11.2, "min": 10, "max": 45}}, "top_categories": {"Failure_Code": [{"code": "F02", "count": 4}, {"code": "F01", "count": 3}]}, "downtime_by_machine": {"M01": 15, "M02": 80, "M03": 75, "M04": 10}, "operator_frequency": {"John D": 4, "Susan": 4, "Mike": 4, "Karen": 2}, "correlation_downtime_shift": {"Morning": 13.3, "Afternoon": 25, "Night": 38}}}
{"purpose": "Analyze user engagement patterns and content types on a social media platform.", "raw_table": "user_id,post_date,post_type,likes,comments,shares,content_length\nU001,2023-07-01,Image,100,15,5,250\nU002,07/02/2023,video,200,,10,180\nu003,2023/07/03,Text,50,5,,100\nU004,2023-7-04,IMAGE,NaN,7,3,300\nU005,2023-07-05,video,150,20,8,210\nU006,07-06-2023,text,80,NaN,2,90\nu007,,image,120,10,6,270\nU008,2023-07-08,Poll,60,3,1,50\nU009,2023/07/09,Video,NaN,12,4,190\nU010,July 10 2023,text,70,8,2,110", "eda_steps": ["Check and summarize missing values per column", "Standardize post_type categories to consistent capitalization", "Convert post_date into a uniform date format", "Compute descriptive statistics for numeric columns: likes, comments, shares, content_length", "Calculate value counts for post_type", "Identify posts with missing engagement metrics (likes, comments, shares)", "Analyze the distribution of content_length by post_type", "Determine correlation between likes, comments, shares, and content_length"], "eda_results": {"missing_values": {"user_id": 0, "post_date": 1, "post_type": 0, "likes": 2, "comments": 2, "shares": 1, "content_length": 0}, "standardized_post_type_counts": {"image": 3, "video": 3, "text": 3, "poll": 1}, "post_date_format_sample": ["2023-07-01", "2023-07-02", "2023-07-03", "2023-07-04", "2023-07-05", "2023-07-06", "2023-07-07 (missing, filled as null)", "2023-07-08", "2023-07-09", "2023-07-10"], "summary_stats": {"likes": {"count": 8, "mean": 105, "std": 56.7, "min": 50, "max": 200}, "comments": {"count": 8, "mean": 10, "std": 5.6, "min": 3, "max": 20}, "shares": {"count": 7, "mean": 5, "std": 3, "min": 1, "max": 10}, "content_length": {"count": 10, "mean": 165, "std": 84, "min": 50, "max": 300}}, "posts_missing_engagement_metrics": 3, "content_length_by_post_type": {"image": {"mean": 273, "min": 250, "max": 300}, "video": {"mean": 193, "min": 180, "max": 210}, "text": {"mean": 100, "min": 90, "max": 110}, "poll": {"mean": 50, "min": 50, "max": 50}}, "correlations": {"likes_comments": 0.85, "likes_shares": 0.78, "comments_shares": 0.69, "likes_content_length": 0.45}}}
{"purpose": "Analyze customer call behavior and service subscription patterns to identify potential churn risks.", "raw_table": "CustomerID,CallDuration,CallDate,SubscriptionPlan,Churned,Region\n1001,15.5,2023-01-15,basic,no,North\n1002,,01/18/2023,Premium,Yes,South\n1003,7.2,2023/01/20,BASIC,No,East\n1004,NaN,Jan 22 2023,premium,yes,west\n1005,3.8,2023-01-25,BasIc,No,south\n1006,22.1,2023-01-28,Premium,,North\n1007,5,01-30-2023,basic,no,Unknown\n1008,13.7,2023/02/01,PREMIUM,No,East\n1009,4.4,2023-02-02,basic,No,South\n1010,NaN,02/05/2023,basic,Yes,North\n1011,9.0,2023-02-07,premium,No,East\n1012,8.3,2023-02-10,,No,West\n1013,20.0,,Premium,Yes,South", "eda_steps": ["Check the data types and convert CallDate to a consistent date format", "Identify and count missing values in each column", "Compute descriptive statistics for CallDuration", "Generate value counts for SubscriptionPlan and Churned columns", "Examine unique values and inconsistencies in Region and SubscriptionPlan columns", "Calculate the percentage of churned customers by SubscriptionPlan", "Summarize average CallDuration by Region", "Identify rows with missing critical data for potential removal or imputation"], "eda_results": {"missing_values": {"CustomerID": 0, "CallDuration": 3, "CallDate": 1, "SubscriptionPlan": 1, "Churned": 1, "Region": 1}, "data_type_corrections": {"CallDate": "Converted mixed date formats to ISO standard YYYY-MM-DD"}, "summary_stats": {"CallDuration": {"count": 12, "mean": 10.41, "std": 6.45, "min": 3.8, "25%": 5.05, "50%": 8.65, "75%": 15.6, "max": 22.1}}, "value_counts": {"SubscriptionPlan": {"basic": 7, "premium": 5, "": 1}, "Churned": {"No": 7, "Yes": 4, "": 1}}, "inconsistencies": {"SubscriptionPlan": ["basic", "BASIC", "BasIc", "premium", "Premium", "PREMIUM", ""], "Region": ["North", "South", "East", "west", "Unknown", "West"]}, "churn_rate_by_subscription": {"basic": "28.6% (2 out of 7)", "premium": "60.0% (3 out of 5)"}, "average_call_duration_by_region": {"North": 18.8, "South": 4.1, "East": 10.0, "West": 8.3, "west": null, "Unknown": 5.0}, "rows_with_missing_critical_data": [2, 3, 5, 11, 12]}}
{"purpose": "Analyze customer churn patterns and usage behavior in a telecom dataset to identify potential risk factors.", "raw_table": "CustomerID,JoinDate,MonthlySpend,Churn,ContractType,DataUsageGB,CustomerServiceCalls\nC001,2021/01/15,45.50,No,monthly,5.3,1\nC002,15-02-2021,NA,yes,Annual,12.1,3\nc003,2021-03-10,30.00,No,MONTHLY,2.5,\nC004,2021/04/05,25.75,no,Monthly,NA,0\nC005,2021-05-20,60.10,Yes,annual,20.0,2\nC006,,55.00,No,Monthly,15.4,1\nC007,2021/07/30,NaN,No,monthly,7.8,4\nc008,2021-08-15,40,YES,Annual,NA,2\nC009,2021-09-10,50.75,No,monthly,8.2,NaN\nC010,10/10/2021,35,,Monthly,4.9,1\nC011,2021-11-05,48.60,No,Monthly,6.7,0\nC012,2021/12/25,,Yes,annual,14.3,3", "eda_steps": ["Check and summarize missing values for each column", "Standardize date formats in JoinDate column", "Convert Churn and ContractType columns to consistent capitalization", "Compute descriptive statistics for MonthlySpend and DataUsageGB", "Count unique customers by ContractType", "Calculate value counts for Churn column", "Analyze correlation between MonthlySpend, DataUsageGB and CustomerServiceCalls", "Identify customers with more than 2 customer service calls and their churn status"], "eda_results": {"missing_values": {"CustomerID": 0, "JoinDate": 1, "MonthlySpend": 3, "Churn": 1, "ContractType": 0, "DataUsageGB": 3, "CustomerServiceCalls": 2}, "standardized_dates_sample": ["2021-01-15", "2021-02-15", "2021-03-10", "2021-04-05", "2021-05-20", "missing", "2021-07-30"], "value_counts": {"Churn": {"No": 7, "Yes": 4, "missing": 1}, "ContractType": {"Monthly": 8, "Annual": 4}}, "descriptive_statistics": {"MonthlySpend": {"count": 9, "mean": 43.17, "std": 10.64, "min": 25.75, "25%": 35.0, "50%": 45.5, "75%": 50.75, "max": 60.1}, "DataUsageGB": {"count": 9, "mean": 9.11, "std": 6.03, "min": 2.5, "25%": 5.3, "50%": 7.8, "75%": 14.3, "max": 20.0}}, "unique_customers_by_contract": {"Monthly": 8, "Annual": 4}, "correlations": {"MonthlySpend_DataUsageGB": 0.91, "MonthlySpend_CustomerServiceCalls": -0.25, "DataUsageGB_CustomerServiceCalls": -0.31}, "high_service_calls_customers": {"count": 3, "churned_count": 1, "not_churned_count": 2}}}
{"purpose": "Explore patient demographics and clinical measurements to identify data quality issues and distribution patterns.", "raw_table": "Patient_ID,Age,Gender,Diagnosis,Blood_Pressure,Cholesterol,Admission_Date,Discharge_Date\n001,45,male,Hypertension,130/85,200,2022-01-15,2022-01-20\n002,38,Female,diabetes,120/80,NA,15-02-2022,20-02-2022\n003,54,M,Asthma,140/90,180,2022/03/05,2022/03/10\n004,,female,Hypertension,125/82,190,2022-04-12,2022-04-18\n005,29,F,Diabetes,118/75,170,2022-05-20,2022-05-25\n006,61,Male,Hypertension,135/88,210,,2022-06-30\n007,48,m,F,bp:128/84,195,2022-07-15,2022-07-20\n008,50,M,asthma,142/92,205,2022-08-10,2022-08-15\n009,43,Female,Hypertension,130/85,198,2022-09-05,09/09/2022\n010,37,male,Diabetes,,185,2022-10-12,2022-10-18\n011,55,F,Hypertension,132/86,202,2022-11-22,2022-11-28\n012,60,M,Asthma,138/89,,2022-12-01,2022-12-07\n", "eda_steps": ["Check for missing values in all columns", "Standardize Gender column values to consistent lowercase labels", "Parse and standardize Admission_Date and Discharge_Date formats to ISO", "Compute descriptive statistics for Age and Cholesterol columns", "Extract systolic and diastolic values from Blood_Pressure and compute their summary statistics", "Generate value counts for Diagnosis categories", "Calculate length of stay in days for each patient", "Identify any outlier values in Age and Cholesterol"], "eda_results": {"missing_values": {"Patient_ID": 0, "Age": 1, "Gender": 0, "Diagnosis": 0, "Blood_Pressure": 1, "Cholesterol": 2, "Admission_Date": 1, "Discharge_Date": 0}, "standardized_gender_counts": {"male": 4, "female": 5, "m": 3, "f": 2}, "date_format_issues": {"Admission_Date": 3, "Discharge_Date": 1}, "age_summary_stats": {"count": 14, "mean": 47.14, "std": 9.78, "min": 29, "25%": 38, "50%": 48, "75%": 55, "max": 61}, "cholesterol_summary_stats": {"count": 12, "mean": 192.5, "std": 12.3, "min": 170, "25%": 185, "50%": 195, "75%": 202, "max": 210}, "blood_pressure_extraction": {"systolic": {"mean": 131, "min": 118, "max": 142, "missing": 1}, "diastolic": {"mean": 85, "min": 75, "max": 92, "missing": 1}}, "diagnosis_value_counts": {"Hypertension": 5, "Diabetes": 3, "Asthma": 4}, "length_of_stay_days": {"mean": 5.2, "min": 5, "max": 6, "missing_admission_date": 1}, "outliers_detected": {"Age": false, "Cholesterol": false}}}
{"purpose": "Analyze customer call patterns and identify data quality issues in call logs.", "raw_table": "call_id,customer_id,call_start,call_duration,call_type,call_cost\n1,1234,2023-05-01 08:15:00,300,VOICE,2.50\n2,1235,05/02/2023 09:30,180,voice,1.80\n3,1234,2023-05-3 10:00:00,missing,VOICE,2.50\n4,1236,2023-05-04 11:20,240,VOICE,2.00\n5,1237,2023/05/05 12:00:00,120,VoIP,1.00\n6,1238,,60,VOIP,0.80\n7,1239,2023-05-07 14:15:00,NA,voice,1.20\n8,1240,2023-05-08 15:45:00,200,SMS,0.50\n9,1241,2023-05-09 16:00:00,100,sms,0.50\n10,1242,2023-05-10 17:30:00,150,Voice,1.75\n", "eda_steps": ["Check for missing values in each column", "Standardize the call_type column values to uppercase", "Parse and standardize call_start dates to a consistent datetime format", "Calculate descriptive statistics for call_duration and call_cost", "Count unique customers and calls per customer", "Identify the frequency of each call_type", "Analyze the distribution of call durations", "Check for any correlation between call_duration and call_cost"], "eda_results": {"missing_values": {"call_id": 0, "customer_id": 0, "call_start": 1, "call_duration": 2, "call_type": 0, "call_cost": 0}, "standardized_call_type_value_counts": {"VOICE": 6, "VOIP": 2, "SMS": 2}, "call_start_date_range": {"min": "2023-05-01 08:15:00", "max": "2023-05-10 17:30:00"}, "call_duration_stats": {"count": 8, "mean": 176.25, "std": 81.73, "min": 60, "max": 300}, "call_cost_stats": {"count": 10, "mean": 1.455, "std": 0.73, "min": 0.5, "max": 2.5}, "unique_customers": 10, "calls_per_customer": {"1234": 2, "others": 8}, "call_type_frequency": {"VOICE": 6, "VOIP": 2, "SMS": 2}, "correlation_call_duration_cost": 0.97}}
{"purpose": "Analyze temperature and precipitation patterns across different climate zones to identify seasonal trends and data quality issues.", "raw_table": "Date,Location,Climate_Zone,Temperature_C,Precipitation_mm\n2023-01-05,New York,Temperate,3.4,5.2\n01/12/2023,los angeles,Arid,18.5,0\n2023-02-20,Miami,Tropical,26.7,112.3\n2023-03-15,Seattle,temperate,10.2,25.1\n2023-04-10,Denver,Alpine,12.0,NaN\n2023-05-01,Houston,TROPICAL,28.1,85.6\n2023-06-14,las vegas,Arid,40.3,1.4\n2023-07-22,Anchorage,Polar,15.0,7.8\n2023-08-30,Miami,Tropical,29.9,MISSING\n2023-09-10,Denver,Alpine,17.5,10.2\n2023-10-05,Seattle,Temperate,13.0,30.0\n2023/11/20,New York,Temperate,7.8,8.3\n2023-12-31,Houston,Tropical,N/A,90.1\n", "eda_steps": ["Parse the Date column to a standard date format", "Standardize capitalization in Location and Climate_Zone columns", "Identify and quantify missing values in Temperature_C and Precipitation_mm", "Compute summary statistics (mean, median, std) for numeric columns", "Generate value counts for Climate_Zone", "Check distribution skewness for Temperature_C", "Identify any outliers in Temperature_C using IQR method", "Calculate correlation between Temperature_C and Precipitation_mm where data is available"], "eda_results": {"missing_values": {"Temperature_C": 1, "Precipitation_mm": 2}, "summary_stats": {"Temperature_C": {"mean": 18.4, "median": 15.0, "std_dev": 9.6, "min": 3.4, "max": 40.3}, "Precipitation_mm": {"mean": 29.2, "median": 10.2, "std_dev": 37.1, "min": 0, "max": 112.3}}, "value_counts": {"Climate_Zone": {"Temperate": 4, "Tropical": 4, "Arid": 2, "Alpine": 2, "Polar": 1}}, "skewness": {"Temperature_C": 0.85}, "outliers_temperature_C": [{"Date": "2023-06-14", "Location": "Las Vegas", "Temperature_C": 40.3}], "correlations": {"Temperature_C_Precipitation_mm": -0.23}}}
{"purpose": "Examine patient demographics and vital signs to identify patterns in hospital admissions.", "raw_table": "Patient_ID,Admission_Date,Age,Gender,Blood_Pressure,Heart_Rate,Diagnosis\n001,2023-01-15,45,Male,120/80,72,Hypertension\n002,15/02/2023,38,Female,130/85,80,diabetes\n003,,58,male,140/90,88,Hypertension\n004,2023/03/10,NA,Female,110/70,75,Asthma\n005,2023-03-22,29,F,115/75,NA,bronchitis\n006,04/15/2023,33,Male,125/82,78,\n007,2023-04-18,47,MALE,135/89,,Diabetes\n008,2023-04-20,52,Female,NA,85,Hypertension\n009,2023-04-22,61,Female,142/95,90,COPD\n010,2023-04-25,55,Male,138/85,82,asthma\n011,2023-04-27,44,F,120/80,77,Diabetes", "eda_steps": ["Check for missing values in each column", "Normalize and clean inconsistent capitalization in Gender and Diagnosis columns", "Parse and standardize Admission_Date formats", "Compute descriptive statistics for Age and Heart_Rate columns", "Extract systolic and diastolic values from Blood_Pressure and analyze separately", "Generate value counts for Diagnosis categories", "Analyze distribution of Age by Gender", "Identify entries with missing or inconsistent vital sign data"], "eda_results": {"missing_values": {"Patient_ID": 0, "Admission_Date": 1, "Age": 1, "Gender": 0, "Blood_Pressure": 2, "Heart_Rate": 2, "Diagnosis": 1}, "cleaned_categories": {"Gender": {"Male": 5, "Female": 5}, "Diagnosis": {"Hypertension": 3, "Diabetes": 3, "Asthma": 2, "Bronchitis": 1, "COPD": 1, "Missing": 1}}, "admission_dates": {"standardized_format": "YYYY-MM-DD", "missing_dates": ["003"]}, "age_stats": {"count": 13, "mean": 46.7, "min": 29, "max": 61, "missing": 1}, "heart_rate_stats": {"count": 9, "mean": 81.2, "min": 72, "max": 90, "missing": 2}, "blood_pressure_parsed": {"systolic": {"mean": 128.4, "min": 110, "max": 142, "missing": 2}, "diastolic": {"mean": 83.4, "min": 70, "max": 95, "missing": 2}}, "diagnosis_counts": {"Hypertension": 3, "Diabetes": 3, "Asthma": 2, "Bronchitis": 1, "COPD": 1, "Missing": 1}, "age_by_gender": {"Male": {"mean_age": 43.6, "count": 5}, "Female": {"mean_age": 49.8, "count": 5}}, "inconsistent_vital_signs": {"entries_missing_heart_rate": ["005", "007"], "entries_missing_blood_pressure": ["008", "003"]}}}
{"purpose": "Analyze customer purchase behavior and product category performance in the online store.", "raw_table": "OrderID,CustomerID,OrderDate,ProductCategory,Quantity,PricePerUnit,PaymentMethod,DeliveryStatus\n1001,C001,2023/01/15,Electronics,2,299.99,Credit Card,delivered\n1002,c002,15-01-2023,Home & kitchen,1,89.50,Paypal,Delivered\n1003,C003,2023-01-16,Electronics, ,149.99,CREDIT card,Pending\n1004,C004,01/17/2023,beauty,3,19.99,Cash,delivered\n1005,C001,2023-01-18,Home & Kitchen,2,,Paypal,Delivered\n1006,C005,2023-01-18,beauty,1,25.00,Cash,pending\n1007,C006,,electronics,1,299.99,Credit Card,Delivered\n1008,C007,2023/01/20,TOYS,4,15.00,Credit Card,Delivered\n1009,C008,2023-01-21,beauty,2,19.99,Paypal,delivered\n1010,C003,2023-1-22,home & kitchen,1,89.50,Credit Card,Delivered\n1011,C009,2023/01/22,Electronics,1,299.99,Credit card,Cancelled\n1012,,2023-01-23,Electronics,2,299.99,Credit Card,Delivered\n1013,C010,2023/01/24,TOYS,,15.00,Credit Card,Delivered", "eda_steps": ["Check the number of missing values per column", "Standardize the capitalization of categorical columns ProductCategory, PaymentMethod, and DeliveryStatus", "Identify unique product categories and count their occurrences", "Compute descriptive statistics for Quantity and PricePerUnit columns", "Calculate total sales amount per order as Quantity multiplied by PricePerUnit", "Find frequency counts of different PaymentMethod values", "Summarize delivery status counts", "Identify orders with missing Quantity or PricePerUnit and count them", "Analyze correlation between Quantity and PricePerUnit"], "eda_results": {"missing_values": {"OrderID": 0, "CustomerID": 1, "OrderDate": 1, "ProductCategory": 0, "Quantity": 3, "PricePerUnit": 1, "PaymentMethod": 0, "DeliveryStatus": 0}, "standardized_categories": {"ProductCategory": ["Electronics", "Home & Kitchen", "Beauty", "Toys"], "PaymentMethod": ["Credit Card", "Paypal", "Cash"], "DeliveryStatus": ["Delivered", "Pending", "Cancelled"]}, "value_counts_product_category": {"Electronics": 6, "Home & Kitchen": 3, "Beauty": 3, "Toys": 2}, "descriptive_stats": {"Quantity": {"count": 12, "mean": 1.75, "std": 1.05, "min": 1, "25%": 1, "50%": 1.5, "75%": 2, "max": 4}, "PricePerUnit": {"count": 14, "mean": 129.13, "std": 129.62, "min": 15.0, "25%": 19.99, "50%": 89.5, "75%": 299.99, "max": 299.99}}, "total_sales_per_order": {"1001": 599.98, "1002": 89.5, "1003": null, "1004": 59.97, "1005": null, "1006": 25.0, "1007": 299.99, "1008": 60.0, "1009": 39.98, "1010": 89.5, "1011": 299.99, "1012": 599.98, "1013": null}, "payment_method_counts": {"Credit Card": 8, "Paypal": 3, "Cash": 2}, "delivery_status_counts": {"Delivered": 9, "Pending": 2, "Cancelled": 1}, "orders_with_missing_quantity_or_price": 4, "correlation_quantity_price": -0.15}}
{"purpose": "Explore temperature and precipitation patterns across different climate zones to identify data quality issues and preliminary trends.", "raw_table": "Date,Location,Climate_Zone,Avg_Temperature_C,Precipitation_mm\n2024-01-15,Tropica, Tropical, 29.5, 120\n15/02/2024,Arctic,polar, -15.2, \n2024-Mar-10,Desert,Arid,35.1,2\n2024-04-05, tropica,Tropical, 30.2 ,150\n2024/05/20,Arctic,Polar,-12.5, 5\n2024-06-18,DESERT,arid,40.6, 0\n2024-07-10,Tropica,tropical,, 80\n2024-08-01,desert,Arid,39.2,0.5\n2024-09-12,Arctic,Polar,-10, 7\n2024-10-30,Tropica,Tropical,28.0, 100\n2024-11-15,Desert,Arid,38.0, 0\n2024-12-25,Arctic,polar,-13.3, 4.2", "eda_steps": ["Standardize the Climate_Zone column capitalization", "Parse and unify the Date column into YYYY-MM-DD format", "Identify and count missing values in Avg_Temperature_C and Precipitation_mm", "Compute descriptive statistics for Avg_Temperature_C and Precipitation_mm", "Generate value counts for Climate_Zone", "Calculate mean temperature and precipitation per Climate_Zone", "Check correlation between Avg_Temperature_C and Precipitation_mm"], "eda_results": {"missing_values": {"Avg_Temperature_C": 1, "Precipitation_mm": 1}, "value_counts": {"Climate_Zone": {"Tropical": 4, "Polar": 4, "Arid": 4}}, "summary_stats": {"Avg_Temperature_C": {"count": 11, "mean": 20.1, "std": 21.8, "min": -15.2, "25%": -12.5, "50%": 28.0, "75%": 38.0, "max": 40.6}, "Precipitation_mm": {"count": 11, "mean": 37.6, "std": 56.3, "min": 0, "25%": 0, "50%": 5, "75%": 100, "max": 150}}, "mean_by_climate_zone": {"Tropical": {"Avg_Temperature_C": 29.25, "Precipitation_mm": 112.5}, "Polar": {"Avg_Temperature_C": -12.75, "Precipitation_mm": 4.8}, "Arid": {"Avg_Temperature_C": 38.225, "Precipitation_mm": 0.625}}, "correlations": {"Avg_Temperature_C_vs_Precipitation_mm": -0.74}}}
{"purpose": "Analyze student performance and attendance patterns across different courses to identify trends and data quality issues.", "raw_table": "Student_ID,Name,Course,Score,Attendance_Percentage,Enrollment_Date\n101,alice,Mathematics,85,95,2023-01-15\n102,Bob,physics,78,,2023/01/20\n103,CHARLIE,Chemistry,92,88,15-02-2023\n104,diana,mathematics,missing,90,2023-02-18\n105,Edward,Physics,85,85,2023-02-20\n106,frank,chemistry,88,missing,2023/02/25\n107,Gina,Mathematics,91,93,2023-03-01\n108,harry,,75,80,2023-03-05\n109,Ian,Physics,89,87,03/10/2023\n110,Julia,Chemistry,95,100,2023.03.12\n111,Kate,Mathematics,NA,92,2023-03-15\n112,Luke,Physics,82,84,2023/03/18", "eda_steps": ["Standardize course names to title case", "Convert Enrollment_Date to a consistent date format", "Identify and count missing values in each column", "Compute descriptive statistics for the Score and Attendance_Percentage columns", "Generate value counts for the Course column", "Assess correlation between Score and Attendance_Percentage", "Identify records with missing or non-numeric Score values", "Summarize enrollment dates by month"], "eda_results": {"missing_values": {"Student_ID": 0, "Name": 0, "Course": 1, "Score": 3, "Attendance_Percentage": 2, "Enrollment_Date": 0}, "value_counts": {"Course": {"Mathematics": 4, "Physics": 4, "Chemistry": 3, "": 1}}, "summary_stats": {"Score": {"count": 9, "mean": 86.78, "std": 6.62, "min": 75, "25%": 82, "50%": 87, "75%": 91, "max": 95}, "Attendance_Percentage": {"count": 10, "mean": 89.4, "std": 6.6, "min": 80, "25%": 85, "50%": 89, "75%": 93, "max": 100}}, "correlations": {"Score_Attendance_Percentage": 0.72}, "non_numeric_scores": [104, 111], "enrollment_by_month": {"January 2023": 2, "February 2023": 4, "March 2023": 6}}}
{"purpose": "Analyze citizen complaints received by the city government to identify common issues and data quality.", "raw_table": "Complaint_ID,Received_Date,Department,Complaint_Type,Status,Priority,Response_Time_Days\nC001,2023-01-15,Sanitation,Trash Pickup,Closed,High,2\nC002,01/20/2023,water,Water Leak,closed,Medium,5\nc003,2023/02/05,Transportation,Pothole,Open,,\nC004,,sanitation,Trash pickup,Closed,Low,1\nC005,2023-02-20,Water,Water Quality,Open,High,NA\nC006,2023-03-02,TRANSPORTATION,Street Light,Closed,Medium,3\nC007,15-03-2023,Health,Noise Complaint,Open,Low,7\nC008,2023-03-18,health,Noise complaint,Closed,Low,2\nC009,2023-03-20,Sanitation,Illegal Dumping,Closed,High,4\nC010,2023-03-25,water,Water Leak,Open,Medium,NA\nC011,03/27/2023,Transportation,Pothole,Open,High,6\nC012,2023-03-30,Health,,Closed,Medium,3\nC013,2023-04-01,Sanitation,Trash pickup,Closed,Low,1\nC014,2023-04-05,Water,Water Leak,Closed,High,2\nC015,2023-04-08,,Street Light,Open,Medium,5", "eda_steps": ["Check for missing values in each column and calculate their percentages", "Standardize the capitalization of categorical columns: Department, Complaint_Type, Status, Priority", "Parse and standardize the Received_Date column to a uniform date format", "Generate value counts for Department and Complaint_Type columns", "Compute descriptive statistics for the numeric column Response_Time_Days", "Identify how many complaints are still open versus closed", "Analyze the average response time by Priority level", "Find inconsistent or missing Complaint_Type entries"], "eda_results": {"missing_values": {"Complaint_ID": "0%", "Received_Date": "6.7%", "Department": "6.7%", "Complaint_Type": "6.7%", "Status": "0%", "Priority": "6.7%", "Response_Time_Days": "20%"}, "standardized_departments": {"Sanitation": 4, "Water": 4, "Transportation": 3, "Health": 3, "": 1}, "standardized_complaint_types": {"Trash Pickup": 3, "Water Leak": 4, "Pothole": 2, "Water Quality": 1, "Street Light": 2, "Noise Complaint": 2, "Illegal Dumping": 1, "": 1}, "status_counts": {"Open": 7, "Closed": 8}, "response_time_stats": {"count": 12, "mean": 3.42, "std_dev": 1.85, "min": 1, "max": 7}, "average_response_time_by_priority": {"High": 3.25, "Medium": 3.5, "Low": 2.0, "": null}, "inconsistent_complaint_types": 1}}
{"purpose": "Analyze customer purchase behavior and product category popularity in an ecommerce store.", "raw_table": "order_id,customer_id,order_date,product_category,quantity,price_usd,payment_method\n1001,C001,2023-01-15,Electronics,2,299.99,Credit Card\n1002,C002,15/01/2023,home & Kitchen,1,89.5,Paypal\n1003,C003,2023/01/16,electronics,3,,Credit Card\n1004,C004,01-17-2023,Books,2,15.99,CREDIT card\n1005,C005,2023-01-18,Toys,,23.5,Cash\n1006,C006,2023-01-18,Home & kitchen,1,,paypal\n1007,C007,,Books,1,12.5,Cash\n1008,,2023-01-19,Electronics,1,199.99,Credit card\n1009,C009,2023-01-20,Toys,4,23.5,cash\n1010,C010,2023/01/21,Books,2,15.5,Paypal\n1011,C011,2023-1-22,Fashion,1,49.99,Credit card\n1012,C012,2023-01-22,Fashion,2,39.99,Credit Card\n1013,C013,01/23/2023,Electronics,1,299.99,credit card\n1014,C014,2023-01-23,Toys,3,23.5,CASH", "eda_steps": ["Check for missing values in all columns", "Standardize product_category names to title case", "Parse order_date into a consistent date format", "Compute descriptive statistics for quantity and price_usd", "Generate value counts for product_category", "Identify unique payment_method types with counts", "Calculate total sales per product_category", "Find orders with missing customer_id or order_date"], "eda_results": {"missing_values": {"order_id": 0, "customer_id": 1, "order_date": 1, "product_category": 0, "quantity": 2, "price_usd": 2, "payment_method": 0}, "standardized_product_categories": {"Electronics": 5, "Home & Kitchen": 2, "Books": 3, "Toys": 4, "Fashion": 2}, "order_date_consistency": "All dates parsed to ISO format YYYY-MM-DD except 1 missing", "summary_stats": {"quantity": {"count": 13, "mean": 1.77, "std": 1.01, "min": 1, "25%": 1, "50%": 2, "75%": 2, "max": 4}, "price_usd": {"count": 12, "mean": 112.58, "std": 115.58, "min": 12.5, "25%": 23.5, "50%": 39.99, "75%": 89.5, "max": 299.99}}, "product_category_value_counts": {"Electronics": 5, "Toys": 4, "Books": 3, "Home & Kitchen": 2, "Fashion": 2}, "payment_method_counts": {"Credit Card": 6, "Paypal": 2, "Cash": 4}, "total_sales_per_category": {"Electronics": 1799.94, "Home & Kitchen": 89.5, "Books": 59.98, "Toys": 141.0, "Fashion": 129.97}, "orders_missing_customer_or_date": {"missing_customer_id": [1008], "missing_order_date": [1007]}}}
{"purpose": "Analyze user engagement patterns and content types on a social media platform.", "raw_table": "post_id,user_id,post_date,content_type,likes,comments,shares\n001,UserA,2023/03/15,Image,120,15,5\n002,userb,15-03-2023,video,85,,2\n003,USERC,2023-03-16,Text,45,7,NaN\n004,UserD,03/17/2023,IMAGE,NaN,3,1\n005,userE,2023/3/18,video,100,18,4\n006,,2023-03-19,text,60,6,3\n007,userG,2023-03-20,Poll,30,2,0\n008,UserH,2023/03/21,video,NaN,,2\n009,userI,2023-03-22,Image,75,12,3\n010,UserJ,23-03-2023,Video,90,9,4\n011,userK,2023/03/24,text,50,5,1\n012,userL,2023/03/25,Story,,1,0\n013,userM,2023-03-26,story,40,,1\n014,userN,2023/03/27,Image,110,20,6", "eda_steps": ["Check and report missing values in each column", "Normalize content_type values to lowercase and group similar types", "Convert post_date to a consistent date format", "Calculate descriptive statistics for likes, comments, and shares", "Generate value counts for content_type", "Identify posts with missing user_id", "Find correlation between likes, comments, and shares", "Find top 3 most engaged posts by total interactions (likes + comments + shares)"], "eda_results": {"missing_values": {"post_id": 0, "user_id": 1, "post_date": 0, "content_type": 0, "likes": 3, "comments": 4, "shares": 1}, "normalized_content_types": {"image": 4, "video": 4, "text": 3, "poll": 1, "story": 2}, "post_date_format": "All dates converted to yyyy-mm-dd", "summary_stats": {"likes": {"count": 11, "mean": 71.36, "std": 29.25, "min": 30, "25%": 45, "50%": 75, "75%": 100, "max": 120}, "comments": {"count": 10, "mean": 8.6, "std": 6.01, "min": 1, "25%": 3, "50%": 7, "75%": 15, "max": 20}, "shares": {"count": 13, "mean": 2.46, "std": 1.91, "min": 0, "25%": 1, "50%": 2, "75%": 4, "max": 6}}, "posts_missing_user_id": ["006"], "correlations": {"likes_comments": 0.91, "likes_shares": 0.84, "comments_shares": 0.76}, "top_3_engaged_posts": [{"post_id": "001", "total_interactions": 140}, {"post_id": "014", "total_interactions": 136}, {"post_id": "005", "total_interactions": 122}]}}
{"purpose": "Examine public service request patterns and data quality in a city government 311 system.", "raw_table": "Request_ID,Request_Type,Date_Received,Status,Priority,Department\n1001,Street Light Out,2024-03-01,Closed,High,Public Works\n1002,Pothole,3/5/2024,open,medium,public works\n1003,Graffiti,2024/03/07,Closed,,Parks & Recreation\n1004,Noise Complaint,03-08-2024,In Progress,Low,Police\n1005,Street light out,2024-03-09,Closed,High,Public works\n1006,,2024-03-10,Closed,Medium,Public Works\n1007,Pothole,March 11 2024,Open,Medium,Public Works\n1008,Graffiti,2024-03-12,closed,High,Parks & Recreation\n1009,Noise Complaint,2024/03/13,In progress,,Police\n1010,Pothole,,Open,Medium,Public Works\n1011,Street Light Out,2024-03-15,Closed,High,Public Works\n1012,Graffiti,2024-03-16,Closed,Medium,parks & recreation\n1013,Noise Complaint,2024-03-17,Open,Low,Police\n1014,Pothole,2024-03-18,Closed,Medium,Public Works", "eda_steps": ["Check the number of missing values in each column", "Standardize capitalization in Request_Type and Department columns", "Identify frequency counts of each Request_Type", "Calculate the percentage of requests by Status", "Summarize the distribution of Priority values", "Parse Date_Received to a consistent date format and check for missing dates", "Count requests per Department", "Analyze the relationship between Priority and Status"], "eda_results": {"missing_values": {"Request_ID": 0, "Request_Type": 1, "Date_Received": 1, "Status": 0, "Priority": 2, "Department": 0}, "standardized_request_types": {"Street Light Out": 4, "Pothole": 5, "Graffiti": 4, "Noise Complaint": 4, "Missing": 1}, "status_counts": {"Closed": 7, "Open": 4, "In Progress": 2, "In progress": 1}, "priority_distribution": {"High": 4, "Medium": 6, "Low": 2, "Missing": 2}, "parsed_dates_missing": 1, "requests_per_department": {"Public Works": 8, "Parks & Recreation": 4, "Police": 4}, "priority_vs_status": {"High": {"Closed": 4, "Open": 0, "In Progress": 0}, "Medium": {"Closed": 3, "Open": 4, "In Progress": 0}, "Low": {"Closed": 0, "Open": 1, "In Progress": 2}, "Missing": {"Closed": 0, "Open": 1, "In Progress": 0}}}}
{"purpose": "Analyze temperature and precipitation patterns across multiple climate zones over a one-week period.", "raw_table": "Date,Location,Climate_Zone,Temperature_C,Precipitation_mm\n2024-06-01,Tahoe,Temperate,22,0.0\n2024/06/02,tahoe,temperate,21.5,0\n2024-6-03,Tahoe,Temperate,,0.2\n2024-06-04,Death valley,Arid,45.3,0\n2024-06-05,Death Valley,arid,44.8,0.1\n06-06-2024,Siberia,Polar,-15,5\n2024-06-07,Siberia,polar,-14.2,\n2024-06-07,Amazon,Rainforest,30.1,12.4\n2024-06-01,amazon,Rainforest,29.8,13\n2024-06-03,Amazon,Rainforest,31.2,11.7\n2024-06-02,Sahara,Desert,40,,\n2024-06-04,sahara,Desert,42.1,0\n2024-06-05,Greenland,Polar,-5.5,3.2\n2024-06-05,Greenland,polar,-6.0,3.1\n2024-06-06,Death Valley,arid,46,0", "eda_steps": ["Standardize the date format to YYYY-MM-DD", "Normalize capitalization in Location and Climate_Zone columns", "Identify and count missing values in Temperature_C and Precipitation_mm", "Compute summary statistics (mean, median, std) for Temperature_C by Climate_Zone", "Calculate total precipitation by Location", "Generate value counts for Climate_Zone", "Check for duplicate rows", "Analyze correlation between Temperature_C and Precipitation_mm", "Identify dates with missing Temperature_C or Precipitation_mm values"], "eda_results": {"missing_values": {"Temperature_C": 1, "Precipitation_mm": 3}, "value_counts": {"Climate_Zone": {"Polar": 4, "Temperate": 3, "Arid": 4, "Rainforest": 3, "Desert": 2}}, "summary_stats_temperature_by_climate_zone": {"Temperate": {"mean": 21.75, "median": 21.5, "std": 0.354}, "Arid": {"mean": 45.02, "median": 45.15, "std": 0.53}, "Polar": {"mean": -10.18, "median": -10.75, "std": 4.8}, "Rainforest": {"mean": 30.37, "median": 30.1, "std": 0.7}, "Desert": {"mean": 41.05, "median": 41.05, "std": 1.5}}, "total_precipitation_by_location": {"Tahoe": 0.2, "Death Valley": 0.1, "Siberia": 5, "Amazon": 37.1, "Sahara": 0, "Greenland": 6.3}, "duplicates_found": 0, "correlation_temperature_precipitation": -0.45, "dates_with_missing_values": {"Temperature_C": ["2024-06-03"], "Precipitation_mm": ["2024-06-07", "2024-06-02"]}}}
{"purpose": "Analyze user engagement patterns and content types on a social media platform.", "raw_table": "User_ID,Post_Date,Content_Type,Likes,Comments,Shares,User_Age,User_Location\n001,2023/03/01,photo,120,15,5,25,New york\n002,03-02-2023,Video,80,,2,30,los angeles\n003,2023-03-03,text,45,7,0,22,San Francisco\n004,2023/03/04,link,20,2,1,NaN,Boston\n005,03/05/2023,PHOTO,150,20,10,28,New York\n006,2023-03-06,video,,5,3,27,\n007,2023-3-07,text,60,8,,24,Chicago\n008,03-08-2023,photo,130,18,7,26,new york\n009,March 9, 2023,Link,25,3,1,31,Los Angeles\n010,2023/03/10,text,50,6,0,23,San Francisco\n011,2023/03/11,video,90,10,4,,Boston\n012,2023-03-12,photo,140,22,8,29,New York\n013,03-13-2023,video,85,9,3,26,ChIcAgO\n014,2023/03/14,text,55,7,1,25,Boston", "eda_steps": ["Check and summarize missing values for each column", "Standardize Content_Type values to lowercase", "Standardize User_Location names capitalization", "Parse Post_Date into a uniform date format", "Compute descriptive statistics for numeric columns: Likes, Comments, Shares, User_Age", "Generate value counts for Content_Type", "Generate value counts for User_Location", "Calculate correlation between Likes, Comments, Shares, and User_Age", "Identify top 3 most common Content_Type categories"], "eda_results": {"missing_values": {"User_ID": 0, "Post_Date": 0, "Content_Type": 0, "Likes": 2, "Comments": 1, "Shares": 2, "User_Age": 2, "User_Location": 1}, "standardized_content_type_counts": {"photo": 5, "video": 4, "text": 4, "link": 2}, "standardized_user_location_counts": {"New York": 4, "Los Angeles": 2, "San Francisco": 2, "Boston": 3, "Chicago": 2, "": 1}, "post_date_uniform_format": ["2023-03-01", "2023-03-02", "2023-03-03", "2023-03-04", "2023-03-05", "2023-03-06", "2023-03-07", "2023-03-08", "2023-03-09", "2023-03-10", "2023-03-11", "2023-03-12", "2023-03-13", "2023-03-14"], "summary_stats": {"Likes": {"count": 12, "mean": 84.58, "std": 44.77, "min": 20, "25%": 50, "50%": 82.5, "75%": 130, "max": 150}, "Comments": {"count": 13, "mean": 10.23, "std": 6.03, "min": 2, "25%": 6, "50%": 7, "75%": 15, "max": 22}, "Shares": {"count": 12, "mean": 3.83, "std": 3.2, "min": 0, "25%": 1, "50%": 3, "75%": 5, "max": 10}, "User_Age": {"count": 12, "mean": 26.25, "std": 2.89, "min": 22, "25%": 24, "50%": 26, "75%": 28, "max": 31}}, "correlations": {"Likes_Comments": 0.95, "Likes_Shares": 0.87, "Likes_User_Age": 0.12, "Comments_Shares": 0.85, "Comments_User_Age": 0.18, "Shares_User_Age": 0.1}, "top_3_content_types": ["photo", "video", "text"]}}
{"purpose": "Analyze monthly stock trading data to identify missing values, distribution of trade volumes, and sector-based price trends.", "raw_table": "Date,StockSymbol,Sector,Price,Volume\n2023-01-05,AAPL,technology,150.25,10000\n01/12/2023,GOOGL,Technology,2750.5,8500\n2023-02-15,MSFT,technology,295.1,NaN\n2023-02-20,AMZN,Consumer Discretionary,3300.45,12000\n2023-03-01,TSLA,Consumer discretionary,700.3,abc\n03-10-2023,BRK.B,Financials,285.67,4500\n2023/03/15,JPM,financials,157.55,7000\n2023-04-01,V,Financials,,8900\n2023-04-07,NFLX,Communication Services,510.75,5300\n2023-04-12,FB,communication services,260.4,4800\n2023-04-15,GOOGL,Technology,2800.15,9000\n2023-05-01,AAPL,Technology,NaN,11000\n2023-05-05,MSFT,Technology,305.2,9500", "eda_steps": ["Check for and summarize missing values in each column", "Standardize the 'Date' column to ISO format YYYY-MM-DD", "Convert 'Volume' to numeric and handle non-numeric entries", "Generate descriptive statistics for 'Price' and 'Volume'", "Calculate value counts for 'Sector' and identify unique categories", "Analyze average 'Price' by 'Sector'", "Identify records with inconsistent capitalization in 'Sector'", "Compute correlation between 'Price' and 'Volume' after cleaning"], "eda_results": {"missing_values": {"Date": 0, "StockSymbol": 0, "Sector": 0, "Price": 2, "Volume": 2}, "date_standardization": {"dates_standardized": true, "unique_formats_found": ["YYYY-MM-DD", "MM/DD/YYYY", "MM-DD-YYYY", "YYYY/MM/DD"]}, "volume_conversion_issues": {"non_numeric_entries": ["abc"], "converted_missing": 1}, "summary_stats": {"Price": {"count": 13, "mean": 987.7, "median": 295.1, "min": 150.25, "max": 3300.45, "std": 1112.48}, "Volume": {"count": 12, "mean": 7950.0, "median": 7450.0, "min": 4500, "max": 12000, "std": 2540.97}}, "value_counts_sector": {"Technology": 6, "Consumer Discretionary": 2, "Financials": 3, "Communication Services": 2}, "sector_capitalization_issues": {"Consumer discretionary": 1, "communication services": 1, "financials": 1}, "average_price_by_sector": {"Technology": 1133.66, "Consumer Discretionary": 2000.38, "Financials": 147.61, "Communication Services": 385.58}, "price_volume_correlation": 0.58}}
{"purpose": "Analyze retail transaction patterns and identify data quality issues in sales records.", "raw_table": "TransactionID,Date,ProductCategory,Quantity,Price,CustomerID,StoreLocation\nT001,2024-01-15,Electronics,2,399.99,C123,New York\nT002,01/16/2024,home appliances,1,89.5,C124,los angeles\nT003,2024/01/17,Clothing,3,29.99,C125,Chicago\nT004,,electronics,1,,C126,New york\nT005,2024-01-19,Furniture,1,199.99,,Houston\nT006,January 20 2024,Clothing,2,19.99,C128,\nT007,2024-01-21,Toys,5,9.99,C129,Chicago\nT008,2024/01/22,Toys,NaN,12.5,C130,Los Angeles\nT009,2024-01-23,Home Appliances,1,89.5,C131,Houston\nT010,2024-01-24,clothing,2,29.99,C132,CHICAGO\nT011,2024-01-25,Furniture,1,199.99,C133,New York\nT012,2024-01-26,Electronics,1,399.99,C134,Los Angeles\nT013,2024/01/27,Home appliances,,89.5,C135,Houston\nT014,01-28-2024,Toys,3,9.99,C136,Chicago", "eda_steps": ["Check for missing values in each column", "Standardize date formats and parse as datetime objects", "Normalize ProductCategory capitalization", "Compute descriptive statistics for Quantity and Price", "Generate value counts for StoreLocation and ProductCategory", "Identify transactions with missing CustomerID or StoreLocation", "Calculate total sales per ProductCategory", "Find top 3 StoreLocations by number of transactions", "Check for duplicates in TransactionID"], "eda_results": {"missing_values": {"TransactionID": 0, "Date": 1, "ProductCategory": 0, "Quantity": 2, "Price": 1, "CustomerID": 1, "StoreLocation": 1}, "date_standardization": {"all_dates_parsed": true, "notes": "Mixed formats including YYYY-MM-DD, MM/DD/YYYY, YYYY/MM/DD, textual dates, and missing values handled"}, "product_category_normalized_counts": {"Electronics": 3, "Home Appliances": 3, "Clothing": 3, "Furniture": 2, "Toys": 4}, "quantity_price_stats": {"Quantity": {"count": 12, "mean": 2.25, "median": 2, "min": 1, "max": 5}, "Price": {"count": 13, "mean": 116.17, "median": 89.5, "min": 9.99, "max": 399.99}}, "store_location_value_counts": {"Chicago": 4, "New York": 3, "Los Angeles": 3, "Houston": 3, "": 1}, "missing_customer_or_store": {"missing_customerID": ["T005"], "missing_storeLocation": ["T006"]}, "total_sales_per_product_category": {"Electronics": 1199.97, "Home Appliances": 268.5, "Clothing": 179.94, "Furniture": 399.98, "Toys": 80.91}, "top_3_store_locations_by_transactions": ["Chicago", "New York", "Los Angeles"], "duplicate_transaction_ids": []}}
{"purpose": "Examine trip patterns and vehicle types in urban transportation data to identify missing data issues and common usage trends.", "raw_table": "Trip_ID,Vehicle_Type,Start_Time,End_Time,Distance_km,Fare,Driver_Rating\nT001,Car,2023/05/01 08:00 AM,2023-05-01 08:30,15.2,23.50,4.7\nT002,bike,05-01-2023 09:15,05-01-2023 09:45 AM,,12.00,4.2\nT003,Taxi,2023-05-01 10:00,2023/05/01 10:20,8.5,,3.9\nT004,BUS,2023/05/01 11:00 AM,2023/05/01 11:45 AM,20,18.75,4.5\nT005,car,,2023-05-01 12:30,10.0,15,4.0\nT006,Taxi,2023-05-01 13:00,2023-05-01 13:25,12.3,22.00,NA\nT007,bike,2023/05/01 14:00,2023/05/01 14:30,5.5,8.5,4.8\nT008,Bus,2023-05-01 15:00,2023/05/01 15:50,25,20.00,4.1\nT009,car,05/01/2023 16:00,05/01/2023 16:20,7.8,13.50,4.3\nT010,Taxi,2023-05-01 17:00,2023-05-01 17:30,14.0,24.00,4.6", "eda_steps": ["Check the percentage of missing values in each column", "Standardize the Vehicle_Type column capitalization", "Compute descriptive statistics for numeric columns: Distance_km, Fare, Driver_Rating", "Generate value counts for Vehicle_Type", "Analyze time coverage by parsing and summarizing Start_Time and End_Time", "Identify any discrepancies or missing values in date/time columns", "Compute correlation between Distance_km and Fare", "Identify top 2 Vehicle_Types by number of trips"], "eda_results": {"missing_values": {"Trip_ID": 0, "Vehicle_Type": 0, "Start_Time": 1, "End_Time": 0, "Distance_km": 1, "Fare": 1, "Driver_Rating": 1}, "value_counts": {"Vehicle_Type": {"car": 3, "bike": 2, "taxi": 3, "bus": 2}}, "summary_stats": {"Distance_km": {"count": 13, "mean": 13.04, "std": 6.15, "min": 5.5, "25%": 7.8, "50%": 12.3, "75%": 15.2, "max": 25}, "Fare": {"count": 14, "mean": 17.32, "std": 5.03, "min": 8.5, "25%": 13.5, "50%": 18.75, "75%": 22, "max": 24}, "Driver_Rating": {"count": 14, "mean": 4.31, "std": 0.33, "min": 3.9, "25%": 4.1, "50%": 4.3, "75%": 4.65, "max": 4.8}}, "time_coverage": {"earliest_start": "2023-05-01 08:00", "latest_end": "2023-05-01 17:30"}, "date_time_issues": ["One missing Start_Time (Trip_ID T005)", "Mixed date formats detected (e.g., '2023/05/01', '05-01-2023', '05/01/2023')"], "correlations": {"Distance_km_vs_Fare": 0.97}, "top_categories": {"Vehicle_Type": [{"car": 3}, {"taxi": 3}]}}}
{"purpose": "Analyze crop yield patterns and farm characteristics to understand factors affecting productivity.", "raw_table": "Farm_ID,Crop_Type,Planting_Date,Harvest_Date,Yield_kg,Area_ha,Soil_Type,Fertilizer_Used\nF001,Wheat,2023-03-15,2023-08-20,1500,2.5,Loam,Yes\nF002,corn,15/04/2023,20/09/2023,2000,3.0,sandy,No\nF003,Rice,2023/03/10,2023/09/01,1800,,Clay,Yes\nF004,Barley,Mar 20 2023,Aug 25 2023,,1.8,Loam,yes\nF005,Wheat,2023-03-18,,1700,2.7,loam,No\nF006,Corn,2023-04-17,2023-09-22,2100,3.2,Sandy,YES\nF007,rice,2023-03-12,2023-09-05,1750,2.9,clay,No\nF008,Barley,2023-03-19,2023-08-30,1400,1.7,loam,No\nF009,Wheat,2023-03-16,2023-08-22,1600,2.6,Loam,\nF010,Corn,2023-04-14,2023-09-18,1950,3.1,Sandy,Yes\n", "eda_steps": ["Check and summarize missing values for each column", "Standardize Crop_Type and Soil_Type capitalization", "Parse and unify date formats in Planting_Date and Harvest_Date", "Compute descriptive statistics for numeric columns: Yield_kg and Area_ha", "Calculate the duration of crop growth in days for each record", "Generate value counts for Fertilizer_Used", "Analyze average Yield_kg by Crop_Type", "Check correlation between Area_ha and Yield_kg"], "eda_results": {"missing_values": {"Yield_kg": 1, "Area_ha": 1, "Harvest_Date": 1, "Fertilizer_Used": 1}, "standardized_categories": {"Crop_Type": {"Wheat": 3, "Corn": 3, "Rice": 2, "Barley": 2}, "Soil_Type": {"Loam": 5, "Sandy": 3, "Clay": 2}}, "date_parsing": {"Planting_Date": "All dates converted to YYYY-MM-DD", "Harvest_Date": "All dates converted to YYYY-MM-DD, except one missing"}, "summary_stats": {"Yield_kg": {"count": 9, "mean": 1777.78, "std_dev": 237.17, "min": 1400, "max": 2100}, "Area_ha": {"count": 9, "mean": 2.72, "std_dev": 0.5, "min": 1.7, "max": 3.2}, "Growth_Days": {"count": 9, "mean": 158, "std_dev": 5, "min": 149, "max": 164}}, "fertilizer_counts": {"Yes": 5, "No": 4}, "avg_yield_by_crop": {"Wheat": 1600, "Corn": 2016.67, "Rice": 1775, "Barley": 1400}, "correlations": {"Area_ha_vs_Yield_kg": 0.89}}}
{"purpose": "Analyze ride-sharing trip data to identify patterns in trip duration, passenger count, and payment types.", "raw_table": "Trip_ID,Start_Time,End_Time,Passenger_Count,Payment_Type,Trip_Distance_miles,Driver_Rating\nT001,2023-04-01 08:30,2023/04/01 08:55,2,Cash,5.2,4.8\nT002,04-01-2023 09:00,2023-04-01 09:20,1,Credit Card,3.5,5\nT003,2023/04/01 09:15,2023-04-01 09:40,,Cash,6.7,4.5\nT004,2023-04-01 10:00,2023-04-01 10:30,3,Credit,4.1,4.7\nT005,2023-04-01 10:45,2023-04-01 11:05,1,credit card,3.8,Missing\nT006,2023-04-01 11:15,2023-04-01 11:50,2,CASH,7.3,4.9\nT007,01-04-2023 12:00,2023-04-01 12:25,4,Cash,5.0,4.6\nT008,2023-04-01 12:30,2023-04-01 12:50,2,Credit Card,3.2,5\nT009,2023-04-01 13:15,2023-04-01 13:40,1,Cash,5.5,4.8\nT010,2023-04-01 13:45,2023-04-01 14:10,,CREDIT CARD,6.0,4.7\nT011,2023-04-01 14:20,2023-04-01 14:45,2,Cash,NaN,4.9\nT012,2023-04-01 15:00,2023-04-01 15:25,3,cash,4.9,4.4", "eda_steps": ["Check and standardize date formats in Start_Time and End_Time columns", "Identify and count missing values in all columns", "Convert Payment_Type to consistent capitalization and categorize unique payment methods", "Calculate trip durations in minutes from Start_Time and End_Time", "Compute summary statistics for numeric columns: Passenger_Count, Trip_Distance_miles, Driver_Rating, and Trip Duration", "Generate value counts for Passenger_Count and Payment_Type", "Check for correlations between Trip_Distance_miles, Trip Duration, Passenger_Count, and Driver_Rating", "Identify trips with missing or anomalous data"], "eda_results": {"missing_values": {"Passenger_Count": 2, "Payment_Type": 0, "Trip_Distance_miles": 1, "Driver_Rating": 1}, "standardized_payment_types": {"Cash": 6, "Credit Card": 5, "Credit": 1}, "date_formats_standardized": true, "trip_durations_minutes": {"min": 20, "max": 35, "mean": 26.58, "median": 25}, "summary_stats": {"Passenger_Count": {"min": 1, "max": 4, "mean": 2.1, "median": 2}, "Trip_Distance_miles": {"min": 3.2, "max": 7.3, "mean": 5.21, "median": 5.1}, "Driver_Rating": {"min": 4.4, "max": 5.0, "mean": 4.71, "median": 4.75}}, "value_counts": {"Passenger_Count": {"1": 4, "2": 4, "3": 2, "4": 1, "missing": 2}, "Payment_Type": {"Cash": 6, "Credit Card": 5, "Credit": 1}}, "correlations": {"Trip_Distance_miles_vs_Trip_Duration": 0.89, "Passenger_Count_vs_Trip_Duration": 0.15, "Driver_Rating_vs_Trip_Distance_miles": -0.12, "Driver_Rating_vs_Passenger_Count": 0.05}, "anomalous_trips": ["T003 (missing Passenger_Count)", "T005 (Driver_Rating missing)", "T011 (Trip_Distance_miles missing)", "T010 (missing Passenger_Count)"]}}
{"purpose": "Analyze customer call behavior and service usage patterns in a telecom dataset to identify data quality issues and usage trends.", "raw_table": "CustomerID,CallDuration,CallDate,ServicePlan,DataUsed_GB,Region\nC001,12,2023-01-05 14:23,Premium,1.5,North\nc002,7,,Basic,0.5,South\nC003,,01/10/2023,PREMIUM,2.1,East\nC004,15,2023-01-15,Basic,abc,west\nc005,9,2023/01/18,BASIC,0.8,East\nC006,11,2023-1-20,Premium,1.3,NORTH\nc007,5,15-01-2023,basic,0.4,South\nC008,20,2023-01-22,Premium,2.9,West\nC009,14,,Basic,,east\nc010,nan,2023-01-25,Enterprise,3.5,North\nC011,8,2023-01-26,enterprise,NaN,South\nc012,10,2023-01-27,,1.0,South\n", "eda_steps": ["Check and summarize missing values per column", "Standardize the capitalization of the ServicePlan and Region columns", "Parse CallDate into a consistent date format and identify parsing issues", "Compute descriptive statistics for CallDuration and DataUsed_GB", "Generate value counts for ServicePlan and Region columns", "Identify rows with non-numeric DataUsed_GB values", "Calculate the correlation between CallDuration and DataUsed_GB ignoring missing or invalid values", "Identify top 2 most common ServicePlans", "Summarize the distribution skewness of CallDuration"], "eda_results": {"missing_values": {"CallDuration": 2, "CallDate": 2, "ServicePlan": 1, "DataUsed_GB": 3, "Region": 0}, "standardized_categories": {"ServicePlan": ["Premium", "Basic", "Enterprise", "Unknown"], "Region": ["North", "South", "East", "West"]}, "call_date_parsing_issues": 1, "summary_stats": {"CallDuration": {"count": 11, "mean": 11.0, "std": 4.75, "min": 5, "25%": 8, "50%": 11, "75%": 14, "max": 20, "skewness": 0.47}, "DataUsed_GB": {"count": 9, "mean": 1.51, "std": 0.91, "min": 0.4, "25%": 0.8, "50%": 1.3, "75%": 2.1, "max": 3.5}}, "value_counts": {"ServicePlan": {"Basic": 4, "Premium": 4, "Enterprise": 2, "Unknown": 1}, "Region": {"North": 3, "South": 4, "East": 3, "West": 3}}, "non_numeric_DataUsed_GB_rows": ["C004"], "correlations": {"CallDuration_DataUsed_GB": 0.89}, "top_categories": {"ServicePlan": ["Basic", "Premium"]}}}
{"purpose": "Analyze patterns in public assistance program enrollment and demographics to identify data quality issues and key participant characteristics.", "raw_table": "Participant_ID,Name,Age,Program,Enrollment_Date,State,Income,Status\n001,John Doe,34,SNAP,2023-01-15,NY,25000,active\n002,jane smith,28,snap,15/02/2023,ca,22000,Active\n003,,45,MEDICAID,2023-03-01,TX,NA,active\n004,Michael Brown,NA,Medicaid,2023/03/20,tx,18000,inactive\n005,Lisa Ray,39,SNAP,2023-04-10,FL,27000,Active\n006,Tom Hardy,52,Unemployment,03-15-2023,ny,NA,active\n007,Anna White,31,Unemployment,2023-04-01,CA,15000,ACTIVE\n008,George Black,29,Medicaid,2023.02.28,fl,20000,Inactive\n009,Sarah Green,NA,SNAP,2023-01-25,ny,23000,active\n010,David Wilson,40,unemployment,2023-03-30,TX,19500,Inactive\n", "eda_steps": ["Check for missing values in each column", "Standardize capitalization in categorical columns (Program, State, Status)", "Convert Enrollment_Date to a consistent date format", "Compute descriptive statistics for Age and Income columns", "Generate value counts for Program and Status columns", "Identify number of unique States and list them", "Check for any duplicate Participant_IDs", "Summarize missing data patterns in Name and Age columns", "Identify top 2 programs by number of participants"], "eda_results": {"missing_values": {"Participant_ID": 0, "Name": 1, "Age": 3, "Program": 0, "Enrollment_Date": 0, "State": 0, "Income": 2, "Status": 0}, "standardized_programs": {"SNAP": 4, "MEDICAID": 3, "UNEMPLOYMENT": 3}, "standardized_states": {"NY": 3, "CA": 2, "TX": 3, "FL": 2}, "standardized_status": {"ACTIVE": 6, "INACTIVE": 4}, "enrollment_dates_consistent_format": true, "age_summary_stats": {"count": 12, "mean": 36.0, "min": 28, "max": 52, "missing": 3}, "income_summary_stats": {"count": 8, "mean": 21625, "min": 15000, "max": 27000, "missing": 2}, "unique_states": ["NY", "CA", "TX", "FL"], "duplicate_participant_ids": 0, "missing_data_patterns": {"Name": ["003"], "Age": ["004", "009"]}, "top_programs": {"SNAP": 4, "MEDICAID": 3}}}
{"purpose": "Explore patient demographics and diagnosis patterns in a cardiology outpatient dataset.", "raw_table": "Patient_ID,Age,Gender,Diagnosis,Visit_Date,Cholesterol_Level,Blood_Pressure\n001,45,Male,hypertension,2023-01-15,200,120/80\n002,37,Female,Coronary artery disease,15/02/2023,180,130/85\n003,52,Male,Hypertension,,220,140/90\n004,29,Female,,2023-03-05,NaN,115/75\n005,NaN,Male,Arrhythmia,2023-02-30,190,110/70\n006,41,female,hypertension,03-10-2023,210,135/88\n007,48,Male,coronary artery disease,2023/04/12,NaN,140/95\n008,36,Male,arrhythmia,2023-04-20,195,125/80\n009,NaN,Female,Hypertension,2023-05-01,205,130/85\n010,50,Male,hypertension,May 5, 2023,215,NaN", "eda_steps": ["Check for missing values in each column", "Standardize date formats in 'Visit_Date' column", "Convert all 'Diagnosis' entries to consistent capitalization", "Compute summary statistics for 'Age' and 'Cholesterol_Level'", "Generate value counts for 'Diagnosis' and 'Gender'", "Identify patients with missing or invalid 'Visit_Date'", "Extract systolic and diastolic values from 'Blood_Pressure' and compute their means", "Analyze correlation between 'Age' and 'Cholesterol_Level'"], "eda_results": {"missing_values": {"Patient_ID": 0, "Age": 2, "Gender": 0, "Diagnosis": 1, "Visit_Date": 2, "Cholesterol_Level": 2, "Blood_Pressure": 1}, "standardized_dates": ["2023-01-15", "2023-02-15", "missing", "2023-03-05", "invalid", "2023-10-03", "2023-04-12", "2023-04-20", "2023-05-01", "2023-05-05"], "diagnosis_value_counts": {"hypertension": 4, "coronary artery disease": 2, "arrhythmia": 2, "missing": 1}, "gender_value_counts": {"Male": 6, "Female": 4}, "age_summary_stats": {"count": 8, "mean": 42.4, "std": 7.8, "min": 29, "max": 52}, "cholesterol_level_summary_stats": {"count": 8, "mean": 201.9, "std": 14.9, "min": 180, "max": 220}, "blood_pressure_means": {"systolic_mean": 128.1, "diastolic_mean": 83.8}, "correlation_age_cholesterol": 0.65}}
{"purpose": "Analyze taxi trip data to understand trip duration, missing data, and common pickup locations.", "raw_table": "Trip_ID,Pickup_Location,Dropoff_Location,Trip_Duration,Trip_Date,Fare_Amount,Passenger_Count\n1,downtown,airport,15,2023-01-05,25.5,2\n2,Uptown,Downtown,NA,01/06/2023,18.0,1\n3,downtown,,22,2023-1-07,22.0,three\n4,Midtown,Airport,19,2023/01/08,21.5,1\n5,,downtown,12,2023-01-09,16.0,2\n6,downtown,Midtown,18,,20.0,1\n7,Uptown,Midtown,20,2023-01-11,19.5,\n8,midtown,Downtown,NaN,2023-01-12,17.0,1\n9,Downtown,Uptown,25,1/13/2023,30.0,2\n10,Downtown,Midtown,16,2023-01-14,abc,2\n11,Airport,Downtown,21,2023-01-15,23.5,1\n12,downtown,airport,14,2023-01-16,24.0,1\n13,,Airport,17,2023-01-17,22.5,2", "eda_steps": ["Check for missing values in each column", "Standardize and clean the Trip_Date column to a consistent date format", "Convert Trip_Duration to numeric and identify invalid entries", "Summarize descriptive statistics for Trip_Duration and Fare_Amount", "Calculate value counts for Pickup_Location and Dropoff_Location", "Identify and count unusual or inconsistent categories in categorical columns", "Convert Passenger_Count to numeric and handle invalid values", "Analyze correlations between Trip_Duration, Fare_Amount, and Passenger_Count", "Identify top 3 most common pickup locations"], "eda_results": {"missing_values": {"Trip_ID": 0, "Pickup_Location": 2, "Dropoff_Location": 1, "Trip_Duration": 1, "Trip_Date": 1, "Fare_Amount": 1, "Passenger_Count": 2}, "date_format_cleaned": {"standardized_dates": 13, "unparseable_dates": 0}, "trip_duration_stats": {"count": 12, "mean": 18.58, "std": 3.7, "min": 12, "max": 25}, "fare_amount_stats": {"count": 12, "mean": 21.63, "std": 4.31, "min": 16.0, "max": 30.0}, "pickup_location_counts": {"downtown": 5, "uptown": 2, "midtown": 3, "airport": 1, "": 2}, "dropoff_location_counts": {"downtown": 3, "airport": 3, "midtown": 3, "uptown": 1, "": 1}, "unusual_categories_found": {"Passenger_Count": ["three", "", "NA", "NaN"], "Fare_Amount": ["abc"], "Trip_Duration": ["NA", "NaN"]}, "passenger_count_cleaned": {"count": 11, "mean": 1.55, "std": 0.52, "min": 1, "max": 3}, "correlations": {"Trip_Duration_Fare_Amount": 0.92, "Trip_Duration_Passenger_Count": 0.15, "Fare_Amount_Passenger_Count": 0.12}, "top_pickup_locations": ["downtown", "midtown", "uptown"]}}
{"purpose": "Analyze customer call patterns and identify data quality issues in call logs.", "raw_table": "CustomerID,CallDate,CallDuration,CallType,Location,PlanType,CallCost\nA123,2024/01/15,300,Voice,New York,Premium,5.50\nB456,15-02-2024,,SMS,los angeles,Basic,\nC789,2024-03-20,45,voice,Chicago,Premium,1.25\nD012,2024/04/05,120,Data,houston,,3.75\nE345,03/25/2024,NaN,VOICE,New york,Basic,2.00\nF678,2024-04-15,90,Voice,los Angeles,Premium,4.00\nG901,,60,SMS,Dallas,Basic,0.50\nH234,2024/05/01,180,Voice,boston,Premium,6.00\nI567,2024-05-10,NaN,voice,CHICAGO,Basic,2.75\nJ890,05-15-2024,200,DATA,Miami,Premium,5.00\nK123,2024/05/20,abc,Voice,Seattle,Basic,3.25", "eda_steps": ["Check missing value percentages for each column", "Compute descriptive statistics (mean, median) for CallDuration and CallCost", "Generate value counts for CallType and PlanType columns", "Identify inconsistent capitalization in Location and CallType columns", "Find and report non-numeric or invalid entries in CallDuration", "Summarize the distribution of CallDuration by PlanType", "Calculate correlation between CallDuration and CallCost", "Identify top 3 locations by number of calls"], "eda_results": {"missing_values": {"CustomerID": 0, "CallDate": 1, "CallDuration": 3, "CallType": 0, "Location": 0, "PlanType": 1, "CallCost": 2}, "summary_stats": {"CallDuration": {"mean": 126.25, "median": 105}, "CallCost": {"mean": 3.79, "median": 3.75}}, "value_counts": {"CallType": {"Voice": 7, "SMS": 2, "Data": 2}, "PlanType": {"Basic": 5, "Premium": 6, "missing": 1}}, "inconsistent_capitalization": {"Location": ["New York", "los angeles", "New york", "houston", "los Angeles", "boston", "CHICAGO", "Miami", "Seattle"], "CallType": ["Voice", "voice", "VOICE", "DATA"]}, "invalid_entries": {"CallDuration": ["NaN", "", "abc"]}, "duration_by_plan": {"Basic": {"mean_duration": 66.25, "count": 5}, "Premium": {"mean_duration": 165, "count": 6}}, "correlations": {"CallDuration_CallCost": 0.94}, "top_locations": {"New York": 2, "los angeles": 2, "Chicago": 2}}}
{"purpose": "Analyze property listing characteristics and identify data quality issues in a real estate dataset.", "raw_table": "ListingID,Price,Location,PropertyType,DateListed,Bedrooms,Bathrooms,SquareFeet\n1,350000,Downtown,Condo,2023-01-15,2,2,950\n2,450000,Suburb,House,15/02/2023,3,2,1500\n3,,uptown,Townhouse,2023/03/10,3,,1300\n4,550000,Downtown,condo,03-20-2023,3,2.5,1100\n5,NaN,suburb,House,2023-04-01,4,3,2000\n6,300000,DOWNTOWN,Apartment,April 5 2023,1,1,700\n7,400000,Uptown,Townhouse,2023-04-15,3,2, 1350\n8,500000,Suburb,house,2023-05-01,4,3,1800\n9,475000,Downtown,Condo,,2,2,1000\n10,425000,Uptown,TOWNHOUSE,2023-05-10,3,2,1400\n11,380000,Suburb,House,2023-05-15,3,2,1600\n12,NaN,Downtown,Apartment,2023-05-20,1,1,750", "eda_steps": ["Check for missing values in each column", "Standardize capitalization in 'Location' and 'PropertyType' columns", "Parse and unify date formats in 'DateListed' column", "Compute descriptive statistics for numeric columns: Price, Bedrooms, Bathrooms, SquareFeet", "Generate value counts for 'Location' and 'PropertyType' columns", "Identify listings with missing 'Price' or 'DateListed' values", "Calculate correlation matrix for numeric features", "Find top 2 most common property types", "Summarize distribution skewness of 'Price'"], "eda_results": {"missing_values": {"ListingID": 0, "Price": 3, "Location": 0, "PropertyType": 0, "DateListed": 1, "Bedrooms": 0, "Bathrooms": 1, "SquareFeet": 0}, "standardized_categories": {"Location": {"Downtown": 4, "Suburb": 4, "Uptown": 3}, "PropertyType": {"Condo": 3, "House": 4, "Townhouse": 3, "Apartment": 2}}, "date_format_summary": {"original_formats_found": 5, "unified_format": "YYYY-MM-DD", "missing_dates": 1}, "summary_stats": {"Price": {"count": 9, "mean": 425555.56, "std": 68980.85, "min": 300000, "25%": 380000, "50%": 425000, "75%": 475000, "max": 550000}, "Bedrooms": {"count": 12, "mean": 2.92, "std": 0.79, "min": 1, "25%": 2, "50%": 3, "75%": 3, "max": 4}, "Bathrooms": {"count": 11, "mean": 2.18, "std": 0.59, "min": 1, "25%": 2, "50%": 2, "75%": 2.5, "max": 3}, "SquareFeet": {"count": 12, "mean": 1304.58, "std": 448.44, "min": 700, "25%": 950, "50%": 1350, "75%": 1600, "max": 2000}}, "value_counts": {"Location": {"Downtown": 4, "Suburb": 4, "Uptown": 3}, "PropertyType": {"House": 4, "Condo": 3, "Townhouse": 3, "Apartment": 2}}, "listings_with_missing_price_or_date": {"ListingIDs": [3, 5, 12, 9]}, "correlations": {"Price-Bedrooms": 0.92, "Price-Bathrooms": 0.85, "Price-SquareFeet": 0.94, "Bedrooms-Bathrooms": 0.87, "Bedrooms-SquareFeet": 0.8, "Bathrooms-SquareFeet": 0.75}, "top_categories": {"PropertyType": [{"Category": "House", "Count": 4}, {"Category": "Condo", "Count": 3}]}, "price_skewness": 0.15}}
{"purpose": "Examine crop yield patterns and factors affecting wheat production across different farms.", "raw_table": "Farm_ID,Crop,Planting_Date,Yield_tons_per_ha,Fertilizer_Used,Soil_Type\nF001,wheat,2023/03/15,3.5,Nitrogen,Loamy\nF002,Wheat,15-03-2023,3.8,,Clay\nF003,Rice,2023-03-16,2.1,Phosphorus,Sandy\nF004,wheat,3/17/2023,?,Nitrogen,Loamy\nF005,WHEAT,2023/03/15,4.0,Potassium,loamy\nF006,Barley,2023/03/18,2.5,Nitrogen,Clay\nF007,Wheat,2023-03-15,3.7,Nitrogen,Sandy\nF008,Rice,17/03/2023,2.3,,Clay\nF009,Wheat,March 15 2023,3.9,Nitrogen,Loamy\nF010,Wheat,,3.6,Phosphorus,Sandy", "eda_steps": ["Standardize 'Crop' and 'Soil_Type' column values to lowercase", "Parse 'Planting_Date' to a uniform date format and identify missing dates", "Check and report missing values in each column", "Compute descriptive statistics for 'Yield_tons_per_ha' after handling missing or invalid values", "Generate value counts for 'Fertilizer_Used' and 'Crop'", "Identify average yield per soil type", "Calculate correlation between fertilizer usage categories and yield", "Find top 3 farms with highest yield"], "eda_results": {"missing_values": {"Farm_ID": 0, "Crop": 0, "Planting_Date": 1, "Yield_tons_per_ha": 1, "Fertilizer_Used": 2, "Soil_Type": 0}, "standardized_categories": {"Crop": {"wheat": 7, "rice": 2, "barley": 1}, "Soil_Type": {"loamy": 4, "clay": 3, "sandy": 3}}, "date_parsing": {"missing_dates": ["F010"]}, "yield_stats": {"count": 9, "mean": 3.44, "std": 0.63, "min": 2.3, "25%": 3.3, "50%": 3.7, "75%": 3.8, "max": 4.0}, "value_counts_fertilizer_used": {"nitrogen": 5, "phosphorus": 2, "potassium": 1, "missing": 2}, "average_yield_per_soil_type": {"loamy": 3.8, "clay": 2.95, "sandy": 2.8}, "top_farms_by_yield": [{"Farm_ID": "F005", "Yield_tons_per_ha": 4.0}, {"Farm_ID": "F009", "Yield_tons_per_ha": 3.9}, {"Farm_ID": "F002", "Yield_tons_per_ha": 3.8}]}}
{"purpose": "Explore property listing data to understand pricing patterns and data quality issues.", "raw_table": "PropertyID,ListingDate,Location,Price,Bedrooms,Bathrooms,PropertyType\n101,2023-01-05,Downtown,350000,2,1,Apartment\n102,01/12/2023,Suburb, 450000 ,3,2,House\n103,2023/02/15,,400000,2,,Condo\n104,March 3 2023,Midtown,NaN,1,1,apartment\n105,2023-04-01,suburb,475000,4,3,House\n106,2023-04-15,Downtown,380000,,1,Apartment\n107,2023-04-20,Midtown,425000,3,2,Condo\n108,2023-05-05,Suburb,460000,3,2,House\n109,,Downtown,390000,2,1,Apartment\n110,2023-05-15,Midtown,NaN,3,2,condo\n111,2023-05-20,Suburb,500000,4,3,House\n112,2023-05-25,Downtown,385000,2,1,Apartment", "eda_steps": ["Check missing value percentages for each column", "Standardize the Location and PropertyType columns capitalization", "Parse the ListingDate column into a consistent date format", "Compute descriptive statistics for numeric columns: Price, Bedrooms, Bathrooms", "Generate value counts for categorical columns: Location, PropertyType", "Identify records with missing Price values", "Analyze average Price by PropertyType", "Compute correlation matrix among numeric columns"], "eda_results": {"missing_values": {"PropertyID": 0, "ListingDate": 1, "Location": 1, "Price": 2, "Bedrooms": 2, "Bathrooms": 0, "PropertyType": 0}, "standardized_categories": {"Location": ["Downtown", "Suburb", "Midtown", "Missing"], "PropertyType": ["Apartment", "House", "Condo"]}, "listing_date_parsing_success_rate": "11 out of 12 records parsed successfully", "summary_stats": {"Price": {"count": 10, "mean": 426500, "std": 46291, "min": 350000, "25%": 385000, "50%": 425000, "75%": 460000, "max": 500000}, "Bedrooms": {"count": 10, "mean": 2.6, "std": 1.1, "min": 1, "25%": 2, "50%": 3, "75%": 3.25, "max": 4}, "Bathrooms": {"count": 12, "mean": 1.58, "std": 0.67, "min": 1, "25%": 1, "50%": 2, "75%": 2, "max": 3}}, "value_counts": {"Location": {"Downtown": 4, "Suburb": 4, "Midtown": 3, "Missing": 1}, "PropertyType": {"Apartment": 5, "House": 4, "Condo": 3}}, "missing_price_records": [104, 110], "average_price_by_property_type": {"Apartment": 373750, "House": 471250, "Condo": 412500}, "correlations": {"Price-Bedrooms": 0.85, "Price-Bathrooms": 0.8, "Bedrooms-Bathrooms": 0.95}}}
{"purpose": "Analyze daily ride-sharing trip patterns and identify data quality issues.", "raw_table": "Trip_ID,Driver_ID,Rider_ID,Trip_Date,Start_Time,End_Time,Distance_km,Fare_USD,Payment_Type\nT001,D123,r789,2024-04-01,08:15,08:45,12.5,25.00,Cash\nT002,d124,R790,04/01/2024,09:00,09:30,10,,credit card\nT003,D125,R791,2024-04-01,8:45 AM,9:15 AM,15.2,30.50,CARD\nT004,D126,R792,2024/04/02,10:00,10:25,8.0,18.00,Cash\nT005,,r793,2024-04-02,10:30,,7.5,16.00,Cash\nT006,D128,R794,2024-4-02,11:00,11:30,NaN,20.00,Credit\nT007,D129,R795,2024-04-03,11:15,11:45,9.0,19.00,card\nT008,D130,R796,2024-04-03,12:00,12:30,11.0,22.00,CASH\nT009,D131,R797,April 3 2024,12:15,12:45,10.5,21.50,Credit card\nT010,D132,R798,2024-04-04,13:00,13:40,14.0,28.00,Cash\nT011,D133,R799,2024-04-04,13:15,13:45,,24.00,Card\nT012,D134,R800,2024-04-04,14:00,14:30,13.5,27.00,Credit\n", "eda_steps": ["Check for missing values in each column", "Standardize and count unique values in Payment_Type", "Convert Trip_Date to a consistent date format", "Calculate trip durations in minutes", "Compute descriptive statistics for Distance_km and Fare_USD", "Identify any negative or zero trip durations", "Generate value counts for Driver_ID", "Summarize correlation between Distance_km and Fare_USD"], "eda_results": {"missing_values": {"Trip_ID": 0, "Driver_ID": 1, "Rider_ID": 0, "Trip_Date": 0, "Start_Time": 0, "End_Time": 1, "Distance_km": 2, "Fare_USD": 1, "Payment_Type": 0}, "payment_type_counts": {"cash": 4, "credit card": 3, "card": 3, "credit": 2}, "trip_dates_standardized": ["2024-04-01", "2024-04-01", "2024-04-01", "2024-04-02", "2024-04-02", "2024-04-02", "2024-04-03", "2024-04-03", "2024-04-03", "2024-04-04", "2024-04-04", "2024-04-04"], "trip_durations_minutes": {"T001": 30, "T002": 30, "T003": 30, "T004": 25, "T005": null, "T006": 30, "T007": 30, "T008": 30, "T009": 30, "T010": 40, "T011": 30, "T012": 30}, "descriptive_stats": {"Distance_km": {"count": 10, "mean": 11.12, "std": 2.63, "min": 7.5, "max": 15.2}, "Fare_USD": {"count": 11, "mean": 23.77, "std": 4.15, "min": 16, "max": 30.5}}, "invalid_trip_durations": {"T005": "End_Time missing, duration cannot be calculated"}, "driver_id_counts": {"D123": 1, "d124": 1, "D125": 1, "D126": 1, "D128": 1, "D129": 1, "D130": 1, "D131": 1, "D132": 1, "D133": 1, "D134": 1, "null": 1}, "correlation_distance_fare": 0.95}}
{"purpose": "Analyze housing listing characteristics to understand price distribution and data quality issues.", "raw_table": "ListingID,Price,Location,DateListed,Bedrooms,Bathrooms,SquareFeet,PropertyType\n1,350000,Downtown,2023-03-15,3,2,1500,Single Family\n2,275000,suburbs,15/04/2023,2,,1200,Condo\n3,NaN,Uptown,2023/05/01,4,3,2000,Townhouse\n4,420000,Downtown,2023-03-20,3,2,NaN,Single family\n5,310000,Suburbs,2023-04-10,,2,1400,condo\n6,295000,Uptown,April 5 2023,3,2,1300,TownHouse\n7,330000,downtown,2023-03-18,3,2,1600,Single Family\n8,NaN,Unknown,2023-03-25,2,1,1100,Apartment\n9,280000,Suburbs,04/20/2023,2,2,1250,Condo\n10,400000,Uptown,2023-05-05,4,3,2100,Townhouse\n11,310000,DOWNTOWN,2023-03-17,3,2,1550,single family\n12,290000,Suburbs,,2,2,1300,Condo\n13,350000,Uptown,2023-05-02,4,3,,Townhouse", "eda_steps": ["Check the percentage of missing values in each column", "Standardize the capitalization of categorical columns: Location and PropertyType", "Compute descriptive statistics for numeric columns: Price, Bedrooms, Bathrooms, SquareFeet", "Generate value counts for Location and PropertyType columns", "Analyze the distribution of DateListed and identify inconsistent date formats", "Calculate the correlation matrix for numeric features", "Identify listings with missing or anomalous Price values", "Summarize the count of listings by number of bedrooms"], "eda_results": {"missing_values": {"ListingID": "0%", "Price": "15.38%", "Location": "7.69%", "DateListed": "7.69%", "Bedrooms": "7.69%", "Bathrooms": "7.69%", "SquareFeet": "15.38%", "PropertyType": "0%"}, "standardized_categories": {"Location": ["Downtown", "Suburbs", "Uptown", "Unknown"], "PropertyType": ["Single Family", "Condo", "Townhouse", "Apartment"]}, "summary_stats": {"Price": {"count": 11, "mean": 323636.36, "std": 44336.73, "min": 275000, "25%": 290000, "50%": 310000, "75%": 350000, "max": 420000}, "Bedrooms": {"count": 12, "mean": 3.0, "std": 0.82, "min": 2, "25%": 2, "50%": 3, "75%": 4, "max": 4}, "Bathrooms": {"count": 12, "mean": 2.17, "std": 0.62, "min": 1, "25%": 2, "50%": 2, "75%": 3, "max": 3}, "SquareFeet": {"count": 11, "mean": 14909.09, "std": 319.21, "min": 1100, "25%": 1250, "50%": 1500, "75%": 1600, "max": 2100}}, "value_counts": {"Location": {"Downtown": 4, "Suburbs": 4, "Uptown": 4, "Unknown": 1}, "PropertyType": {"Single Family": 4, "Condo": 4, "Townhouse": 4, "Apartment": 1}}, "date_format_issues": {"inconsistent_formats": ["15/04/2023", "2023/05/01", "April 5 2023", "04/20/2023"], "missing_dates": 1}, "correlations": {"Price_Bedrooms": 0.91, "Price_Bathrooms": 0.88, "Price_SquareFeet": 0.85, "Bedrooms_Bathrooms": 0.95}, "listings_missing_price": [3, 8], "bedrooms_distribution": {"2": 4, "3": 5, "4": 4, "null": 1}}}
{"purpose": "Explore patient demographics and diagnosis patterns in a small clinic dataset to identify missing data and common conditions.", "raw_table": "Patient_ID,Age,Gender,Admission_Date,Diagnosis,Height_cm,Weight_kg\nP001,34,male,2023/01/15,Hypertension,175,82\nP002,28,Female,15-02-2023,diabetes,NaN,70\nP003,,Female,2023-03-05,Asthma,160,60\np004,45,Male,2023-03-20,hypertension,180,NA\nP005,50,female,,COPD,165,68\nP006,38,M,2023/04/01,diabetes,172,77.5\nP007,31,F,,asthma,158,55\nP008,29,FEMALE,2023-04-15,unknown,165,63\nP009,NaN,male,2023/02/28,COPD,170,80\nP010,42,Male,2023-03-10,Diabetes,178,85", "eda_steps": ["Check for missing values in each column", "Standardize gender values to lowercase and consistent categories", "Parse and standardize admission dates into a uniform format", "Compute descriptive statistics (mean, median) for Age, Height_cm, and Weight_kg", "Generate value counts for Diagnosis column", "Identify the number of unique patients", "Check correlation between Height_cm and Weight_kg", "List patients with missing Admission_Date or Age"], "eda_results": {"missing_values": {"Patient_ID": 0, "Age": 2, "Gender": 0, "Admission_Date": 2, "Diagnosis": 0, "Height_cm": 1, "Weight_kg": 1}, "standardized_gender_counts": {"male": 4, "female": 5, "m": 1}, "admission_date_formats": {"YYYY/MM/DD": 3, "DD-MM-YYYY": 1, "YYYY-MM-DD": 5, "missing": 2}, "age_stats": {"mean": 36.86, "median": 35.5, "count": 8}, "height_stats": {"mean": 169.44, "median": 170, "count": 9}, "weight_stats": {"mean": 71.39, "median": 70, "count": 9}, "diagnosis_value_counts": {"hypertension": 2, "diabetes": 3, "asthma": 2, "copd": 2, "unknown": 1}, "unique_patients_count": 10, "height_weight_correlation": 0.95, "patients_missing_age": ["P003", "P009"], "patients_missing_admission_date": ["P005", "P007"]}}
{"purpose": "Analyze monthly sales performance and product category distribution in a retail store.", "raw_table": "OrderID,Product,Category,Quantity,UnitPrice,OrderDate,CustomerID\n1001,Widget A,Electronics,10,19.99,2023-01-15,CUST001\n1002,widget b,Home Goods,5, ,15/01/2023,CUST002\n1003,Gadget X,Electronics, ,29.99,2023/01/16,CUST003\n1004,Thingamajig,TOys,3,9.99,01-17-2023,CUST004\n1005,widget a,Electronics,7,19.99,2023-1-18,CUST005\n1006,Doohickey,home goods,2,14.99,2023-01-19,CUST006\n1007,Widget B,Home Goods,4,24.99,,CUST007\n1008,Gadget X,Electronics,6,29.99,2023-01-20,CUST008\n1009,Thingamajig,Toys, ,9.99,2023-01-21,CUST009\n1010,Widget A,Electronics,11,19.99,2023-01-22,CUST010\n1011,Doohickey,Home goods,1,14.99,2023-01-23,CUST011\n1012,Gadget X,Electronics,8,29.99,2023-01-24,CUST012\n1013,Thingamajig,Toys,5, ,2023-01-25,CUST013\n1014,Widget a,Electronics,9,19.99,2023-01-26,CUST014", "eda_steps": ["Check for missing values in each column and calculate percentages", "Standardize the 'Category' column values (case and spelling)", "Convert 'OrderDate' to a consistent date format", "Compute summary statistics for 'Quantity' and 'UnitPrice'", "Calculate total sales per order as Quantity multiplied by UnitPrice", "Identify the top 3 selling products by total quantity", "Generate value counts for 'Category'", "Analyze sales trends over the order dates"], "eda_results": {"missing_values": {"OrderID": "0%", "Product": "0%", "Category": "0%", "Quantity": "14.3%", "UnitPrice": "14.3%", "OrderDate": "7.1%", "CustomerID": "0%"}, "standardized_categories": {"Electronics": 7, "Home Goods": 4, "Toys": 3}, "date_conversion_success": "100%", "summary_stats": {"Quantity": {"count": 12, "mean": 6.25, "median": 6.5, "min": 1, "max": 11}, "UnitPrice": {"count": 12, "mean": 21.91, "median": 19.99, "min": 9.99, "max": 29.99}}, "total_sales_per_order": {"1001": 199.9, "1002": "NaN", "1003": "NaN", "1004": 29.97, "1005": 139.93, "1006": 29.98, "1007": 99.96, "1008": 179.94, "1009": "NaN", "1010": 219.89, "1011": 14.99, "1012": 239.92, "1013": "NaN", "1014": 179.91}, "top_3_products_by_quantity": {"Widget A": 37, "Gadget X": 14, "Thingamajig": 8}, "category_value_counts": {"Electronics": 7, "Home Goods": 4, "Toys": 3}, "sales_trends": {"2023-01-15": 199.9, "2023-01-16": "NaN", "2023-01-17": 29.97, "2023-01-18": 139.93, "2023-01-19": 29.98, "2023-01-20": 179.94, "2023-01-21": "NaN", "2023-01-22": 219.89, "2023-01-23": 14.99, "2023-01-24": 239.92, "2023-01-25": "NaN", "2023-01-26": 179.91}}}
{"purpose": "Explore student performance and attendance patterns to identify factors affecting academic success.", "raw_table": "StudentID,Name,Grade,Subject,Score,Attendance,ExamDate\n101,john doe,10,Math,85,Present,2023-05-12\n102,Jane SMITH,10,english,78,Absent,05/15/2023\n103,Mike Brown,11,Math,,Present,2023/05/16\n104,Lisa White,11,Science,92,present,16-May-2023\n105,Tom Clark,10,English,88,Absent,2023-05-14\n106,Amy Adams,12,science,95,Present,2023-5-17\n107,,12,Math,80,Present,2023-05-13\n108,Chris Black,11,english,70,,2023-05-12\n109,Anna Lee,10,Science,85,Absent,2023/05/15\n110,Ben King,12,Math,90,present,2023-05-14", "eda_steps": ["Check for missing values in each column", "Standardize the 'Attendance' column entries", "Convert 'ExamDate' to a uniform date format", "Compute descriptive statistics for the 'Score' column", "Generate value counts for the 'Subject' and 'Grade' columns", "Calculate average scores by Subject", "Identify students with missing names or scores", "Analyze correlation between Attendance and Score"], "eda_results": {"missing_values": {"StudentID": 0, "Name": 1, "Grade": 0, "Subject": 0, "Score": 1, "Attendance": 1, "ExamDate": 0}, "attendance_standardized": {"Present": 7, "Absent": 3}, "exam_date_uniform_format": ["2023-05-12", "2023-05-15", "2023-05-16", "2023-05-16", "2023-05-14", "2023-05-17", "2023-05-13", "2023-05-12", "2023-05-15", "2023-05-14"], "score_descriptive_stats": {"count": 9, "mean": 83.9, "std_dev": 8.92, "min": 70, "max": 95, "median": 85}, "value_counts": {"Subject": {"Math": 4, "English": 3, "Science": 3}, "Grade": {"10": 4, "11": 3, "12": 3}}, "average_scores_by_subject": {"Math": 86.25, "English": 78.67, "Science": 90.67}, "missing_names_or_scores": {"missing_name_student_ids": [107], "missing_score_student_ids": [103]}, "attendance_score_correlation": 0.42}}
{"purpose": "Analyze customer call patterns and identify data quality issues in call records.", "raw_table": "CustomerID,CallDate,CallDuration,CallType,Network,DataUsedMB\nC101,2023/01/15,300,voice,4G,NaN\nc102,15-01-2023,120,Voice,3g,50.5\nC103,2023-01-16,,SMS,4G,0\nC104,01/17/2023,200,Voice,5G,30.1\nC105,2023-01-17 10:00,NA,voice,4G,75.0\nc106,2023.01.18,180,VOICE,4g,20\nC107,,240,voice,3G,45.2\nc108,2023-01-19,90,sms,4G,0\nC109,2023-01-20,NaN,Voice,unknown,60.5\nC110,2023-01-21,150,voice,5g,NaN\n", "eda_steps": ["Check missing values percentage for each column", "Standardize capitalization in categorical columns: CallType and Network", "Convert CallDate to a consistent date format", "Compute descriptive statistics for CallDuration and DataUsedMB", "Generate value counts for CallType and Network columns", "Identify unique CustomerIDs and check for duplicates", "Analyze correlation between CallDuration and DataUsedMB"], "eda_results": {"missing_values": {"CustomerID": 0, "CallDate": 1, "CallDuration": 3, "CallType": 0, "Network": 1, "DataUsedMB": 2}, "standardized_categories": {"CallType": {"voice": 8, "sms": 2}, "Network": {"4g": 5, "3g": 2, "5g": 2, "unknown": 1}}, "date_format_consistency": {"converted_dates": 9, "missing_or_invalid": 1}, "summary_stats": {"CallDuration": {"count": 7, "mean": 180, "min": 90, "max": 300, "median": 180}, "DataUsedMB": {"count": 8, "mean": 33.91, "min": 0, "max": 75, "median": 37.65}}, "value_counts": {"CallType": {"voice": 8, "sms": 2}, "Network": {"4g": 5, "3g": 2, "5g": 2, "unknown": 1}}, "unique_customers": {"total": 10, "duplicates": 0}, "correlations": {"CallDuration_DataUsedMB": 0.75}}}
{"purpose": "Examine household electricity consumption patterns and data quality issues in a small sample dataset.", "raw_table": "HouseholdID,Date,Time,Energy_kWh,Region,Device_Type\n001,2024-01-15,08:00,3.5,North,HVAC\n002,15/01/2024,09:00,4.1,south,Lighting\n003,2024/01/15,10:00,,East,Appliance\n004,2024-01-15,11:00,5.2,West,HVAC\n005,01-16-2024,12:00,2.8,North,Lighting\n006,2024-01-16,13:00,3.0,South,appliance\n007,2024-01-16,14:00,NaN,East,HVAC\n008,2024-01-16,15:00,4.4,west,Lighting\n009,2024-01-17,08:00,3.6,North,APPLIANCE\n010,17/01/2024,09:00,4.0,South,Lighting\n", "eda_steps": ["Check for and report missing values in each column", "Standardize the Region and Device_Type categorical values to consistent capitalization", "Convert all Date entries to a uniform date format", "Calculate basic descriptive statistics for Energy_kWh", "Count the number of unique households and devices", "Generate value counts for the Region and Device_Type columns", "Identify rows with missing or invalid Energy_kWh values", "Examine the distribution skewness of Energy_kWh"], "eda_results": {"missing_values": {"HouseholdID": 0, "Date": 0, "Time": 0, "Energy_kWh": 2, "Region": 0, "Device_Type": 0}, "standardized_categories": {"Region": ["North", "South", "East", "West"], "Device_Type": ["HVAC", "Lighting", "Appliance"]}, "date_format_standardized": true, "summary_stats": {"Energy_kWh": {"count": 8, "mean": 3.82, "std": 0.76, "min": 2.8, "25%": 3.3, "50%": 3.55, "75%": 4.25, "max": 5.2}}, "unique_counts": {"HouseholdID": 10, "Device_Type": 3}, "value_counts": {"Region": {"North": 3, "South": 3, "East": 2, "West": 2}, "Device_Type": {"HVAC": 3, "Lighting": 4, "Appliance": 3}}, "invalid_energy_rows": [3, 7], "energy_skewness": 0.65}}
{"purpose": "Analyze production line efficiency and defect rates to identify bottlenecks and quality issues.", "raw_table": "Production_ID,Line,Operator,Start_Date,End_Date,Units_Produced,Defects,Shift\n1001,Line A,john doe,2023/04/01,2023-04-01,500,5,Morning\n1002,line b,Jane Smith,04-02-2023,2023-04-02,450,,Night\n1003,Line A,JOHN DOE,2023-04-03,2023/04/03,480,8,morning\n1004,Line C,Mark Lee,2023-04-01,2023-04-01,300,2,Afternoon\n1005,Line B,Jane smith,,2023-04-04,470,7,Night\n1006,line c,Mark lee,2023/04/05,2023-04-05,320,zero,afternoon\n1007,LINE A,john doe,2023-04-06,2023-04-06,510,4,morning\n1008,Line B,Jane Smith,2023-04-07,07/04/2023,455,6,Night\n1009,Line C,Mark Lee,2023-4-08,2023-04-08,310,3,Afternoon\n1010,Line A,John Doe,2023-04-09,2023-04-09, ,9,Morning", "eda_steps": ["Standardize the capitalization of categorical columns such as Line, Operator, and Shift", "Parse and unify date columns (Start_Date and End_Date) into a consistent date format", "Identify and count missing values in each column", "Convert the Defects column to numeric, handling non-numeric entries", "Compute descriptive statistics for numeric columns Units_Produced and Defects", "Generate value counts for categorical columns Line, Operator, and Shift", "Calculate defect rate as Defects divided by Units_Produced for each record", "Identify the average defect rate per production line", "Check for any correlations between Units_Produced and Defects"], "eda_results": {"missing_values": {"Production_ID": 0, "Line": 0, "Operator": 0, "Start_Date": 1, "End_Date": 0, "Units_Produced": 1, "Defects": 1, "Shift": 0}, "value_counts": {"Line": {"Line A": 4, "Line B": 3, "Line C": 3}, "Operator": {"John Doe": 4, "Jane Smith": 3, "Mark Lee": 3}, "Shift": {"Morning": 4, "Night": 3, "Afternoon": 3}}, "summary_stats": {"Units_Produced": {"count": 9, "mean": 435.56, "std": 85.89, "min": 300, "25%": 320, "50%": 455, "75%": 480, "max": 510}, "Defects": {"count": 9, "mean": 5.33, "std": 2.5, "min": 2, "25%": 4, "50%": 5, "75%": 7, "max": 9}}, "defect_rate_per_record": [0.01, null, 0.0167, 0.0067, 0.0149, 0, 0.0078, 0.0132, 0.0097, null], "average_defect_rate_per_line": {"Line A": 0.0113, "Line B": 0.0141, "Line C": 0.0067}, "correlations": {"Units_Produced_vs_Defects": 0.71}}}
{"purpose": "Analyze ridership patterns and operational details of city bus routes to identify data quality issues and summarize key metrics.", "raw_table": "RouteID,RouteName,Day,Date,StartTime,EndTime,DistanceKM,Passengers,Status\n101,downtown express,Mon,2023-03-01,06:30,09:00,15.2,120,active\n102,Airport shuttle,TUE,03/01/2023,07:00,09:45,25.5,NaN,Active\n103,Suburban Line,wed,2023-03-01,06:45 AM,10:15 AM,30,85,active\n104,city Circle,Thurs,01-03-2023,08:00,11:00,22.7,,inactive\n105,River Route,FRI,3/1/2023,07:15,10:00,18.9,95,Active\n106,Mountain Trail,sat,2023/03/01,09:00,,28.3,50,active\n107,East Side Loop,Sun,2023-03-01,08:30,11:30,20.1,fifty,Inactive\n108,West End,Mon,2023-3-1,06:00,08:30,16,110,Active\n109,Old Town,Tuesday,2023-03-01,07:00 AM,09:30 AM,14.5,80,active\n110,Airport shuttle,Tue,2023-03-01,18:00,20:45,25.5,130,active", "eda_steps": ["Check for missing values in each column", "Standardize the capitalization in categorical columns RouteName and Status", "Convert Date and Time columns to proper datetime formats", "Identify and handle non-numeric values in Passengers column", "Compute descriptive statistics for numeric columns DistanceKM and Passengers", "Generate value counts for Status and Day columns", "Calculate total passengers per RouteName", "Find routes with incomplete time data (missing StartTime or EndTime)"], "eda_results": {"missing_values": {"RouteID": 0, "RouteName": 0, "Day": 0, "Date": 0, "StartTime": 0, "EndTime": 1, "DistanceKM": 0, "Passengers": 2, "Status": 0}, "standardized_categories": {"RouteName_unique": ["Downtown Express", "Airport Shuttle", "Suburban Line", "City Circle", "River Route", "Mountain Trail", "East Side Loop", "West End", "Old Town"], "Status_unique": ["Active", "Inactive"]}, "datetime_conversion_issues": {"Date": 0, "StartTime": 0, "EndTime": 1}, "passengers_numeric_conversion": {"converted_values": [120, null, 85, null, 95, 50, null, 110, 80, 130], "non_numeric_found": ["NaN", "", "fifty"]}, "summary_stats": {"DistanceKM": {"count": 10, "mean": 20.03, "min": 14.5, "max": 30}, "Passengers": {"count": 7, "mean": 90, "min": 50, "max": 130}}, "value_counts": {"Status": {"Active": 7, "Inactive": 3}, "Day": {"Mon": 2, "Tue": 2, "Wed": 1, "Thurs": 1, "Fri": 1, "Sat": 1, "Sun": 1, "Tuesday": 1}}, "total_passengers_per_route": {"Downtown Express": 120, "Airport Shuttle": 130, "Suburban Line": 85, "City Circle": 0, "River Route": 95, "Mountain Trail": 50, "East Side Loop": 0, "West End": 110, "Old Town": 80}, "incomplete_time_data_routes": ["Mountain Trail"]}}
{"purpose": "Analyze the distribution and characteristics of public service complaint records to identify common issues and data quality problems.", "raw_table": "Complaint_ID,Department,Complaint_Type,Date_Received,Status,Resolution_Time_Days,Resident_Age\n101,Health,Noise,2024-01-15,Closed,5,34\n102,transport,Trash collection,15/02/2024,Open,,28\n103,Public Safety,Noise,2024/03/02,Closed,3,47\n104,Health,Water Leakage,,Pending,7,NA\n105,health,noise,2024-02-28,closed,4,52\n106,Transport,trash collection,2024-03-10,Open,NA,41\n107,Public safety,Noise,2024-03-11,Closed,2,39\n108,Education,Building Damage,20-03-2024,Closed,10,63\n109,Education,building damage,2024-03-21,Open,NA,58\n110,Health,Noise,March 25 2024,Closed,1,45\n111,,Noise,2024-03-26,Closed,3,50\n112,Public safety,Noise,2024-03-27,Closed,3,44\n113,Transport,Trash Collection,2024-03-28,Closed,6,NA", "eda_steps": ["Check for missing values in each column and calculate their percentages", "Standardize the 'Department' and 'Complaint_Type' columns to consistent capitalization", "Convert 'Date_Received' to a uniform date format and identify any invalid or missing dates", "Generate summary statistics for the 'Resolution_Time_Days' and 'Resident_Age' columns", "Compute value counts for 'Department', 'Complaint_Type', and 'Status' columns", "Identify the average resolution time grouped by department", "Check for correlations between 'Resolution_Time_Days' and 'Resident_Age'", "List complaints with missing 'Department' or 'Date_Received' values", "Determine the top 2 most frequent complaint types"], "eda_results": {"missing_values": {"Complaint_ID": "0%", "Department": "0.9%", "Complaint_Type": "0%", "Date_Received": "7.7%", "Status": "0%", "Resolution_Time_Days": "23.1%", "Resident_Age": "15.4%"}, "standardized_categories": {"Departments": ["Health", "Transport", "Public Safety", "Education", "Unknown"], "Complaint_Types": ["Noise", "Trash Collection", "Water Leakage", "Building Damage"]}, "date_conversion": {"invalid_or_missing_dates": [104]}, "summary_stats": {"Resolution_Time_Days": {"count": 10, "mean": 4.4, "std_dev": 2.6, "min": 1, "max": 10}, "Resident_Age": {"count": 11, "mean": 46.0, "std_dev": 8.9, "min": 28, "max": 63}}, "value_counts": {"Department": {"Health": 3, "Transport": 3, "Public Safety": 3, "Education": 2, "Unknown": 1}, "Complaint_Type": {"Noise": 6, "Trash Collection": 3, "Water Leakage": 1, "Building Damage": 2}, "Status": {"Closed": 9, "Open": 3, "Pending": 1}}, "avg_resolution_time_by_department": {"Health": 3.3, "Transport": 6.0, "Public Safety": 2.7, "Education": 10.0, "Unknown": null}, "correlations": {"Resolution_Time_Days_and_Resident_Age": 0.12}, "missing_key_fields": {"Complaint_IDs_missing_Department": [111], "Complaint_IDs_missing_Date_Received": [104]}, "top_complaint_types": {"Noise": 6, "Trash Collection": 3}}}
{"purpose": "Analyze customer purchase behavior and identify data quality issues in order data.", "raw_table": "Order_ID,Customer_Name,Product_Category,Order_Date,Quantity,Price_per_Unit,Payment_Method\n1001,alice,Electronics,2023/01/15,2,299.99,Credit Card\n1002,Bob,home appliances,15-01-2023,1,149.50,credit card\n1003,,Beauty,2023-01-16,3,19.99,Paypal\n1004,David,Electronics,Jan 17 2023,NaN,299.99,CREDIT CARD\n1005,Eva,Fashion,2023/01/18,2,,paypal\n1006,Frank,beauty,2023-01-19,1,25.00,CASH\n1007,Gina,ELECTRONICS,01/20/2023,1,310.00,Credit Card\n1008,Hank,Toys,2023-01-21,5,15.00,Debit Card\n1009,Ivy,Fashion,2023/01/22,2,49.99,Paypal\n1010,Jack,home Appliances,2023-01-23,1,139.99,Debit card\n1011,Kate,Toys,2023-01-23,,15.00,Cash\n1012,Liam,Electronics,2023/01/24,1,NaN,Credit card\n1013,Mia,Beauty,2023-01-25,2,20.00,paypal\n1014,Nina,Fashion,2023-01-26,1,59.99,PayPal", "eda_steps": ["Check for missing values in each column", "Standardize the capitalization of categorical columns: Product_Category and Payment_Method", "Parse and standardize the Order_Date column to a consistent date format", "Calculate descriptive statistics for Quantity and Price_per_Unit columns", "Identify unique Product_Category values and their frequencies", "Compute total revenue per Product_Category", "Check for duplicates in Order_ID", "Summarize the distribution of Payment_Method usage", "Identify rows with inconsistent or missing Customer_Name values"], "eda_results": {"missing_values": {"Order_ID": 0, "Customer_Name": 1, "Product_Category": 0, "Order_Date": 0, "Quantity": 2, "Price_per_Unit": 2, "Payment_Method": 0}, "standardized_product_categories": ["Electronics", "Home Appliances", "Beauty", "Fashion", "Toys"], "standardized_payment_methods": ["Credit Card", "Paypal", "Cash", "Debit Card"], "descriptive_statistics": {"Quantity": {"count": 13, "mean": 1.85, "std": 1.26, "min": 1, "max": 5}, "Price_per_Unit": {"count": 13, "mean": 109.06, "std": 118.36, "min": 15.0, "max": 310.0}}, "product_category_counts": {"Electronics": 4, "Home Appliances": 2, "Beauty": 3, "Fashion": 3, "Toys": 2}, "total_revenue_per_category": {"Electronics": 1479.98, "Home Appliances": 289.49, "Beauty": 139.96, "Fashion": 209.96, "Toys": 120.0}, "duplicate_order_ids": 0, "payment_method_distribution": {"Credit Card": 4, "Paypal": 4, "Cash": 2, "Debit Card": 2}, "missing_customer_names": [1003]}}
{"purpose": "Analyze student performance and attendance patterns across different courses and identify data quality issues.", "raw_table": "StudentID,Name,Course,Score,Attendance%,ExamDate\n101,john doe,Math,85,90,2023-03-15\n102,Jane Smith,Science,92,,15/03/2023\n103,ALAN Turing,math,78,85,2023/03/16\n104,Maria Garcia,History,88,95,03-17-2023\n105,Liam O'Brien,science,NaN,80,2023-03-18\n106,Emma Li,History,91,100,17 Mar 2023\n107,Noah Brown,Math,87,92,\n108,Olivia Davis,science,89,85,2023-3-19\n109,Ava Wilson,History,83,NaN,2023.03.20\n110,James Miller,Math,NaN,88,March 21, 2023\n111,Isabella Martinez,science,90,87,2023-03-22\n112,Lucas Anderson,History,85,93,2023-03-23\n113,Mia Thomas,math,80,81,2023-03-24", "eda_steps": ["Check and report missing values for all columns", "Standardize course names to consistent capitalization", "Convert ExamDate column to a uniform date format", "Compute descriptive statistics for the Score and Attendance% columns", "Generate value counts for the Course column", "Identify students with missing Score or Attendance% values", "Calculate average Score and Attendance% per Course", "Analyze correlation between Score and Attendance%", "Summarize data quality issues found"], "eda_results": {"missing_values": {"StudentID": 0, "Name": 0, "Course": 0, "Score": 3, "Attendance%": 2, "ExamDate": 1}, "standardized_courses": {"Math": 5, "Science": 4, "History": 4}, "exam_dates_uniform_format": ["2023-03-15", "2023-03-15", "2023-03-16", "2023-03-17", "2023-03-18", "2023-03-17", "2023-03-19", "2023-03-19", "2023-03-20", "2023-03-21", "2023-03-22", "2023-03-23", "2023-03-24"], "score_stats": {"count": 10, "mean": 86.8, "std": 4.57, "min": 78, "25%": 83, "50%": 86, "75%": 90, "max": 92}, "attendance_stats": {"count": 11, "mean": 88.5, "std": 6.33, "min": 80, "25%": 85, "50%": 90, "75%": 93, "max": 100}, "missing_score_students": ["Liam O'Brien", "James Miller", "Noah Brown"], "missing_attendance_students": ["Jane Smith", "Ava Wilson"], "average_per_course": {"Math": {"average_score": 82.5, "average_attendance": 88.75}, "Science": {"average_score": 90.25, "average_attendance": 84.25}, "History": {"average_score": 86.75, "average_attendance": 92.0}}, "score_attendance_correlation": 0.62, "data_quality_issues": {"inconsistent_course_names": true, "missing_scores": 3, "missing_attendance": 2, "inconsistent_date_formats": true, "mixed_types_in_score": true}}}
{"purpose": "Analyze public transportation ridership trends and identify data quality issues in bus service records.", "raw_table": "Route_ID,Route_Name,Date,Boardings,Alightings,Driver_Name,Weather\n101,Downtown Express,2023-05-01,150,140,John Smith,Clear\n102,Suburban Line,5/2/2023,130,125,Mary johnson,Rainy\n103,River Side,2023/05/03,NaN,115,Bob Lee,Cloudy\n104,uptown Local,2023-05-04,90,,Alice Brown,sunny\n101,Downtown Express,05-05-2023,160,155,John Smith,clear\n102,Suburban LINE,2023-05-06,135,130,Mary Johnson,Rain\n105,Airport Shuttle,2023-5-07,80,75,Tom O'Neil,Foggy\n103,Riverside,2023-05-08,110,105,Bob Lee,Cloudy\n104,Uptown local,May 9 2023,95,90,Alice Brown,Sunny\n101,Downtown express,2023-05-10,170,165,John Smith,Clear\n,,2023-05-11,100,98,Unknown,Clear\n105,Airport Shuttle,2023-05-12,82,78,Tom O'Neil,Foggy\n102,Suburban line,2023-13-05,140,135,Mary Johnson,Rainy\n103,Riverside,2023-05-14,115,110,Bob Lee,Cloudy", "eda_steps": ["Compute descriptive statistics for numeric columns Boardings and Alightings", "Generate value counts for Route_Name and Weather columns", "Check missing value percentages for all columns", "Standardize date formats and identify invalid dates", "Identify unique driver names and count their occurrences", "Examine capitalization inconsistencies in Route_Name and Weather columns", "List rows with missing or malformed Route_ID or Route_Name", "Calculate correlation between Boardings and Alightings"], "eda_results": {"summary_stats": {"Boardings": {"count": 14, "mean": 123.21, "std": 30.12, "min": 80, "25%": 95, "50%": 115, "75%": 160, "max": 170}, "Alightings": {"count": 13, "mean": 120.54, "std": 29.21, "min": 75, "25%": 90, "50%": 115, "75%": 155, "max": 165}}, "value_counts": {"Route_Name": {"Downtown Express": 3, "Suburban Line": 3, "Riverside": 4, "Uptown Local": 2, "Airport Shuttle": 2, "": 1}, "Weather": {"Clear": 4, "Rainy": 2, "Cloudy": 4, "Sunny": 2, "Foggy": 2, "Rain": 1}}, "missing_values": {"Route_ID": 1, "Route_Name": 1, "Date": 0, "Boardings": 1, "Alightings": 2, "Driver_Name": 1, "Weather": 0}, "date_issues": {"invalid_dates": ["2023-13-05"], "formats_detected": ["YYYY-MM-DD", "MM/DD/YYYY", "YYYY/MM/DD", "MM-DD-YYYY", "Month D YYYY"]}, "driver_name_counts": {"John Smith": 3, "Mary Johnson": 3, "Bob Lee": 4, "Alice Brown": 2, "Tom O'Neil": 2, "Unknown": 1, "Mary johnson": 1}, "capitalization_inconsistencies": {"Route_Name": ["uptown Local", "Uptown local", "Suburban LINE", "Suburban line", "Downtown express", "Riverside", "River Side"], "Weather": ["clear", "Clear", "Rain", "Rainy", "sunny", "Sunny"]}, "rows_with_missing_route_info": [10], "correlations": {"Boardings_Alightings": 0.99}}}
{"purpose": "Explore housing listings data to understand property characteristics and data quality issues.", "raw_table": "ListingID,Price,Bedrooms,Bathrooms,Location,DateListed,PropertyType\n001,350000,3,2,Downtown,2023-05-15,Apartment\n002,450000,4,,Suburbs,15/06/2023,House\n003,NaN,2,1,suburbs,2023/07/01,townhouse\n004,500000,3,2,Downtown,2023-06-20,Apartment\n005,380000,Three,2,Suburbs,06-25-2023,House\n006,420000,3,2,,2023-07-10,apartment\n007,470000,4,3,Suburbs,2023-07-15,House\n008,390000,2,1,Downtown,07/20/2023,Apartment\n009,440000,3,2,suburbs,,townhouse\n010,350000,2,1,Suburbs,2023-07-22,House\n011,,3,2,DOWNTOWN,2023-07-25,Apartment\n012,360000,2,1,Suburbs,2023-07-30,HOUSE", "eda_steps": ["Check for missing values in each column", "Standardize the Location and PropertyType columns capitalization", "Convert DateListed to a uniform date format", "Convert Bedrooms and Bathrooms to numeric, handling non-numeric values", "Compute descriptive statistics for Price, Bedrooms, and Bathrooms", "Generate value counts for Location and PropertyType", "Identify listings with missing Price or DateListed", "Compute correlation between Price, Bedrooms, and Bathrooms"], "eda_results": {"missing_values": {"ListingID": 0, "Price": 2, "Bedrooms": 1, "Bathrooms": 1, "Location": 1, "DateListed": 1, "PropertyType": 0}, "standardized_values": {"Location": ["Downtown", "Suburbs", "Townhouse (incorrect category)"], "PropertyType": ["Apartment", "House", "Townhouse"]}, "date_format_issues": {"original_formats": ["YYYY-MM-DD", "DD/MM/YYYY", "YYYY/MM/DD", "MM-DD-YYYY", "MM/DD/YYYY", "missing"], "converted_to_iso": true}, "converted_numeric_columns": {"Bedrooms": {"non_numeric_found": ["Three"], "converted_values": [3, 4, 2, 3, 3, 4, 2, 3, 2, 3, 2]}, "Bathrooms": {"missing_values": 1}}, "summary_stats": {"Price": {"count": 10, "mean": 402000, "median": 395000, "min": 350000, "max": 500000, "std": 52079.2}, "Bedrooms": {"count": 11, "mean": 2.91, "median": 3, "min": 2, "max": 4, "std": 0.67}, "Bathrooms": {"count": 10, "mean": 1.8, "median": 2, "min": 1, "max": 3, "std": 0.6}}, "value_counts": {"Location": {"Downtown": 4, "Suburbs": 6, "": 1}, "PropertyType": {"Apartment": 5, "House": 5, "Townhouse": 2}}, "listings_with_missing": {"Price": ["003", "011"], "DateListed": ["009"]}, "correlations": {"Price_Bedrooms": 0.85, "Price_Bathrooms": 0.78, "Bedrooms_Bathrooms": 0.88}}}
{"purpose": "Analyze student performance and attendance patterns across different courses and instructors.", "raw_table": "Student_ID,Student_Name,Course,Instructor,Grade,Attendance,Enrollment_Date\n101,alice smith,Math 101,Dr. Johnson,88,95%,2023-01-15\n102,Bob JONES,English 201,dr. lee,92,89%,15/01/2023\n103,carol white,math 101,Dr. Johnson,NaN,90%,2023/01/16\n104,David Kim,History 101,Dr. O'Brien,78,missing,2023-1-17\n105,Emily Davis,math 101,DR. JOhnson,85,85%,01-18-2023\n106,Frank Moore,English 201,Dr. Lee,91,88%,2023-01-18\n107,Gina Li,History 101,Dr. O'Brien,82,93%,2023/01/19\n108,Helen Chen,Math 101,Dr. johnson,NaN,92%,2023-01-20\n109,ivan petrov,English 201,Dr lee,76,87%,2023-01-20\n110,Jackie Chan,History 101,dr. o'brien,80,missing,2023-01-21\n111,Karen Liu,English 201,Dr. Lee,88,90%,2023-01-22\n112,Liam Smith,Math 101,Dr. Johnson,90,94%,2023-01-22\n113,Mia Wong,english 201,DR. LEE,85,NaN,2023-01-23\n114,Noah Brown,History 101,Dr. O'Brien,83,91%,2023-01-23", "eda_steps": ["Standardize capitalization for Course and Instructor columns", "Check and calculate percentage of missing values in each column", "Compute descriptive statistics for Grade and Attendance columns", "Convert Attendance percentages to numeric values and handle missing data", "Analyze enrollment dates by converting them to a uniform date format", "Generate value counts for unique Courses and Instructors", "Calculate average Grade by Course", "Identify students with missing Grades or Attendance", "Check correlation between Attendance and Grade"], "eda_results": {"missing_values": {"Grade": 2, "Attendance": 3, "Student_ID": 0, "Student_Name": 0, "Course": 0, "Instructor": 0, "Enrollment_Date": 0}, "value_counts": {"Course": {"Math 101": 5, "English 201": 6, "History 101": 4}, "Instructor": {"Dr. Johnson": 5, "Dr. Lee": 6, "Dr. O'Brien": 4}}, "summary_stats": {"Grade": {"count": 12, "mean": 84.75, "std": 5.2, "min": 76, "25%": 80, "50%": 85, "75%": 90, "max": 92}, "Attendance (%)": {"count": 11, "mean": 90.45, "std": 3.68, "min": 85, "25%": 88, "50%": 90, "75%": 93, "max": 95}}, "average_grade_by_course": {"Math 101": 86.25, "English 201": 86.4, "History 101": 80.75}, "students_missing_grade": ["Carol White", "Helen Chen"], "students_missing_attendance": ["David Kim", "Jackie Chan", "Mia Wong"], "correlations": {"Attendance_Grade": 0.67}, "enrollment_date_range": {"min_date": "2023-01-15", "max_date": "2023-01-23"}}}
{"purpose": "Analyze recent temperature and precipitation patterns across different climate zones to identify data quality issues and summarize trends.", "raw_table": "Date,Location,Climate_Zone,Avg_Temperature_C,Precipitation_mm\n2024-01-01,Seattle,temperate,7.5,5.2\n01/02/2024,SEATTLE,Temperate,8.1,NaN\n2024-01-03,Miami,tropical,23.3,12.0\n2024-Jan-04,miami,TROPICAL,24,15.1\n2024-1-05,Denver,Alpine, -2.1,0\n2024-01-06,Denver,alpine,,0.5\n2024-01-07,Houston,subtropical,15.0,20.1\n2024-01-08,Houston,SUBTROPICAL,15.2,19.8\n2024-1-9,New York,Temperate,3.2,NaN\n2024/01/10,new york,temperate,3.0,1.5\n2024-01-11,Anchorage,Polar,-10.5,0\n2024-01-12,Anchorage,Polar,-11,NaN\n2024-01-13,Miami,tropical,NaN,18.2\n2024-01-14,Denver,Alpine,-1.8,0.0", "eda_steps": ["Check and summarize missing values per column", "Standardize capitalization for Location and Climate_Zone columns", "Convert Date column to consistent datetime format", "Compute descriptive statistics for Avg_Temperature_C and Precipitation_mm", "Generate value counts for Climate_Zone", "Identify rows with inconsistent or missing temperature values", "Calculate average temperature and precipitation per Climate_Zone", "Detect outliers in temperature and precipitation using IQR method"], "eda_results": {"missing_values": {"Date": 0, "Location": 0, "Climate_Zone": 0, "Avg_Temperature_C": 2, "Precipitation_mm": 3}, "standardized_locations": ["Seattle", "Seattle", "Miami", "Miami", "Denver", "Denver", "Houston", "Houston", "New York", "New York", "Anchorage", "Anchorage", "Miami", "Denver"], "standardized_climate_zones": ["Temperate", "Temperate", "Tropical", "Tropical", "Alpine", "Alpine", "Subtropical", "Subtropical", "Temperate", "Temperate", "Polar", "Polar", "Tropical", "Alpine"], "date_conversion_success_rate": "100%", "summary_stats": {"Avg_Temperature_C": {"count": 12, "mean": 6.08, "std": 11.24, "min": -11, "25%": -1.85, "50%": 3.1, "75%": 15.1, "max": 24}, "Precipitation_mm": {"count": 11, "mean": 7.13, "std": 7.45, "min": 0, "25%": 0, "50%": 1.5, "75%": 15.1, "max": 20.1}}, "value_counts_climate_zone": {"Temperate": 4, "Tropical": 4, "Alpine": 4, "Subtropical": 2, "Polar": 2}, "inconsistent_temperature_rows": [5, 12], "average_by_climate_zone": {"Temperate": {"Avg_Temperature_C": 5.95, "Precipitation_mm": 2.23}, "Tropical": {"Avg_Temperature_C": 23.65, "Precipitation_mm": 15.1}, "Alpine": {"Avg_Temperature_C": -1.23, "Precipitation_mm": 0.17}, "Subtropical": {"Avg_Temperature_C": 15.1, "Precipitation_mm": 19.95}, "Polar": {"Avg_Temperature_C": -10.75, "Precipitation_mm": 0}}, "detected_outliers": {"Avg_Temperature_C": [24, -11], "Precipitation_mm": [20.1]}}}
{"purpose": "Analyze student performance and attendance patterns to identify potential factors affecting grades.", "raw_table": "Student_ID,Name,Grade,Attendance_Percentage,Enrollment_Date,Favorite_Subject,Extra_Curricular\n101,alice,85,92%,2023-01-12,Math,Drama\n102,Bob,78,88,01/15/2023,science,Soccer\n103,charlie,,85%,2023-1-18,History, \n104,david,92,95%,2023/01/20,math,Basketball\n105,Eva,88,90%,2023-02-01,SCIENCE,Drama\n106,Frank,not available,80%,2023-01-25,English,Swimming\n107,Grace,81,missing,2023-01-30,history,Soccer\n108,Helen,79,87%,2023-01-28,Math,Basketball\n109,Ian,85,91%,2023-15-01,Science,Drama\n110,Jack,90,93%,2023-01-22,,Chess", "eda_steps": ["Check missing values percentage for each column", "Standardize capitalization in 'Favorite_Subject'", "Convert 'Enrollment_Date' to consistent date format", "Identify and handle non-numeric and missing values in 'Grade'", "Compute descriptive statistics for 'Grade' and 'Attendance_Percentage'", "Generate value counts for 'Favorite_Subject' and 'Extra_Curricular'", "Analyze correlation between 'Grade' and 'Attendance_Percentage'", "Identify students with missing or unusual data entries"], "eda_results": {"missing_values": {"Student_ID": "0%", "Name": "0%", "Grade": "18.18%", "Attendance_Percentage": "9.09%", "Enrollment_Date": "0%", "Favorite_Subject": "9.09%", "Extra_Curricular": "9.09%"}, "standardized_subjects": {"math": 3, "science": 3, "history": 2, "english": 1, "": 1}, "date_format_consistency": "All dates converted to YYYY-MM-DD format; one date corrected from invalid '2023-15-01' to '2023-01-15'", "grade_cleaning": {"non_numeric_entries": ["not available"], "missing_entries": [103], "grades_used_for_stats": 9}, "summary_stats": {"Grade": {"count": 9, "mean": 84.44, "std": 4.56, "min": 78, "max": 92}, "Attendance_Percentage": {"count": 11, "mean": 89.18, "std": 4.22, "min": 80, "max": 95}}, "value_counts": {"Favorite_Subject": {"math": 3, "science": 3, "history": 2, "english": 1, "missing": 1}, "Extra_Curricular": {"Drama": 3, "Soccer": 2, "Basketball": 2, "Swimming": 1, "Chess": 1, "missing": 1}}, "correlations": {"Grade_vs_Attendance_Percentage": 0.82}, "unusual_data_entries": {"Students_with_missing_grades": ["Charlie"], "Students_with_non_numeric_grades": ["Frank"], "Students_with_missing_attendance": ["Grace"], "Students_with_missing_subject": ["Jack"]}}}
{"purpose": "Analyze customer purchase behavior and identify common payment methods and product categories.", "raw_table": "OrderID,CustomerID,OrderDate,ProductCategory,PaymentMethod,Quantity,Price\n1001,C001,2023-01-15,Electronics,Credit Card,2,199.99\n1002,C002,01/20/2023,home appliances,Paypal,1,299.50\n1003,C003,2023/01/22,Electronics,CREDIT card,3,99.95\n1004,,2023-01-25,Books,Cash,5,15.00\n1005,C005,2023-1-27,Toys,Credit Card,2,\n1006,C006,2023-01-28,Books,Cash,NaN,12.99\n1007,C007,2023-02-01,fashion,Debit Card,1,49.99\n1008,C002,02-03-2023,Home Appliances,Paypal,2,299.50\n1009,C008,2023-02-05,Toys,Cash,3,20.00\n1010,C009,2023-02-07,Fashion,credit card,1,59.99\n1011,C001,2023-02-10,Electronics,Credit Card,NaN,199.99\n1012,C010,2023-2-12,Books,,4,15.00\n1013,C011,2023-02-15,Home Appliances,Paypal,1,NaN", "eda_steps": ["Standardize the capitalization for ProductCategory and PaymentMethod columns", "Check for missing values in each column and calculate their percentages", "Convert OrderDate to a consistent date format", "Calculate descriptive statistics for Quantity and Price columns after handling missing values", "Generate value counts for ProductCategory and PaymentMethod", "Identify orders with missing CustomerID or PaymentMethod", "Compute total revenue per ProductCategory", "Find the top 2 most frequent PaymentMethods", "Summarize the number of orders per month"], "eda_results": {"missing_values": {"OrderID": "0%", "CustomerID": "7.7%", "OrderDate": "0%", "ProductCategory": "0%", "PaymentMethod": "7.7%", "Quantity": "15.4%", "Price": "15.4%"}, "standardized_categories": {"ProductCategory": ["Electronics", "Home Appliances", "Books", "Toys", "Fashion"], "PaymentMethod": ["Credit Card", "Paypal", "Cash", "Debit Card", "Missing"]}, "descriptive_statistics": {"Quantity": {"count": 11, "mean": 2.27, "std": 1.31, "min": 1, "max": 5}, "Price": {"count": 11, "mean": 118.94, "std": 120.24, "min": 12.99, "max": 299.5}}, "value_counts": {"ProductCategory": {"Electronics": 3, "Home Appliances": 3, "Books": 3, "Toys": 2, "Fashion": 2}, "PaymentMethod": {"Credit Card": 5, "Paypal": 3, "Cash": 3, "Debit Card": 1, "Missing": 1}}, "orders_with_missing": {"Missing CustomerID": [1004], "Missing PaymentMethod": [1012]}, "total_revenue_per_category": {"Electronics": 999.73, "Home Appliances": 1198.0, "Books": 147.99, "Toys": 80.0, "Fashion": 109.98}, "top_payment_methods": ["Credit Card", "Paypal"], "orders_per_month": {"2023-01": 7, "2023-02": 6}}}
{"purpose": "Analyze monthly sales performance and identify data quality issues in retail transactions.", "raw_table": "TransactionID,ProductCategory,SaleAmount,SaleDate,StoreRegion,CustomerAge\nT001,Electronics,1200.50,2023-01-15,North,34\nT002,fashion,89.99,15/01/2023,South,28\nT003,Home & Kitchen,,2023/01/16,west,45\nT004,Electronics,NaN,2023-1-17,North,NaN\nT005,Fashion,59.49,2023-01-18,East,21\nT006,home & kitchen,200.00,01-19-2023,West,52\nT007,Books,15.75,2023-01-20,SOUTH,38\nT008,Books,NaN,20-01-2023,South,NaN\nT009,Electronics,800.00,2023-01-21,North,29\nT010,Toys,45.00,2023/01/22,East,26\nT011,Toys,55.00,2023-01-23,East,\nT012,Fashion,100.00,2023-01-24,North,31\nT013,Home & kitchen,180,2023/01/25,West,49\nT014,Books,NaN,2023-01-26,South,40\nT015,Electronics,950.00,2023-01-27,North,36", "eda_steps": ["Check for missing values in each column", "Standardize the casing in ProductCategory and StoreRegion columns", "Parse and unify SaleDate into a consistent date format", "Compute descriptive statistics for SaleAmount and CustomerAge", "Generate value counts for ProductCategory and StoreRegion", "Identify top 3 ProductCategories by total sales amount", "Check correlation between SaleAmount and CustomerAge", "Summarize how many transactions have missing SaleAmount"], "eda_results": {"missing_values": {"TransactionID": 0, "ProductCategory": 0, "SaleAmount": 4, "SaleDate": 0, "StoreRegion": 0, "CustomerAge": 3}, "standardized_categories": {"ProductCategory": ["Electronics", "Fashion", "Home & Kitchen", "Books", "Toys"], "StoreRegion": ["North", "South", "West", "East"]}, "sale_date_format": "All dates successfully parsed and converted to YYYY-MM-DD", "summary_stats": {"SaleAmount": {"count": 11, "mean": 449.23, "std": 476.82, "min": 15.75, "25%": 55.0, "50%": 89.99, "75%": 800.0, "max": 1200.5}, "CustomerAge": {"count": 12, "mean": 35.25, "std": 8.8, "min": 21, "25%": 29, "50%": 34, "75%": 40, "max": 52}}, "value_counts": {"ProductCategory": {"Electronics": 4, "Fashion": 3, "Home & Kitchen": 3, "Books": 4, "Toys": 2}, "StoreRegion": {"North": 5, "South": 4, "West": 3, "East": 3}}, "top_3_product_categories_by_sales": {"Electronics": 3950.5, "Home & Kitchen": 380.0, "Fashion": 249.48}, "correlations": {"SaleAmount_CustomerAge": 0.62}, "missing_saleamount_transactions": 4}}
{"purpose": "Analyze public housing applications to identify common applicant demographics and application status trends.", "raw_table": "ApplicationID,ApplicantName,Age,Gender,ApplicationDate,Status,Income,City\n1001,John Doe,34,Male,2023-01-15,approved,45000,New York\n1002,jane smith,29,FEMALE,15/02/2023,pending,52000,los angeles\n1003,Mike O'Neil,,Male,2023/03/05,rejected,,Chicago\n1004,Anna Lee,42,Female,04-10-2023,APPROVED,38000,Houston\n1005,Chris P,27,,2023-05-20,pending,NaN,miami\n1006,,31,Male,2023-06-01,Rejected,47000,New york\n1007,Patricia Black,22,female,2023-07-12,pending,33000,Boston\n1008,George King,48,Male,07/15/2023,approved,52000,SAN FRANCISCO\n1009,Linda Green,35,Female,,approved,48000,Seattle\n1010,Robert Brown,40,Male,2023-08-10,pending,45000,boston\n1011,Susan White,38,Female,2023-09-05,Rejected,40000,Chicago\n1012,Mary Johnson,29,Female,2023-10-01,pending,NaN,Los Angeles", "eda_steps": ["Check for missing values in each column", "Standardize and summarize the 'Gender' column values", "Parse and unify the date format in 'ApplicationDate'", "Compute descriptive statistics for the 'Age' and 'Income' columns", "Count the number of applications by each 'Status' category", "Identify the top 3 cities by number of applications", "Calculate average income grouped by application 'Status'", "Assess the distribution of applicant ages", "Check for duplicates in 'ApplicationID'"], "eda_results": {"missing_values": {"ApplicationID": 0, "ApplicantName": 1, "Age": 1, "Gender": 1, "ApplicationDate": 1, "Status": 0, "Income": 3, "City": 0}, "gender_value_counts": {"Female": 6, "Male": 5, "Missing/Unknown": 1}, "application_date_formats_standardized": {"earliest_date": "2023-01-15", "latest_date": "2023-10-01", "missing_dates": 1}, "age_statistics": {"count": 13, "mean": 34.3, "median": 34, "min": 22, "max": 48, "std_dev": 7.35}, "income_statistics": {"count": 9, "mean": 44111, "median": 45000, "min": 33000, "max": 52000, "std_dev": 6870}, "application_status_counts": {"approved": 4, "pending": 5, "rejected": 3}, "top_cities_by_applications": {"Boston": 2, "Chicago": 2, "Los Angeles": 2}, "average_income_by_status": {"approved": 45750, "pending": 44333, "rejected": 43500}, "age_distribution": {"18-25": 1, "26-35": 7, "36-45": 4, "46-55": 1}, "duplicate_application_ids": 0}}
{"purpose": "Analyze monthly temperature and precipitation patterns across different climate zones to identify data quality issues and seasonal trends.", "raw_table": "Date,Climate_Zone,Avg_Temperature_C,Precipitation_mm\n2023-01-15,Tropical,28.4,120\n01/15/2023,temperate,15.2,85\n2023-02-15,ARID,22.1,5\n2023-02-15,Tropical,,130\n15-Mar-2023,TEMPERATE,17.8,N/A\n2023-03-15,Arid,24.3,7\n2023-04-15,Tropical,29,,\n2023-04-15,Temperate,18.4,90\n2023-05-15,arid,25.0,8\n2023-05-15,tropical,27.9,115\nMay 15 2023,Temperate,19.5,95\n2023-06-15,Arid,N/A,6\n2023-06-15,Tropical,30.1,110", "eda_steps": ["Check and summarize missing values in each column", "Standardize Climate_Zone categories by capitalization", "Convert Date column to a consistent datetime format", "Compute descriptive statistics for Avg_Temperature_C and Precipitation_mm", "Generate value counts for Climate_Zone", "Identify rows with inconsistent or missing precipitation values", "Calculate correlation between Avg_Temperature_C and Precipitation_mm", "Summarize monthly average temperature by climate zone"], "eda_results": {"missing_values": {"Date": 0, "Climate_Zone": 0, "Avg_Temperature_C": 2, "Precipitation_mm": 3}, "standardized_climate_zone_counts": {"Tropical": 5, "Temperate": 4, "Arid": 4}, "date_parsing_success": "All dates converted successfully to ISO format YYYY-MM-DD", "summary_stats": {"Avg_Temperature_C": {"count": 11, "mean": 23.95, "std": 5.01, "min": 15.2, "max": 30.1}, "Precipitation_mm": {"count": 10, "mean": 74.1, "std": 48.6, "min": 5, "max": 130}}, "value_counts_precipitation_issues": {"Missing or N/A": 3}, "correlations": {"Avg_Temperature_C_vs_Precipitation_mm": -0.42}, "monthly_avg_temperature_by_zone": {"Tropical": {"Jan": 28.4, "Feb": null, "Apr": 29.0, "May": 27.9, "Jun": 30.1}, "Temperate": {"Jan": 15.2, "Mar": 17.8, "Apr": 18.4, "May": 19.5}, "Arid": {"Feb": 22.1, "Mar": 24.3, "May": 25.0, "Jun": null}}}}
{"purpose": "Analyze monthly stock trading data to identify missing values, distribution of trade volumes, and sector performance.", "raw_table": "Date,Stock,Ticker,Sector,Open,Close,Volume\n2023-01-05,Alpha Inc,ALPH,Technology,120.5,121.3,1000000\n2023/01/06,Beta corp,BETA,Healthcare,85.3,86.1,750000\n2023-01-07,Gamma LLC,GMMa,tech,NaN,95.4,1200000\nJan 08 2023,Delta Ltd,DLTA,Finance,45.2,46.0,NaN\n2023-01-09,Epsilon,EPS,Healthcare,78.0,77.5,800000\n2023-1-10,Zeta Industries,ZETA,TECHNOLOGY,110.0,111.7,1100000\n2023-01-11,eta Corp,ETA,healthcare,82.5,,900000\n2023-01-12,Theta Co,THETA,Finance,50.0,51.2,700000\n20230113,Iota Inc,IOTA,Technology,115.4,116.0,1050000\n2023-01-14,Kappa Ltd,KAPP,Finance,47.8,48.1,680000\n2023-01-15,Lambda LLC,LAMB,Healthcare,NaN,89.2,850000", "eda_steps": ["Check and count missing values in each column", "Standardize the Sector column capitalization", "Compute descriptive statistics for Open, Close, and Volume columns", "Generate value counts for the Sector column", "Identify the date format inconsistencies", "Calculate correlation between Open, Close, and Volume", "Find top 3 stocks by average Close price", "Summarize missing data impact on Volume"], "eda_results": {"missing_values": {"Date": 0, "Stock": 0, "Ticker": 0, "Sector": 0, "Open": 2, "Close": 1, "Volume": 1}, "standardized_sectors": {"Technology": 5, "Healthcare": 4, "Finance": 3}, "summary_stats": {"Open": {"count": 13, "mean": 84.64, "std": 27.39, "min": 45.2, "max": 120.5}, "Close": {"count": 14, "mean": 89.18, "std": 21.37, "min": 46.0, "max": 121.3}, "Volume": {"count": 14, "mean": 882142.9, "std": 161623.6, "min": 680000, "max": 1200000}}, "date_formats": {"YYYY-MM-DD": 10, "YYYY/MM/DD": 1, "MMM DD YYYY": 1, "YYYYMMDD": 1}, "correlations": {"Open_Close": 0.98, "Open_Volume": 0.36, "Close_Volume": 0.31}, "top_stocks_by_avg_close": {"Alpha Inc": 121.3, "Iota Inc": 116.0, "Zeta Industries": 111.7}, "missing_volume_rows": 1, "impact_of_missing_volume": "1 out of 15 rows missing volume data, representing 6.7% missingness which may affect volume-based analysis."}}
{"purpose": "Analyze monthly transaction patterns and client segmentation in retail banking.", "raw_table": "Client_ID,Transaction_Date,Transaction_Amount,Account_Type,Branch,Transaction_Type\nC001,2023-01-15,250.75,Checking,New York,deposit\nc002,01/20/2023,,-SAVINGS,Boston,Withdrawal\nC003,2023-02-05,500,checking,CHICAGO,transfer\nC004,2023/02/17,NaN,Savings,Los Angeles,Deposit\nc005,March 1 2023,1200.50,SAVINGS,Miami,Withdrawal\nC006,03-15-2023,300,Checking,miami,deposit\nC007,2023-04-01,850,,New york,Transfer\nc008,2023-04-12,,Checking,Boston,withdrawal\nC009,04/20/2023,450,Savings,Chicago,Deposit\nc010,2023-05-05,NaN,Checking,Los Angeles,deposit\nc011,May 15 2023,700,Savings,Boston,withdrawal\nC012,2023-05-20,400,CHECKING,New York,Transfer", "eda_steps": ["Standardize the date format in Transaction_Date column", "Clean and unify Account_Type values to lowercase without special characters", "Identify and count missing values in each column", "Calculate summary statistics for Transaction_Amount", "Count unique values and frequency of Account_Type and Transaction_Type", "Check distribution of transactions by Branch", "Analyze correlation between Transaction_Amount and Account_Type", "Identify top 3 clients by total transaction amount"], "eda_results": {"missing_values": {"Client_ID": 0, "Transaction_Date": 0, "Transaction_Amount": 4, "Account_Type": 1, "Branch": 0, "Transaction_Type": 0}, "summary_stats": {"Transaction_Amount": {"count": 11, "mean": 567.05, "std": 361.14, "min": 250.75, "25%": 400, "50%": 450, "75%": 700, "max": 1200.5}}, "value_counts": {"Account_Type": {"checking": 6, "savings": 5}, "Transaction_Type": {"deposit": 5, "withdrawal": 4, "transfer": 3}}, "branch_distribution": {"New York": 3, "Boston": 3, "Chicago": 2, "Los Angeles": 2, "Miami": 2}, "top_clients_by_transaction_amount": {"c005": 1200.5, "c007": 850, "c011": 700}, "correlations": {"Transaction_Amount_vs_Account_Type_checking": 0.22, "Transaction_Amount_vs_Account_Type_savings": -0.22}}}
{"purpose": "Analyze monthly stock trading volumes and price trends for selected companies to identify data quality issues and basic statistics.", "raw_table": "Ticker,TradeDate,OpenPrice,ClosePrice,Volume,Exchange\nAAPL,2023-01-02,130.5,132.1,1000000,NASDAQ\nmsft,01/03/2023,245.3,247.5,,Nasdaq\nGOOG,2023/01/04,2720,2745.2,500000,NASDAQ\nTSLA,2023-01-05,720.2,710.8,700000,Nasdaq\naapl,2023-1-6,133.0,135,1100000,NasDAQ\nMSFT,2023-01-07,248,250.3,950000,NYSE\nGOOG,01-08-2023,2740,,480000,NASDAQ\nTSLA,2023/01/09,,715.6,680000,Nasdaq\nFB,2023-01-10,320,325.5,600000,Nasdaq\nGOOG,,2735,2750,520000,Nasdaq\nAAPL,2023-01-12,134.5,136.7,1050000,NASDAQ\nfb,2023-01-13,322,326,590000,NASDAQ\nTSLA,2023-01-14,710.5,720.1,,nasdaq\nMSFT,2023-01-15,251,252.4,970000,NYSE", "eda_steps": ["Check for missing values in each column", "Standardize the date format in the TradeDate column", "Normalize the Ticker and Exchange columns to consistent capitalization", "Compute descriptive statistics for numeric columns OpenPrice, ClosePrice, and Volume", "Generate value counts for the Ticker and Exchange columns", "Identify rows with missing or inconsistent data", "Calculate the correlation matrix for numeric columns", "Summarize the distribution skewness for Volume", "Identify top 2 tickers by average trading volume"], "eda_results": {"missing_values": {"Ticker": 0, "TradeDate": 1, "OpenPrice": 2, "ClosePrice": 1, "Volume": 3, "Exchange": 0}, "standardized_dates": ["2023-01-02", "2023-01-03", "2023-01-04", "2023-01-05", "2023-01-06", "2023-01-07", "2023-01-08", "2023-01-09", "2023-01-10", null, "2023-01-12", "2023-01-13", "2023-01-14", "2023-01-15"], "normalized_tickers": ["AAPL", "MSFT", "GOOG", "TSLA", "AAPL", "MSFT", "GOOG", "TSLA", "FB", "GOOG", "AAPL", "FB", "TSLA", "MSFT"], "normalized_exchanges": {"nasdaq": "NASDAQ", "Nasdaq": "NASDAQ", "NYSE": "NYSE"}, "summary_stats": {"OpenPrice": {"count": 12, "mean": 654.08, "std": 1064.42, "min": 130.5, "25%": 245.3, "50%": 320.0, "75%": 720.2, "max": 2720.0}, "ClosePrice": {"count": 13, "mean": 658.06, "std": 1065.19, "min": 132.1, "25%": 250.3, "50%": 326.0, "75%": 715.6, "max": 2750.0}, "Volume": {"count": 11, "mean": 748181.82, "std": 205361.64, "min": 480000, "25%": 600000, "50%": 700000, "75%": 970000, "max": 1100000}}, "value_counts": {"Ticker": {"AAPL": 3, "MSFT": 3, "GOOG": 3, "TSLA": 3, "FB": 2}, "Exchange": {"NASDAQ": 11, "NYSE": 3}}, "rows_with_issues": [1, 6, 7, 9, 12], "correlations": {"OpenPrice_ClosePrice": 0.999, "OpenPrice_Volume": 0.15, "ClosePrice_Volume": 0.13}, "volume_skewness": 0.12, "top_2_tickers_by_avg_volume": {"AAPL": 1050000, "MSFT": 973333}}}
{"purpose": "Analyze customer purchase behavior and identify data quality issues in transaction records.", "raw_table": "OrderID,CustomerID,ProductCategory,OrderDate,Quantity,UnitPrice,PaymentMethod\n1001,C123,Electronics,2023-1-5,2,199.99,Credit Card\n1002,c123,electronics,05/01/2023,1,199.99,cash\n1003,C124,Home & Kitchen,2023/01/07,,59.99,credit\n1004,C125,Books,07-01-2023,3,,PayPal\n1005,C126,Toys,Jan 8 2023,5,15.0,Credit card\n1006,C127,Home & kitchen,2023-01-09,2,69.99,CASH\n1007,C128,Books,2023-13-01,1,12.99,Paypal\n1008,C129,toys,2023-01-10,4,14.5,Credit Card\n1009,C130,Electronics,,1,199.99,Credit\n1010,,Books,2023-01-12,2,10.99,Cash\n1011,C131,Home & Kitchen,2023-01-13,1,65.0,Credit Card\n1012,C132,Toys,2023-01-14,3,16.5,Credit card", "eda_steps": ["Check for missing values in all columns", "Standardize capitalization in ProductCategory and PaymentMethod columns", "Parse and standardize OrderDate formats", "Calculate descriptive statistics for Quantity and UnitPrice", "Generate value counts for ProductCategory and PaymentMethod", "Identify rows with inconsistent or invalid dates", "Compute total sales (Quantity * UnitPrice) per order", "Identify top 2 product categories by total sales"], "eda_results": {"missing_values": {"OrderID": 0, "CustomerID": 1, "ProductCategory": 0, "OrderDate": 1, "Quantity": 1, "UnitPrice": 1, "PaymentMethod": 0}, "standardized_categories": {"ProductCategory": ["Electronics", "Home & Kitchen", "Books", "Toys"], "PaymentMethod": ["Credit Card", "Cash", "Credit", "PayPal"]}, "invalid_dates": ["2023-13-01"], "descriptive_statistics": {"Quantity": {"count": 14, "mean": 2.21, "std": 1.36, "min": 1, "max": 5}, "UnitPrice": {"count": 13, "mean": 88.45, "std": 78.57, "min": 10.99, "max": 199.99}}, "value_counts": {"ProductCategory": {"Electronics": 3, "Home & Kitchen": 3, "Books": 3, "Toys": 3}, "PaymentMethod": {"Credit Card": 5, "Cash": 2, "Credit": 2, "PayPal": 2}}, "total_sales_per_order": {"1001": 399.98, "1002": 199.99, "1003": null, "1004": null, "1005": 75.0, "1006": 139.98, "1007": 12.99, "1008": 58.0, "1009": 199.99, "1010": 21.98, "1011": 65.0, "1012": 49.5}, "top_categories_by_sales": {"Electronics": 799.96, "Home & Kitchen": 204.98}}}
{"purpose": "Analyze customer call behavior and identify patterns related to call duration and call type.", "raw_table": "CustomerID,CallDate,CallType,CallDurationMinutes,Region\n1001,2023/01/15,Local,13,North\n1002,15-02-2023,international,45,South\n1003,2023-03-05,Local,,East\n1004,2023/04/20,Roaming,60,west\n1005,2023-04-22,local,25,South\n1006,2023/05/10,INTERNATIONAL,NaN,North\n1007,2023-06-01,Roaming,NaN,East\n1008,2023/06/07,Local,15,\n1009,07-07-2023,Local,20,South\n1010,2023-07-15,International,55,North\n1011,2023/08/01,Roaming,35,West\n1012,08/15/2023,local,10,East\n1013,,International,40,South\n", "eda_steps": ["Check and report the percentage of missing values in each column", "Standardize the CallType values to have consistent capitalization", "Parse and standardize CallDate into a uniform date format", "Calculate descriptive statistics (mean, median, std) for CallDurationMinutes", "Generate value counts for CallType and Region columns", "Identify the number of unique customers", "Find correlation between CallDurationMinutes and CallType categories encoded as numbers", "Summarize call duration distribution by CallType", "Identify rows with missing CallDurationMinutes and investigate their CallType and Region"], "eda_results": {"missing_values": {"CustomerID": 0, "CallDate": 1, "CallType": 0, "CallDurationMinutes": 3, "Region": 1}, "standardized_calltype_counts": {"Local": 6, "International": 4, "Roaming": 3}, "standardized_calldate_sample": ["2023-01-15", "2023-02-15", "2023-03-05", "2023-04-20", "2023-04-22"], "callduration_stats": {"count": 10, "mean": 31.8, "median": 25, "std_dev": 17.8, "min": 10, "max": 60}, "value_counts_region": {"North": 3, "South": 4, "East": 3, "West": 2, "": 1}, "unique_customers": 13, "calltype_encoded_correlation_with_duration": 0.78, "call_duration_by_calltype": {"Local": {"mean": 16.6, "count": 6}, "International": {"mean": 46.7, "count": 4}, "Roaming": {"mean": 51.7, "count": 3}}, "missing_duration_calltype_region": [{"CallType": "Local", "Region": "East"}, {"CallType": "International", "Region": "North"}, {"CallType": "Roaming", "Region": "East"}]}}
{"purpose": "Analyze citizen complaints data to identify common complaint types and assess data quality issues.", "raw_table": "Complaint_ID,Date_Received,Department,Complaint_Type,Status,Resolution_Time_Days\n1001,2023/01/15,Public Works,Noise,Closed,5\n1002,15-02-2023,public safety,Traffic,Open,\n1003,,Sanitation,Trash Collection,Closed,3\n1004,2023-03-20,Public Works,noise,Closed,7\n1005,2023/04/01,Health,Food Safety,In Progress,NA\n1006,2023-04-15,Public Works,Street Light,Closed,2\n1007,2023-04-30,Public safety,Traffic,Closed,4\n1008,2023-05-05,Sanitation,Trash collection,Closed,missing\n1009,2023-05-10,HEALTH,food safety,Closed,6\n1010,2023-05-15,Transportation,Bus Service,Open,\n1011,05/20/2023,Transportation,Bus service,Closed,3\n1012,2023/05/22,Public Works,Street light,Closed,2\n1013,2023/06/01,Health,Food safety,Closed,4\n1014,2023-06-05,Public Works,Noise,Closed,5\n1015,2023-06-10,Sanitation,Trash Collection,Open,\n", "eda_steps": ["Check for missing values in each column", "Standardize the Department and Complaint_Type columns to consistent capitalization", "Convert Date_Received to a uniform date format", "Calculate summary statistics for Resolution_Time_Days, excluding missing and invalid entries", "Generate value counts for Department and Complaint_Type", "Identify the number of complaints by Status", "Analyze average Resolution_Time_Days by Department", "Find complaints with missing or invalid Resolution_Time_Days"], "eda_results": {"missing_values": {"Complaint_ID": 0, "Date_Received": 1, "Department": 0, "Complaint_Type": 0, "Status": 0, "Resolution_Time_Days": 5}, "standardized_departments": {"public works": 6, "public safety": 2, "sanitation": 3, "health": 3, "transportation": 2}, "standardized_complaint_types": {"noise": 3, "traffic": 2, "trash collection": 3, "food safety": 3, "street light": 2, "bus service": 2}, "date_format_conversion": {"successfully_converted": 14, "failed_conversion": 1}, "resolution_time_stats": {"count": 10, "mean": 4.1, "median": 4.5, "min": 2, "max": 7}, "status_counts": {"Closed": 10, "Open": 3, "In Progress": 1}, "avg_resolution_time_by_department": {"public works": 4.33, "public safety": 4.0, "sanitation": 3.0, "health": 5.0, "transportation": 3.0}, "invalid_resolution_time_complaints": [1002, 1005, 1008, 1010, 1015]}}
{"purpose": "Analyze customer call and data usage patterns to identify service usage trends and data quality issues in the telecommunications dataset.", "raw_table": "CustomerID,CallDurationMinutes,DataUsageGB,SubscriptionType,LastCallDate,DeviceType\nC001,35,2.5,PREMIUM,2023/03/15,iPhone\nC002,12,,basic,15-03-2023,Samsung\nc003,45,5.1,Premium,2023-03-14,Xiaomi\nC004,NaN,1.2,BASIC,03/13/2023,iPhone\nC005,8,0.8,premium,,OnePlus\nC006,22,3.0,Basic,2023.03.16,Samsung\nc007,60,7.5,PREMIUM,16-Mar-2023,iPhone\nC008,NaN,NaN,basic,2023/03/17,Samsung\nC009,30,4.0,Basic,2023-03-14,Samsung\nC010,15,2.2,Premium,2023-3-14,XIAOMI", "eda_steps": ["Check for missing values in each column", "Standardize the capitalization in SubscriptionType and DeviceType columns", "Convert LastCallDate to consistent date format", "Compute descriptive statistics for CallDurationMinutes and DataUsageGB", "Generate value counts for SubscriptionType and DeviceType", "Identify customers with missing CallDurationMinutes or DataUsageGB", "Calculate correlation between CallDurationMinutes and DataUsageGB"], "eda_results": {"missing_values": {"CustomerID": 0, "CallDurationMinutes": 2, "DataUsageGB": 2, "SubscriptionType": 0, "LastCallDate": 1, "DeviceType": 0}, "standardized_categories": {"SubscriptionType": {"basic": 5, "premium": 5}, "DeviceType": {"iphone": 3, "samsung": 4, "xiaomi": 2, "oneplus": 1}}, "last_call_dates_standardized": ["2023-03-15", "2023-03-15", "2023-03-14", "2023-03-13", null, "2023-03-16", "2023-03-16", "2023-03-17", "2023-03-14", "2023-03-14"], "summary_stats": {"CallDurationMinutes": {"count": 8, "mean": 29.625, "min": 8, "max": 60, "std": 17.47}, "DataUsageGB": {"count": 8, "mean": 3.5375, "min": 0.8, "max": 7.5, "std": 2.26}}, "value_counts": {"SubscriptionType": {"basic": 5, "premium": 5}, "DeviceType": {"iphone": 3, "samsung": 4, "xiaomi": 2, "oneplus": 1}}, "customers_missing_usage": ["C002", "C004", "C008", "C005"], "correlations": {"CallDurationMinutes_vs_DataUsageGB": 0.95}}}
{"purpose": "Analyze bike-sharing trip data to understand usage patterns and data quality issues.", "raw_table": "Trip_ID,Start_Date,End_Date,User_Type,Start_Station,End_Station,Duration_Minutes,Distance_km\n1,2023-05-01 08:15,2023-05-01 08:45,Subscriber,Central Park,central park,30,5.2\n2,05/02/2023 09:00 AM,05/02/2023 09:20 AM,customer,3rd ave,5th Ave,20,3.1\n3,2023-05-03 07:50,2023-05-03 08:05,Subscriber,,Downtown,15,\n4,2023-05-04 18:15,,Subscriber,Central Park,Central Park,45,7.0\n5,2023-05-05 12:00,2023-05-05 12:30,Customer,5th Ave,3rd Ave,thirty,4.8\n6,2023-05-06 14:20,2023-05-06 14:50,Subscriber,Central park,5th ave,30,5.5\n7,May 7 2023 10:00 AM,May 7 2023 10:25 AM,subscriber,3rd Ave,Downtown,25,4.2\n8,2023-05-08 16:00,2023-05-08 16:45,CUSTOMER,Downtown,CENTRAL PARK,45,6.8\n9,2023/05/09 13:30,2023-05-09 13:50,Customer,5th Ave,,20,3.6\n10,2023-05-10 09:10,2023-05-10 09:40,Subscriber,Central Park,3rd Ave,30,5.0\n", "eda_steps": ["Check and report the percentage of missing values per column", "Standardize the capitalization of the 'User_Type' column and count unique categories", "Identify inconsistent or missing entries in 'Start_Station' and 'End_Station'", "Convert 'Duration_Minutes' to numeric, handling non-numeric values and report summary statistics", "Parse and standardize 'Start_Date' and 'End_Date' columns into datetime format", "Calculate trip durations from 'Start_Date' and 'End_Date' to validate 'Duration_Minutes' column", "Generate value counts for the most common start stations", "Compute correlation between 'Duration_Minutes' and 'Distance_km'"], "eda_results": {"missing_values": {"Trip_ID": "0%", "Start_Date": "0%", "End_Date": "10%", "User_Type": "0%", "Start_Station": "10%", "End_Station": "20%", "Duration_Minutes": "10%", "Distance_km": "10%"}, "user_type_counts": {"subscriber": 6, "customer": 4}, "station_issues": {"start_station_missing": 1, "end_station_missing": 2, "inconsistent_capitalization": ["central park", "Central Park", "CENTRAL PARK", "Central park", "3rd ave", "3rd Ave", "5th ave", "5th Ave", "Downtown", "DOWNTOWN"]}, "duration_minutes_stats": {"count": 9, "mean": 29.44, "std_dev": 9.03, "min": 15, "max": 45, "non_numeric_entries": 1}, "dates_parsed": {"start_date_parsed": "success for all rows", "end_date_parsed": "missing or invalid for 1 row"}, "trip_duration_validation": {"matching_rows": 8, "discrepant_rows": 1, "details": "One row had 'thirty' as duration; calculated duration is 30 min"}, "top_start_stations": {"Central Park": 4, "3rd Ave": 3, "5th Ave": 2, "Downtown": 1}, "correlation_duration_distance": 0.98}}
{"purpose": "Analyze household electricity consumption patterns and identify data quality issues.", "raw_table": "Household_ID,Date,Energy_kWh,Region,Customer_Type\nH001,2023-01-01,34.5,North,residential\nh002,01/02/2023,45.2,south,Residential\nH003,2023-01-03,,East,Commercial\nH004,2023/01/04,27.1,West,commercial\nH005,2023-1-05,38.0,North,Residential\nH006,2023-01-06,49.9,,Industrial\nH007,2023-01-07,abc,South,industrial\nH008,2023-01-08,41.5,East,Residential\nH009,2023-01-09,39.3,west,Commercial\nH010,2023-01-10,44.0,North,residential\nH011,,36.7,South,Commercial\nH012,2023-01-12,40.2,North,commercial\nh013,2023-01-13,43.3,East,Residential", "eda_steps": ["Check the total number of rows and columns in the dataset", "Identify and count missing values per column", "Standardize and summarize the Region and Customer_Type categorical columns", "Convert Date column to a uniform date format and check for invalid or missing entries", "Compute descriptive statistics (mean, median, min, max) for Energy_kWh", "Identify and flag non-numeric or invalid Energy_kWh values", "Generate value counts for Region and Customer_Type", "Analyze the distribution of Energy_kWh across different Customer_Type categories"], "eda_results": {"data_shape": {"rows": 14, "columns": 5}, "missing_values": {"Household_ID": 0, "Date": 1, "Energy_kWh": 1, "Region": 1, "Customer_Type": 0}, "invalid_energy_entries": 1, "date_issues": {"non_uniform_formats": true, "missing_dates": 1}, "energy_kwh_stats": {"count": 12, "mean": 40.91, "median": 40.85, "min": 27.1, "max": 49.9}, "region_value_counts": {"North": 4, "South": 3, "East": 3, "West": 2, "": 1}, "customer_type_value_counts": {"Residential": 6, "Commercial": 4, "Industrial": 2, "commercial": 2, "industrial": 1}, "energy_by_customer_type_avg": {"Residential": 40.88, "Commercial": 37.1, "Industrial": 49.9, "commercial": 33.65, "industrial": 0}}}
{"purpose": "Analyze student performance and attendance patterns in a high school semester.", "raw_table": "Student_ID,Name,Grade,Attendance_Percentage,Subject,Test_Score,Test_Date,Passed\n101,alice Smith,10,95.5,Math,88,2023-03-15,Yes\n102,Bob Johnson,10,89.0,english,92,15/03/2023,Yes\n103,carol Lee,11,NA,Science,76,2023/03/16,No\n104,Dave O'Neil,10,82.3,Math,NA,03-17-2023,No\n105,Emily Davis,11,88.7,Math,85,2023-03-15,Yes\n106,frank Moore,12,91.2,English,90,2023-03-15,Yes\n107,Gina King,10,87,science,78,2023.03.16,No\n108,Hank Miller,12,NA,Math,NA,2023-03-15,No\n109,Ivy Chen,11,93.4,english,91,15-03-2023,Yes\n110,jackie Liu,12,85.6,Science,82,2023/03/16,Yes", "eda_steps": ["Check for missing values in each column", "Compute descriptive statistics for Test_Score and Attendance_Percentage", "Generate value counts for Subject and Passed columns", "Standardize and count unique formats in Test_Date", "Analyze correlation between Attendance_Percentage and Test_Score", "Identify students with missing Test_Score or Attendance_Percentage", "Summarize average Test_Score by Grade and Subject"], "eda_results": {"missing_values": {"Student_ID": 0, "Name": 0, "Grade": 0, "Attendance_Percentage": 2, "Subject": 0, "Test_Score": 2, "Test_Date": 0, "Passed": 0}, "summary_stats": {"Test_Score": {"count": 8, "mean": 85.25, "std": 5.32, "min": 76, "25%": 82, "50%": 87, "75%": 90.5, "max": 92}, "Attendance_Percentage": {"count": 8, "mean": 88.53, "std": 4.52, "min": 82.3, "25%": 86.5, "50%": 89, "75%": 92.35, "max": 95.5}}, "value_counts": {"Subject": {"Math": 4, "English": 3, "Science": 3}, "Passed": {"Yes": 7, "No": 3}}, "test_date_formats": {"YYYY-MM-DD": 6, "DD/MM/YYYY": 1, "YYYY/MM/DD": 2, "MM-DD-YYYY": 1, "YYYY.MM.DD": 1, "DD-MM-YYYY": 1}, "correlations": {"Attendance_Percentage_vs_Test_Score": 0.76}, "students_missing_data": {"Missing_Test_Score": ["104", "108"], "Missing_Attendance_Percentage": ["103", "108"]}, "avg_test_score_by_grade_subject": {"10": {"Math": 88, "English": 92, "Science": 78}, "11": {"Math": 85, "English": 91, "Science": 76}, "12": {"English": 90, "Science": 82, "Math": null}}}}
{"purpose": "Explore temperature and precipitation patterns across different climate zones to identify data quality issues and initial trends.", "raw_table": "Date,Location,Climate_Zone,Avg_Temperature_C,Precipitation_mm\n2023-01-01,New York,Temperate,3.4,5\n2023/01/02,los angeles,Mediterranean,15.2,0\n2023-01-03,Chicago,Continental,-5.1,12\n01-04-2023,Houston,Subtropical,12.3,NaN\n2023-01-05,Phoenix,desert,18.5,0\n2023-01-06,Seattle,Temperate,7.1,8.2\n2023-01-07,Miami,subtropical,21.0,14\n2023-01-08,Denver,Continental,0,2.5\n2023-1-09,Boston,Temperate,,6\n2023-01-10,San Francisco,Mediterranean,13.8,1\n2023-01-11,Las Vegas,Desert,17.9,0\n2023-01-12,Atlanta,SubTropical,10.6,NaN\n2023-01-13,Portland,temperate,6.2,7.1\n2023-01-14,New orleans,Subtropical,16.5,13\n", "eda_steps": ["Standardize the 'Date' column to a consistent date format", "Normalize capitalization in 'Location' and 'Climate_Zone' columns", "Check for missing values in all columns", "Compute descriptive statistics for 'Avg_Temperature_C' and 'Precipitation_mm'", "Generate value counts for the 'Climate_Zone' column", "Identify rows with inconsistent or unusual categories in 'Climate_Zone'", "Calculate correlation between 'Avg_Temperature_C' and 'Precipitation_mm'", "Summarize precipitation distribution skewness", "List locations with missing temperature values"], "eda_results": {"missing_values": {"Date": 0, "Location": 0, "Climate_Zone": 0, "Avg_Temperature_C": 1, "Precipitation_mm": 2}, "value_counts_Climate_Zone": {"Temperate": 4, "Mediterranean": 2, "Continental": 2, "Subtropical": 4, "Desert": 2}, "summary_stats_Avg_Temperature_C": {"count": 13, "mean": 11.08, "std": 7.44, "min": -5.1, "25%": 6.2, "50%": 12.3, "75%": 16.5, "max": 21.0}, "summary_stats_Precipitation_mm": {"count": 12, "mean": 6.11, "std": 5.12, "min": 0, "25%": 0, "50%": 5.5, "75%": 8.2, "max": 14}, "inconsistent_climate_zone_entries": ["desert", "subtropical", "SubTropical", "temperate", "New orleans"], "correlation_Avg_Temperature_C_Precipitation_mm": -0.15, "precipitation_skewness": 1.2, "locations_missing_temperature": ["Boston"]}}
{"purpose": "Analyze customer transaction patterns and identify data quality issues in recent banking transactions.", "raw_table": "Transaction_ID,Customer_ID,Transaction_Date,Transaction_Type,Amount,Currency,Branch\nT1001,C123,2023-01-15,Deposit,1000.50,USD,New York\nT1002,c124,15/01/2023,withdrawal,500,usd,Boston\nT1003,C125,2023/01/16,TRANSFER,250.75,EUR,los angeles\nT1004,C126,,deposit,300,USD,Chicago\nT1005,C127,2023-01-18,Withdrawal,,Usd,Miami\nT1006,C128,01-19-2023,deposit,100,usd,\nT1007,C129,2023-01-20,transfer,-150,EUR,New york\nT1008,C130,2023-01-21,Deposit,200,USD,Chicago\nT1009,C131,2023-01-22,WithdrawAL,400.25,USD,boston\nT1010,C132,2023-01-23,Deposit,NaN,USD,Miami\nT1011,C133,2023-01-24,Withdrawal,350,usd,LOS ANGELES\nT1012,C134,2023-01-25,Transfer,100,eur,Chicago\nT1013,C135,2023-01-26,deposit,450,USD,New York\nT1014,C136,01/27/2023,WithdrawAl,600,Usd,MIAMI", "eda_steps": ["Check data types of each column", "Identify and count missing values per column", "Standardize transaction type capitalization and categorize unique types", "Standardize branch names capitalization and identify unique branches", "Convert Transaction_Date to a uniform datetime format", "Compute descriptive statistics for the Amount column", "Calculate value counts for Transaction_Type and Branch columns", "Identify negative or zero transaction amounts and their rows", "Check currency consistency and list unique currency values"], "eda_results": {"data_types": {"Transaction_ID": "string", "Customer_ID": "string", "Transaction_Date": "string/object (mixed formats)", "Transaction_Type": "string", "Amount": "float/object (some missing and invalid)", "Currency": "string", "Branch": "string"}, "missing_values": {"Transaction_ID": 0, "Customer_ID": 0, "Transaction_Date": 1, "Transaction_Type": 0, "Amount": 2, "Currency": 0, "Branch": 1}, "transaction_type_unique": ["Deposit", "Withdrawal", "Transfer"], "branch_unique": ["New York", "Boston", "Los Angeles", "Chicago", "Miami", null], "transaction_date_uniform_format_sample": ["2023-01-15", "2023-01-15", "2023-01-16", null, "2023-01-18", "2023-01-19", "2023-01-20", "2023-01-21", "2023-01-22", "2023-01-23", "2023-01-24", "2023-01-25", "2023-01-26", "2023-01-27"], "amount_descriptive_stats": {"count": 12, "mean": 350.44, "std": 255.22, "min": -150, "25%": 200, "50%": 325.13, "75%": 450, "max": 1000.5}, "transaction_type_counts": {"Deposit": 5, "Withdrawal": 5, "Transfer": 3}, "branch_counts": {"New York": 3, "Boston": 2, "Los Angeles": 2, "Chicago": 3, "Miami": 3, "null": 1}, "invalid_amount_rows": [{"Transaction_ID": "T1007", "Amount": -150}, {"Transaction_ID": "T1005", "Amount": null}, {"Transaction_ID": "T1010", "Amount": null}], "currency_unique_values": ["USD", "usd", "EUR", "eur"]}}
{"purpose": "Analyze customer purchase behavior and identify missing data patterns in retail transactions.", "raw_table": "TransactionID,CustomerID,ProductCategory,PurchaseAmount,PurchaseDate,StoreLocation\n1001,cust_001,Electronics,499.99,2023-01-15,New York\n1002,CUST_002,Clothing,89.50,15/01/2023,Boston\n1003,cust-003,home appliances,120.75,2023/01/16,los angeles\n1004,,Electronics,NaN,2023-01-17,NEW YORK\n1005,cust_005,Toys,35.00,01-18-2023,Chicago\n1006,cust_006,clothing,NaN,2023-01-19,ChicAgo\n1007,cust007,Electronics,250.00,2023-01-20,\n1008,cust_008,TOYS,45.99,2023-01-21,Boston\n1009,cust_009,Home Appliances,130.00,2023-01-22,Los Angeles\n1010,cust_010,Clothing,75,2023.01.23,New york\n1011,cust_011,Electronics,NaN,2023-01-24,New York\n1012,cust_012,Toys,40.00,2023-01-25,Boston", "eda_steps": ["Check for missing values in each column", "Standardize and normalize the 'StoreLocation' and 'ProductCategory' categories", "Convert 'PurchaseDate' to a consistent date format", "Compute descriptive statistics for 'PurchaseAmount'", "Generate value counts for 'ProductCategory' and 'StoreLocation'", "Identify transactions with missing 'PurchaseAmount' and analyze their distribution", "Calculate average purchase amount by product category", "Analyze the frequency of purchases by customer"], "eda_results": {"missing_values": {"TransactionID": 0, "CustomerID": 1, "ProductCategory": 0, "PurchaseAmount": 3, "PurchaseDate": 0, "StoreLocation": 1}, "standardized_categories": {"StoreLocation": ["New York", "Boston", "Los Angeles", "Chicago"], "ProductCategory": ["Electronics", "Clothing", "Home Appliances", "Toys"]}, "purchase_date_consistency": "All dates converted to YYYY-MM-DD format", "summary_stats_purchase_amount": {"count": 11, "mean": 141.67, "std": 146.99, "min": 35.0, "25%": 45.0, "50%": 89.5, "75%": 130.0, "max": 499.99}, "value_counts_product_category": {"Electronics": 4, "Clothing": 3, "Toys": 3, "Home Appliances": 2}, "value_counts_store_location": {"New York": 4, "Boston": 3, "Los Angeles": 2, "Chicago": 2}, "missing_purchase_amount_distribution": {"Electronics": 2, "Clothing": 1, "Toys": 0, "Home Appliances": 0}, "average_purchase_amount_by_category": {"Electronics": 416.66, "Clothing": 82.25, "Toys": 40.33, "Home Appliances": 125.38}, "purchase_frequency_by_customer": {"cust_001": 1, "CUST_002": 1, "cust-003": 1, "cust_005": 1, "cust_006": 1, "cust007": 1, "cust_008": 1, "cust_009": 1, "cust_010": 1, "cust_011": 1, "cust_012": 1, "missing": 1}}}
{"purpose": "Examine customer purchase patterns and identify missing data issues in retail transactions.", "raw_table": "TransactionID,CustomerID,ProductCategory,PurchaseDate,Quantity,PricePerUnit,PaymentMethod\n1001,cust01,Electronics,2023/01/15,2,299.99,Credit Card\n1002,CUST02,apparel,01-17-2023,1,49.5,Paypal\n1003,cust03,Home & kitchen,,3,15.99,credit card\n1004,cust04,appliances,2023-01-20,1,399.00,CASH\n1005,cust05,Electronics,2023/01/21, ,199.99,Credit card\n1006,CUST06,Apparel,01/22/2023,2,49.5,\n1007,cust07,Toys,2023-01-23,1,14.99,Debit Card\n1008,cust08,home & Kitchen,2023-01-24,1,17.99,Credit Card\n1009,cust09,Electronics,2023-01-25,1,299.99,Credit card\n1010,cust10,Apparel,2023-01-26,2,49.5,PayPal\n1011,cust11,Toys,01/27/2023,1,14.99,debit card\n1012,cust12,Appliances,2023/01/28,1,399.00,Cash\n1013,CUST13,Electronics,2023-01-29,1,,Credit Card\n1014,cust14,home & kitchen,2023/01/30,2,15.99,credit card", "eda_steps": ["Check for missing values in all columns", "Standardize capitalization in categorical columns ProductCategory and PaymentMethod", "Convert PurchaseDate to a consistent date format", "Compute descriptive statistics for numeric columns Quantity and PricePerUnit", "Generate value counts for ProductCategory and PaymentMethod", "Identify transactions with missing or zero Quantity or PricePerUnit", "Calculate total revenue per transaction as Quantity multiplied by PricePerUnit", "Find the top 3 most frequent ProductCategory values"], "eda_results": {"missing_values": {"TransactionID": 0, "CustomerID": 0, "ProductCategory": 0, "PurchaseDate": 1, "Quantity": 2, "PricePerUnit": 1, "PaymentMethod": 1}, "standardized_categories": {"ProductCategory": ["Electronics", "Apparel", "Home & Kitchen", "Appliances", "Toys"], "PaymentMethod": ["Credit Card", "Paypal", "Cash", "Debit Card"]}, "purchase_date_format": "All dates converted to yyyy-mm-dd, missing dates remain null", "descriptive_statistics": {"Quantity": {"count": 13, "mean": 1.46, "min": 1, "max": 3, "median": 1}, "PricePerUnit": {"count": 13, "mean": 139.7, "min": 14.99, "max": 399.0, "median": 49.5}}, "value_counts": {"ProductCategory": {"Electronics": 4, "Apparel": 3, "Home & Kitchen": 3, "Appliances": 2, "Toys": 2}, "PaymentMethod": {"Credit Card": 6, "Paypal": 2, "Cash": 2, "Debit Card": 2, "missing": 1}}, "transactions_with_missing_or_zero": {"Quantity": [1005, 1006], "PricePerUnit": [1013]}, "total_revenue_per_transaction": {"1001": 599.98, "1002": 49.5, "1003": 47.97, "1004": 399.0, "1007": 14.99, "1008": 17.99, "1009": 299.99, "1010": 99.0, "1011": 14.99, "1012": 399.0, "1014": 31.98}, "top_3_product_categories": ["Electronics", "Apparel", "Home & Kitchen"]}}
{"purpose": "Analyze daily ridership and trip duration patterns for a city bike-share system to identify usage trends and data quality issues.", "raw_table": "Trip_ID,Start_Date,Start_Time,End_Time,User_Type,Trip_Duration,Start_Station,End_Station\n1,2023-07-01,08:15 AM,08:45 AM,Subscriber,30,Central Park,central park\n2,07/01/2023,09:00,09:25,Customer,25,Main St,Main st\n3,2023/07/01,10:30 AM,11:00 AM,Subscriber,thirty,5th Ave,5th ave\n4,,11:15,11:45,subscriber,30,Central Park,Main St\n5,2023-07-02,07:50 AM,,Customer,NA,main st,Central Park\n6,2023-07-02,09:20 AM,09:50 AM,Subscriber,30,5th ave,5th ave\n7,2023-07-02,10:00 AM,10:30 AM,Subscriber,30,,Main St\n8,2023-07-02,11:15 AM,11:45 AM,customer,30,Central Park,5th ave\n9,2023-07-03,08:00 AM,08:30 AM,Subscriber,30,Main St,Central Park\n10,2023-07-03,09:30 AM,10:00 AM,Customer,30,5th Ave,main st\n", "eda_steps": ["Check for missing values in all columns", "Standardize capitalization in categorical columns (User_Type, Start_Station, End_Station)", "Convert Start_Date and Start_Time to datetime objects", "Convert Trip_Duration to numeric and identify non-numeric entries", "Compute descriptive statistics for Trip_Duration", "Generate value counts for User_Type", "Identify most frequent Start_Station and End_Station", "Calculate average trip duration by User_Type"], "eda_results": {"missing_values": {"Trip_ID": 0, "Start_Date": 1, "Start_Time": 0, "End_Time": 1, "User_Type": 0, "Trip_Duration": 1, "Start_Station": 1, "End_Station": 0}, "standardized_categories": {"User_Type": {"subscriber": 6, "customer": 4}, "Start_Station": {"central park": 4, "main st": 4, "5th ave": 4}, "End_Station": {"central park": 3, "main st": 4, "5th ave": 3}}, "datetime_conversion_issues": {"Start_Date": 1, "Start_Time": 0}, "trip_duration_conversion": {"non_numeric_entries": ["thirty", "NA"], "converted_to_numeric": 8}, "trip_duration_stats": {"count": 8, "mean": 29.38, "min": 25, "max": 30, "std_dev": 1.79}, "user_type_counts": {"subscriber": 6, "customer": 4}, "top_start_stations": {"central park": 4, "main st": 4, "5th ave": 4}, "top_end_stations": {"main st": 4, "central park": 3, "5th ave": 3}, "average_trip_duration_by_user_type": {"subscriber": 30, "customer": 27.5}}}
{"purpose": "Analyze customer purchase behavior and product category trends in a retail store.", "raw_table": "OrderID,CustomerID,ProductCategory,Quantity,Price,OrderDate,StoreLocation\n1001,C001,Electronics,2,299.99,2023-01-15,New York\n1002,C002,Home Appliances,,199.99,15/01/2023,Boston\n1003,c003,electronics,1,399.50,2023/01/16,LOS ANGELES\n1004,C004,Fashion,3,49.99,2023-01-17,Chicago\n1005,C005,fashion,2,,2023-01-18,Chicago\n1006,C006,Outdoor,1,89.95,2023-1-19,San Francisco\n1007,C007,home appliances,1,NaN,01-20-2023,Boston\n1008,C008,Toys,5,19.99,2023-01-21,New york\n1009,C009,Electronics,NaN,299.99,2023-01-22,Los Angeles\n1010,C010,,2,15.99,2023-01-23,Miami\n1011,C011,TOYS,3,25.50,2023-01-24,Miami\n1012,C012,Outdoor,4,85.00,2023-01-25,san francisco\n1013,C013,Electronics,1,NaN,2023-01-26,NEW YORK\n1014,C014,Fashion,NaN,59.99,2023-01-27,Chicago", "eda_steps": ["Check for missing values in each column", "Standardize capitalization in ProductCategory and StoreLocation columns", "Convert OrderDate to a consistent date format", "Calculate descriptive statistics for Quantity and Price columns", "Generate value counts for ProductCategory", "Identify top 3 StoreLocations by number of orders", "Calculate total sales amount per ProductCategory (Quantity * Price)", "Summarize missing data impact on Quantity and Price", "Check correlation between Quantity and Price"], "eda_results": {"missing_values": {"OrderID": 0, "CustomerID": 0, "ProductCategory": 1, "Quantity": 3, "Price": 4, "OrderDate": 0, "StoreLocation": 0}, "standardized_categories": {"ProductCategory": ["Electronics", "Home Appliances", "Electronics", "Fashion", "Fashion", "Outdoor", "Home Appliances", "Toys", "Electronics", null, "Toys", "Outdoor", "Electronics", "Fashion"], "StoreLocation": ["New York", "Boston", "Los Angeles", "Chicago", "Chicago", "San Francisco", "Boston", "New York", "Los Angeles", "Miami", "Miami", "San Francisco", "New York", "Chicago"]}, "date_conversion": {"all_dates_parsed": true, "earliest_order": "2023-01-15", "latest_order": "2023-01-27"}, "descriptive_stats": {"Quantity": {"count": 11, "mean": 2.36, "std": 1.42, "min": 1, "25%": 1.0, "50%": 2.0, "75%": 3.0, "max": 5}, "Price": {"count": 10, "mean": 136.49, "std": 135.99, "min": 15.99, "25%": 49.99, "50%": 89.95, "75%": 299.99, "max": 399.5}}, "value_counts_product_category": {"Electronics": 4, "Fashion": 3, "Home Appliances": 2, "Outdoor": 2, "Toys": 2, "null": 1}, "top_store_locations": {"Chicago": 3, "New York": 3, "Boston": 2}, "total_sales_per_category": {"Electronics": 1799.47, "Fashion": 299.96, "Home Appliances": 199.99, "Outdoor": 434.8, "Toys": 180.47, "null": 31.98}, "missing_data_impact": {"Quantity_missing_rows": [1002, 1009, 1014], "Price_missing_rows": [1005, 1007, 1013, 1009]}, "correlations": {"Quantity_Price_correlation": 0.24}}}
{"purpose": "Analyze recent transaction patterns and customer demographics in retail banking.", "raw_table": "TransactionID,CustomerID,TransactionDate,TransactionType,Amount,AccountType,Branch,Status\n1001,C123,2024/01/05,Deposit,5000,Savings,New york,completed\n1002,c124,01-15-2024,withdrawal,3000,Current,Boston,Completed\n1003,C125,2024-02-10,Transfer,,Savings,los angeles,completed\n1004,C126,2/14/2024,DEPOSIT,1500,Current,Chicago,Pending\n1005,C127,2024-03-01,withdrawal,-500,Savings,New York,completed\n1006,C128,2024/03/15,Withdrawal,2000,Current,Boston,Failed\n1007,C129,,Deposit,2500,Savings,Chicago,completed\n1008,C130,2024-03-20,Transfer,1000,Checking,Los Angeles,completed\n1009,C131,03-25-2024,deposit,3500,Current,boston,completed\n1010,C132,2024/04/01,Withdrawal,NaN,Savings,New York,Completed\n1011,C133,2024-04-05,Withdrawal,1200,Current,Chicago,completed\n1012,C134,04/07/2024,deposit,4000,Savings,los angeles,completed", "eda_steps": ["Check missing value percentages for each column", "Standardize capitalization in TransactionType, AccountType, Branch, and Status columns", "Calculate descriptive statistics for Amount including mean, median, min, and max", "Generate value counts for TransactionType and AccountType", "Identify rows with negative or zero transaction Amounts", "Count unique customers involved in transactions", "Summarize transactions by Branch and Status", "Examine date format inconsistencies and count missing TransactionDate entries", "Calculate correlation between Amount and TransactionType if numeric encoding is applied"], "eda_results": {"missing_values": {"TransactionID": 0, "CustomerID": 0, "TransactionDate": 1, "TransactionType": 0, "Amount": 2, "AccountType": 0, "Branch": 0, "Status": 0}, "standardized_categories": {"TransactionType": ["Deposit", "Withdrawal", "Transfer"], "AccountType": ["Savings", "Current", "Checking"], "Branch": ["New York", "Boston", "Los Angeles", "Chicago"], "Status": ["Completed", "Pending", "Failed"]}, "summary_stats_amount": {"count": 13, "mean": 2215.38, "median": 2000, "min": -500, "max": 5000}, "value_counts_transaction_type": {"Deposit": 5, "Withdrawal": 6, "Transfer": 2}, "value_counts_account_type": {"Savings": 6, "Current": 5, "Checking": 1}, "negative_or_zero_amounts": 1, "unique_customers_count": 12, "transactions_by_branch_and_status": {"New York": {"Completed": 3, "Pending": 0, "Failed": 0}, "Boston": {"Completed": 2, "Pending": 0, "Failed": 1}, "Los Angeles": {"Completed": 3, "Pending": 0, "Failed": 0}, "Chicago": {"Completed": 2, "Pending": 1, "Failed": 0}}, "date_issues": {"inconsistent_formats": true, "missing_dates": 1}, "correlation_amount_transactiontype_encoded": {"correlation_coefficient": -0.2}}}
{"purpose": "Analyze operational data of a city bike-sharing system to identify usage patterns and data quality issues.", "raw_table": "Trip_ID,Start_Date,End_Date,User_Type,Start_Station,End_Station,Duration_Minutes,Distance_km\n101,2024-01-15 08:23,2024-01-15 08:45,Subscriber,Central Park,central park,22,5.4\n102,1/16/2024 09:00 AM,1/16/2024 09:30 AM,casual,Riverside,river side,30,7.1\n103,2024-01-17 18:10:00,2024-01-17 18:40:00,Subscriber,,Downtown,30,6.0\n104,2024-01-18 07:00,2024-01-18 07:20,Casual,Downtown,Downtown,20,NaN\n105,2024-01-18 20:05,2024-01-18 20:25,Subscriber,Central Park,Central park,20,5.3\n106,2024-01-19 14:00,2024-01-19 14:25,,Central Park,Riverside,25,6.5\n107,01-20-2024 13:30,01-20-2024 13:55,Subscriber,Riverside,Central Park,25,6.7\n108,2024-01-21 08:15:00,2024-01-21 08:40:00,casual,Riverside,Downtown,25,7.0\n109,2024-01-21 09:00,2024-01-21 09:22,Subscriber,central park,Riverside,22,5.5\n110,2024-01-22 07:45,2024-01-22 08:10,Subscriber,Downtown,Central Park,25,6.8\n111,2024-01-22 09:00:00,,Casual,Riverside,Downtown,15,4.2\n112,2024-01-23 10:00,2024-01-23 10:20,Subscriber,Riverside,,20,5.9\n113,2024/01/24 11:00,2024/01/24 11:30,Casual,Downtown,Central Park,30,7.2", "eda_steps": ["Check for missing values in each column", "Standardize capitalization in categorical columns: User_Type, Start_Station, End_Station", "Parse and unify date formats for Start_Date and End_Date columns", "Calculate descriptive statistics for Duration_Minutes and Distance_km", "Generate value counts for User_Type", "Identify rows with inconsistent or missing Start_Station or End_Station", "Check correlation between Duration_Minutes and Distance_km", "Identify top 3 most frequent Start_Stations"], "eda_results": {"missing_values": {"Trip_ID": 0, "Start_Date": 0, "End_Date": 1, "User_Type": 1, "Start_Station": 1, "End_Station": 2, "Duration_Minutes": 0, "Distance_km": 1}, "standardized_categories": {"User_Type": {"Subscriber": 7, "Casual": 5}, "Start_Station": {"Central Park": 5, "Riverside": 5, "Downtown": 5}, "End_Station": {"Central Park": 4, "Riverside": 4, "Downtown": 4, "": 2}}, "date_formats_unified": true, "summary_stats": {"Duration_Minutes": {"count": 14, "mean": 24.5, "std": 4.24, "min": 15, "25%": 20, "50%": 25, "75%": 25, "max": 30}, "Distance_km": {"count": 13, "mean": 6.24, "std": 1.07, "min": 4.2, "25%": 5.3, "50%": 6.5, "75%": 7.1, "max": 7.2}}, "value_counts_User_Type": {"Subscriber": 7, "Casual": 5, "Missing": 1}, "inconsistent_start_end_station_rows": [3, 5, 11], "correlations": {"Duration_Minutes_vs_Distance_km": 0.93}, "top_start_stations": {"Central Park": 5, "Riverside": 5, "Downtown": 5}}}
{"purpose": "Explore crop yield patterns and assess data quality of farm records across various soil types and planting dates.", "raw_table": "Farm_ID,Crop,Planting_Date,Soil_Type,Yield_kg,Water_Used_L,Harvest_Grade\n101,Wheat,2023-03-15,Loamy,3500,1200,Good\n102,Corn,15/03/2023,Clay,4200,1400,excellent\n103,Rice,2023/03/16,SANDY,,1300,Fair\n104,Wheat,03-17-2023,loamy,3600,1150,good\n105,corn,2023-3-18,clay,4300,,Excellent\n106,Rice,2023-03-18,Sandy,3900,1250,FAIR\n107,Wheat,,Loamy,3550,1180,Good\n108,Barley,2023-03-19,Silt,2800,1100,poor\n109,barley,2023-03-19,silt,2750,1080,Poor\n110,Corn,2023/03/20,Clay,4450,1450,EXCELLENT\n111,Rice,2023-03-21,SANDY,3950,1280,Fair\n112,Wheat,2023-03-22,LOAMY,3550,1190,good", "eda_steps": ["Check the total number of records and columns", "Identify missing values in each column and their percentages", "Standardize capitalization in Crop and Soil_Type columns", "Parse and unify Planting_Date formats into ISO standard YYYY-MM-DD", "Compute descriptive statistics for Yield_kg and Water_Used_L", "Generate value counts for the Harvest_Grade column", "Examine correlation between Yield_kg and Water_Used_L", "Identify top 2 crops by average Yield_kg", "Summarize distribution skewness for Yield_kg"], "eda_results": {"total_records": 12, "total_columns": 7, "missing_values": {"Planting_Date": "8.33%", "Yield_kg": "8.33%", "Water_Used_L": "8.33%"}, "standardized_categories": {"Crop": ["Wheat", "Corn", "Rice", "Barley"], "Soil_Type": ["Loamy", "Clay", "Sandy", "Silt"], "Harvest_Grade": ["Good", "Excellent", "Fair", "Poor"]}, "date_parsing_success_rate": "100%", "yield_stats": {"count": 11, "mean": 3732, "std": 520, "min": 2750, "25%": 3500, "50%": 3600, "75%": 4200, "max": 4450, "skewness": 0.45}, "water_used_stats": {"count": 11, "mean": 1255, "std": 110, "min": 1080, "25%": 1150, "50%": 1200, "75%": 1400, "max": 1450}, "harvest_grade_counts": {"Good": 4, "Excellent": 3, "Fair": 3, "Poor": 2}, "yield_water_correlation": 0.89, "top_crops_by_yield": {"Corn": 4312, "Rice": 3916}}}
{"purpose": "Analyze machine downtime causes and durations to identify patterns affecting production efficiency.", "raw_table": "MachineID,Date,DowntimeMinutes,Cause,Shift,Operator\nM001,2023-01-15,120,Overheat,Day,John\nm002,15/01/2023,45,,Night,alice\nM003,2023-01-16,NA,Mechanical Failure,DAY,Bob\nM004,01-17-2023,30,Power Outage,Night,Charlie\nM001,2023/01/18,60,overheat,Day,john\nM005,2023-01-18,90,Mechanical failure,day,ALICE\nM002,2023-01-19,15,Operator Error,Night,alice\nm003,2023-01-20,NA,Mechanical failure,,bob\nM004,2023-01-20,50,,night,Charlie\nM005,2023-01-21,70,OverHeat,Day,ALICE\nM001,2023-01-22,,Power outage,Day,John\nM002,,40,Operator Error,Night,alice", "eda_steps": ["Check for and report missing values in each column", "Standardize capitalization for categorical columns: Cause, Shift, Operator", "Parse and unify Date formats to a single consistent format", "Compute descriptive statistics for DowntimeMinutes, ignoring missing values", "Generate value counts for Cause and Shift columns", "Identify the number of unique operators and their frequency", "Analyze correlation between Shift and DowntimeMinutes", "Summarize missing value percentages per column", "Determine top 3 most common downtime causes"], "eda_results": {"missing_values": {"MachineID": 0, "Date": 1, "DowntimeMinutes": 2, "Cause": 2, "Shift": 1, "Operator": 0}, "standardized_columns_sample": {"Cause": ["Overheat", "Mechanical Failure", "Power Outage", "Operator Error", null], "Shift": ["Day", "Night", null], "Operator": ["John", "Alice", "Bob", "Charlie"]}, "date_formats_unified_sample": ["2023-01-15", "2023-01-15", "2023-01-16", "2023-01-17", "2023-01-18", "2023-01-19", "2023-01-20", "2023-01-21", "2023-01-22"], "summary_stats": {"DowntimeMinutes": {"count": 11, "mean": 57.27, "std": 36.62, "min": 15, "25%": 40, "50%": 50, "75%": 70, "max": 120}}, "value_counts": {"Cause": {"Overheat": 3, "Mechanical Failure": 3, "Power Outage": 2, "Operator Error": 2, "Missing": 2}, "Shift": {"Day": 6, "Night": 6, "Missing": 1}}, "unique_operators_count": 4, "operator_frequency": {"John": 3, "Alice": 4, "Bob": 2, "Charlie": 2}, "correlation_shift_downtime": {"Day": 66.67, "Night": 44.17}, "missing_value_percentages": {"MachineID": 0, "Date": 8.33, "DowntimeMinutes": 16.67, "Cause": 16.67, "Shift": 8.33, "Operator": 0}, "top_3_downtime_causes": ["Overheat", "Mechanical Failure", "Power Outage"]}}
{"purpose": "Analyze patterns in government employee work hours and department distribution.", "raw_table": "Employee_ID,Department,Work_Hours,Start_Date,Status\nE001,Finance,40,2023-01-10,Active\nE002,healthcare,38,1/15/2023,active\nE003,Education,,2023-02-01,Inactive\nE004,FINANCE,45,2023-01-20,Active\nE005,Transportation, Forty,2023-02-15,Active\nE006,Healthcare,40,2023/03/01,Active\nE007,education,39,03-05-2023,active\nE008,Transportation,42,,inactive\nE009,Finance,40,2023-01-25,Active\nE010,HEALTHCARE,41,2023-02-28,Active\nE011,Education,38,2023-03-10,Inactive\nE012,Transportation,43,2023-02-20,Active\nE013,Finance,,2023-01-30,Active\nE014,HealthCare,40,2023/03/05,active", "eda_steps": ["Check missing value percentages for each column", "Standardize the 'Department' and 'Status' columns to consistent capitalization", "Convert 'Work_Hours' to numeric, handling non-numeric values and missing data", "Parse and standardize 'Start_Date' to ISO format (YYYY-MM-DD)", "Compute descriptive statistics for 'Work_Hours'", "Generate value counts for 'Department' and 'Status' columns", "Identify employees missing 'Work_Hours' or 'Start_Date'", "Summarize the distribution of employees by department and status"], "eda_results": {"missing_values": {"Employee_ID": 0, "Department": 0, "Work_Hours": 2, "Start_Date": 1, "Status": 0}, "standardized_departments": {"Finance": 4, "Healthcare": 4, "Education": 3, "Transportation": 3}, "standardized_status": {"Active": 10, "Inactive": 3}, "work_hours_conversion_issues": {"non_numeric_entries": ["Forty"], "missing_entries": 2}, "work_hours_statistics": {"count": 12, "mean": 40.75, "median": 40, "min": 38, "max": 45, "std_dev": 2.17}, "start_date_parsing_issues": {"missing_dates": 1}, "employees_missing_work_hours": ["E003", "E005", "E013"], "employees_missing_start_date": ["E008"], "department_status_distribution": {"Finance": {"Active": 4, "Inactive": 0}, "Healthcare": {"Active": 4, "Inactive": 0}, "Education": {"Active": 0, "Inactive": 3}, "Transportation": {"Active": 3, "Inactive": 0}}}}
{"purpose": "Analyze customer purchase behavior and identify patterns in product categories and sales over time.", "raw_table": "OrderID,CustomerID,OrderDate,ProductCategory,Quantity,UnitPrice,PaymentMethod\n1001,C001,2023/01/15,Electronics,2,299.99,Credit Card\n1002,c002,15-01-2023,Home & Garden,1,89.5,credit card\n1003,C003,2023-01-17,electronics,3,not available,PayPal\n1004,C004,,Clothing,2,45.0,CASH\n1005,C005,2023/01/20,Clothing,1,45,Cash\n1006,C006,2023-01-22,Toys,5,15.99,Paypal\n1007,C007,01-23-2023,Toys,2,15.99,Credit card\n1008,C008,2023/01/24,home & garden,,89.50,Debit Card\n1009,C009,2023-01-25,Electronics,1,299.99,CREDIT CARD\n1010,C010,2023/1/26,Books,4,12.99,Debit Card\n1011,C011,2023-01-27,books,3,12.99,DEBIT card\n1012,C012,2023/01/28,Clothing,2,,Credit Card\n1013,C013,2023/1/29,Toys,3,15.99,cash\n1014,C014,2023/01/30,Home & Garden,1,89.50,PayPal", "eda_steps": ["Check for missing values in each column", "Standardize capitalization in ProductCategory and PaymentMethod columns", "Convert OrderDate to a consistent date format", "Identify and handle non-numeric or missing values in Quantity and UnitPrice columns", "Calculate total sales (Quantity * UnitPrice) for each order", "Generate descriptive statistics for Quantity, UnitPrice, and total sales", "Calculate value counts for ProductCategory and PaymentMethod", "Identify top 2 product categories by total sales", "Check correlation between Quantity and UnitPrice"], "eda_results": {"missing_values": {"OrderID": 0, "CustomerID": 0, "OrderDate": 1, "ProductCategory": 0, "Quantity": 1, "UnitPrice": 2, "PaymentMethod": 0}, "standardized_categories": {"ProductCategory": ["Electronics", "Home & Garden", "Electronics", "Clothing", "Clothing", "Toys", "Toys", "Home & Garden", "Electronics", "Books", "Books", "Clothing", "Toys", "Home & Garden"], "PaymentMethod": ["Credit Card", "Credit Card", "PayPal", "Cash", "Cash", "PayPal", "Credit Card", "Debit Card", "Credit Card", "Debit Card", "Debit Card", "Credit Card", "Cash", "PayPal"]}, "order_dates_parsed": ["2023-01-15", "2023-01-15", "2023-01-17", null, "2023-01-20", "2023-01-22", "2023-01-23", "2023-01-24", "2023-01-25", "2023-01-26", "2023-01-27", "2023-01-28", "2023-01-29", "2023-01-30"], "cleaned_numeric": {"Quantity": [2, 1, 3, 2, 1, 5, 2, null, 1, 4, 3, 2, 3, 1], "UnitPrice": [299.99, 89.5, null, 45.0, 45.0, 15.99, 15.99, 89.5, 299.99, 12.99, 12.99, null, 15.99, 89.5]}, "total_sales": [599.98, 89.5, null, 90.0, 45.0, 79.95, 31.98, null, 299.99, 51.96, 38.97, null, 47.97, 89.5], "descriptive_statistics": {"Quantity": {"count": 13, "mean": 2.54, "median": 2, "min": 1, "max": 5}, "UnitPrice": {"count": 12, "mean": 83.35, "median": 45.0, "min": 12.99, "max": 299.99}, "TotalSales": {"count": 11, "mean": 86.5, "median": 45.0, "min": 31.98, "max": 599.98}}, "value_counts": {"ProductCategory": {"Electronics": 3, "Home & Garden": 3, "Clothing": 3, "Toys": 4, "Books": 2}, "PaymentMethod": {"Credit Card": 5, "Cash": 3, "PayPal": 3, "Debit Card": 3}}, "top_categories_by_sales": {"Electronics": 899.97, "Home & Garden": 268.5}, "correlation_quantity_unitprice": -0.21}}
{"purpose": "Analyze customer call patterns and service usage to identify potential churn indicators.", "raw_table": "CustomerID,CallDate,CallDuration,PlanType,Region,DataUsage,Churn\nC001,2023/01/15,300,Premium,North,2.5,No\nc002,15-01-2023,180,standard,South,1.2,Yes\nC003,2023-01-16,,Basic,East,,No\nC004,01/16/2023,240,basic,west,0.9,No\nC005,2023.01.17,360,Premium,South,3.1, yes\nC006,2023-01-18,NaN,STANDARD,North,1.7,No\nC007,18/01/2023,150,Premium,East,2.2,No\nc008,2023-01-19,200,standard,,1.5,No\nC009,2023-01-20,400,Basic,North,,Yes\nC010,20-01-2023,100,Premium,South,2.8,no\nC011,2023/01/21,220,Standard,East,1.0,No", "eda_steps": ["Standardize date formats in CallDate column", "Convert all PlanType values to lowercase", "Check and count missing values per column", "Compute descriptive statistics for CallDuration and DataUsage", "Generate value counts for PlanType and Region", "Analyze Churn distribution", "Identify correlation between CallDuration and DataUsage", "Check for inconsistencies in Churn values", "Summarize average CallDuration and DataUsage by PlanType"], "eda_results": {"missing_values": {"CallDuration": 1, "DataUsage": 3, "Region": 1}, "value_counts": {"PlanType": {"basic": 3, "premium": 4, "standard": 4}, "Region": {"north": 3, "south": 3, "east": 3, "west": 1, "missing": 1}, "Churn": {"no": 7, "yes": 3}}, "summary_stats": {"CallDuration": {"count": 11, "mean": 237.27, "std": 91.05, "min": 100, "max": 400}, "DataUsage": {"count": 9, "mean": 1.79, "std": 0.82, "min": 0.9, "max": 3.1}}, "correlations": {"CallDuration_DataUsage": 0.76}, "churn_inconsistencies": {"original_values": ["No", "Yes", " yes", "no"], "standardized": ["no", "yes"]}, "average_by_plan_type": {"basic": {"CallDuration": 280, "DataUsage": 0.95}, "premium": {"CallDuration": 252.5, "DataUsage": 2.65}, "standard": {"CallDuration": 175, "DataUsage": 1.35}}}}
{"purpose": "Analyze customer churn patterns and usage metrics to identify key factors influencing churn in a telecom dataset.", "raw_table": "CustomerID,SignupDate,PlanType,MonthlyCharges,DataUsageGB,Churn,SupportCalls\n001,2022-01-15,Premium,79.99,45.7,Yes,3\n002,15/02/2022,standard,59.5,30.2,no,1\n003,2022/03/10,Basic,25.00,,No,0\n004,04-04-2022,Premium,85.00,50.3,YES,5\n005,,Standard,60,28.5,No,2\n006,2022-06-01,basic,27.5,12.0,No,na\n007,2022-07-20,PREMIUM,82,47.0,yes,4\n008,2022-08-15,basic,25,,No,0\n009,2022-09-10,Standard,62,33.1,No,1\n010,2022-10-05,,70,40.0,No,3\n011,2022-11-11,Premium,77.5,48.8,yes,\n012,11-12-2022,Basic,26,15.5,No,1", "eda_steps": ["Check and standardize date formats in SignupDate column", "Identify and count missing values in each column", "Standardize categorical values in PlanType and Churn columns to consistent capitalization", "Compute descriptive statistics for MonthlyCharges and DataUsageGB", "Generate value counts for PlanType and Churn columns", "Calculate correlation between MonthlyCharges, DataUsageGB, and SupportCalls", "Identify average number of SupportCalls for churned vs non-churned customers", "Summarize distribution skewness for numeric columns"], "eda_results": {"missing_values": {"CustomerID": 0, "SignupDate": 1, "PlanType": 1, "MonthlyCharges": 0, "DataUsageGB": 3, "Churn": 0, "SupportCalls": 2}, "standardized_plantype_counts": {"Basic": 4, "Standard": 3, "Premium": 5}, "standardized_churn_counts": {"Yes": 5, "No": 7}, "date_format_issues": 3, "summary_stats": {"MonthlyCharges": {"count": 12, "mean": 58.96, "std": 22.1, "min": 25.0, "max": 85.0}, "DataUsageGB": {"count": 9, "mean": 34.45, "std": 13.82, "min": 12.0, "max": 50.3}, "SupportCalls": {"count": 10, "mean": 2.0, "std": 1.72, "min": 0, "max": 5}}, "correlations": {"MonthlyCharges_vs_DataUsageGB": 0.98, "MonthlyCharges_vs_SupportCalls": 0.65, "DataUsageGB_vs_SupportCalls": 0.6}, "avg_support_calls_by_churn": {"Yes": 3.8, "No": 0.8}, "skewness": {"MonthlyCharges": 0.05, "DataUsageGB": -0.14, "SupportCalls": 0.88}}}
{"purpose": "Analyze monthly sales performance and product popularity in a retail store.", "raw_table": "OrderID,ProductName,Category,Quantity,Price,OrderDate,CustomerSegment\n1001,Wireless mouse,Electronics,2,25.99,2023/01/05,Consumer\n1002,USB cable,electronics,3,5.5,01-10-2023,corporate\n1003,Office Chair,Furniture,1,89.99,2023-01-12,Home Office\n1004,Laptop Stand,Furniture,,45.0,2023/01/15,Consumer\n1005,Desk Lamp,Furniture,1,27.5,15-01-2023,consumer\n1006,Wireless Mouse,Electronics,1,25.99,2023/02/01,Consumer\n1007,Desk Organizer,office supplies,2,12.0,2023-02-04,Corporate\n1008,Stapler,Office Supplies,5,7.25,2023/02/08,Consumer\n1009,Notebook,Office supplies,10,2.5,2023/02/10,consumer\n1010,Pen,Office Supplies,25,1.0,2023-02-12,Consumer\n1011,Monitor,Electronics,1,199.99,2023-02-15,Corporate\n1012,Lamp Desk,Furniture,1,27.50,2023/02/18,consumer\n1013,Wireless mouse,electronics,1,25.99,,Consumer\n1014,Desk Lamp,Furniture,2,27.5,2023-02-20,Consumer", "eda_steps": ["Check missing values in each column", "Standardize capitalization in 'Category' and 'CustomerSegment' columns", "Convert 'OrderDate' to a consistent date format", "Calculate total sales amount per order as Quantity multiplied by Price", "Compute descriptive statistics for 'Quantity' and 'Price'", "Generate value counts for 'Category' and 'CustomerSegment'", "Identify top-3 best-selling products by total quantity sold", "Summarize number of orders per month", "Check for any orders with missing 'Quantity' or 'OrderDate'"], "eda_results": {"missing_values": {"OrderID": 0, "ProductName": 0, "Category": 0, "Quantity": 1, "Price": 0, "OrderDate": 1, "CustomerSegment": 0}, "standardized_categories": {"electronics": 5, "furniture": 5, "office supplies": 5}, "standardized_customer_segments": {"Consumer": 9, "Corporate": 3, "Home Office": 1}, "order_date_format": "All dates converted to YYYY-MM-DD format; one missing date remains missing", "total_sales_per_order": {"1001": 51.98, "1002": 16.5, "1003": 89.99, "1004": null, "1005": 27.5, "1006": 25.99, "1007": 24.0, "1008": 36.25, "1009": 25.0, "1010": 25.0, "1011": 199.99, "1012": 27.5, "1013": 25.99, "1014": 55.0}, "descriptive_stats": {"Quantity": {"count": 14, "mean": 5.07, "std": 7.38, "min": 1, "25%": 1, "50%": 2, "75%": 5, "max": 25}, "Price": {"count": 15, "mean": 41.07, "std": 58.62, "min": 1.0, "25%": 7.25, "50%": 25.99, "75%": 27.5, "max": 199.99}}, "value_counts": {"Category": {"Electronics": 5, "Furniture": 5, "Office Supplies": 5}, "CustomerSegment": {"Consumer": 9, "Corporate": 3, "Home Office": 1}}, "top_products_by_quantity": {"Pen": 25, "Notebook": 10, "Stapler": 5}, "orders_per_month": {"2023-01": 5, "2023-02": 9}, "orders_with_missing_values": {"Quantity_missing": ["1004"], "OrderDate_missing": ["1013"]}}}
{"purpose": "Analyze customer purchase behaviors and product category distribution in a retail store over a two-week period.", "raw_table": "OrderID,CustomerID,ProductCategory,PurchaseDate,Quantity,UnitPrice,PaymentMethod,StoreLocation\n1001,C001,Electronics,2024/04/01,2,299.99,Credit Card,New york\n1002,C002,clothing,2024-04-01,1,49.5,Cash,Boston\n1003,C003,Home & Kitchen,04-02-2024,3,15.0,credit card,Los Angeles\n1004,,Toys,2024-04-02,1,,Debit Card,San Francisco\n1005,C005,Electronics,2024/04/03,1,199.99,Cash,new york\n1006,C006,clothing,2024-4-03,2,49.5,Debit card,Boston\n1007,C007,Books,2024-04-04,5,9.99,Credit Card,Los angeles\n1008,C008,Toys,2024/04/05,,19.99,Cash,San Francisco\n1009,C009,Books,2024-04-06,3,9.99,Credit card,Los Angeles\n1010,C010,Home & kitchen,2024-04-07,4,15.0,Debit Card,boston\n1011,C011,Clothing,,2,55.0,Cash,new york\n1012,C012,Electronics,2024-04-08,1,299.99,Credit Card,New York\n1013,C013,Books,2024/04/08,1,9.99,Cash,los angeles\n1014,C014,Toys,04/09/2024,2,19.99,Debit card,san francisco\n", "eda_steps": ["Check for missing values in each column", "Standardize capitalization in ProductCategory and StoreLocation columns", "Parse and standardize PurchaseDate to YYYY-MM-DD format", "Compute descriptive statistics for Quantity and UnitPrice", "Generate value counts for ProductCategory", "Identify top 3 StoreLocations by number of orders", "Calculate total sales (Quantity * UnitPrice) per ProductCategory", "Summarize payment method distribution", "Check for duplicated OrderIDs"], "eda_results": {"missing_values": {"OrderID": 0, "CustomerID": 1, "ProductCategory": 0, "PurchaseDate": 1, "Quantity": 1, "UnitPrice": 1, "PaymentMethod": 0, "StoreLocation": 0}, "standardized_categories": {"ProductCategory": ["Electronics", "Clothing", "Home & Kitchen", "Toys", "Books"], "StoreLocation": ["New York", "Boston", "Los Angeles", "San Francisco"]}, "date_range": {"earliest": "2024-04-01", "latest": "2024-04-09"}, "quantity_stats": {"count": 13, "mean": 2.38, "min": 1, "max": 5, "missing": 1}, "unit_price_stats": {"count": 13, "mean": 77.21, "min": 9.99, "max": 299.99, "missing": 1}, "product_category_counts": {"Electronics": 3, "Clothing": 3, "Home & Kitchen": 2, "Toys": 3, "Books": 4}, "top_store_locations": {"Los Angeles": 4, "New York": 4, "Boston": 3}, "total_sales_per_category": {"Electronics": 1099.96, "Clothing": 198.0, "Home & Kitchen": 90.0, "Toys": 79.96, "Books": 89.91}, "payment_method_distribution": {"Credit Card": 5, "Cash": 5, "Debit Card": 4}, "duplicate_order_ids": 0}}
{"purpose": "Explore crop yield factors and identify data quality issues in farm records.", "raw_table": "Farm_ID,Crop,Planting_Date,Harvest_Date,Yield_kg,Irrigation_Type,Soil_Quality,Notes\nF001,Wheat,2023-03-15,2023/08/20,2500,Drip,Good,NA\nF002,Corn,15-Apr-2023,2023-09-30,3000,Sprinkler,Average,Some pest damage\nf003,soybean,,2023-10-05,2800,drip,Bad,little weed\nF004,Corn,2023-04-10,2023-09-25,3200,Sprinkler,average,\nF005,Wheat,2023/03/20,2023-08-22,2450,,Good,NA\nF006,Rice,2023-05-01,2023-10-10,2900,Flood,Bad,\nF007,Corn,2023-04-12,2023-09-28,NaN,sprinkler,Average,missing yield\nF008,soybean,2023-03-25,2023-09-30,2700,Drip,good,\nF009,Rice,2023-05-05,2023-10-12,2950,Flood,Bad,disease reported\nF010,Wheat,2023-03-18,2023/08/24,2550,Drip,GOOD,\n", "eda_steps": ["Identify and count missing values in each column", "Standardize capitalization in categorical columns: Crop, Irrigation_Type, Soil_Quality", "Parse and standardize date formats in Planting_Date and Harvest_Date columns", "Calculate descriptive statistics for Yield_kg, excluding missing values", "Generate value counts for Crop and Irrigation_Type columns", "Check for inconsistencies or unusual categories in Soil_Quality", "Compute the difference in days between Planting_Date and Harvest_Date to find crop duration", "Summarize notes column for common issues mentioned", "Calculate correlation between Yield_kg and crop duration"], "eda_results": {"missing_values": {"Farm_ID": 0, "Crop": 0, "Planting_Date": 1, "Harvest_Date": 0, "Yield_kg": 1, "Irrigation_Type": 1, "Soil_Quality": 0, "Notes": 3}, "standardized_categories": {"Crop": {"Wheat": 3, "Corn": 3, "Soybean": 2, "Rice": 2}, "Irrigation_Type": {"Drip": 4, "Sprinkler": 3, "Flood": 2, "": 1}, "Soil_Quality": {"Good": 4, "Average": 3, "Bad": 3}}, "date_parsing": {"Planting_Date": {"parsed_dates": 9, "failed_to_parse": 1}, "Harvest_Date": {"parsed_dates": 10, "failed_to_parse": 0}}, "yield_stats": {"count": 9, "mean": 2761.11, "std": 255.44, "min": 2450, "25%": 2695, "50%": 2750, "75%": 2987.5, "max": 3200}, "value_counts": {"Crop": {"Corn": 3, "Wheat": 3, "Soybean": 2, "Rice": 2}, "Irrigation_Type": {"Drip": 4, "Sprinkler": 3, "Flood": 2, "": 1}}, "soil_quality_categories": {"Good": 4, "Average": 3, "Bad": 3}, "crop_duration_days": {"mean": 158.3, "std": 5.1, "min": 150, "max": 165}, "notes_summary": {"NA": 2, "Some pest damage": 1, "little weed": 1, "missing yield": 1, "disease reported": 1, "blank": 4}, "correlation_yield_duration": 0.82}}
{"purpose": "Examine student performance and attendance patterns in a middle school semester.", "raw_table": "StudentID,Name,Grade,Attendance,TestScore,EnrollmentDate\n1001,alice smith,8,95%,88,2023/01/10\n1002,Bob Johnson,8,90%,92,2023-01-15\n1003,CHARLIE lee,7,85%,,01-20-2023\n1004,Dana White,7,75%,,2023/1/25\n1005,eric brown,8,ninety,79,2023-01-30\n1006,Fiona Green,7,80%,85,2023-02-01\n1007,Gary Black,8,,91,2023/02/05\n1008,Helen King,7,88%,88,2023-02-10\n1009,ian Clark,7,92%,94,2023-02-12\n1010,Jane Doe,8,95%,90,2023/2/14", "eda_steps": ["Check for missing values in each column", "Standardize capitalization in the Name column", "Convert Attendance percentages to numeric values", "Parse EnrollmentDate into a consistent date format", "Compute descriptive statistics for TestScore and Attendance", "Generate value counts for Grade", "Identify students with missing TestScore or Attendance data", "Examine correlation between Attendance and TestScore"], "eda_results": {"missing_values": {"StudentID": 0, "Name": 0, "Grade": 0, "Attendance": 2, "TestScore": 2, "EnrollmentDate": 0}, "standardized_names": ["Alice Smith", "Bob Johnson", "Charlie Lee", "Dana White", "Eric Brown", "Fiona Green", "Gary Black", "Helen King", "Ian Clark", "Jane Doe"], "attendance_numeric": [95, 90, 85, 75, null, 80, null, 88, 92, 95], "parsed_enrollment_dates": ["2023-01-10", "2023-01-15", "2023-01-20", "2023-01-25", "2023-01-30", "2023-02-01", "2023-02-05", "2023-02-10", "2023-02-12", "2023-02-14"], "summary_stats": {"TestScore": {"count": 8, "mean": 88.375, "min": 79, "max": 94, "std_dev": 5.02}, "Attendance": {"count": 8, "mean": 87.5, "min": 75, "max": 95, "std_dev": 7.42}}, "value_counts": {"Grade": {"7": 5, "8": 5}}, "students_missing_data": {"Missing TestScore": ["CHARLIE lee", "Dana White"], "Missing Attendance": ["eric brown", "Gary Black"]}, "correlations": {"Attendance_vs_TestScore": 0.88}}}
{"purpose": "Explore patient admission data to identify patterns in diagnoses and length of stay.", "raw_table": "Patient_ID,Admission_Date,Diagnosis,Length_of_Stay,Gender,Age,Discharge_Status\nP001,2023-01-15,Covid-19,5,Male,45,Discharged\nP002,15/02/2023,flu,3,Female,34,discharged\nP003,2023-03-01,,7,M,50,Discharged\nP004,03-20-2023,Hypertension,2,female,NaN,Transferred\nP005,2023/04/05,Covid-19,ten,Male,60,Deceased\nP006,2023-04-15,Asthma,,F,28,Discharged\nP007,,Diabetes,4,Male,55,Discharged\nP008,2023-05-10,Flu,3,female,40,discharged\nP009,2023-05-15,hypertension,1,Male,62,Transferred\nP010,2023-06-01,Asthma,NaN,Female,30,Discharged", "eda_steps": ["Check data types of each column", "Identify and count missing values per column", "Standardize 'Admission_Date' to a uniform date format", "Compute descriptive statistics for 'Length_of_Stay' and 'Age'", "Generate value counts for 'Diagnosis' and 'Discharge_Status' columns", "Analyze gender distribution", "Calculate average length of stay by diagnosis", "Identify inconsistent capitalization in categorical fields"], "eda_results": {"data_types": {"Patient_ID": "string", "Admission_Date": "string", "Diagnosis": "string", "Length_of_Stay": "mixed (string and integer)", "Gender": "string", "Age": "float", "Discharge_Status": "string"}, "missing_values": {"Admission_Date": 1, "Diagnosis": 1, "Length_of_Stay": 3, "Age": 1}, "standardized_Admission_Date": ["2023-01-15", "2023-02-15", "2023-03-01", "2023-03-20", "2023-04-05", "2023-04-15", null, "2023-05-10", "2023-05-15", "2023-06-01"], "length_of_stay_stats": {"count": 7, "mean": 3.57, "median": 3, "min": 1, "max": 7}, "age_stats": {"count": 9, "mean": 44.9, "median": 45, "min": 28, "max": 62}, "value_counts": {"Diagnosis": {"Covid-19": 2, "flu": 2, "Hypertension": 2, "Asthma": 2, "Diabetes": 1, "null": 1}, "Discharge_Status": {"Discharged": 6, "discharged": 2, "Transferred": 2, "Deceased": 1}}, "gender_distribution": {"Male": 5, "Female": 4, "M": 1, "F": 1, "female": 2, "male": 0}, "average_length_of_stay_by_diagnosis": {"Covid-19": 7.5, "flu": 3.0, "Hypertension": 1.5, "Asthma": 0, "Diabetes": 4.0, "null": 7.0}, "capitalization_issues": {"Diagnosis": ["flu", "Flu", "hypertension", "Hypertension", "Asthma", "asthma"], "Discharge_Status": ["Discharged", "discharged", "Transferred", "Deceased"]}}}
{"purpose": "Analyze daily energy consumption patterns across different regions and energy sources to identify usage trends and data quality issues.", "raw_table": "Date,Region,Energy_Source,Consumption_MWh,Peak_Hour\n2024-01-01,North,solar,1500,18\n01/02/2024,South,Wind,1300,19\n2024/01/03,East,Coal,2000,20\n2024-01-04,west,solar,NaN,18\n2024-01-05,North,wind,1600,17\n2024-01-06,South,Coal,2100,\n2024-01-07,East,Solar,1800,18\n2024-01-08,West,Wind,NaN,20\n2024-01-09,North,Coal,1900,19\n2024-1-10,South,Solar,1700,17\n2024-01-11,East,coal,1950,21\n2024-01-12,west,WIND,1400,20\n2024-01-13,North,solar,1550,18\n13-01-2024,South,Wind,1350,19", "eda_steps": ["Standardize date formats in the Date column", "Normalize capitalization in Region and Energy_Source columns", "Check and report missing values for all columns", "Compute descriptive statistics for Consumption_MWh", "Generate value counts for Region and Energy_Source columns", "Identify the most common Peak_Hour values", "Calculate correlation between Consumption_MWh and Peak_Hour where data is available", "Summarize the distribution skewness of Consumption_MWh"], "eda_results": {"missing_values": {"Date": 0, "Region": 0, "Energy_Source": 0, "Consumption_MWh": 2, "Peak_Hour": 1}, "value_counts": {"Region": {"North": 4, "South": 4, "East": 3, "West": 3}, "Energy_Source": {"Solar": 5, "Wind": 5, "Coal": 4}}, "summary_stats": {"Consumption_MWh": {"count": 12, "mean": 1704.17, "std": 263.95, "min": 1300, "25%": 1550, "50%": 1700, "75%": 1950, "max": 2100, "skewness": 0.15}, "Peak_Hour": {"mode": [18, 19], "missing": 1}}, "correlations": {"Consumption_MWh_Peak_Hour": 0.42}, "top_categories": {"Peak_Hour": [18, 19]}}}
{"purpose": "Explore patterns in patient demographics and lab results to identify data quality issues and distributions.", "raw_table": "Patient_ID,Age,Gender,Diagnosis,Admission_Date,Discharge_Date,Glucose_Level,Blood_Pressure\nP001,34,Male,Diabetes,2023-01-15,2023-01-20,145,130/85\nP002,29,Female,hypertension,01/18/2023,2023-01-25,NaN,120/80\nP003,52,M,Heart Failure,2023/02/10,2023/02/15,180,140/90\nP004,,female,COPD,2023-02-14,2023-02-20,110,NaN\np005,45,Male,diabetes,2/20/2023,2/28/2023,150,135/88\nP006,38,F,Hypertension,2023-03-01,2023-03-05,NaN,125/\nP007,60,,Heart failure,2023-03-10,2023-03-15,200,145/95\nP008,55,Female,COPD,,2023-03-21,115,130/85\nP009,47,Male,Diabetes,03-20-2023,03-25-23,160,140/90\nP010,50,Female,Unknown,2023-04-01,2023-04-05,130,120/80\n", "eda_steps": ["Check for missing values in each column and calculate percentages", "Standardize capitalization in categorical columns: Gender and Diagnosis", "Parse and standardize date formats in Admission_Date and Discharge_Date", "Compute descriptive statistics (mean, median, std) for Age and Glucose_Level", "Extract systolic and diastolic values from Blood_Pressure and compute their distributions", "Generate value counts for Diagnosis and Gender columns", "Calculate length of stay in days for each patient", "Identify any inconsistent or unusual categories in Diagnosis"], "eda_results": {"missing_values": {"Patient_ID": 0, "Age": 1, "Gender": 1, "Diagnosis": 0, "Admission_Date": 1, "Discharge_Date": 0, "Glucose_Level": 2, "Blood_Pressure": 1}, "value_counts": {"Diagnosis": {"Diabetes": 3, "Hypertension": 2, "Heart Failure": 2, "COPD": 2, "Unknown": 1}, "Gender": {"Male": 4, "Female": 4, "": 1}}, "standardized_gender": {"Male": 4, "Female": 4, "Missing": 1}, "standardized_diagnosis": {"Diabetes": 3, "Hypertension": 2, "Heart Failure": 2, "COPD": 2, "Unknown": 1}, "age_stats": {"mean": 47.86, "median": 48.5, "std_dev": 9.56, "count": 14}, "glucose_stats": {"mean": 148.57, "median": 145, "std_dev": 28.74, "count": 11}, "blood_pressure_extracted": {"systolic": {"mean": 133.33, "median": 135, "std_dev": 9.57, "count": 12}, "diastolic": {"mean": 86.25, "median": 88, "std_dev": 5.42, "count": 11}}, "length_of_stay_days": {"mean": 5.9, "median": 6, "min": 4, "max": 8, "count": 14}, "date_standardization_issues": ["Mixed date formats detected in Admission_Date and Discharge_Date; some missing Admission_Date values"], "unusual_diagnosis_categories": ["Unknown"]}}
{"purpose": "Analyze patient demographics and lab test results to identify data quality issues and key summary statistics.", "raw_table": "Patient_ID,Age,Gender,Diagnosis,Admission_Date,Cholesterol_mg_dL,Blood_Pressure\n001,45,Male,Hypertension,2023/01/15,220,130/85\n002,,female,Diabetes,15-02-2023,180,120/80\n003,39,M,hypertension,2023-03-10,NaN,135/90\n004,52,Female,diabetes,03/20/2023,195,NaN\n005,29,Male,Asthma,,210,125/82\n006,41,F,hypertension,2023/04/05,205,140/90\n007,NaN,female,diabetes,2023-04-10,NaN,128/88\n008,33,Male,asthma,2023/05/01,190,118/79\n009,47,male,Hypertension,05-15-2023,NaN,132/85\n010,50,F,Diabetes,2023/06/01,200,138/92", "eda_steps": ["Check for missing values in each column and calculate their percentages", "Standardize the 'Gender' and 'Diagnosis' categorical values to consistent capitalization", "Parse and standardize the 'Admission_Date' to YYYY-MM-DD format", "Compute descriptive statistics (mean, median, std) for numeric columns: Age and Cholesterol_mg_dL", "Generate value counts for the 'Diagnosis' and 'Gender' columns", "Identify rows with inconsistent or missing Blood_Pressure readings", "Analyze relationship between Age and Cholesterol levels using correlation coefficient"], "eda_results": {"missing_values": {"Patient_ID": "0%", "Age": "20%", "Gender": "0%", "Diagnosis": "0%", "Admission_Date": "10%", "Cholesterol_mg_dL": "30%", "Blood_Pressure": "10%"}, "standardized_categories": {"Gender": {"Male": 5, "Female": 5}, "Diagnosis": {"Hypertension": 4, "Diabetes": 4, "Asthma": 2}}, "dates_standardized": ["2023-01-15", "2023-02-15", "2023-03-10", "2023-03-20", null, "2023-04-05", "2023-04-10", "2023-05-01", "2023-05-15", "2023-06-01"], "summary_stats": {"Age": {"mean": 42.1, "median": 44, "std": 7.6}, "Cholesterol_mg_dL": {"mean": 199, "median": 200, "std": 15}}, "value_counts": {"Diagnosis": {"Hypertension": 4, "Diabetes": 4, "Asthma": 2}, "Gender": {"Male": 5, "Female": 5}}, "inconsistent_blood_pressure": {"missing": 1, "format_issues": 0}, "correlations": {"Age_vs_Cholesterol_mg_dL": 0.62}}}
{"purpose": "Analyze retail sales data to identify product performance and data quality issues.", "raw_table": "OrderID,Product,Category,Quantity,Price,OrderDate,CustomerRegion\n1001,widget A,Electronics,10,25.5,2023-01-15,North\n1002,Widget B,Electronics,5,,15/01/2023,South\n1003,widget a,Electronics,NaN,25.5,2023/01/16,North\n1004,Gadget X,Home,3,45.0,2023-1-17,East\n1005,gadget x,home,,45,01-18-2023,East\n1006,Widget B,Electronics,7,30.0,2023-01-19,West\n1007,Tool Y,Hardware,4,15.0,2023-01-20,South\n1008,Tool y,hardware,2,15,2023/01/21,South\n1009,Widget C,Electronics,8,27.5,,north\n1010,Gadget Z,Home,6,50.0,2023-01-22,East\n1011,widget C,Electronics,3,27.5,2023-01-23,North\n1012,Tool Y,Hardware,NaN,15.0,2023-01-24,South\n1013,Gadget x,HOME,5,45.0,2023-01-25,East", "eda_steps": ["Check the data types and identify inconsistent types in each column", "Calculate the percentage of missing values per column", "Standardize capitalization for 'Product' and 'Category' columns", "Compute descriptive statistics for 'Quantity' and 'Price' columns", "Generate value counts for the 'CustomerRegion' column", "Identify unique products after standardization", "Analyze date formats and convert 'OrderDate' to a consistent datetime format", "Find correlation between 'Quantity' and 'Price'"], "eda_results": {"data_types": {"OrderID": "integer", "Product": "string", "Category": "string", "Quantity": "float", "Price": "float", "OrderDate": "string", "CustomerRegion": "string"}, "missing_values_percent": {"Quantity": 15.38, "Price": 7.69, "OrderDate": 7.69, "CustomerRegion": 7.69}, "standardized_categories": {"Product": ["Widget A", "Widget B", "Widget C", "Gadget X", "Gadget Z", "Tool Y"], "Category": ["Electronics", "Home", "Hardware"]}, "descriptive_statistics": {"Quantity": {"count": 11, "mean": 5.27, "std": 2.64, "min": 2.0, "25%": 3.0, "50%": 5.0, "75%": 7.0, "max": 10.0}, "Price": {"count": 12, "mean": 32.88, "std": 11.78, "min": 15.0, "25%": 25.5, "50%": 27.5, "75%": 45.0, "max": 50.0}}, "value_counts_CustomerRegion": {"North": 4, "South": 4, "East": 4, "West": 1}, "unique_products_count": 6, "order_date_conversion": {"successful_conversions": 12, "failed_conversions": 1}, "correlation_Quantity_Price": 0.65}}
{"purpose": "Analyze patient demographic and clinical characteristics in a cardiology outpatient dataset.", "raw_table": "Patient_ID,Age,Gender,Visit_Date,Diagnosis,Cholesterol_mg_dl,Blood_Pressure\n001,45,Male,2023-01-15,Hypertension,220,140/90\n002,52,Female,15-02-2023,CAD,250,130/85\n003,,female,2023/03/05,Hypertension,NaN,135/88\n004,60,Male,2023-04-20,cardiomyopathy,180,145/95\n005,37,Male,2023-05-18,,210,NaN\n006,29,Female,2023-06-11,Hypertension,NaN,120/80\n007,55,MALE,2023-07-01,Stroke,240,150/100\n008,48,Female,2023-07-15,CAD,230,135/87\n009,50,Male,,Hypertension,NaN,140/90\n010,44,Female,2023-08-09,CAD,225,130/85\n011,51,male,2023-08-21,Hypertension,215,138/89\n012,58,Female,2023-09-10,cardiomyopathy,195,142/92\n013,63,Male,2023-09-25,Stroke,NaN,NaN\n014,35,Female,09-30-2023,Hypertension,205,125/80", "eda_steps": ["Check for missing values in each column", "Standardize the Gender column to consistent capitalization", "Parse and standardize Visit_Date to ISO format (YYYY-MM-DD)", "Compute descriptive statistics for Age and Cholesterol_mg_dl", "Generate value counts for Diagnosis categories", "Extract systolic and diastolic values from Blood_Pressure and summarize", "Identify rows with missing key clinical data", "Calculate correlation between Age and Cholesterol_mg_dl"], "eda_results": {"missing_values": {"Patient_ID": 0, "Age": 1, "Gender": 0, "Visit_Date": 1, "Diagnosis": 1, "Cholesterol_mg_dl": 4, "Blood_Pressure": 2}, "gender_value_counts": {"male": 7, "female": 7}, "standardized_dates": ["2023-01-15", "2023-02-15", "2023-03-05", "2023-04-20", "2023-05-18", "2023-06-11", "2023-07-01", "2023-07-15", "NaN", "2023-08-09", "2023-08-21", "2023-09-10", "2023-09-25", "2023-09-30"], "age_descriptive_stats": {"count": 14, "mean": 49.0, "std_dev": 9.78, "min": 29, "max": 63}, "cholesterol_descriptive_stats": {"count": 10, "mean": 218.0, "std_dev": 22.7, "min": 180, "max": 250}, "diagnosis_value_counts": {"Hypertension": 6, "CAD": 3, "cardiomyopathy": 2, "Stroke": 2, "NaN": 1}, "blood_pressure_summary": {"systolic": {"mean": 136.6, "min": 120, "max": 150, "missing": 2}, "diastolic": {"mean": 87.7, "min": 80, "max": 100, "missing": 2}}, "rows_with_missing_clinical_data": [3, 5, 8, 12], "age_cholesterol_correlation": 0.68}}
{"purpose": "Explore housing market data to understand price trends and property features.", "raw_table": "Property_ID,Location,Price,Bedrooms,Bathrooms,Size_sqft,Year_Built,Sale_Date\nP001,Downtown,350000,3,2,1500,1998,2021-06-15\nP002,suburbs,425000,4,3,,2005,15/07/2021\nP003,UPTOWN,NaN,2,1,900,1990,2021/08/01\nP004,Downtown,500000,4,,1800,2010,2021-9-10\nP005,Suburbs,NaN,3,2,1300,,2021-09-25\nP006,uptown,300000,2,1,850,1985,2021-10-05\nP007,Downtown,475000,3,2,1400,2000,2021/11/12\nP008,Suburbs,520000,5,3,2100,2015,\nP009,,400000,3,2,1600,2008,2021-12-01\nP010,Uptown,310000,2,1,900,1988,2021-12-15\nP011,Downtown,NaN,3,2,1500,1995,2022-01-10\nP012,Suburbs,450000,4,3,1900,2012,01-02-2022", "eda_steps": ["Check the percentage of missing values in each column", "Standardize the Location column capitalization", "Compute descriptive statistics for Price and Size_sqft", "Generate value counts for the number of Bedrooms", "Analyze distribution of Sale_Date formats and convert to a uniform date format", "Identify correlation between Price and Size_sqft", "List properties with missing Price values", "Summarize the range and median of Year_Built"], "eda_results": {"missing_values": {"Property_ID": "0%", "Location": "8.3%", "Price": "25%", "Bedrooms": "0%", "Bathrooms": "8.3%", "Size_sqft": "8.3%", "Year_Built": "16.7%", "Sale_Date": "8.3%"}, "location_value_counts": {"downtown": 4, "suburbs": 4, "uptown": 4, "missing": 1}, "price_stats": {"count": 9, "mean": 417222, "std": 77550, "min": 300000, "25%": 350000, "50%": 425000, "75%": 475000, "max": 520000}, "size_sqft_stats": {"count": 12, "mean": 1508, "std": 420, "min": 850, "25%": 900, "50%": 1500, "75%": 1800, "max": 2100}, "bedrooms_value_counts": {"2": 4, "3": 4, "4": 3, "5": 1}, "sale_date_formats": {"YYYY-MM-DD": 8, "DD/MM/YYYY": 1, "YYYY/MM/DD": 2, "MM-DD-YYYY": 1, "missing": 1}, "price_size_correlation": 0.88, "missing_price_properties": ["P003", "P005", "P011"], "year_built_summary": {"min": 1985, "median": 1998, "max": 2015, "count_non_missing": 10}}}
{"purpose": "Analyze machine downtime causes and duration to improve maintenance scheduling.", "raw_table": "MachineID,Date,DowntimeMinutes,Shift,Operator,Reason\nM001,2024-01-01,120,Day,alice,Maintenance\nm002,01/02/2024,45,Night,bob,Power Failure\nM003,2024-1-03,,Day,Charlie,Operator Error\nM004,2024/01/04,30,day,ALICE,Material Shortage\nM005,2024-01-05,60,NIGHT,Bob,Power failure\nM006,,15,Night,Dave,Operator error\nm007,2024-01-07,90,Day,,Maintenance\nM008,2024-01-08,NaN,Night,Eve,Material shortage\nM009,2024-01-09,20,Day,Fred,unknown\nM010,20240110,50,Day,George,Maintenance", "eda_steps": ["Check for missing values in each column", "Standardize the 'Shift' and 'Reason' columns to consistent capitalization", "Convert 'Date' strings to a uniform date format", "Compute descriptive statistics for 'DowntimeMinutes'", "Generate value counts for 'Reason' and 'Shift' columns", "Identify rows with missing 'DowntimeMinutes' or 'Operator' information", "Calculate average downtime per shift", "Summarize unique operators count and their frequency"], "eda_results": {"missing_values": {"MachineID": 0, "Date": 1, "DowntimeMinutes": 2, "Shift": 0, "Operator": 1, "Reason": 0}, "standardized_categories": {"Shift": {"Day": 6, "Night": 4}, "Reason": {"Maintenance": 3, "Power Failure": 2, "Operator Error": 2, "Material Shortage": 2, "Unknown": 1}}, "date_format_uniformed": {"earliest_date": "2024-01-01", "latest_date": "2024-01-10"}, "summary_stats": {"DowntimeMinutes": {"count": 11, "mean": 52.73, "std": 33.96, "min": 15, "25%": 30, "50%": 45, "75%": 90, "max": 120}}, "value_counts": {"Reason": {"Maintenance": 3, "Power Failure": 2, "Operator Error": 2, "Material Shortage": 2, "Unknown": 1}, "Shift": {"Day": 6, "Night": 4}}, "missing_critical_rows": {"missing_DowntimeMinutes_rows": [3, 8], "missing_Operator_rows": [6]}, "average_downtime_per_shift": {"Day": 61.0, "Night": 33.75}, "operators_summary": {"unique_operators": 7, "operator_frequency": {"Alice": 2, "Bob": 2, "Charlie": 1, "Dave": 1, "Eve": 1, "Fred": 1, "George": 1}}}}
{"purpose": "Analyze trip durations and identify common transportation modes used by commuters.", "raw_table": "trip_id,start_time,end_time,mode_of_transport,distance_km,user_age\n1,2023-04-01 08:15,2023-04-01 08:45,Bus,5.2,34\n2,04/02/2023 09:00,2023-04-02 09:30,car,12.5,28\n3,2023-4-03 07:50,2023-04-03 08:10,Bike,3.1,NaN\n4,2023/04/04 18:20,2023/04/04 18:50,Bus,,45\n5,2023-04-05 07:15,2023-04-05 07:45,Taxi,8.4,37\n6,2023-04-06 08:00,,bus,4.8,22\n7,2023-04-07 08:30,2023-04-07 09:10,Train,15.0,41\n8,2023/04/08 06:45,2023/04/08 07:05,bike,3.0,26\n9,2023-04-09 17:00,2023-04-09 17:25,,6.5,30\n10,2023-04-10 08:15,2023-04-10 08:40,Car,10.1,29\n11,2023-04-11 09:10,2023-04-11 09:40,Bus,5.5,NaN", "eda_steps": ["Check for missing values in all columns", "Standardize capitalization of mode_of_transport values", "Calculate trip duration in minutes from start_time and end_time", "Compute descriptive statistics for distance_km, user_age, and trip duration", "Generate value counts for mode_of_transport", "Identify rows with missing end_time or distance_km", "Analyze correlation between user_age, distance_km, and trip duration", "Find the top 3 most common modes of transport", "Summarize the distribution skewness of trip duration"], "eda_results": {"missing_values": {"trip_id": 0, "start_time": 0, "end_time": 1, "mode_of_transport": 1, "distance_km": 1, "user_age": 2}, "standardized_modes": ["Bus", "Car", "Bike", "Bus", "Taxi", "Bus", "Train", "Bike", "Missing", "Car", "Bus"], "trip_duration_minutes": {"1": 30, "2": 30, "3": 20, "4": 30, "5": 30, "6": null, "7": 40, "8": 20, "9": 25, "10": 25, "11": 30}, "summary_stats": {"distance_km": {"count": 10, "mean": 7.41, "std": 4.06, "min": 3.0, "25%": 4.8, "50%": 5.35, "75%": 10.1, "max": 15.0}, "user_age": {"count": 9, "mean": 31.78, "std": 7.02, "min": 22, "25%": 28, "50%": 30, "75%": 37, "max": 45}, "trip_duration_minutes": {"count": 10, "mean": 28, "std": 6.48, "min": 20, "25%": 25, "50%": 30, "75%": 30, "max": 40}}, "value_counts_mode_of_transport": {"Bus": 4, "Car": 2, "Bike": 2, "Taxi": 1, "Train": 1, "Missing": 1}, "rows_with_missing_end_time_or_distance": [6, 4], "correlations": {"user_age_distance_km": 0.83, "user_age_trip_duration": 0.75, "distance_km_trip_duration": 0.97}, "top_3_modes": ["Bus", "Car", "Bike"], "trip_duration_skewness": 0.35}}
{"purpose": "Analyze household electricity consumption patterns over a two-week period to identify usage trends and data quality issues.", "raw_table": "HouseholdID,Date,EnergyConsumed_kWh,TimeOfDay,Region\nH001,2024/04/01,12.5,Morning,North\nh002,2024-04-01,,afternoon,South\nH003,01-04-2024,15.7,EVENING,East\nh004,2024/4/2,9.8,Morning,west\nH005,2024-04-02,14.0,Afternoon,North\nh006,2024/04/02,12.1,Night,South\nH007,2024/4/03,abc,MORNING,East\nH008,2024-04-03,13.3,Evening,West\nH009,,11.7,Afternoon,North\nH010,2024-04-04,10.5,Morning,Unknown\nH001,2024-04-05,12.6,Night,North\nH002,2024/04/05,NaN,Afternoon,south\nH003,2024.04.05,15.2,evening,East", "eda_steps": ["Check for missing values in each column", "Standardize the date format to YYYY-MM-DD", "Convert EnergyConsumed_kWh to numeric, handling non-numeric entries", "Compute descriptive statistics for EnergyConsumed_kWh", "Generate value counts for TimeOfDay and Region columns", "Identify duplicate HouseholdID and Date combinations", "Analyze distribution skewness of EnergyConsumed_kWh", "Compute correlation between EnergyConsumed_kWh and TimeOfDay if possible"], "eda_results": {"missing_values": {"HouseholdID": 0, "Date": 1, "EnergyConsumed_kWh": 3, "TimeOfDay": 0, "Region": 1}, "date_format_standardized": ["2024-04-01", "2024-04-01", "2024-04-01", "2024-04-02", "2024-04-02", "2024-04-02", "2024-04-03", "2024-04-03", "null", "2024-04-04", "2024-04-05", "2024-04-05", "2024-04-05"], "energy_consumed_numeric": [12.5, null, 15.7, 9.8, 14.0, 12.1, null, 13.3, 11.7, 10.5, 12.6, null, 15.2], "summary_stats_energy_kWh": {"count": 10, "mean": 12.72, "std": 1.77, "min": 9.8, "25%": 11.7, "50%": 12.55, "75%": 14.0, "max": 15.7}, "value_counts_timeofday": {"Morning": 4, "Afternoon": 3, "Evening": 3, "Night": 2}, "value_counts_region": {"North": 4, "South": 3, "East": 3, "West": 2, "Unknown": 1}, "duplicate_household_date_pairs": [], "skewness_energy_kWh": 0.42, "correlation_energy_timeofday": {"note": "TimeOfDay is categorical; correlation not computed directly"}}}
{"purpose": "Analyze customer purchase behavior and product category distribution in the last quarter", "raw_table": "OrderID,CustomerID,OrderDate,ProductCategory,Quantity,UnitPrice,PaymentMethod\n1001,C001,2023-10-01,electronics,2,199.99,Credit Card\n1002,c002,10/05/2023,Home & Garden,1,89.50,Paypal\n1003,C003,2023/10/07,Electronics,NaN,149.99,Credit card\n1004,C004,2023-10-12,beauty,3,19.99,Credit Card\n1005,C005,13-10-2023,Toys,5,15.00,cash\n1006,c006,2023-10-15,home & garden,,89.50,Paypal\n1007,C007,2023-10-18,Beauty,2,NaN,Credit Card\n1008,C008,10-20-2023,Toys,1,15.0,Cash\n1009,C009,2023/10/22,ELECTRONICS,1,199.99,credit card\n1010,C001,2023-10-25,Home & garden,4,89.50,PayPal\n1011,C010,,Toys,3,15.00,Cash\n1012,C011,2023-10-28,Books,2,12.99,Credit Card\n1013,C012,2023-10-30,books,,12.99,Credit card", "eda_steps": ["Check missing value percentages for each column", "Standardize capitalization in ProductCategory and PaymentMethod columns", "Convert OrderDate to a consistent date format", "Compute descriptive statistics for Quantity and UnitPrice", "Generate value counts for ProductCategory", "Identify top 3 PaymentMethods used", "Calculate total revenue per ProductCategory", "Find number of unique customers", "Summarize frequency of orders per week"], "eda_results": {"missing_values": {"OrderID": 0, "CustomerID": 0, "OrderDate": 1, "ProductCategory": 0, "Quantity": 3, "UnitPrice": 2, "PaymentMethod": 0}, "standardized_categories": {"ProductCategory": ["Electronics", "Home & Garden", "Beauty", "Toys", "Books"], "PaymentMethod": ["Credit Card", "Paypal", "Cash"]}, "order_date_range": {"min": "2023-10-01", "max": "2023-10-30", "missing_dates": 1}, "descriptive_stats": {"Quantity": {"count": 12, "mean": 2.5, "min": 1, "max": 5, "median": 2}, "UnitPrice": {"count": 13, "mean": 79.24, "min": 12.99, "max": 199.99, "median": 89.5}}, "value_counts_ProductCategory": {"Electronics": 3, "Home & Garden": 3, "Beauty": 2, "Toys": 4, "Books": 2}, "top_payment_methods": {"Credit Card": 6, "Paypal": 3, "Cash": 3}, "total_revenue_per_category": {"Electronics": 749.96, "Home & Garden": 536.99, "Beauty": 99.95, "Toys": 135.0, "Books": 51.96}, "unique_customers": 12, "orders_per_week": {"Week 1 (Oct 1-7)": 3, "Week 2 (Oct 8-14)": 3, "Week 3 (Oct 15-21)": 3, "Week 4 (Oct 22-28)": 3, "Week 5 (Oct 29-31)": 1}}}
{"purpose": "Analyze public transportation ridership patterns and identify data quality issues in bus trip records.", "raw_table": "Trip_ID,Route,Boarding_Time,Passenger_Count,Fare,Driver_ID\n1,A1,2023-03-01 08:15,25,2.50,D100\n2,a1,03/01/2023 09:00,30,2.50,D101\n3,B2,2023-03-01 08:45,,3.00,d102\n4,C3,2023-3-01 10:00,15,3.00,D103\n5,B2,2023/03/01 10:30,20,3,d104\n6,A1,2023-03-01 11:00,28,2.50, D100\n7,c3,Mar 01 2023 11:15,12,3.00,D105\n8,B2,,22,3.00,D102\n9,A1,2023-03-01 12:00,NaN,2.50,D100\n10,D4,2023-03-01 12:30,18,3.50,D106\n11,d4,2023-03-01 13:00,20,3.50,D106\n12,,2023-03-01 13:30,15,3.00,D107\n13,C3,2023-03-01 14:00,10,3.00,D103", "eda_steps": ["Check for missing values in all columns and calculate their percentages", "Standardize the 'Route' column capitalization", "Parse and standardize the 'Boarding_Time' column to a consistent datetime format", "Compute descriptive statistics for 'Passenger_Count' and 'Fare'", "Generate value counts for the 'Route' and 'Driver_ID' columns", "Identify records with invalid or missing 'Route' or 'Boarding_Time' entries", "Calculate the correlation between 'Passenger_Count' and 'Fare'"], "eda_results": {"missing_values": {"Trip_ID": "0%", "Route": "7.7%", "Boarding_Time": "7.7%", "Passenger_Count": "15.4%", "Fare": "0%", "Driver_ID": "0%"}, "route_value_counts": {"A1": 4, "B2": 3, "C3": 3, "D4": 2, "": 1}, "driver_id_value_counts": {"D100": 3, "D101": 1, "D102": 2, "D103": 2, "D104": 1, "D105": 1, "D106": 2, "D107": 1}, "passenger_count_stats": {"count": 11, "mean": 19.55, "std": 6.74, "min": 10, "25%": 15, "50%": 20, "75%": 25, "max": 30}, "fare_stats": {"count": 13, "mean": 2.92, "std": 0.37, "min": 2.5, "25%": 2.5, "50%": 3.0, "75%": 3.0, "max": 3.5}, "boarding_time_standardized_formats": ["2023-03-01T08:15:00", "2023-03-01T09:00:00", "2023-03-01T08:45:00", "2023-03-01T10:00:00", "2023-03-01T10:30:00", "2023-03-01T11:00:00", "2023-03-01T11:15:00", null, "2023-03-01T12:00:00", "2023-03-01T12:30:00", "2023-03-01T13:00:00", "2023-03-01T13:30:00", "2023-03-01T14:00:00"], "invalid_or_missing_routes": [12], "correlation_passenger_fare": 0.68}}
{"purpose": "Analyze monthly transaction patterns and customer demographics for a retail bank.", "raw_table": "TransactionID,CustomerID,TransactionDate,Amount,TransactionType,CustomerSegment\nTXN001,CUST1001,2023-01-15,1200.50,Deposit,Premium\nTXN002,CUST1002,01/20/2023,,- withdrawal ,Standard\nTXN003,CUST1003,2023/02/10,500.00,Deposit,standard\nTXN004,CUST1004,2023-02-15,300,Withdrawal,Premium\nTXN005,CUST1005,15-03-2023,abc,Deposit,Unknown\nTXN006,CUST1001,2023-03-20,700.00,Deposit,PREMIUM\nTXN007,CUST1003,2023-03-25,400.75,Withdrawal,standard\nTXN008,CUST1006,,600,Deposit,Standard\nTXN009,CUST1007,2023-04-10,-200,withdrawal,standard\nTXN010,CUST1008,2023-04-15,1000,Deposit,VIP\nTXN011,CUST1009,2023-04-20,250.50,Withdrawal,Vip\nTXN012,CUST1010,2023-04-25,350.00,Deposit,Standard\nTXN013,CUST1011,04/30/2023,450.00,Withdrawal,premium", "eda_steps": ["Check missing values in all columns", "Standardize the TransactionType and CustomerSegment values to consistent capitalization", "Parse and standardize TransactionDate into a uniform date format", "Identify and handle invalid Amount values (non-numeric or missing)", "Compute descriptive statistics for the Amount column", "Generate value counts for TransactionType and CustomerSegment", "Summarize monthly transaction volumes", "Calculate the correlation between Amount and TransactionType (encoded numerically)", "Identify the top 2 CustomerSegments by total transaction Amount"], "eda_results": {"missing_values": {"TransactionID": 0, "CustomerID": 0, "TransactionDate": 1, "Amount": 2, "TransactionType": 0, "CustomerSegment": 1}, "standardized_categories": {"TransactionType": {"Deposit": 6, "Withdrawal": 6}, "CustomerSegment": {"Premium": 4, "Standard": 5, "Unknown": 1, "VIP": 2}}, "invalid_amounts": {"non_numeric": ["abc"], "negative_values": [-200]}, "summary_stats": {"count": 11, "mean": 544.77, "std": 335.14, "min": 250.5, "25%": 400.75, "50%": 500, "75%": 700, "max": 1200.5}, "monthly_volumes": {"2023-01": 2, "2023-02": 2, "2023-03": 3, "2023-04": 4}, "correlations": {"Amount_vs_TransactionType": -0.45}, "top_customer_segments_by_amount": {"Premium": 2551.75, "Standard": 1900.5}}}
{"purpose": "Analyze customer purchase patterns and identify missing data issues in retail transactions.", "raw_table": "TransactionID,CustomerID,ProductCategory,PurchaseAmount,PurchaseDate,StoreLocation\nT001,C123,Electronics,299.99,2023/01/15,New york\nT002,C234,Clothing,,15-02-2023,Los Angeles\nT003,C345,home & garden,89.5,2023-03-05,CHICAGO\nT004,C123,Electronics,199.99,Mar 10 2023,New York\nT005,C456,clothing,49.99,2023-03-11,Houston\nT006,C567,Toys,20,2023/03/15,Houston\nT007,C234,Home & Garden,,2023-03-20,los angeles\nT008,C678,Electronics,150,03/22/2023,New york\nT009,C789,Toys,15.5,March 25 2023,Chicago\nT010,C890,Clothing,75.25,2023-03-27,houston\nT011,C901,Electronics,NaN,2023/03/29,New York\nT012,C234,Toys,22,2023-04-01,Los Angeles", "eda_steps": ["Standardize ProductCategory capitalization", "Parse PurchaseDate into a uniform date format", "Check for missing values in PurchaseAmount and ProductCategory", "Compute descriptive statistics for PurchaseAmount", "Generate value counts for StoreLocation and ProductCategory", "Identify transactions with zero or missing PurchaseAmount", "Calculate total sales per StoreLocation", "Find top 2 ProductCategories by total sales"], "eda_results": {"missing_values": {"PurchaseAmount": 3, "ProductCategory": 0}, "standardized_categories": ["Electronics", "Clothing", "Home & Garden", "Electronics", "Clothing", "Toys", "Home & Garden", "Electronics", "Toys", "Clothing", "Electronics", "Toys"], "purchase_amount_summary": {"count": 9, "mean": 109.27, "std": 95.38, "min": 15.5, "25%": 22, "50%": 75.25, "75%": 199.99, "max": 299.99}, "store_location_counts": {"New York": 4, "Los Angeles": 3, "Chicago": 2, "Houston": 3}, "product_category_counts": {"Electronics": 4, "Clothing": 3, "Home & Garden": 2, "Toys": 3}, "zero_or_missing_purchase_amount_transactions": ["T002", "T007", "T011"], "total_sales_per_store": {"New York": 649.98, "Los Angeles": 22, "Chicago": 105, "Houston": 145.24}, "top_2_categories_by_sales": {"Electronics": 649.98, "Clothing": 125.24}}}
{"purpose": "Analyze movie ratings and release trends to understand audience preferences and data quality issues.", "raw_table": "MovieID,Title,Genre,ReleaseDate,Rating,Duration,Director\n1,The Great Escape,Action,2021-07-15,8.2,132,John Sturges\n2,Romance in paris,romance,15/08/2020,7.5,105,Sophie Marceau\n3,Shadow Realm,Fantasy,2020/12/01,8.7,140,kevin smith\n4,Speedster,Action,2021-03-22,,110,Michael Bay\n5,Love & War,ROMANCE,2020-07-30,6.8,NaN,Anna Lee\n6,Haunted Nights,Horror,2021/10/31,7.9,115,Edgar Wright\n7,Comedy Club,comedy,2021-05-10,6.5,98,Adam Sandler\n8,Under The Sea,fAntasy,2022-01-15,8.0,125,Hans Zimmer\n9,Last Stand,action,2021-13-01,7.1,130,John Woo\n10,Sunset Boulevard,Drama,July 20 2020,8.3,112,Billy Wilder\n11,Midnight Run,Action,,7.6,118,Martin Brest\n12,Romantic Getaway,Romance,2020-11-25,NaN,102,Sophie Marceau", "eda_steps": ["Check for missing values in each column", "Standardize the Genre column to consistent capitalization", "Parse and standardize ReleaseDate to a uniform date format", "Calculate descriptive statistics for Rating and Duration", "Generate value counts for Genre", "Identify rows with invalid or missing ReleaseDate", "Find top 3 directors by number of movies", "Calculate average rating by Genre"], "eda_results": {"missing_values": {"MovieID": 0, "Title": 0, "Genre": 0, "ReleaseDate": 2, "Rating": 2, "Duration": 1, "Director": 0}, "genre_standardized_counts": {"Action": 4, "Romance": 3, "Fantasy": 2, "Horror": 1, "Comedy": 1, "Drama": 1}, "invalid_release_dates": [9, 11], "rating_stats": {"count": 10, "mean": 7.66, "std": 0.68, "min": 6.5, "max": 8.7}, "duration_stats": {"count": 11, "mean": 117.5, "std": 13.2, "min": 98, "max": 140}, "top_directors": {"Sophie Marceau": 2, "John Sturges": 1, "Kevin Smith": 1}, "average_rating_by_genre": {"Action": 7.75, "Romance": 7.15, "Fantasy": 8.35, "Horror": 7.9, "Comedy": 6.5, "Drama": 8.3}}}
{"purpose": "Analyze customer transaction patterns and data quality issues in retail banking accounts.", "raw_table": "Account_ID,Transaction_Date,Transaction_Type,Amount,Currency,Account_Status\n1001,2023-01-15,deposit,500.00,USD,Active\n1002,01/20/2023,Withdrawal,200,usd,active\n1003,2023/01/25,Deposit,,USD,Active\n1004,15-02-2023,withdrawal,-150,Usd,Inactive\n1005,2023-02-18,Transfer,300,EUR,Active\n1006,2023-2-20,deposit,1000,usd,active\n1007,2023-02-21,WITHDRAWAL,NaN,USD,Active\n1008,02/25/2023,Withdraw,250,USD,Active\n1009,,deposit,400,USD,Active\n1010,2023-02-28,transfer,150,eur,Inactive\n1011,2023-03-01,deposit,350,USD,active\n1012,2023-03-03,withdrawal,100,USD,Active\n1013,03/05/2023,Transfer,NaN,USD,Active\n1014,2023-03-07,Deposit,450,USD,active", "eda_steps": ["Check the percentage of missing values per column", "Standardize the Transaction_Date format to YYYY-MM-DD", "Normalize capitalization in Transaction_Type, Currency, and Account_Status", "Compute descriptive statistics (mean, median) for Amount", "Count unique values and frequencies for Transaction_Type and Account_Status", "Identify and count negative and missing amounts", "Evaluate currency distribution", "Calculate correlation between Amount and Account_Status encoded as binary", "Summarize transaction counts per Account_Status"], "eda_results": {"missing_values": {"Account_ID": "0%", "Transaction_Date": "7.1%", "Transaction_Type": "0%", "Amount": "14.3%", "Currency": "0%", "Account_Status": "0%"}, "standardized_dates": ["2023-01-15", "2023-01-20", "2023-01-25", "2023-02-15", "2023-02-18", "2023-02-20", "2023-02-21", "2023-02-25", null, "2023-02-28", "2023-03-01", "2023-03-03", "2023-03-05", "2023-03-07"], "normalized_categories": {"Transaction_Type": {"deposit": 5, "withdrawal": 5, "transfer": 3}, "Currency": {"USD": 11, "EUR": 2}, "Account_Status": {"active": 11, "inactive": 2}}, "amount_statistics": {"mean": 346.67, "median": 350, "negative_amounts": 1, "missing_amounts": 2}, "currency_distribution": {"USD": 11, "EUR": 2}, "correlations": {"Amount_vs_Account_Status": 0.12}, "transaction_counts_per_account_status": {"active": 11, "inactive": 2}}}
{"purpose": "Analyze student performance and attendance patterns in a high school class.", "raw_table": "StudentID,Name,Grade,Math_Score,English_Score,Attendance_Percentage,Enrollment_Date,Status\n101,alice,10,88,92,95,2022-09-01,active\n102,Bob,10,76,81,,9/3/2022,Active\n103,CHARLIE,11,89,missing,85,2022/09/05,active\n104,Diana,10,92,88,98,2022-09-03,Active\n105,edward,11,55,60,70,Sept 7 2022,inactive\n106,Felicity,10,83,79,90,2022-9-01,Active\n107,George,12,missing,85,88,2022-09-02,Active\n108,Helen,11,74,78,missing,2022-09-08,Inactive\n109,ian,12,91,93,96,09-05-2022,ACTIVE\n110,Julia,10,85,missing,89,2022-09-04,active", "eda_steps": ["Check the percentage of missing values per column", "Standardize the capitalization in the 'Name' and 'Status' columns", "Convert 'Enrollment_Date' to a consistent date format", "Compute descriptive statistics for Math_Score and English_Score", "Generate value counts for 'Grade' and 'Status'", "Calculate the correlation between Math_Score and English_Score", "Identify the top 3 students by Math_Score", "Analyze attendance percentage distribution", "Summarize student counts per enrollment date"], "eda_results": {"missing_values": {"StudentID": 0, "Name": 0, "Grade": 0, "Math_Score": 2, "English_Score": 3, "Attendance_Percentage": 2, "Enrollment_Date": 0, "Status": 0}, "standardized_categories": {"Status": {"active": 7, "inactive": 2}, "Name": ["Alice", "Bob", "Charlie", "Diana", "Edward", "Felicity", "George", "Helen", "Ian", "Julia"]}, "enrollment_dates_converted": ["2022-09-01", "2022-09-03", "2022-09-05", "2022-09-03", "2022-09-07", "2022-09-01", "2022-09-02", "2022-09-08", "2022-09-05", "2022-09-04"], "summary_stats": {"Math_Score": {"count": 8, "mean": 81.75, "std": 12.51, "min": 55, "25%": 76, "50%": 85, "75%": 89, "max": 92}, "English_Score": {"count": 7, "mean": 82.57, "std": 11.67, "min": 60, "25%": 78, "50%": 85, "75%": 88, "max": 93}}, "value_counts": {"Grade": {"10": 5, "11": 3, "12": 2}, "Status": {"active": 7, "inactive": 2}}, "correlations": {"Math_Score_vs_English_Score": 0.82}, "top_categories": {"Top_3_Math_Scores": [{"StudentID": 104, "Name": "Diana", "Math_Score": 92}, {"StudentID": 109, "Name": "Ian", "Math_Score": 91}, {"StudentID": 103, "Name": "Charlie", "Math_Score": 89}]}, "attendance_distribution": {"mean": 89.13, "median": 90, "min": 70, "max": 98, "missing_count": 2}, "enrollment_date_counts": {"2022-09-01": 2, "2022-09-02": 1, "2022-09-03": 2, "2022-09-04": 1, "2022-09-05": 2, "2022-09-07": 1, "2022-09-08": 1}}}
{"purpose": "Explore user engagement patterns and content types on the social media platform.", "raw_table": "user_id,post_date,post_type,likes,comments,shares\nU001,2023-03-15,Photo,120,15,5\nU002,15/03/2023,video,300,45,20\nU003,2023/03/16,text,85,,2\nU004,03-17-2023,Photo,,10,3\nU005,2023-3-18,Live,250,30,N/A\nu006,2023-03-18,Video,400,60,25\nU007,2023/03/19,text,50,5,1\nU008,19-03-2023,photo,90,8,4\nU009,,Video,330,40,18\nU010,2023-03-20,Text,60,7,2\nU011,2023-03-20,live,200,,15\nU012,2023-3-21,Unknown,75,12,3\nU013,03/22/2023,Photo,110,14,6\nU014,2023-03-22,video,350,50,22", "eda_steps": ["Standardize the date format in the post_date column", "Identify and count missing values in each column", "Normalize the capitalization in user_id and post_type columns", "Calculate descriptive statistics for likes, comments, and shares", "Generate frequency counts for the post_type categories", "Find the top 3 post types by average likes", "Check for any unusual or unknown post_type categories", "Analyze correlation between likes, comments, and shares"], "eda_results": {"missing_values": {"user_id": 0, "post_date": 1, "post_type": 1, "likes": 1, "comments": 2, "shares": 1}, "value_counts_post_type": {"photo": 4, "video": 4, "text": 3, "live": 2, "unknown": 1}, "top_post_types_by_avg_likes": {"video": 345, "live": 225, "photo": 103.75}, "descriptive_stats": {"likes": {"count": 14, "mean": 170.71, "std": 118.25, "min": 50, "25%": 90, "50%": 110, "75%": 300, "max": 400}, "comments": {"count": 12, "mean": 22.75, "std": 18.04, "min": 5, "25%": 8, "50%": 14, "75%": 45, "max": 60}, "shares": {"count": 13, "mean": 10.92, "std": 8.16, "min": 1, "25%": 3, "50%": 5, "75%": 18, "max": 25}}, "correlations": {"likes_comments": 0.95, "likes_shares": 0.92, "comments_shares": 0.89}, "unusual_post_types": ["unknown"]}}
{"purpose": "Analyze customer purchase behaviors and product popularity in an ecommerce store.", "raw_table": "OrderID,CustomerID,ProductCategory,PurchaseDate,Quantity,Price,PaymentMethod,CustomerRegion\n1001,C001,electronics,2023-01-05,2,199.99,Credit Card,North\n1002,C002,Apparel,01/10/2023,1,29.99,PayPal,South\n1003,C003,home & kitchen,2023/01/12,,49.95,CREDIT CARD,East\n1004,C001,Electronics,2023-01-15,1,199.99,Credit card,North\n1005,C004,Apparel,15-Jan-2023,3,NaN,PayPal,west\n1006,C005,Toys,2023-1-18,2,15.5,Debit Card,East\n1007,C006,Home & Kitchen,2023/01/20,1,55.0,credit card,South\n1008,C007,apparel,2023-01-22,NaN,35.0,PayPal,South\n1009,C008,Electronics,2023-01-25,1,199.99,Debit card,North\n1010,C009,Toys,2023-01-27,4,15.5,PayPal,East\n1011,C010,Home & kitchen,2023-01-30,2,50.0,Credit Card,south\n1012,,Electronics,2023-02-01,1,199.99,Credit Card,North\n1013,C011,Apparel,2023/02/03,,39.99,Paypal,South", "eda_steps": ["Check the number of missing values in each column", "Standardize the ProductCategory values to consistent capitalization", "Convert PurchaseDate to a uniform date format", "Calculate summary statistics for Quantity and Price columns", "Generate value counts for PaymentMethod and CustomerRegion", "Identify orders with missing Quantity or Price and count them", "Find the top 3 most purchased ProductCategory by total Quantity", "Compute correlation between Quantity and Price", "Summarize the distribution of PurchaseDate over time"], "eda_results": {"missing_values": {"OrderID": 0, "CustomerID": 1, "ProductCategory": 0, "PurchaseDate": 0, "Quantity": 3, "Price": 1, "PaymentMethod": 0, "CustomerRegion": 0}, "standardized_product_categories": {"Electronics": 5, "Apparel": 4, "Home & Kitchen": 3, "Toys": 2}, "purchase_date_range": {"min": "2023-01-05", "max": "2023-02-03"}, "summary_stats": {"Quantity": {"count": 12, "mean": 1.75, "std": 1.05, "min": 1, "25%": 1, "50%": 1.5, "75%": 2, "max": 4}, "Price": {"count": 14, "mean": 78.46, "std": 81.24, "min": 15.5, "25%": 29.99, "50%": 49.98, "75%": 199.99, "max": 199.99}}, "value_counts_payment_method": {"Credit Card": 5, "PayPal": 5, "Debit Card": 2}, "value_counts_customer_region": {"North": 5, "South": 5, "East": 3, "West": 1}, "orders_missing_quantity_or_price": 4, "top_3_product_categories_by_quantity": {"Apparel": 8, "Electronics": 5, "Home & Kitchen": 3}, "correlation_quantity_price": 0.42, "purchase_date_distribution": {"2023-01": 11, "2023-02": 3}}}
{"purpose": "Analyze user engagement patterns and content types on a social media platform.", "raw_table": "post_id,user_id,post_date,likes,comments,shares,post_type\n1,U1001,2023-01-05,15,2,,Photo\n2,u1002,01/07/2023,20,,5,video\n3,U1003,2023-1-08,8,1,0,Status\n4,,2023-01-09,12,3,1,Photo\n5,U1005,01-10-2023,NaN,0,2,link\n6,U1006,2023/01/11,30,5,7,Video\n7,U1007,2023-01-12,25,,3,PHOTO\n8,U1008,2023-13-01,10,1,NaN,status\n9,U1009,2023-01-14,18,2,4,Link\n10,U1010,,22,4,6,video\n11,U1011,2023-01-16,NaN,NaN,NaN,Photo\n12,U1012,2023-01-17,14,3,NaN,Status", "eda_steps": ["Check and summarize missing values in each column", "Standardize 'post_date' format and identify invalid dates", "Compute descriptive statistics for numeric columns (likes, comments, shares)", "Generate value counts for 'post_type' with case normalization", "Identify posts with missing user_id", "Analyze distribution of engagement metrics by post_type", "Check correlations among likes, comments, and shares", "Identify posts with zero or missing engagement values"], "eda_results": {"missing_values": {"post_id": 0, "user_id": 1, "post_date": 1, "likes": 2, "comments": 2, "shares": 3, "post_type": 0}, "invalid_dates": ["2023-13-01"], "summary_stats": {"likes": {"count": 10, "mean": 17.2, "std": 7.39, "min": 8, "25%": 12, "50%": 17, "75%": 22.5, "max": 30}, "comments": {"count": 10, "mean": 2.1, "std": 1.59, "min": 0, "25%": 1, "50%": 2, "75%": 3, "max": 5}, "shares": {"count": 9, "mean": 3.33, "std": 2.37, "min": 0, "25%": 1, "50%": 3, "75%": 5, "max": 7}}, "value_counts_post_type": {"photo": 4, "video": 3, "status": 3, "link": 2}, "missing_user_id_posts": [4], "engagement_by_post_type": {"photo": {"avg_likes": 15.67, "avg_comments": 2.33, "avg_shares": 2}, "video": {"avg_likes": 24, "avg_comments": 3, "avg_shares": 6}, "status": {"avg_likes": 10.67, "avg_comments": 1.67, "avg_shares": 0}, "link": {"avg_likes": 14, "avg_comments": 1, "avg_shares": 3}}, "correlations": {"likes_comments": 0.89, "likes_shares": 0.85, "comments_shares": 0.8}, "posts_with_zero_or_missing_engagement": [3, 5, 8, 11]}}
{"purpose": "Analyze citizen complaints received by a city government department to identify common issues and data quality problems.", "raw_table": "ComplaintID,Date_Received,Department,Complaint_Type,Status,Resolution_Time_Days\n001,2023-01-15,Sanitation,littering,Closed,5\n002,15-Feb-2023,Public Safety,Noise complaint,closed,3\n003,03/10/2023,Transportation,Traffic light malfunction,Open,\n004,2023/03/15,Sanitation,Illegal dumping,Closed,8\n005,2023-04-01,public safety,Noise Complaint,Resolved,4\n006,,Transportation,road pothole,Open,\n007,2023-04-12,Sanitation,LITTERing,Closed,6\n008,2023-04-20,Public Safety,Noise complaint,Open,2\n009,2023-04-21,Transportation,,Open,7\n010,2023-04-22,Sanitation,Illegal dumping,Closed,NA\n011,2023-04-23,Public safety,Noise complaint,Resolved,3", "eda_steps": ["Check the total number of records and identify missing values in each column", "Standardize the 'Department' and 'Complaint_Type' categorical values for consistency", "Calculate descriptive statistics for 'Resolution_Time_Days'", "Generate value counts for 'Status' and 'Department' columns", "Identify the number of unique complaint types and their frequencies", "Analyze the distribution of complaints over the Date_Received field", "Assess correlation between 'Resolution_Time_Days' and complaint 'Status'"], "eda_results": {"missing_values": {"ComplaintID": 0, "Date_Received": 1, "Department": 0, "Complaint_Type": 1, "Status": 0, "Resolution_Time_Days": 3}, "standardized_categories": {"Departments": ["Sanitation", "Public Safety", "Transportation"], "Complaint_Types": ["Littering", "Noise Complaint", "Traffic Light Malfunction", "Illegal Dumping", "Road Pothole"]}, "summary_stats_Resolution_Time_Days": {"count": 8, "mean": 5.125, "std_dev": 1.87, "min": 2, "max": 8}, "value_counts_Status": {"Closed": 4, "Open": 4, "Resolved": 3}, "value_counts_Department": {"Sanitation": 4, "Public Safety": 4, "Transportation": 3}, "complaint_type_frequencies": {"Noise Complaint": 4, "Littering": 2, "Illegal Dumping": 2, "Traffic Light Malfunction": 1, "Road Pothole": 1}, "complaints_over_time": {"2023-01": 1, "2023-02": 1, "2023-03": 2, "2023-04": 6}, "correlation_Resolution_Status": "Resolution_Time_Days tends to be lower for 'Resolved' and higher for 'Closed' complaints; insufficient data for statistical correlation."}}
{"purpose": "Analyze customer purchase behavior and product popularity in a retail store.", "raw_table": "OrderID,CustomerID,ProductCategory,Quantity,Price,OrderDate,PaymentMethod\n1001,C001,Electronics,2,299.99,2023-01-15,Credit Card\n1002,C002,Clothing,1,49.5,15/01/2023,credit card\n1003,,Home Appliances,1,89.99,2023/01/16,CASH\n1004,C004,electronics,3,,2023-1-17,Debit Card\n1005,C002,Clothing,2,39.99,17-01-2023,Debit card\n1006,C005,Toys,4,15.5,01/18/2023,Cash\n1007,C006,Toys,,13.5,2023-01-19,Credit Card\n1008,C007,Books,5,7.99,2023-01-20,creditCard\n1009,C001,Clothing,1,55,20-01-2023,Credit Card\n1010,C008,Electronics,1,199.99,2023/01/21,creditcard\n1011,C009,Books,3,,2023-01-22,CASH\n1012,C010,Home appliances,2,99.99,22/01/2023,Debit Card\n1013,C011,Toys,2,15.5,2023-01-23,Credit card\n1014,C002,Clothing,1,45,2023-01-24,Cash\n", "eda_steps": ["Check for missing values in each column and calculate their percentages", "Standardize the capitalization of the 'ProductCategory' and 'PaymentMethod' columns", "Convert 'OrderDate' to a uniform date format", "Compute descriptive statistics for 'Quantity' and 'Price' columns", "Calculate total sales per 'ProductCategory'", "Generate value counts for 'PaymentMethod'", "Identify orders with missing 'Quantity' or 'Price' and handle them", "Determine the number of unique customers", "Find the most popular product category by quantity sold"], "eda_results": {"missing_values": {"OrderID": 0, "CustomerID": 1, "ProductCategory": 0, "Quantity": 1, "Price": 3, "OrderDate": 0, "PaymentMethod": 0}, "standardized_categories": {"ProductCategory": ["Electronics", "Clothing", "Home Appliances", "Toys", "Books"], "PaymentMethod": ["Credit Card", "Cash", "Debit Card"]}, "order_date_format": "All dates converted to YYYY-MM-DD format", "summary_stats": {"Quantity": {"count": 13, "mean": 2.15, "median": 2, "min": 1, "max": 5}, "Price": {"count": 11, "mean": 86.26, "median": 45, "min": 7.99, "max": 299.99}}, "total_sales_per_category": {"Electronics": 999.96, "Clothing": 270.48, "Home Appliances": 289.97, "Toys": 157.5, "Books": 39.95}, "payment_method_counts": {"Credit Card": 5, "Cash": 3, "Debit Card": 3}, "orders_with_missing_quantity_or_price": [1004, 1007, 1011], "unique_customers": 11, "most_popular_category_by_quantity": "Books"}}
{"purpose": "Analyze housing listings data to understand pricing trends and identify data quality issues.", "raw_table": "ListingID,DateListed,Location,Price,Bedrooms,Bathrooms,SquareFeet,PropertyType\n001,2023-01-15,New york,850000,3,2,1500,Condo\n002,1/22/2023,los Angeles, ,4,3,2500,Single Family\n003,2023/02/10,Chicago,450000,2,1,1000,Condo\n004,02-18-2023,Houston,350000,,2,1200,Townhouse\n005,2023-03-01,NEW YORK,950000,4,3,2000,Condo\n006,3/5/2023,Chicago,,3,,1400,Single family\n007,2023-03-10,los angeles,780000,3,2,1700,Single Family\n008,2023-03-12,Houston,420000,2,2,,Townhouse\n009,03-15-2023,Miami,600000,3,2,1800,Condo\n010,2023-03-18,miami,620000,3,2,1850,Condo\n011,2023-03-20,new york,870000,3,2,1550,Condo\n012,2023-03-22,HOUSTON,400000,2,1,1300,Townhouse\n", "eda_steps": ["Standardize capitalization in 'Location' and 'PropertyType' columns", "Check and report missing values in each column", "Convert 'DateListed' to consistent datetime format", "Compute descriptive statistics for numeric columns: Price, Bedrooms, Bathrooms, SquareFeet", "Generate value counts for 'PropertyType' and 'Location'", "Identify listings with missing or zero values in key numeric fields", "Calculate correlation matrix for numeric columns", "Find the average price per property type"], "eda_results": {"missing_values": {"Price": 2, "Bedrooms": 1, "Bathrooms": 2, "SquareFeet": 1}, "value_counts": {"PropertyType": {"Condo": 6, "Single Family": 3, "Townhouse": 3}, "Location": {"New York": 3, "Los Angeles": 2, "Chicago": 2, "Houston": 3, "Miami": 2}}, "summary_stats": {"Price": {"count": 10, "mean": 626000, "std": 217234, "min": 350000, "25%": 420000, "50%": 610000, "75%": 850000, "max": 950000}, "Bedrooms": {"count": 11, "mean": 2.91, "std": 0.75, "min": 2, "25%": 2, "50%": 3, "75%": 3, "max": 4}, "Bathrooms": {"count": 9, "mean": 2, "std": 0.63, "min": 1, "25%": 2, "50%": 2, "75%": 2, "max": 3}, "SquareFeet": {"count": 11, "mean": 1595, "std": 438, "min": 1000, "25%": 1300, "50%": 1550, "75%": 1800, "max": 2500}}, "correlations": {"Price": {"Bedrooms": 0.89, "Bathrooms": 0.65, "SquareFeet": 0.92}}, "average_price_per_property_type": {"Condo": 706667, "Single Family": 815000, "Townhouse": 390000}}}
{"purpose": "Analyze machine downtime patterns and maintenance requests in a manufacturing plant.", "raw_table": "Machine_ID,Date,Shift,Downtime_Minutes,Operator,Maintenance_Requested\nM001,2023-01-10,morning,45,john doe,yes\nM002,10/01/2023,Afternoon,,JANE SMITH,no\nm003,2023/01/11,Night,30,bob lee,YES\nM004,2023-01-12,Morning,20,alice jones,No\nM005,01-13-2023,afternoon,15,,yes\nM006,2023-01-14,Night,NA,carol king,No\nM007,2023-1-15,Morning,60,John Doe,yes\nM008,2023-01-16,Afternoon,25,jane smith,no\nM009,17/01/2023,Night,10,Bob Lee,No\nM010,2023-01-18,MORNING,35,Alice Jones,YES", "eda_steps": ["Standardize date formats in the Date column", "Convert all Operator names to title case", "Check for missing values in all columns", "Compute descriptive statistics for Downtime_Minutes", "Generate value counts for Shift and Maintenance_Requested columns", "Calculate the percentage of downtime records with maintenance requested", "Identify operators with the highest total downtime", "Analyze downtime distribution by Shift"], "eda_results": {"missing_values": {"Machine_ID": 0, "Date": 0, "Shift": 0, "Downtime_Minutes": 2, "Operator": 1, "Maintenance_Requested": 0}, "summary_stats": {"Downtime_Minutes": {"count": 13, "mean": 31.54, "std": 16.96, "min": 10, "25%": 20, "50%": 30, "75%": 45, "max": 60}}, "value_counts": {"Shift": {"Morning": 4, "Afternoon": 3, "Night": 4}, "Maintenance_Requested": {"Yes": 6, "No": 6}}, "maintenance_percentage": 50, "top_operators_by_downtime": {"John Doe": 105, "Bob Lee": 40, "Alice Jones": 55, "Jane Smith": 25, "Carol King": 0, "": 15}, "downtime_by_shift": {"Morning": {"count": 4, "average_downtime": 40}, "Afternoon": {"count": 3, "average_downtime": 20}, "Night": {"count": 4, "average_downtime": 23.75}}}}
{"purpose": "Analyze viewer ratings and genre popularity for recent movie releases to inform content strategy.", "raw_table": "MovieID,Title,Genre,ReleaseDate,Duration,Rating,Views\n101,StarQuest,SCI-FI,2023-07-15,135,8.7,150000\n102,romantic Escape,Romance,15/06/2023,110,7.2,89000\n103,Dark Horizon,Sci-Fi,2023/07/01,142,,120000\n104,The last laugh,Comedy,2023-07-20,95,6.8,65000\n105,Love & war,romance,07/10/2023,105,7.5,98000\n106,Behind the Mask,Horror,,98,6.3,45000\n107,Ghost Town,Horror,2023-07-05,100,6.7,47000\n108,Comic Relief,comedy,2023-07-18,90,7.1,72000\n109,Space Odyssey,Sci-fi,2023-07-22,150,8.9,160000\n110,Hidden Truth,Thriller,2023-07-11,115,7.8,\n111,midnight run,Thriller,2023/07/13,120,7.4,83000\n112,Endless Summer,Romance,2023-6-25,108,7.0,87000\n113,Fun Times,Comedy,2023/07/17,93,6.9,68000\n", "eda_steps": ["Check missing value percentages for all columns", "Standardize the Genre column capitalization", "Compute descriptive statistics for numeric columns: Duration, Rating, and Views", "Generate value counts for the Genre column", "Identify the top 3 movies by Views", "Calculate the average Rating per Genre", "Examine the distribution of ReleaseDate formats", "Summarize the count of movies released per month"], "eda_results": {"missing_values": {"MovieID": 0, "Title": 0, "Genre": 0, "ReleaseDate": 1, "Duration": 0, "Rating": 1, "Views": 1}, "genre_value_counts": {"Sci-Fi": 3, "Romance": 3, "Comedy": 3, "Horror": 2, "Thriller": 2}, "numeric_summary": {"Duration": {"count": 13, "mean": 111.5, "min": 90, "max": 150}, "Rating": {"count": 12, "mean": 7.3, "min": 6.3, "max": 8.9}, "Views": {"count": 12, "mean": 95042, "min": 45000, "max": 160000}}, "top_3_movies_by_views": [{"Title": "Space Odyssey", "Views": 160000}, {"Title": "StarQuest", "Views": 150000}, {"Title": "Dark Horizon", "Views": 120000}], "average_rating_per_genre": {"Sci-Fi": 8.43, "Romance": 7.23, "Comedy": 6.93, "Horror": 6.5, "Thriller": 7.6}, "release_date_formats": {"YYYY-MM-DD": 7, "DD/MM/YYYY": 1, "YYYY/MM/DD": 3, "MM/DD/YYYY": 0, "YYYY-M-D": 1, "missing": 1}, "movies_per_month": {"June 2023": 2, "July 2023": 10, "October 2023": 1, "missing": 1}}}
{"purpose": "Analyze patient demographic and clinical characteristics for a cardiology outpatient dataset.", "raw_table": "PatientID,Age,Gender,VisitDate,Diagnosis,BloodPressure,Cholesterol_mg_dl,Smoker\n001,45,Male,2023/01/15,Hypertension,140/90,200,yes\n002,38,Female,15-02-2023,Diabetes,130/85,NA,No\n003,NA,Male,2023-03-10,hypertension,125/80,180,Yes\n004,50,Female,03/20/2023,Coronary artery disease,150/95,220,No\n005,29,,2023-04-05,Diabetes,NA,210,unknown\n006,67,M,2023/04/25,Heart failure,160/100,230,Yes\n007,55,F,04-30-2023,coronary Artery Disease,145/92,215,No\n008,47,Female,2023.05.10,Hypertension,135/88,190,No\n009,53,Male,05/15/2023,NA,142/90,205,yes\n010,60,Male,2023-06-01,Heart failure,155/98,225,No", "eda_steps": ["Check for missing values in each column", "Standardize the Gender column values", "Parse VisitDate column into consistent date format", "Convert BloodPressure into two numeric columns: Systolic and Diastolic", "Compute descriptive statistics for Age, Systolic BP, Diastolic BP, and Cholesterol", "Generate value counts for Diagnosis and Smoker columns", "Identify the number of patients with missing Diagnosis", "Calculate correlation matrix for numeric variables", "Determine top 2 most common diagnoses"], "eda_results": {"missing_values": {"PatientID": 0, "Age": 1, "Gender": 1, "VisitDate": 0, "Diagnosis": 1, "BloodPressure": 1, "Cholesterol_mg_dl": 1, "Smoker": 1}, "standardized_gender": {"Male": 5, "Female": 4, "Unknown": 1}, "parsed_dates": {"earliest_date": "2023-01-15", "latest_date": "2023-06-01"}, "blood_pressure_split": {"Systolic": [140, 130, 125, 150, null, 160, 145, 135, 142, 155], "Diastolic": [90, 85, 80, 95, null, 100, 92, 88, 90, 98]}, "summary_stats": {"Age": {"count": 9, "mean": 49.8, "std": 11.7, "min": 29, "max": 67}, "Systolic": {"count": 9, "mean": 143.1, "std": 11.7, "min": 125, "max": 160}, "Diastolic": {"count": 9, "mean": 90, "std": 6.5, "min": 80, "max": 100}, "Cholesterol_mg_dl": {"count": 9, "mean": 207.8, "std": 16.7, "min": 180, "max": 230}}, "value_counts": {"Diagnosis": {"Hypertension": 3, "Diabetes": 2, "Coronary artery disease": 2, "Heart failure": 2, "NA": 1}, "Smoker": {"yes": 3, "No": 4, "unknown": 1, "NA": 2}}, "missing_diagnosis_count": 1, "correlations": {"Age-Systolic": 0.78, "Age-Diastolic": 0.65, "Age-Cholesterol_mg_dl": 0.72, "Systolic-Diastolic": 0.95, "Systolic-Cholesterol_mg_dl": 0.6, "Diastolic-Cholesterol_mg_dl": 0.55}, "top_diagnoses": {"Hypertension": 3, "Diabetes": 2}}}
{"purpose": "Explore housing market characteristics and data quality issues in a recent property sales dataset.", "raw_table": "Property_ID,Sale_Date,Location,Property_Type,Bedrooms,Bathrooms,Square_Feet,Price,Agent\n001,2023-01-15,Downtown,Condo,2,1,850,350000,John Smith\n002,15/02/2023,Suburb,Single Family,3,2,1200,450000,jane doe\n003,2023/03/10,DOWNTOWN,Condo,2,,900,360000,John Smith\n004,2023-03-25,Suburb,Townhouse,3,2,1100,,Jane Doe\n005,2023-04-05,Riverside,Single Family,4,3,1600,600000,John Smith\n006,April 15 2023,Riverside,Single Family,4,3,1650,620000,Unknown\n007,2023-05-01,suburb,Townhouse,3,2,1150,480000,jane doe\n008,2023-05-20,Downtown,Condo,2,1,870,355000,John smith\n009,2023-06-10,Riverside,Single Family,,3,1700,610000,John Smith\n010,2023-06-15,Downtown,Condo,2,1,850,345000,John Smith\n011,2023-07-01,Suburb,Single Family,3,Two,1250,470000,Jane Doe\n012,,Riverside,Single Family,4,3,1600,605000,John Smith", "eda_steps": ["Check missing values percentage for each column", "Standardize the date format in Sale_Date column", "Compute descriptive statistics for numeric columns: Bedrooms, Bathrooms, Square_Feet, Price", "Generate value counts for categorical columns: Location, Property_Type, Agent", "Identify inconsistent capitalization in Location and Agent columns and count distinct values before and after correction", "Analyze the distribution of prices by Property_Type", "Check for unusual or mixed type values in Bathrooms column", "Compute correlation matrix between numeric features", "Identify top 3 agents by number of sales", "Summarize number of sales per month"], "eda_results": {"missing_values": {"Property_ID": 0, "Sale_Date": 1, "Location": 0, "Property_Type": 0, "Bedrooms": 1, "Bathrooms": 1, "Square_Feet": 0, "Price": 1, "Agent": 1}, "date_format_standardized": true, "summary_stats": {"Bedrooms": {"count": 11, "mean": 3.0, "min": 2, "max": 4, "missing": 1}, "Bathrooms": {"count": 10, "mean": 2.1, "min": 1, "max": 3, "missing": 1, "mixed_types_detected": true}, "Square_Feet": {"count": 12, "mean": 1258, "min": 850, "max": 1700}, "Price": {"count": 11, "mean": 481364, "min": 345000, "max": 620000, "missing": 1}}, "value_counts": {"Location_before_standardization": {"Downtown": 3, "DOWNTOWN": 1, "Suburb": 3, "suburb": 1, "Riverside": 4}, "Location_after_standardization": {"Downtown": 4, "Suburb": 4, "Riverside": 4}, "Property_Type": {"Condo": 4, "Single Family": 6, "Townhouse": 2}, "Agent_before_standardization": {"John Smith": 5, "John smith": 1, "jane doe": 3, "Jane Doe": 2, "Unknown": 1}, "Agent_after_standardization": {"John Smith": 6, "Jane Doe": 5, "Unknown": 1}}, "price_distribution_by_property_type": {"Condo": {"mean_price": 352500, "count": 4}, "Single Family": {"mean_price": 565000, "count": 6}, "Townhouse": {"mean_price": 480000, "count": 2}}, "bathrooms_mixed_types": {"non_numeric": ["Two"], "count_non_numeric": 1}, "correlations": {"Bedrooms-Square_Feet": 0.94, "Bedrooms-Price": 0.85, "Square_Feet-Price": 0.88, "Bathrooms-Price": 0.79}, "top_agents_by_sales": {"John Smith": 6, "Jane Doe": 5, "Unknown": 1}, "sales_per_month": {"January 2023": 1, "February 2023": 1, "March 2023": 2, "April 2023": 2, "May 2023": 2, "June 2023": 2, "July 2023": 1, "Missing Date": 1}}}
{"purpose": "Analyze monthly temperature and precipitation patterns across different climate zones to identify data quality issues and distribution characteristics.", "raw_table": "Date,Location,Temperature_C,Precipitation_mm,Climate_Zone\n2023-01-15,North Pole, -25.3, 5.2, polar\n15/02/2023,Equator, 30.5, 120, tropical\n2023-03-10,Midwest, 12, , temperate\n2023-04-05,Equator, 31, 85.7, Tropical\n2023-05-20,midwest, 18.4, 50.1, Temperate\n06-15-2023,North pole,-20.1,3.3,Polar\n2023/07/18,Desert, 42,0, Arid\n2023-8-25,Desert, 45.6,0.0,arid\n2023-09-10,Equator,,90.4, tropical\n2023-10-12,Midwest, 8.2, 60, Temperate\n2023-11-30,North Pole,-30, 2, polar\n2023-12-05,desert,43.5, , Arid", "eda_steps": ["Check and report missing values per column", "Standardize date formats to YYYY-MM-DD", "Normalize capitalization in 'Location' and 'Climate_Zone' columns", "Compute descriptive statistics for Temperature_C and Precipitation_mm", "Generate value counts for Climate_Zone", "Identify rows with missing Temperature_C or Precipitation_mm values", "Calculate correlation between Temperature_C and Precipitation_mm", "Summarize temperature distribution skewness"], "eda_results": {"missing_values": {"Date": 0, "Location": 0, "Temperature_C": 1, "Precipitation_mm": 2, "Climate_Zone": 0}, "date_format_standardized": true, "normalized_categories": {"Location": ["North Pole", "Equator", "Midwest", "Desert"], "Climate_Zone": ["Polar", "Tropical", "Temperate", "Arid"]}, "summary_stats": {"Temperature_C": {"count": 11, "mean": 18.54, "std": 22.94, "min": -30, "25%": -20.1, "50%": 12, "75%": 42, "max": 45.6}, "Precipitation_mm": {"count": 10, "mean": 41.59, "std": 43.28, "min": 0, "25%": 3.3, "50%": 50.1, "75%": 85.7, "max": 120}}, "value_counts": {"Climate_Zone": {"Polar": 3, "Tropical": 3, "Temperate": 3, "Arid": 3}}, "rows_with_missing": {"Temperature_C": [8], "Precipitation_mm": [2, 11]}, "correlations": {"Temperature_C_vs_Precipitation_mm": -0.12}, "skewness": {"Temperature_C": 0.55}}}
{"purpose": "Explore recent temperature and precipitation data to identify patterns and data quality issues in regional climate measurements.", "raw_table": "Region,Date,Avg_Temperature_C,Precipitation_mm,Weather_Condition\nNorth,2023-01-01,3.5,12.7,Rainy\nsouth,01/02/2023,15.2,,Sunny\nEast,2023-01-03,,5.6,cloudy\nWEST,2023-1-04,7.8,0.0,Snow\nNorth,2023/01/05,4.1,8.3,Rainy\nSouth,2023-01-06,14.8,missing,sunny\nEast,2023-01-07,6.2,3.1,Cloudy\nwest,2023/01/08,8.0,0.0,sNOW\nNorth,2023-01-09,3.9,11.0,Rainy\nSouth,,16.1,0.0,Sunny\nEast,2023-01-11,6.5,4.0,Cloudy\nWest,2023-01-12,missing,0.0,Snow\nNorth,2023-01-13,4.0,9.5,Rainy", "eda_steps": ["Check for missing values in each column", "Standardize the 'Region' and 'Weather_Condition' categorical columns", "Parse and standardize date formats in the 'Date' column", "Compute descriptive statistics for 'Avg_Temperature_C' and 'Precipitation_mm'", "Generate value counts for 'Region' and 'Weather_Condition'", "Identify rows with inconsistent or missing numeric data", "Visualize distribution skewness for temperature and precipitation"], "eda_results": {"missing_values": {"Region": 1, "Date": 1, "Avg_Temperature_C": 2, "Precipitation_mm": 2, "Weather_Condition": 0}, "standardized_categories": {"Region": ["North", "South", "East", "West"], "Weather_Condition": ["Rainy", "Sunny", "Cloudy", "Snow"]}, "date_parse_issues": 0, "summary_stats": {"Avg_Temperature_C": {"count": 11, "mean": 7.8, "std_dev": 5.1, "min": 3.5, "max": 16.1}, "Precipitation_mm": {"count": 11, "mean": 5.2, "std_dev": 5.6, "min": 0.0, "max": 12.7}}, "value_counts": {"Region": {"North": 4, "South": 3, "East": 3, "West": 3}, "Weather_Condition": {"Rainy": 4, "Sunny": 3, "Cloudy": 3, "Snow": 3}}, "rows_with_inconsistent_numeric": 3, "distribution_skewness": {"Avg_Temperature_C": 0.75, "Precipitation_mm": 1.1}}}
{"purpose": "Analyze ride-sharing trip data to understand usage patterns, identify data quality issues, and summarize key metrics.", "raw_table": "trip_id,driver_id,passenger_id,start_time,end_time,distance_km,vehicle_type,payment_method,rating\nT001,D101,P5001,2024/04/01 08:15,2024-04-01 08:45,12.5,Sedan,Credit Card,4.5\nT002,D102,P5002,2024-04-01 09:00,2024-04-01 09:35,15.0,SUV,cash,4\nT003,D103,P5003,04-01-2024 10:20,2024-04-01 10:50,NaN,Sedan,CREDIT card,5\nT004,D104,P5004,2024/04/01 11:10,11:45 2024-04-01,8.7,Sedan,Paypal,4\nT005,D105,P5005,2024-4-01 13:00,2024/04/01 13:30,10.2,sedan,Cash,NaN\nT006,D106,P5006,2024-04-01 14:15,2024-04-01 14:40,7.5,Convertible,Credit Card,3.5\nT007,D107,P5007,2024-04-01 15:00,2024-04-01 15:25,9.0,SUV,credit card,4\nT008,D108,P5008,2024-04-01 16:30,,11.3,Sedan,Cash,NaN\nT009,D109,P5009,2024-04-01 17:00,2024-04-01 17:35,14.1,SUV,Paypal,5\nT010,D110,P5010,,2024-04-01 18:10,6.8,Sedan,Credit Card,4", "eda_steps": ["Check for missing values in all columns", "Standardize the 'payment_method' column to consistent capitalization", "Convert 'start_time' and 'end_time' columns to datetime format", "Calculate trip duration in minutes for each trip", "Compute descriptive statistics for 'distance_km' and trip duration", "Generate value counts for 'vehicle_type' and 'payment_method' columns", "Identify trips with missing or inconsistent timestamps", "Calculate average rating grouped by 'vehicle_type'", "Summarize number of trips per driver"], "eda_results": {"missing_values": {"trip_id": 0, "driver_id": 0, "passenger_id": 0, "start_time": 1, "end_time": 1, "distance_km": 1, "vehicle_type": 0, "payment_method": 0, "rating": 2}, "payment_method_standardized": {"Credit Card": 5, "Cash": 3, "Paypal": 2}, "trip_duration_minutes": {"T001": 30, "T002": 35, "T003": 30, "T004": 35, "T005": 30, "T006": 25, "T007": 25, "T008": null, "T009": 35, "T010": null}, "descriptive_statistics": {"distance_km": {"count": 9, "mean": 10.23, "std": 2.88, "min": 6.8, "25%": 8.7, "50%": 10.2, "75%": 12.5, "max": 15.0}, "trip_duration_minutes": {"count": 8, "mean": 30.625, "std": 3.54, "min": 25, "25%": 25, "50%": 30, "75%": 35, "max": 35}}, "value_counts": {"vehicle_type": {"Sedan": 5, "SUV": 3, "Convertible": 1, "sedan": 1}}, "trips_with_timestamp_issues": ["T008", "T010"], "average_rating_by_vehicle_type": {"Sedan": 4.25, "SUV": 4.67, "Convertible": 3.5, "sedan": null}, "trips_per_driver": {"D101": 1, "D102": 1, "D103": 1, "D104": 1, "D105": 1, "D106": 1, "D107": 1, "D108": 1, "D109": 1, "D110": 1}}}
{"purpose": "Analyze customer loan application data to identify patterns in loan approval and detect data quality issues.", "raw_table": "CustomerID,ApplicationDate,LoanAmount,LoanType,ApprovalStatus,Income,CreditScore\n001,2023-01-15,25000,Personal,approved,55000,700\n002,15/02/2023,150000,Mortgage,Denied,120000,680\n003,03-05-2023,NaN,Auto,approved,NaN,720\n004,2023/06/10,5000,personal,approved,45000,NaN\n005,2023-07-20,70000,MORTGAGE,Denied,,650\n006,07-25-2023,10000,Auto,Approved,52000,710\n007,2023-08-05,NaN,Personal,Pending,48000,690\n008,2023-09-01,30000,Personal,approved,53000,705\n009,2023-09-15,20000,auto,Denied,49000,NaN\n010,,40000,Mortgage,approved,110000,720\n011,2023-10-10,25000,Business,Denied,75000,700\n012,10/11/2023,15000,personal,approved,54000,705\n013,2023-12-01,NaN,Auto,Denied,47000,695\n014,2023-12-15,35000,PERSONAL,Approved,60000,710\n", "eda_steps": ["Check and summarize missing values for each column", "Standardize the capitalization in LoanType and ApprovalStatus columns", "Parse and standardize ApplicationDate to a consistent date format", "Compute descriptive statistics for numeric columns LoanAmount, Income, and CreditScore", "Generate value counts for LoanType and ApprovalStatus columns", "Calculate correlation matrix for the numeric columns", "Identify top 2 most common LoanTypes", "Summarize distribution skewness for LoanAmount and Income"], "eda_results": {"missing_values": {"ApplicationDate": 1, "LoanAmount": 4, "Income": 2, "CreditScore": 3}, "standardized_categories": {"LoanType": ["Personal", "Mortgage", "Auto", "Business"], "ApprovalStatus": ["Approved", "Denied", "Pending"]}, "application_date_range": {"min": "2023-01-15", "max": "2023-12-15"}, "summary_stats": {"LoanAmount": {"count": 10, "mean": 41600, "std": 44176.06, "min": 5000, "25%": 15000, "50%": 25000, "75%": 35000, "max": 150000}, "Income": {"count": 12, "mean": 64583.33, "std": 23461.92, "min": 45000, "25%": 49500, "50%": 54000, "75%": 75000, "max": 120000}, "CreditScore": {"count": 11, "mean": 697.27, "std": 17.48, "min": 650, "25%": 690, "50%": 700, "75%": 710, "max": 720}}, "value_counts": {"LoanType": {"Personal": 6, "Mortgage": 3, "Auto": 4, "Business": 1}, "ApprovalStatus": {"Approved": 7, "Denied": 5, "Pending": 1}}, "correlations": {"LoanAmount-Income": 0.72, "LoanAmount-CreditScore": 0.15, "Income-CreditScore": 0.05}, "top_categories": {"LoanType": ["Personal", "Auto"]}, "distribution_skewness": {"LoanAmount": 2.3, "Income": 0.8}}}
{"purpose": "Analyze city government employee demographics and department distribution to identify data quality issues and workforce composition.", "raw_table": "Employee_ID,Name,Department,Date_Joined,Salary,Status\nE001,John Smith,Public Works,2015-06-01,55000,Active\nE002,mary jones,Health,03/12/2018,62000,active\nE003,RAJ PATEL,Education,2017/08/15,58000,Active\nE004,Linda,Public works,2019-11-23,NaN,Inactive\nE005,James Lee,Health,2016-05-30,60000,Active\nE006,Susan O'Neil,Education,2018/07/22,59000,\nE007,Tom Clark,Public Works,2015-13-01,56000,Active\nE008,Ana G\u00f3mez,Transportation,2019-02-28,57000,active\nE009,,Health,2020-01-15,63000,Active\nE010,Mike Brown,transportation,2017-03-12,55000,Inactive\nE011,Patricia,K-12 Education,2016-04-20,NaN,Active\nE012,George Hill,Health,,61000,active\nE013,Sarah Connor,Public Works,2018-09-09,60500,Active", "eda_steps": ["Check for missing values in each column", "Standardize the 'Department' column to consistent capitalization and naming", "Parse 'Date_Joined' column into a standard date format and identify invalid dates", "Calculate descriptive statistics for 'Salary' including mean, median, and count of missing", "Generate value counts for 'Status' and identify inconsistent capitalization", "Identify rows with missing or invalid 'Name' values", "Summarize the number of employees per standard department", "Check for duplicate Employee_IDs"], "eda_results": {"missing_values": {"Employee_ID": 0, "Name": 1, "Department": 0, "Date_Joined": 2, "Salary": 2, "Status": 1}, "department_standardization": {"Public Works": 4, "Health": 4, "Education": 2, "Transportation": 2, "K-12 Education": 1}, "invalid_dates": ["E007: 2015-13-01", "E012: missing"], "salary_stats": {"count": 11, "mean": 59090.91, "median": 59000, "missing": 2}, "status_value_counts": {"Active": 7, "Inactive": 2, "active": 3, "": 1}, "missing_or_invalid_names": ["E009"], "employees_per_department": {"Public Works": 4, "Health": 4, "Education": 2, "Transportation": 2, "K-12 Education": 1}, "duplicate_employee_ids": 0}}
{"purpose": "Analyze monthly sales performance and product category distribution for retail items.", "raw_table": "OrderID,ProductName,Category,Quantity,SaleDate,UnitPrice,TotalPrice\n1001,Wireless Mouse,Electronics,2,2023/01/15,25.50,51\n1002,office CHair,Furniture,1,15-01-2023,85.00,85\n1003,Notebook,stationery,5,2023-01-16,2.50,12.5\n1004,Pen,STATIONERY,,2023-01-16,1.20,6\n1005,Desk Lamp,Electronics,1,01/17/2023,45,45\n1006,Wireless mouse,Electronics,3,2023/01/18,25.50,76.5\n1007,Desk, Furniture ,1,2023-01-19,150.00,150\n1008,Pen,Stationery,10,2023/01/19,,12\n1009,Monitor,Electronics,1,2023.01.20,200,200\n1010,Office chair,Furniture,2,2023/01/21,85,170\n1011,Notebook,Stationery,3,2023/01/22,2.5,\n1012,Lamp Desk,Electronics,1,17-01-2023,45,45\n1013,wireless Mouse,Electronics,,2023/01/23,25.50,25.5", "eda_steps": ["Check missing value percentages for all columns", "Standardize and unify the Category column capitalization and spacing", "Calculate descriptive statistics for Quantity, UnitPrice, and TotalPrice columns", "Identify unique ProductName values and count their frequency", "Parse the SaleDate column into a consistent date format", "Compute total sales (sum of TotalPrice) by Category", "Find correlation between Quantity, UnitPrice, and TotalPrice", "Identify rows with inconsistent or missing TotalPrice and recalculate if possible"], "eda_results": {"missing_values": {"OrderID": 0, "ProductName": 0, "Category": 0, "Quantity": 3, "SaleDate": 0, "UnitPrice": 2, "TotalPrice": 2}, "category_value_counts": {"Electronics": 7, "Furniture": 3, "Stationery": 5}, "descriptive_statistics": {"Quantity": {"count": 12, "mean": 2.42, "std": 2.47, "min": 1, "25%": 1, "50%": 2, "75%": 3, "max": 10}, "UnitPrice": {"count": 13, "mean": 56.04, "std": 67.45, "min": 1.2, "25%": 2.5, "50%": 25.5, "75%": 85, "max": 200}, "TotalPrice": {"count": 13, "mean": 74.46, "std": 65.83, "min": 6, "25%": 12, "50%": 45, "75%": 85, "max": 200}}, "unique_product_counts": {"Wireless Mouse": 3, "Office Chair": 2, "Notebook": 2, "Pen": 2, "Desk Lamp": 1, "Desk": 1, "Monitor": 1, "Lamp Desk": 1}, "total_sales_by_category": {"Electronics": 438, "Furniture": 405, "Stationery": 30.5}, "correlations": {"Quantity_UnitPrice": 0.12, "Quantity_TotalPrice": 0.98, "UnitPrice_TotalPrice": 0.87}, "recalculated_rows": [{"OrderID": 4, "Recalculated_TotalPrice": 7.2}, {"OrderID": 11, "Recalculated_TotalPrice": 7.5}, {"OrderID": 13, "Recalculated_TotalPrice": null}]}}
{"purpose": "Examine crop yield patterns and identify data quality issues in farm records.", "raw_table": "Farm_ID,Crop,Planting_Date,Harvest_Date,Yield_tons,Soil_Type,Fertilizer_Used\n001,Wheat,2023-03-15,2023/08/20,4.5,Loamy,Yes\n002,corn,15-04-2023,2023-09-15,5.2,SANDY,YES\n003,Rice,2023/05/01,2023-10-10,,Clay,No\n004,Barley,2023-03-28,2023-08-25,3.8,loamY,Yes\n005,wheat,Mar 20 2023,2023-08-22,4.9,Clay,yes\n006,corn,2023-04-12,Sep 18 2023,5.0,sandy,No\n007,Rice,2023-05-03,2023-10-12,6.1,CLAY,YES\n008,barley,2023/03/30,2023-08-28,3.5,Loamy,No\n009,,2023-04-10,2023-09-20,4.7,Sandy,yes\n010,Wheat,2023-03-18,2023-08-23,4.6,loamy,yes", "eda_steps": ["Check for missing values in each column", "Standardize the Crop names to title case", "Standardize Soil_Type categories to consistent lowercase values", "Convert Planting_Date and Harvest_Date to datetime format", "Calculate the duration in days between Planting_Date and Harvest_Date", "Compute descriptive statistics for Yield_tons", "Generate value counts for Fertilizer_Used", "Identify rows with missing Crop names or Yield values"], "eda_results": {"missing_values": {"Farm_ID": 0, "Crop": 1, "Planting_Date": 0, "Harvest_Date": 0, "Yield_tons": 1, "Soil_Type": 0, "Fertilizer_Used": 0}, "standardized_crops": {"Wheat": 3, "Corn": 2, "Rice": 2, "Barley": 2, "": 1}, "standardized_soil_types": {"loamy": 4, "sandy": 3, "clay": 3}, "date_conversion_issues": 0, "planting_to_harvest_days": {"mean": 158, "min": 147, "max": 161, "std_dev": 5.1}, "yield_statistics": {"count": 9, "mean": 4.81, "median": 4.7, "min": 3.5, "max": 6.1, "std_dev": 0.76}, "fertilizer_usage_counts": {"Yes": 6, "No": 3}, "incomplete_rows": [3, 9]}}
{"purpose": "Examine patient demographics and vital signs to identify data quality issues and summarize key health indicators in a small clinic dataset.", "raw_table": "PatientID,Age,Gender,Weight_kg,Blood_Pressure,Symptom_Onset,Diagnosis\n001,45,Male,82,120/80,2023-1-05,Hypertension\n002,37,female,NA,130/85,5-Jan-2023,Diabetes\n003,29,M,70,118/77,2023/01/07,hypertension\n004,,F,65,125/88,,Diabetes\n005,52,Female,90,140/90,01-08-2023,Unknown\n006,48,male,85,135/85,2023-01-09,Hypertension\n007,33,m,NA,110/70,2023-01-09,\n008,40,Female,78,128/82,07 Jan 2023,Diabetes\n009,NA,Male,88,138/92,2023-01-10,Hypertension\n010,44,f,82,130/88,2023-1-11,HyperTension", "eda_steps": ["Check for missing values in all columns", "Standardize the Gender column values", "Parse and standardize the Symptom_Onset date format", "Compute descriptive statistics for Age and Weight_kg columns", "Extract systolic and diastolic values from Blood_Pressure and compute their means", "Generate value counts for Diagnosis", "Identify rows with inconsistent or unknown diagnosis entries", "Summarize the distribution of patient ages by gender"], "eda_results": {"missing_values": {"PatientID": 0, "Age": 2, "Gender": 0, "Weight_kg": 2, "Blood_Pressure": 0, "Symptom_Onset": 1, "Diagnosis": 1}, "gender_standardization": {"Male": 5, "Female": 4, "Unknown": 0}, "symptom_onset_dates": {"earliest_date": "2023-01-05", "latest_date": "2023-01-11", "missing_dates_count": 1}, "descriptive_statistics": {"Age": {"count": 8, "mean": 40.0, "min": 29, "max": 52, "std_dev": 7.9}, "Weight_kg": {"count": 8, "mean": 80.0, "min": 65, "max": 90, "std_dev": 8.9}}, "blood_pressure_means": {"systolic_mean": 126.4, "diastolic_mean": 84.4}, "diagnosis_value_counts": {"Hypertension": 4, "Diabetes": 3, "Unknown": 1, "": 1}, "inconsistent_diagnosis_rows": [5, 10], "age_distribution_by_gender": {"Male": {"count": 5, "mean_age": 39.6}, "Female": {"count": 4, "mean_age": 40.5}}}}
{"purpose": "Analyze trading activity and price trends for a set of stocks over a short period.", "raw_table": "TradeID,StockSymbol,TradeDate,TradePrice,Volume,Trader\n1,AAPL,2024-04-01,170.5,1000,alice\n2,GOOGL,2024/04/01,2800.1,500,Bob\n3,msft,4-2-2024,295.3,NaN,charlie\n4,TSLA,2024-04-02,NaN,700,ALICE\n5,amzn,2024-04-03,3300.8,3000,bob\n6,AAPL,2024-04-03,171.0,1200,Charlie\n7,GOOGL,2024-4-03,,600,alice\n8,TSLA,2024-04-04,720.5,800,David\n9,MSFT,2024/04/04,298.0,700,Bob\n10,AMZN,2024-04-04,3320.0,,david\n11,aapl,04-05-2024,172.3,1100,Alice\n12,TSLA,2024-04-05,730.0,NaN,Charlie\n13,GOOGL,2024-4-05,2825.0,550,alice\n14,msft,2024-04-05,299.5,650,Bob", "eda_steps": ["Standardize the StockSymbol column to uppercase", "Convert TradeDate to a consistent date format", "Identify and count missing values in TradePrice and Volume", "Compute descriptive statistics for TradePrice and Volume", "Generate value counts for Trader", "Calculate the average TradePrice per StockSymbol", "Determine total traded Volume per StockSymbol", "Check correlation between TradePrice and Volume", "Identify top 2 Traders by number of trades"], "eda_results": {"missing_values": {"TradePrice": 2, "Volume": 3}, "value_counts_trader": {"ALICE": 4, "BOB": 4, "CHARLIE": 3, "DAVID": 2}, "summary_stats_tradeprice": {"count": 12, "mean": 1144.91, "std": 1256.89, "min": 170.5, "25%": 295.3, "50%": 720.5, "75%": 2825.0, "max": 3320.0}, "summary_stats_volume": {"count": 11, "mean": 962.73, "std": 929.91, "min": 300, "25%": 600, "50%": 700, "75%": 1100, "max": 3000}, "average_tradeprice_per_stock": {"AAPL": 171.27, "GOOGL": 2815.37, "MSFT": 297.6, "TSLA": 725.17, "AMZN": 3310.4}, "total_volume_per_stock": {"AAPL": 3300, "GOOGL": 1650, "MSFT": 2050, "TSLA": 1500, "AMZN": 3000}, "correlation_tradeprice_volume": 0.65, "top_2_traders_by_trades": {"ALICE": 4, "BOB": 4}}}
{"purpose": "Analyze streaming viewership data to understand user engagement and content preferences.", "raw_table": "user_id,show_title,genre,watch_date,watch_time_minutes,rating\n001,Stranger Things,Drama,2023-01-15,55,4.5\n002,the crown,history,15/01/2023,50,5\n003,Breaking Bad,Drama,2023-1-16,60,4.8\n004,Black Mirror,SCIFI,01-17-2023,45,4\n005,F.R.I.E.N.D.S,Comedy,2023/01/18,30,4.7\n006,The crown,History,,55,5\n007,Stranger things,Drama,2023-01-19,NA,4\n008,Black Mirror,SciFi,2023-01-19,48,NA\n009,Breaking bad,drama,01/20/2023,61,4.9\n010,F.R.I.E.N.D.S,comedy,2023-01-20,32,4.6\n011,The Mandalorian,SciFi,2023-01-21,50,4.3\n012,The Mandalorian,Sci Fi,2023/01/21,50,4.2\n013,Stranger Things,Drama,21-01-2023,55,4.7\n014,F.R.I.E.N.D.S,Comedy,2023-01-22,28,4.8", "eda_steps": ["Check missing value percentages in each column", "Standardize the genre column to consistent capitalization", "Parse and unify watch_date into a single date format", "Compute descriptive statistics for watch_time_minutes and rating", "Generate value counts for show_title", "Identify top 3 most watched shows", "Calculate average rating by genre", "Summarize total watch time per user", "Analyze correlation between watch_time_minutes and rating"], "eda_results": {"missing_values": {"user_id": 0, "show_title": 0, "genre": 0, "watch_date": 1, "watch_time_minutes": 1, "rating": 1}, "standardized_genre_value_counts": {"Drama": 5, "History": 2, "SciFi": 4, "Comedy": 4}, "date_format_unified_sample": ["2023-01-15", "2023-01-15", "2023-01-16", "2023-01-17", "2023-01-18"], "summary_stats": {"watch_time_minutes": {"count": 13, "mean": 48.5, "std": 11.1, "min": 28, "25%": 45, "50%": 50, "75%": 55, "max": 61}, "rating": {"count": 13, "mean": 4.59, "std": 0.28, "min": 4, "25%": 4.4, "50%": 4.65, "75%": 4.8, "max": 5}}, "show_title_value_counts": {"Stranger Things": 4, "The Crown": 2, "Breaking Bad": 2, "Black Mirror": 2, "F.R.I.E.N.D.S": 4, "The Mandalorian": 2}, "top_3_most_watched_shows": ["Stranger Things", "F.R.I.E.N.D.S", "Black Mirror"], "average_rating_by_genre": {"Drama": 4.62, "History": 5.0, "SciFi": 4.25, "Comedy": 4.7}, "total_watch_time_per_user": {"001": 55, "002": 50, "003": 60, "004": 45, "005": 30, "006": 55, "007": null, "008": 48, "009": 61, "010": 32, "011": 50, "012": 50, "013": 55, "014": 28}, "correlation_watch_time_rating": 0.48}}
{"purpose": "Analyze viewership patterns and ratings of recent streaming shows to identify popular genres and viewer demographics.", "raw_table": "Show_ID,Title,Genre,Release_Date,Avg_Rating,Num_Reviews,Viewer_Age_Group\n1,The Last Stand,Action,2023-05-12,8.2,1245,25-34\n2,love in paris,Romance,05/18/2023,7.5,,18-24\n3,Haunted Nights,HORROR,2023/06/01,6.8,874,35-44\n4,Galactic Wars,Sci-Fi,2023-5-20,8.7,2150,25-34\n5,Funny Bones,Comedy,2023-04-30,7.9,1090,18-24\n6,The Last Stand,action,2023-05-12,8.3,1300,25-34\n7,Sing Along,Musical,2023-06-10,,450,18-24\n8,The Last Stand,Action,2023-05-12,8.1,1180,NA\n9,Deep Dive,Documentary,2023-05-25,8.5,980,45-54\n10,Haunted Nights,Horror,2023-06-01,6.9,900,35-44\n11,Unknown Territories,Sci-fi,2023/06/15,7.8,670,25-34\n12,Love In Paris,romance,2023-05-18,7.6,500,18-24", "eda_steps": ["Check for missing values in each column", "Standardize Genre capitalization and consolidate similar categories", "Calculate descriptive statistics for Avg_Rating and Num_Reviews", "Count number of unique shows by Title and Show_ID", "Analyze distribution of Viewer_Age_Group", "Identify top 3 genres by average rating", "Examine duplicate entries and their effect on ratings", "Summarize release date formats and convert to a consistent format"], "eda_results": {"missing_values": {"Avg_Rating": 1, "Num_Reviews": 1, "Viewer_Age_Group": 1}, "standardized_genres": {"Action": 3, "Romance": 3, "Horror": 2, "Sci-Fi": 3, "Comedy": 1, "Musical": 1, "Documentary": 1}, "descriptive_stats": {"Avg_Rating": {"count": 11, "mean": 7.76, "std": 0.72, "min": 6.8, "max": 8.7}, "Num_Reviews": {"count": 13, "mean": 1073, "std": 514, "min": 450, "max": 2150}}, "unique_shows": {"by_Title": 9, "by_Show_ID": 12}, "viewer_age_distribution": {"18-24": 4, "25-34": 5, "35-44": 2, "45-54": 1, "NA": 1}, "top_genres_by_avg_rating": {"Sci-Fi": 8.17, "Documentary": 8.5, "Action": 8.2}, "duplicate_entries": {"Show_Title": "The Last Stand", "Number_of_Duplicates": 3, "Average_Rating_Range": [8.1, 8.3]}, "release_date_formats": {"YYYY-MM-DD": 9, "YYYY/M/D": 2, "MM/DD/YYYY": 1}}}
{"purpose": "Analyze user engagement patterns on a social media platform to understand activity trends and content preferences.", "raw_table": "user_id,post_date,post_type,likes,comments,shares,content_length\n101,2024-03-15,Photo,150,20,5,245\n102,15/03/2024,video,200,,10,180\n103,2024/03/16,Text,,5,0,75\n104,2024-3-15,photo,NaN,8,2,230\n105,2024-03-17,VIDEO,300,25,,400\n106,,Link,50,2,1,100\n107,2024-03-15,text,80,7,1,60\n108,03-18-2024,Photo,120,NaN,3,210\n109,2024-03-19,video,NaN,15,7,350\n110,2024-03-20,Poll,40,1,0,20\n111,2024-03-20,Poll,35,0,0,18\n112,2024-3-21,photo,130,10,4,225\n113,2024/03/21,text,60,NaN,0,70", "eda_steps": ["Check for missing values in all columns", "Standardize post_type values to lowercase", "Parse post_date into a consistent date format", "Calculate descriptive statistics for numeric columns: likes, comments, shares, content_length", "Compute value counts for post_type", "Identify posts with missing likes or comments", "Calculate average likes and comments grouped by post_type", "Find the correlation between likes, comments, shares, and content_length", "Determine the top 3 users by total likes received"], "eda_results": {"missing_values": {"user_id": 0, "post_date": 1, "post_type": 0, "likes": 3, "comments": 3, "shares": 2, "content_length": 0}, "standardized_post_type_counts": {"photo": 4, "video": 3, "text": 3, "link": 1, "poll": 2}, "parsed_dates": {"min_date": "2024-03-15", "max_date": "2024-03-21"}, "summary_stats": {"likes": {"count": 9, "mean": 128.9, "std": 86.2, "min": 35, "max": 300}, "comments": {"count": 9, "mean": 9.0, "std": 7.9, "min": 0, "max": 25}, "shares": {"count": 10, "mean": 3.3, "std": 2.7, "min": 0, "max": 10}, "content_length": {"count": 13, "mean": 170.5, "std": 118.4, "min": 18, "max": 400}}, "missing_likes_or_comments_posts": [102, 103, 104, 106, 108, 109, 113], "average_engagement_by_post_type": {"photo": {"avg_likes": 125, "avg_comments": 11.25}, "video": {"avg_likes": 250, "avg_comments": 20}, "text": {"avg_likes": 70, "avg_comments": 6}, "link": {"avg_likes": 50, "avg_comments": 2}, "poll": {"avg_likes": 37.5, "avg_comments": 0.5}}, "correlations": {"likes_comments": 0.89, "likes_shares": 0.82, "likes_content_length": 0.63, "comments_shares": 0.79, "comments_content_length": 0.55, "shares_content_length": 0.58}, "top_users_by_likes": {"105": 300, "102": 200, "104": NaN, "101": 150}}}
{"purpose": "Analyze sales performance and customer purchase patterns in a retail store over a two-week period.", "raw_table": "OrderID,CustomerName,Product,Category,Quantity,Price,OrderDate,PaymentMethod\n1001,john doe,Smartphone,Electronics,2,699,2024-04-01,Credit Card\n1002,Jane Smith,jeans,Clothing,1,49.99,04/02/2024,credit card\n1003,Bob Johnson,LED TV,Electronics,,399.99,2024/04/03,Paypal\n1004,Mary Lee,coffee Maker,Home appliances,1,79.5,4-4-2024,Cash\n1005,adam o'neill,smartphone,Electronics,1,699,2024.04.05,Credit Card\n1006,,blender,Home Appliances,1,65,2024/04/06,Paypal\n1007,tina brown,Jeans,clothing,2,49.99,2024-4-07,credit Card\n1008,George White,Vacuum Cleaner,home appliances,1,120,,Cash\n1009,Lisa Ray,SmartWatch,Electronics,1,199.99,2024-04-08,Credit card\n1010,Paul Green,jeans,Clothing,,49.99,2024-04-09,Paypal\n1011,Sara O'Connor,Coffee maker,Home Appliances,1,79.5,2024-04-10,CASH\n1012,Michael Brown,SmartWatch,Electronics,1,199.99,04-11-2024,Credit Card\n1013,Alice Cooper,Socks,clothing,3,5.99,2024-04-12,Cash\n1014,John Doe,Smartphone,Electronics,1,699,,Credit Card", "eda_steps": ["Check the number of missing values in each column", "Standardize capitalization in categorical columns such as Product, Category, and PaymentMethod", "Convert OrderDate to a consistent date format", "Calculate descriptive statistics for Quantity and Price", "Generate value counts for Category and PaymentMethod", "Identify the top 3 products by total quantity sold", "Calculate the total sales amount per order (Quantity * Price) and summarize", "Check for duplicate OrderIDs", "Analyze the distribution of orders across different dates"], "eda_results": {"missing_values": {"OrderID": 0, "CustomerName": 1, "Product": 0, "Category": 0, "Quantity": 2, "Price": 0, "OrderDate": 2, "PaymentMethod": 0}, "standardized_categories": {"Category": {"Electronics": 6, "Clothing": 4, "Home Appliances": 4}, "PaymentMethod": {"Credit Card": 7, "Paypal": 3, "Cash": 4}}, "corrected_dates": {"range": "2024-04-01 to 2024-04-12", "missing_dates_filled": 0}, "descriptive_stats": {"Quantity": {"count": 12, "mean": 1.42, "min": 1, "max": 3, "missing": 2}, "Price": {"count": 14, "mean": 240.35, "min": 5.99, "max": 699.0}}, "value_counts": {"Category": {"Electronics": 6, "Clothing": 4, "Home Appliances": 4}, "PaymentMethod": {"Credit Card": 7, "Paypal": 3, "Cash": 4}}, "top_products_by_quantity": {"Smartphone": 4, "Jeans": 3, "SmartWatch": 2}, "total_sales_per_order_summary": {"mean": 354.14, "min": 17.97, "max": 1398, "missing_quantities_ignored": true}, "duplicate_orderids": 0, "orders_per_day_distribution": {"2024-04-01": 1, "2024-04-02": 1, "2024-04-03": 1, "2024-04-04": 1, "2024-04-05": 1, "2024-04-06": 1, "2024-04-07": 1, "2024-04-08": 1, "2024-04-09": 1, "2024-04-10": 1, "2024-04-11": 1, "2024-04-12": 1, "missing_dates": 1}}}
{"purpose": "Analyze household electricity consumption patterns and identify data quality issues.", "raw_table": "Household_ID,Measurement_Date,Energy_Consumed_kWh,Energy_Source,City\nH001,2023/01/15,35.2,solar,New York\nh002,15-01-2023,NaN,Wind,new york\nH003,01/16/2023,42.5,Coal,Los Angeles\nH004,2023-01-17,38,,LOS ANGELES\nH005,2023/01/18,invalid,solar,Chicago\nH006,2023/01/19,40.0,Wind,CHICAGO\nH007,,37.1,coal,New York\nH008,2023-01-21,39.5,Solar,los angeles\nH009,21-01-2023,36.7,wind,Chicago\nh010,2023/01/22,41.3,Coal,New york", "eda_steps": ["Check the percentage of missing values in each column", "Standardize the date format in Measurement_Date column", "Calculate descriptive statistics for Energy_Consumed_kWh, excluding invalid or missing entries", "Generate value counts for Energy_Source to identify category inconsistencies", "Generate value counts for City to identify inconsistent capitalization", "Identify rows with invalid numeric values in Energy_Consumed_kWh", "Compute the mean Energy_Consumed_kWh by Energy_Source", "Summarize the distribution skewness of Energy_Consumed_kWh"], "eda_results": {"missing_values": {"Household_ID": 0, "Measurement_Date": 1, "Energy_Consumed_kWh": 2, "Energy_Source": 1, "City": 0}, "date_standardization": {"formats_found": ["YYYY/MM/DD", "DD-MM-YYYY", "MM/DD/YYYY", "YYYY-MM-DD"], "standardized": "all dates converted to YYYY-MM-DD except missing"}, "summary_stats": {"count": 11, "valid_numeric_entries": 9, "mean": 38.56, "std_dev": 2.34, "min": 35.2, "max": 42.5}, "value_counts_Energy_Source": {"solar": 3, "wind": 3, "coal": 3, "": 1}, "value_counts_City": {"new york": 4, "los angeles": 3, "chicago": 3}, "invalid_numeric_rows": ["H005"], "mean_energy_by_source": {"solar": 37.35, "wind": 37.93, "coal": 40.63}, "energy_skewness": 0.12}}
{"purpose": "Analyze recent transactions to detect patterns in expenses and income categories.", "raw_table": "TransactionID,Date,Amount,Category,Account,Description\n1,2023-01-15,1500,Salary,Checking,Monthly pay\n2,01/20/2023,-200,Groceries,checking,Supermarket\n3,2023/01/22,-150,utilities,Savings,Electric Bill\n4,2023-01-25,,Entertainment,Checking,Movie tickets\n5,2023-2-01,-50,groceries,Checking,Local market\n6,2023-02-05,500,Bonus,savings,Performance Bonus\n7,, -75,Travel,Checking,Taxi fare\n8,2023-02-15,-300,Utilities,Checking,Water Bill\n9,2023-02-18,-,Misc,Checking,Unknown expense\n10, 2023-02-20,1000,Salary,Checking,Bonus\n11,2023-02-22,-120,entertainment,checking,Concert\n12,2023-02-25,-45,Groceries,Checking,Snacks\n13,2023-02-28,-60,Travel,savings,Bus ticket\n14,2023-02-28,-90,Utilities,Checking,Gas Bill", "eda_steps": ["Check missing value percentages for each column", "Standardize date formats and parse dates", "Compute descriptive statistics for the Amount column", "Generate value counts for the Category column", "Check unique accounts and standardize capitalization", "Identify transactions with missing or invalid Amount values", "Calculate total expenses and income by Category", "Compute correlation between Amount and Date converted to numeric", "Summarize distribution skewness of Amount"], "eda_results": {"missing_values": {"TransactionID": 0, "Date": 1, "Amount": 2, "Category": 0, "Account": 0, "Description": 0}, "date_parsing": {"parsed_dates_success": 13, "failed": 1}, "summary_stats": {"Amount": {"count": 12, "mean": 293.75, "std": 752.82, "min": -300, "25%": -90, "50%": -60, "75%": 500, "max": 1500, "skewness": 2.15}}, "value_counts": {"Category": {"Groceries": 3, "Utilities": 3, "Entertainment": 2, "Salary": 2, "Travel": 2, "Bonus": 1, "Misc": 1}, "Account": {"Checking": 10, "Savings": 2}}, "standardized_accounts": ["Checking", "Checking", "Savings", "Checking", "Checking", "Savings", "Checking", "Checking", "Checking", "Checking", "Checking", "Checking", "Savings", "Checking"], "invalid_amount_transactions": [4, 9], "total_by_category": {"Groceries": -295, "Utilities": -510, "Entertainment": -120, "Salary": 2500, "Travel": -135, "Bonus": 500, "Misc": 0}, "correlations": {"Amount_DateNumeric": -0.07}}}
{"purpose": "Analyze public bus transportation data to identify usage patterns and data quality issues.", "raw_table": "Trip_ID,Route,Boarding_Time,Passenger_Count,Fare,Driver_Name\nT001,Route 5,2023-04-01 08:15 AM,12,2.5,alice\nT002,route 7,04/01/2023 09:00,15,3.0,Bob\nT003,ROUTE 5,2023-4-01 10:30 am,,2.5,alice\nT004,Route 8,2023/04/01 11:00 AM,8,three,Charlie\nT005,route 5,2023-04-01 12:15 PM,20,2.5,\nT006,Route 7,2023-04-01 13:00,NaN,3.0,bob\nT007,Route 8,2023-04-01 01:45 pm,5,2.8,Charlie\nT008,Route 5,2023-04-01 14:30,18,2.5,Alice\nT009,route 9,2023-04-01 15:00,10,3.2,David\nT010,Route 7,April 1, 2023 16:15,14,3.0,Bob", "eda_steps": ["Check for missing values and their percentages in each column", "Standardize the 'Route' column capitalization", "Convert 'Boarding_Time' to a consistent datetime format", "Convert 'Fare' to numeric, handling non-numeric values", "Generate descriptive statistics for 'Passenger_Count' and 'Fare'", "Count unique drivers and their frequency", "Identify top 3 most frequent routes by trip count", "Calculate average passenger count per route"], "eda_results": {"missing_values": {"Trip_ID": 0, "Route": 0, "Boarding_Time": 0, "Passenger_Count": 2, "Fare": 1, "Driver_Name": 1}, "standardized_routes": ["Route 5", "Route 7", "Route 5", "Route 8", "Route 5", "Route 7", "Route 8", "Route 5", "Route 9", "Route 7"], "boarding_time_parsed": true, "fare_numeric_conversion": {"non_numeric_entries": ["three"], "converted_fare": [2.5, 3.0, 2.5, null, 2.5, 3.0, 2.8, 2.5, 3.2, 3.0]}, "summary_stats": {"Passenger_Count": {"count": 8, "mean": 12.75, "std_dev": 5.03, "min": 5, "max": 20}, "Fare": {"count": 9, "mean": 2.78, "std_dev": 0.22, "min": 2.5, "max": 3.2}}, "driver_counts": {"Alice": 3, "Bob": 3, "Charlie": 2, "David": 1, "": 1}, "top_routes": {"Route 5": 4, "Route 7": 3, "Route 8": 2}, "avg_passengers_per_route": {"Route 5": 16.7, "Route 7": 14.5, "Route 8": 6.5, "Route 9": 10.0}}}
{"purpose": "Explore and summarize housing listing characteristics and identify data quality issues in a real estate dataset.", "raw_table": "ListingID,Price,Location,DateListed,PropertyType,Bedrooms,Bathrooms,SquareFeet\n1001,350000,New york,2023-01-15,Apartment,2,1,850\n1002,450,000,Boston,15/02/2023,Condo,3,,1200\n1003,525000,los angeles,2023-03-10,house,4,3,2000\n1004,NaN,Chicago,2023/04/05,Apartment,1,1,600\n1005,475000,Houston,,Townhouse,3,2,1500\n1006,400000,Philadelphia,2023-06-01,condo,2,2,1100\n1007,390000,NEW YORK,2023-07-12,Apartment,2,1,900\n1008,NaN,miami,07-15-2023,House,,2,1800\n1009,600000,Seattle,2023-08-01,House,5,4,2500\n1010,425000,boston,2023-08-15,Townhouse,3,2,1600\n1011,430000,Chicago,2023-09-01,Apartment,2,1.5,950\n1012,410000,Los Angeles,2023/09/10,Condo,3,2,1200\n", "eda_steps": ["Check missing value percentages for all columns", "Standardize capitalization in Location and PropertyType columns and generate value counts", "Convert Price to numeric, identify and count non-numeric or malformed entries", "Compute descriptive statistics for numeric columns: Price, Bedrooms, Bathrooms, SquareFeet", "Analyze date formats in DateListed and convert to consistent yyyy-mm-dd format", "Calculate correlation matrix for numeric variables", "Identify top 3 most common PropertyType categories", "Summarize number of listings per Location"], "eda_results": {"missing_values": {"Price": 2, "Location": 0, "DateListed": 1, "PropertyType": 0, "Bedrooms": 2, "Bathrooms": 1, "SquareFeet": 0}, "value_counts": {"Location": {"new york": 2, "boston": 2, "los angeles": 2, "chicago": 2, "houston": 1, "philadelphia": 1, "miami": 1, "seattle": 1}, "PropertyType": {"apartment": 3, "condo": 3, "house": 3, "townhouse": 2}}, "price_conversion_issues": {"non_numeric_entries": 1, "malformed_entries": ["450,000"]}, "summary_stats": {"Price": {"count": 11, "mean": 453181.82, "std": 66763.64, "min": 350000, "25%": 400000, "50%": 430000, "75%": 475000, "max": 600000}, "Bedrooms": {"count": 10, "mean": 2.8, "std": 1.17, "min": 1, "25%": 2, "50%": 3, "75%": 3, "max": 5}, "Bathrooms": {"count": 11, "mean": 1.86, "std": 0.87, "min": 1, "25%": 1, "50%": 2, "75%": 2, "max": 4}, "SquareFeet": {"count": 13, "mean": 1373.08, "std": 575.18, "min": 600, "25%": 900, "50%": 1200, "75%": 1600, "max": 2500}}, "date_format_summary": {"formats_found": ["yyyy-mm-dd", "dd/mm/yyyy", "yyyy/mm/dd", "mm-dd-yyyy", "missing"], "standardized_missing_dates": 1}, "correlations": {"Price_Bedrooms": 0.89, "Price_Bathrooms": 0.87, "Price_SquareFeet": 0.92, "Bedrooms_Bathrooms": 0.85, "Bedrooms_SquareFeet": 0.88, "Bathrooms_SquareFeet": 0.9}, "top_categories": {"PropertyType": [{"category": "apartment", "count": 3}, {"category": "condo", "count": 3}, {"category": "house", "count": 3}]}, "listings_per_location": {"new york": 2, "boston": 2, "los angeles": 2, "chicago": 2, "houston": 1, "philadelphia": 1, "miami": 1, "seattle": 1}}}
{"purpose": "Explore housing listing data to understand price distributions, neighborhood popularity, and data quality issues.", "raw_table": "ListingID,Neighborhood,Price,Bedrooms,Bathrooms,ListDate,SaleStatus\n101,Downtown,450000,3,2,2023-01-15, Sold\n102,Suburbia, 350000, 4,3,15/02/2023,Pending\n103,midtown ,500000,,2,2023-03-10,SOLD\n104,Uptown,NaN,2,1,2023-03-20,Available\n105,Downtown,480000,3,2,2023/04/01,Sold\n106,suburbia,370000,4,3,2023-04-15,pending\n107,MidTown,520000,3,two,2023-04-20,Sold\n108,Old Town,300000,2,1,04-25-2023,Available\n109,Downtown,460000,3,2,2023-05-01,sold\n110,Old town,,2,1,2023-05-10,Sold\n111,UPTOWN,490000,3,2,2023-05-15,PENDING\n112,Midtown,505000,3,,2023-05-20,Sold", "eda_steps": ["Check for missing values in each column", "Standardize capitalization in categorical columns Neighborhood and SaleStatus", "Convert ListDate to a consistent date format", "Compute descriptive statistics for Price, Bedrooms, and Bathrooms", "Generate value counts for Neighborhood and SaleStatus", "Identify rows with inconsistent data types (e.g., Bathrooms as 'two')", "Calculate correlation between Price and number of Bedrooms", "Summarize the distribution skewness of Price"], "eda_results": {"missing_values": {"ListingID": 0, "Neighborhood": 0, "Price": 2, "Bedrooms": 1, "Bathrooms": 2, "ListDate": 0, "SaleStatus": 0}, "value_counts": {"Neighborhood": {"Downtown": 3, "Suburbia": 2, "Midtown": 3, "Uptown": 2, "Old Town": 2}, "SaleStatus": {"Sold": 6, "Pending": 3, "Available": 2}}, "descriptive_stats": {"Price": {"count": 10, "mean": 447500, "std": 76014, "min": 300000, "25%": 370000, "50%": 465000, "75%": 495000, "max": 520000}, "Bedrooms": {"count": 11, "mean": 3.09, "std": 0.62, "min": 2, "25%": 3, "50%": 3, "75%": 4, "max": 4}, "Bathrooms": {"count": 9, "mean": 1.89, "std": 0.52, "min": 1, "25%": 1.5, "50%": 2, "75%": 2, "max": 3}}, "inconsistent_data_rows": {"Bathrooms_as_text": [107], "Missing_Bedrooms": [103]}, "correlations": {"Price_Bedrooms": 0.87}, "skewness": {"Price": 0.12}}}
{"purpose": "Analyze daily public transportation usage patterns and vehicle types in a city bus network.", "raw_table": "Date,Route,Vehicle_Type,Passengers,On_Time_Status\n2024-01-01,12A,BUS,56,On time\n01/02/2024,7b,Bus,48,Late\n2024/01/03,12A,Minibus,30,On Time\n2024-01-04,15C,Bus,,Delayed\n2024-01-05,7B,bus,52,ontime\n2024-01-06,15c,MiniBus,29,late\n2024-01-07,12a,Bus,NaN,On Time\n2024-01-08,7B,Bus,55,On time\n2024-01-09,15C,bus,49,DELAYED\n2024-01-10,12A,Minibus,31,On time\n2024-01-11,7b,bus,NaN,LATE\n2024-1-12,15C,Bus,50,On Time\n", "eda_steps": ["Standardize date formats to a single consistent format", "Clean and unify capitalization in categorical columns (Route, Vehicle_Type, On_Time_Status)", "Check for missing values in each column", "Compute descriptive statistics for the Passengers column", "Generate value counts for Route, Vehicle_Type, and On_Time_Status columns", "Calculate the percentage of on-time vs late/delayed trips", "Identify routes with highest average passenger count", "Summarize the number of missing passenger entries per route"], "eda_results": {"missing_values": {"Date": 0, "Route": 0, "Vehicle_Type": 0, "Passengers": 3, "On_Time_Status": 0}, "value_counts": {"Route": {"7B": 4, "12A": 4, "15C": 4}, "Vehicle_Type": {"Bus": 8, "Minibus": 3, "MiniBus": 1}, "On_Time_Status": {"On time": 7, "Late": 3, "Delayed": 2}}, "summary_stats": {"Passengers": {"count": 9, "mean": 45.56, "std": 10.03, "min": 29, "25%": 30, "50%": 49, "75%": 52, "max": 56}}, "on_time_percentage": 58.33, "average_passengers_per_route": {"7B": 51.67, "12A": 39.0, "15C": 49.5}, "missing_passengers_per_route": {"7B": 1, "12A": 1, "15C": 1}}}
{"purpose": "Analyze crop yield and fertilizer usage patterns across different farm regions to identify data quality issues and basic trends.", "raw_table": "Farm_ID,Region,Planting_Date,Crop_Type,Yield_kg,Fertilizer_Used_kg\nF001,North,2023-03-15,Wheat,1200,50\nF002,south,15/03/2023,maize,1100,45\nF003, EAST ,2023/03/16,Barley,950,NaN\nF004,West,Mar 17 2023,Wheat,,55\nF005,north,2023-3-18,Maize,1300,48\nF006,South,2023-03-19,Rice,800,40\nF007,East,2023-03-20,barley,1000,42\nF008,West,,Wheat,1250,50\nF009,South,03-21-2023,Rice,850,38\nF010,north,2023/03/22,MAIZE,1150,47", "eda_steps": ["Check and summarize missing values in each column", "Standardize the Region and Crop_Type column capitalization", "Parse and standardize Planting_Date to YYYY-MM-DD format", "Compute descriptive statistics for Yield_kg and Fertilizer_Used_kg", "Generate value counts for Crop_Type and Region", "Calculate correlation between Yield_kg and Fertilizer_Used_kg", "Identify farms with missing Yield_kg or Planting_Date", "Summarize average Yield_kg by Crop_Type"], "eda_results": {"missing_values": {"Farm_ID": 0, "Region": 0, "Planting_Date": 1, "Crop_Type": 0, "Yield_kg": 1, "Fertilizer_Used_kg": 1}, "standardized_categories": {"Region": ["North", "South", "East", "West"], "Crop_Type": ["Wheat", "Maize", "Barley", "Rice"]}, "parsed_dates": {"invalid_dates": 1, "date_range": ["2023-03-15", "2023-03-22"]}, "summary_stats": {"Yield_kg": {"count": 9, "mean": 1055.56, "std": 169.74, "min": 800, "max": 1300}, "Fertilizer_Used_kg": {"count": 9, "mean": 46.11, "std": 5.01, "min": 38, "max": 55}}, "value_counts": {"Crop_Type": {"Wheat": 3, "Maize": 3, "Barley": 2, "Rice": 2}, "Region": {"North": 3, "South": 3, "East": 2, "West": 2}}, "correlations": {"Yield_kg_Fertilizer_Used_kg": 0.78}, "missing_data_farms": {"Missing_Yield_kg": ["F004"], "Missing_Planting_Date": ["F008"], "Missing_Fertilizer_Used_kg": ["F003"]}, "average_yield_by_crop": {"Wheat": 1225, "Maize": 1183.33, "Barley": 975, "Rice": 825}}}
{"purpose": "Analyze customer purchase patterns and identify missing data issues in retail transactions.", "raw_table": "TransactionID,CustomerID,Product,Quantity,Price,PurchaseDate,StoreLocation\nT001,C123,apple,2,0.5,2023-01-15,New York\nT002,C124,Banana,5,0.3,15/01/2023,los angeles\nT003,C125,,3,0.7,2023/01/16,NEW YORK\nT004,C126,Orange,NaN,0.4,2023-01-17,Chicago\nT005,C127,banana,7,0.3,2023-01-18,LOS ANGELES\nT006,C128,Apple,1,0.5,01-19-2023,New york\nT007,C129,orange,2,,2023-01-20,Chicago\nT008,C130,Grapes,10,1.2,2023-01-21,Boston\nT009,C131,apple,4,0.5,2023-01-22,Boston\nT010,C132,banana,3,0.3,,los angeles\nT011,C133,Grapes,NaN,1.1,2023-01-24,Boston\nT012,C134,APPLE,6,0.5,2023-01-25,new york\nT013,C135,Orange,2,0.4,2023-01-26,CHICAGO", "eda_steps": ["Check for missing values in each column", "Standardize capitalization for Product and StoreLocation columns", "Convert PurchaseDate to a uniform date format", "Calculate descriptive statistics for Quantity and Price columns", "Generate value counts for Product and StoreLocation columns", "Identify transactions with missing Quantity or Price", "Compute total revenue per product", "Summarize date range of purchases"], "eda_results": {"missing_values": {"TransactionID": 0, "CustomerID": 0, "Product": 1, "Quantity": 2, "Price": 1, "PurchaseDate": 1, "StoreLocation": 0}, "standardized_categories": {"Product": ["Apple", "Banana", "Orange", "Grapes", null], "StoreLocation": ["New York", "Los Angeles", "Chicago", "Boston"]}, "date_range": {"min_date": "2023-01-15", "max_date": "2023-01-26"}, "descriptive_stats": {"Quantity": {"count": 11, "mean": 3.91, "std": 2.81, "min": 1, "max": 10}, "Price": {"count": 12, "mean": 0.59, "std": 0.32, "min": 0.3, "max": 1.2}}, "value_counts": {"Product": {"Apple": 4, "Banana": 3, "Orange": 3, "Grapes": 2, "missing": 1}, "StoreLocation": {"New York": 4, "Los Angeles": 3, "Chicago": 3, "Boston": 3}}, "transactions_with_missing_quantity_or_price": ["T004", "T007", "T011"], "total_revenue_per_product": {"Apple": 6.5, "Banana": 4.5, "Orange": 3.2, "Grapes": 13.2, "missing": 0}}}
{"purpose": "Analyze daily electricity consumption patterns and identify missing data issues in residential and commercial sectors.", "raw_table": "Date,Customer_Type,Consumption_kWh,Peak_Hour_Usage,Region\n01/02/2024,residential,23.5,5.2,north\n2024-01-03,Commercial,45.8,12.1,South\n01-04-2024,Residential,18.9,,north\n01/05/2024,commercial,NaN,10.5,South\n01/06/24,Residential,21.0,5.0,NORTH\n01/07/2024,residential,19.5,4.8,East\nJanuary 8, 2024,Commercial,47.3,11.9,south\n01/09/2024,Residential,22.1,5.4,East\n01/10/2024,commercial,44.0,10.2,west\n01/11/2024,Residential,20.8,,West\n01/12/2024,residential,NaN,5.1,West\n01/13/2024,Commercial,48.2,12.3,South", "eda_steps": ["Standardize the Date column to a uniform date format", "Normalize Customer_Type values to consistent capitalization", "Check and report the percentage of missing values per column", "Generate descriptive statistics for Consumption_kWh and Peak_Hour_Usage", "Compute value counts for Customer_Type and Region", "Identify correlation between Consumption_kWh and Peak_Hour_Usage", "Summarize consumption statistics grouped by Customer_Type", "List top 2 regions with highest average consumption"], "eda_results": {"missing_values": {"Date": 0, "Customer_Type": 0, "Consumption_kWh": 2, "Peak_Hour_Usage": 2, "Region": 0}, "value_counts": {"Customer_Type": {"Residential": 7, "Commercial": 5}, "Region": {"South": 4, "North": 3, "East": 2, "West": 3}}, "summary_stats": {"Consumption_kWh": {"count": 10, "mean": 31.61, "std": 13.29, "min": 18.9, "25%": 21.0, "50%": 22.1, "75%": 44.0, "max": 48.2}, "Peak_Hour_Usage": {"count": 11, "mean": 8.6, "std": 3.1, "min": 4.8, "25%": 5.0, "50%": 5.4, "75%": 11.9, "max": 12.3}}, "correlations": {"Consumption_kWh_vs_Peak_Hour_Usage": 0.98}, "consumption_by_customer_type": {"Residential": {"mean_consumption_kWh": 21.6, "count": 7}, "Commercial": {"mean_consumption_kWh": 46.3, "count": 5}}, "top_regions_by_avg_consumption": {"South": 46.08, "West": 32.27}}}
{"purpose": "Analyze customer call and data usage patterns to identify common issues and usage trends.", "raw_table": "CustomerID,CallDuration,DataUsage,ServiceType,CallDate,IssueReported\nC001,300,1.5,GSM,2023-03-15 14:30,No\nC002,NA,2.1,LTE,15/03/2023 15:00,Yes\nc003,120,abc,GSM,2023/03/16,No\nC004,450,3.7,4G,2023-03-16 17:00,Yes\nC005,200,,lte,03-17-2023,No\nC006,180,0.5,GSM,2023-03-17 18:15,NO\nc007,NA,1.0,5g,2023/03/18 19:00,yes\nC008,600,4.2,4G,18-03-2023 20:00,yes\nC009,90,0.7,GSM,,No\nC010,250,1.8,Lte,2023-03-19 14:00,Yes", "eda_steps": ["Check missing value percentages for each column", "Standardize capitalization in the ServiceType column", "Convert CallDuration and DataUsage columns to numeric, coercing errors", "Parse CallDate column into a consistent datetime format", "Generate descriptive statistics for CallDuration and DataUsage", "Count value frequencies for ServiceType and IssueReported columns", "Identify rows with inconsistent or missing CallDate values", "Analyze correlation between CallDuration and DataUsage", "Identify top 2 most common ServiceType categories"], "eda_results": {"missing_values": {"CustomerID": 0, "CallDuration": 2, "DataUsage": 2, "ServiceType": 0, "CallDate": 1, "IssueReported": 0}, "service_type_standardized": {"GSM": 4, "LTE": 3, "4G": 2, "5G": 1}, "numeric_conversion": {"CallDuration": {"converted_non_numeric": 0, "missing_after_conversion": 2}, "DataUsage": {"converted_non_numeric": 1, "missing_after_conversion": 2}}, "call_date_parsing": {"successfully_parsed": 9, "failed_to_parse": 1}, "descriptive_statistics": {"CallDuration": {"count": 8, "mean": 285.0, "std": 160.5, "min": 90, "max": 600}, "DataUsage": {"count": 8, "mean": 1.79, "std": 1.29, "min": 0.5, "max": 4.2}}, "value_counts": {"IssueReported": {"Yes": 5, "No": 5}}, "missing_call_date_rows": ["C009"], "correlations": {"CallDuration_DataUsage": 0.89}, "top_categories": {"ServiceType": [{"category": "GSM", "count": 4}, {"category": "LTE", "count": 3}]}}}
{"purpose": "Understand user engagement patterns and content popularity on a social media platform.", "raw_table": "post_id,user_id,post_date,content_type,likes,comments,shares\n101,U01,2023-01-15,Image,150,10,5\n102,u02,15/01/2023,video,200,,8\n103,U03,2023/01/16,Text,NaN,2,1\n104,U04,2023-01-16,image,100,5,NaN\n105,U05,17-01-2023,Video,250,15,12\n106,U06,2023-01-17,text,75,,3\n107,u07,2023-01-18,IMage,NaN,7,2\n108,U08,18/01/2023,video,300,20,15\n109,U09,2023-01-18,text,50,1,0\n110,u10,,Image,120,4,3\n111,U11,2023-01-19,Video,275,18,9\n112,U12,2023-1-19,text,90,3,1", "eda_steps": ["Check and report missing values in each column", "Standardize the content_type column to lowercase", "Parse post_date column into a consistent date format", "Compute descriptive statistics (mean, median) for likes, comments, and shares", "Generate value counts for content_type", "Identify top 2 posts by likes", "Calculate the correlation matrix for likes, comments, and shares", "Summarize number of posts per user_id"], "eda_results": {"missing_values": {"post_id": 0, "user_id": 0, "post_date": 1, "content_type": 0, "likes": 2, "comments": 3, "shares": 1}, "content_type_standardized_counts": {"image": 4, "video": 4, "text": 4}, "post_date_parsing_issues": 0, "likes_stats": {"mean": 160.45, "median": 150, "count": 10}, "comments_stats": {"mean": 8.0, "median": 5, "count": 9}, "shares_stats": {"mean": 6.9, "median": 5, "count": 11}, "top_posts_by_likes": [{"post_id": "108", "likes": 300}, {"post_id": "105", "likes": 250}], "correlations": {"likes_comments": 0.95, "likes_shares": 0.89, "comments_shares": 0.85}, "posts_per_user": {"U01": 1, "u02": 1, "U03": 1, "U04": 1, "U05": 1, "U06": 1, "u07": 1, "U08": 1, "U09": 1, "u10": 1, "U11": 1, "U12": 1}}}
{"purpose": "Evaluate production line efficiency and quality metrics to identify bottlenecks and defects.", "raw_table": "Machine_ID,Shift,Production_Date,Units_Produced,Defects,Operator,Line\nM01,morning,2023/04/01,120,5,John Doe,Line-1\nm02,Night,04-01-2023,130,,Jane Smith,line-2\nM03,morning,2023-04-01,115,3,alice jones,Line-1\nM01,Afternoon,2023/4/02,NaN,2,John Doe,Line-1\nm02,night,2023-04-02,125,4,,Line-2\nM03,MORNING,04/02/2023,110,1,Alice Jones,line-1\nM01,Afternoon,2023-04-03,118,0,John Doe,Line-1\nm02,Night,2023/4/03,128,7,Jane Smith,line-2\nM03,morning,2023-04-03,112,NaN,Alice Jones,Line-1\nM04,Morning,04-03-2023,105,2,Michael B,Line-3\nM04,afternoon,2023/04/03,108,3,Michael B,LINE-3", "eda_steps": ["Check the percentage of missing values per column", "Standardize the date format in Production_Date", "Normalize categorical values for Shift and Line columns", "Compute descriptive statistics for Units_Produced and Defects columns", "Generate value counts for Operator and Shift", "Identify rows with missing or inconsistent data", "Calculate correlation between Units_Produced and Defects", "Summarize total units produced per production line"], "eda_results": {"missing_values": {"Machine_ID": 0, "Shift": 0, "Production_Date": 0, "Units_Produced": 1, "Defects": 2, "Operator": 1, "Line": 0}, "date_format_standardized": true, "normalized_categories": {"Shift": ["morning", "afternoon", "night"], "Line": ["line-1", "line-2", "line-3"]}, "summary_stats": {"Units_Produced": {"count": 14, "mean": 117.36, "std": 7.71, "min": 105, "max": 130}, "Defects": {"count": 12, "mean": 3.42, "std": 2.01, "min": 0, "max": 7}}, "value_counts": {"Operator": {"John Doe": 4, "Jane Smith": 3, "Alice Jones": 3, "Michael B": 2, "": 1}, "Shift": {"morning": 6, "afternoon": 3, "night": 4}}, "inconsistent_rows": [{"row_index": 3, "issue": "Units_Produced missing"}, {"row_index": 4, "issue": "Operator missing"}, {"row_index": 8, "issue": "Defects missing"}, {"row_index": 1, "issue": "Defects missing"}], "correlations": {"Units_Produced_vs_Defects": -0.41}, "total_units_per_line": {"line-1": 575, "line-2": 383, "line-3": 213}}}
{"purpose": "Analyze user engagement patterns and content types on a social media platform to identify trends and data quality issues.", "raw_table": "post_id,user_id,post_date,content_type,likes,comments,shares\n101,U001,2023-04-01,Photo,120,15,5\n102,u002,04/02/2023,video,85,,3\n103,U003,2023/04/03,text,NaN,2,0\n104,U004,2023-4-04,Photo,150,20,8\n105,u005,April 5 2023,link,60,NaN,NaN\n106,U006,2023-04-06,VIDEO,95,10,4\n107,U007,,photo,130,12,6\n108,u008,2023-04-08,Text,70,7,2\n109,u009,2023-04-09,photo,,5,1\n110,U010,2023/04/10,Poll,50,3,0\n111,U011,2023-13-11,photo,80,8,3\n112,U012,2023-04-12,Video,100,NaN,7\n113,U013,2023-04-13,text,110,9,4", "eda_steps": ["Check for missing values in all columns", "Standardize content_type values to lowercase", "Parse and standardize post_date to ISO format, identify invalid dates", "Compute descriptive statistics for likes, comments, and shares", "Generate value counts for content_type", "Identify posts with missing or zero engagement (likes, comments, shares)", "Calculate correlation matrix between likes, comments, and shares", "Identify top 3 posts by total engagement (likes + comments + shares)"], "eda_results": {"missing_values": {"post_date": 1, "likes": 2, "comments": 3, "shares": 2}, "content_type_standardized_counts": {"photo": 5, "video": 3, "text": 3, "link": 1, "poll": 1}, "invalid_dates": ["2023-13-11"], "descriptive_stats": {"likes": {"count": 11, "mean": 93.18, "std": 27.57, "min": 50, "25%": 70, "50%": 95, "75%": 110, "max": 150}, "comments": {"count": 10, "mean": 9.1, "std": 5.7, "min": 2, "25%": 5, "50%": 8, "75%": 12, "max": 20}, "shares": {"count": 11, "mean": 3.55, "std": 2.55, "min": 0, "25%": 2, "50%": 4, "75%": 6, "max": 8}}, "posts_missing_or_zero_engagement": [103, 105, 109, 110], "correlations": {"likes_comments": 0.89, "likes_shares": 0.84, "comments_shares": 0.78}, "top_3_posts_by_engagement": [{"post_id": 104, "total_engagement": 178}, {"post_id": 107, "total_engagement": 148}, {"post_id": 101, "total_engagement": 140}]}}
{"purpose": "Analyze customer call behavior and identify missing data patterns for telecom service improvement.", "raw_table": "CustomerID,CallStart,CallDuration,CallType,Network,DataUsedMB\n001,2024-05-01 08:15,300,voice,4G,50\n002,05/02/2024 09:45 AM,180,VOICE,3g,NaN\n003,2024-05-03 14:00,missing,data,5G,150\n004,2024/05/04 20:30,240,Voice,4g,75\n005,2024-05-05 07:50,,Data,Unknown,100\n006,May 6 2024 11:10,90,voice,5G,80\n007,2024-5-07 22:05,60,DATA,3G,\n008,2024-05-08 13:00,120,Voice,4G,NaN\n009,,200,voice,4g,55\n010,2024-05-10 18:25,300,voice,3g,45", "eda_steps": ["Check for missing values in each column", "Standardize capitalization in categorical columns 'CallType' and 'Network'", "Convert 'CallStart' to a consistent datetime format", "Compute descriptive statistics for 'CallDuration' and 'DataUsedMB'", "Generate value counts for 'Network' and 'CallType'", "Identify rows with missing 'CallStart' or 'CallDuration'", "Calculate correlation between 'CallDuration' and 'DataUsedMB'", "Find top 3 most frequent 'Network' types"], "eda_results": {"missing_values": {"CustomerID": 0, "CallStart": 1, "CallDuration": 2, "CallType": 0, "Network": 1, "DataUsedMB": 3}, "standardized_categories": {"CallType": {"voice": 7, "data": 3}, "Network": {"4g": 5, "3g": 3, "5g": 2, "unknown": 1}}, "datetime_conversion": {"successful": 9, "failed": 1}, "summary_stats": {"CallDuration": {"count": 8, "mean": 193.75, "std": 93.48, "min": 60, "max": 300}, "DataUsedMB": {"count": 7, "mean": 75, "std": 37.2, "min": 45, "max": 150}}, "value_counts": {"Network": {"4g": 5, "3g": 3, "5g": 2, "unknown": 1}, "CallType": {"voice": 7, "data": 3}}, "missing_data_rows": {"CallStart": [9], "CallDuration": [5, 3]}, "correlations": {"CallDuration_DataUsedMB": 0.64}, "top_categories": {"Network": [{"4g": 5}, {"3g": 3}, {"5g": 2}]}}}
{"purpose": "Examine crop yield patterns across different farm locations and soil types to identify factors affecting productivity.", "raw_table": "Farm_ID,Crop,Planting_Date,Harvest_Date,Soil_Type,Yield_kg,Water_Usage_liters\n001,Wheat,2023-03-15,2023/08/20,Loam,1200,15000\n002,Corn,15-04-2023,2023-09-10,SANDY,1100,NA\n003,Rice,,2023-10-05,Clay,900,12000\n004,Wheat,2023-03-20,2023-08-25,loam,1300,16000\n005,corn,2023/04/16,09/15/2023,LoAm,1150,14000\n006,Rice,2023-05-01,2023/10/10,clay,,13000\n007,Barley,2023-03-10,,Silty,800,12500\n008,Barley,2023-03-12,2023-08-30,Silty,850,13000\n009,WHEAT,2023-03-18,2023-08-22,Loam,1250,15500\n010,Maize,2023/04/20,2023-09-18,SANDY,1000,14500", "eda_steps": ["Standardize Crop and Soil_Type column values to consistent capitalization", "Parse Planting_Date and Harvest_Date into datetime format, identify missing or malformed dates", "Calculate growing period in days by subtracting Planting_Date from Harvest_Date", "Check for missing values in each column and report percentages", "Compute descriptive statistics for numeric columns Yield_kg, Water_Usage_liters, and growing period", "Generate value counts for Crop and Soil_Type categories", "Identify correlation between Yield_kg and Water_Usage_liters", "Find top 2 crops by average Yield_kg", "Summarize number of records with incomplete date information"], "eda_results": {"standardized_categories": {"Crop": {"Wheat": 3, "Corn": 2, "Rice": 2, "Barley": 2, "Maize": 1}, "Soil_Type": {"Loam": 4, "Sandy": 2, "Clay": 2, "Silty": 2}}, "missing_values_percent": {"Planting_Date": 10, "Harvest_Date": 10, "Yield_kg": 10, "Water_Usage_liters": 10}, "growing_period_days": {"mean": 155, "min": 147, "max": 160, "missing": 2}, "descriptive_statistics": {"Yield_kg": {"count": 13, "mean": 1064.6, "std": 178.2, "min": 800, "max": 1300}, "Water_Usage_liters": {"count": 14, "mean": 14142.9, "std": 1566.4, "min": 12000, "max": 16000}}, "value_counts": {"Crop": {"Wheat": 3, "Corn": 2, "Rice": 2, "Barley": 2, "Maize": 1}, "Soil_Type": {"Loam": 4, "Sandy": 2, "Clay": 2, "Silty": 2}}, "correlations": {"Yield_kg_Water_Usage_liters": 0.72}, "top_crops_by_yield": {"Wheat": 1250, "Corn": 1125}, "incomplete_dates_count": 2}}
{"purpose": "Analyze customer call patterns and service usage in a telecom dataset to identify data quality issues and usage trends.", "raw_table": "CustomerID,CallDate,CallDuration,PlanType,Region,DataUsageMB,Churn\n001,2023-01-15,300,Premium,North,500,No\n002,1/20/2023,NaN,BASIC,south,250,Yes\n003,2023/01/22,180,Premium,East,,No\n004,15-01-2023,45,Basic,west,100,No\n005,01-25-2023,200,Premium,North,750,No\n006,2023-01-30,NaN,PREMIUM,South,,Yes\n007,2023-02-01,120,standard,East,300,No\n008,,60,Basic,West,150,No\n009,2023-02-05,NaN,Standard,Unknown,200,Yes\n010,2023-02-07,90,Basic,north,NaN,No\n011,2023-02-08,30,PREMIUM,SOUTH,400,No\n012,2023-02-10,NaN,Standard,East,350,No", "eda_steps": ["Check missing value percentages for all columns", "Standardize the 'PlanType' and 'Region' columns capitalization", "Compute descriptive statistics for 'CallDuration' and 'DataUsageMB'", "Generate value counts for 'PlanType' and 'Region'", "Identify the number of unique customers", "Analyze churn distribution", "Calculate correlation between 'CallDuration' and 'DataUsageMB'", "Summarize earliest and latest call dates"], "eda_results": {"missing_values": {"CustomerID": 0, "CallDate": 1, "CallDuration": 4, "PlanType": 0, "Region": 1, "DataUsageMB": 4, "Churn": 0}, "standardized_categories": {"PlanType": ["Premium", "Basic", "Standard"], "Region": ["North", "South", "East", "West", "Unknown"]}, "summary_stats": {"CallDuration": {"count": 11, "mean": 131.36, "std": 91.01, "min": 30, "25%": 60, "50%": 120, "75%": 200, "max": 300}, "DataUsageMB": {"count": 9, "mean": 350, "std": 204.12, "min": 100, "25%": 200, "50%": 300, "75%": 400, "max": 750}}, "value_counts": {"PlanType": {"Basic": 4, "Premium": 5, "Standard": 3}, "Region": {"North": 3, "South": 3, "East": 3, "West": 2, "Unknown": 1}}, "unique_customers": 12, "churn_distribution": {"No": 8, "Yes": 4}, "correlations": {"CallDuration_vs_DataUsageMB": 0.89}, "call_date_range": {"earliest": "2023-01-15", "latest": "2023-02-10"}}}
{"purpose": "Analyze customer churn patterns and service usage in a telecom dataset to identify factors affecting retention.", "raw_table": "CustomerID,SignupDate,PlanType,MonthlySpend,DataUsageGB,Churned\n001,2023-01-15,Premium,75.5,15.2,No\n002,15/02/2023,basic,20,,Yes\n003,2023-03-05,Standard,45.0,8.5,No\n004,3-15-2023,Premium,80.0,20.1,No\n005,2023/04/01,BASIC,19.5,5.0,yes\n006,2023-05-23,standard,50.0,9.0,No\n007,,Premium,85,,No\n008,2023-07-01,Basic,22.0,6.2,No\n009,2023-07-15,Standard,,10.0,Yes\n010,2023-08-01,PREMIUM,90.0,18.3,No\n011,2023-08-15,Basic,21.0,4.8,No\n012,2023-09-01,Standard,48,,No\n013,2023-09-10,Unknown,30.0,7.0,Yes", "eda_steps": ["Check and summarize missing values for each column", "Standardize the PlanType column capitalization", "Convert SignupDate to a uniform date format and identify missing dates", "Calculate descriptive statistics for MonthlySpend and DataUsageGB", "Generate value counts for PlanType and Churned columns", "Identify correlation between MonthlySpend and DataUsageGB", "Find the proportion of customers who churned by PlanType"], "eda_results": {"missing_values": {"CustomerID": 0, "SignupDate": 1, "PlanType": 1, "MonthlySpend": 1, "DataUsageGB": 4, "Churned": 0}, "plan_type_standardized": {"Premium": 4, "Basic": 4, "Standard": 4, "Unknown": 1}, "signup_date_missing": 1, "monthly_spend_stats": {"count": 13, "mean": 48.85, "std": 23.19, "min": 19.5, "25%": 21.0, "50%": 45.0, "75%": 80.0, "max": 90.0}, "data_usage_stats": {"count": 9, "mean": 10.11, "std": 5.5, "min": 4.8, "25%": 6.2, "50%": 8.5, "75%": 15.2, "max": 20.1}, "value_counts": {"PlanType": {"Premium": 4, "Basic": 4, "Standard": 4, "Unknown": 1}, "Churned": {"No": 9, "Yes": 4}}, "correlations": {"MonthlySpend_vs_DataUsageGB": 0.92}, "churn_rate_by_plan": {"Premium": "0%", "Basic": "25%", "Standard": "25%", "Unknown": "100%"}}}
{"purpose": "Analyze movie box office performance and audience ratings to identify trends and data quality issues.", "raw_table": "MovieID,Title,Genre,ReleaseDate,BoxOffice_Million,AudienceRating,Director\n1,The Last Stand,Action,2023-01-15,150.5,8.2,John Doe\n2,romantic escape,Romance,02/14/2023,85.3,7.5,Jane Smith\n3,Haunted Nights,Horror,2023/03/10,,6.8,Mary Johnson\n4,Comedy Central,comedy,2023-03-20,45.0,7.1,Chris Lee\n5,Space Odyssey,Sci-Fi,March 25 2023,220.4,9.0,John Doe\n6,The Last Stand,action,2023-01-15,152.0,8.3,John Doe\n7,Drama Life,Drama,2023-04-01,35a,7.0,Anna Kim\n8,Children's World,Animation,2023-04-10,60.2,,Tom Brown\n9,romantic escape,romance,2023-02-14,88.7,7.6,Jane Smith\n10,Unknown Genre,Thriller,2023-05-05,50.0,6.9,\n", "eda_steps": ["Check missing value percentages in all columns", "Standardize the 'Genre' column capitalization", "Parse and standardize 'ReleaseDate' into ISO format", "Convert 'BoxOffice_Million' to numeric and handle conversion errors", "Compute descriptive statistics for 'BoxOffice_Million' and 'AudienceRating'", "Generate value counts for 'Genre' and 'Director'", "Identify duplicate movie entries based on 'Title' and 'ReleaseDate'", "Summarize missing values in 'AudienceRating'", "Calculate the correlation between 'BoxOffice_Million' and 'AudienceRating'"], "eda_results": {"missing_values": {"MovieID": 0, "Title": 0, "Genre": 0, "ReleaseDate": 0, "BoxOffice_Million": 2, "AudienceRating": 2, "Director": 1}, "genre_value_counts": {"Action": 2, "Romance": 2, "Horror": 1, "Comedy": 1, "Sci-Fi": 1, "Drama": 1, "Animation": 1, "Thriller": 1}, "box_office_stats": {"count": 13, "mean": 90.3, "std_dev": 58.1, "min": 45, "max": 220.4}, "audience_rating_stats": {"count": 8, "mean": 7.4, "std_dev": 0.8, "min": 6.8, "max": 9.0}, "duplicate_movies": [{"Title": "The Last Stand", "ReleaseDate": "2023-01-15", "occurrences": 2}, {"Title": "romantic escape", "ReleaseDate": "2023-02-14", "occurrences": 2}], "box_office_audience_correlation": 0.65, "director_value_counts": {"John Doe": 3, "Jane Smith": 2, "Mary Johnson": 1, "Chris Lee": 1, "Anna Kim": 1, "Tom Brown": 1, "": 1}}}
{"purpose": "Analyze daily household energy consumption trends and identify data quality issues.", "raw_table": "Date,Household_ID,Energy_Consumption_kWh,Energy_Source,Peak_Usage_Time\n2024-01-01,H001,12.5,solar,18:00\n01/02/2024,h002,15.3,Wind,19:30\n2024-01-03,H003,,solar,17:45\n2024-1-4,H004,NaN,grid,20:00\n2024-01-05,H005,13.8,Solar,18:15\n2024-01-06,H006,11.9,GRID,18:45\n2024-01-07,h007,9.5,wind,18:30\n2024/01/08,H008,14.2,solar,\n2024-01-09,H009,16.1,Grid,19:00\n2024-01-10,H010,missing,solar,18:00\n2024-01-11,H011,13.0,Wind,17:30\n2024-01-12,H012,14.5,solar,18:00\n2024-01-13,h013,15.0,Grid,19:15", "eda_steps": ["Check and standardize date formats in the Date column", "Identify and count missing values in each column", "Standardize capitalization in categorical columns Energy_Source and Household_ID", "Convert Energy_Consumption_kWh to numeric and handle non-numeric entries", "Compute descriptive statistics for Energy_Consumption_kWh", "Generate value counts for Energy_Source", "Identify peak usage time distribution and check for missing values", "Calculate correlation between Energy_Consumption_kWh and Peak_Usage_Time hour", "Visualize energy consumption trends by date (not included in JSON)"], "eda_results": {"missing_values": {"Date": 0, "Household_ID": 0, "Energy_Consumption_kWh": 3, "Energy_Source": 0, "Peak_Usage_Time": 1}, "standardized_energy_source_counts": {"solar": 6, "wind": 3, "grid": 4}, "energy_consumption_stats": {"count": 12, "mean": 13.88, "std": 2.18, "min": 9.5, "25%": 12.5, "50%": 14.35, "75%": 15.3, "max": 16.1}, "peak_usage_time_missing": 1, "peak_usage_time_distribution": {"17:30": 1, "17:45": 1, "18:00": 3, "18:15": 1, "18:30": 1, "18:45": 1, "19:00": 1, "19:15": 1, "19:30": 1, "20:00": 1}, "correlation_energy_consumption_peak_hour": 0.45}}
{"purpose": "Analyze viewer ratings and release trends for a selection of popular TV shows.", "raw_table": "Show_Title,Genre,Release_Date,Seasons,Avg_Rating,Viewer_Count,Platform\nStranger Things,Sci-fi,2016-07-15,4,8.7,23000000,Netflix\nBreaking bad,Crime,01/20/2008,5,9.5,12000000,netflix\nThe Crown,Drama,2016/11/04,3,8.6,,Netflix\nGame of Thrones,Fantasy,2011-04-17,8,9.3,30000000,HBO\nThe Office,comedy,2005-03-24,9,8.9,15000000,netFlix\nBlack Mirror,Sci-Fi,2011-12-04,5,N/A,9000000,Netflix\nFriends,Comedy,1994-09-22,10,8.9,25000000,Hulu\nWestworld,SCI-FI,2016-10-02,3,8.7,10000000,HBO\nThe Mandalorian,Action,2019-11-12,2,8.8,15000000,Disney+\nSherlock,Crime,2010-07-25,4,9.1,11000000,BBC\n\n", "eda_steps": ["Check the data types of each column", "Standardize the 'Genre' and 'Platform' columns to consistent capitalization", "Parse and unify the 'Release_Date' format to YYYY-MM-DD", "Identify and report missing values in all columns", "Compute descriptive statistics for numeric columns: Seasons, Avg_Rating, Viewer_Count", "Generate value counts for the 'Genre' and 'Platform' columns", "Identify the top 3 shows by Viewer_Count", "Calculate the correlation between Seasons, Avg_Rating, and Viewer_Count"], "eda_results": {"data_types": {"Show_Title": "string", "Genre": "string", "Release_Date": "string", "Seasons": "integer", "Avg_Rating": "mixed (float and string 'N/A')", "Viewer_Count": "mixed (integer and missing)", "Platform": "string"}, "missing_values": {"Avg_Rating": 1, "Viewer_Count": 1}, "standardized_columns": {"Genre": {"Sci-fi": 3, "Crime": 2, "Drama": 1, "Fantasy": 1, "Comedy": 2, "Action": 1}, "Platform": {"Netflix": 5, "HBO": 2, "Hulu": 1, "Disney+": 1, "BBC": 1}}, "parsed_release_dates_format": "All dates converted to YYYY-MM-DD", "summary_stats": {"Seasons": {"count": 14, "mean": 4.71, "min": 2, "max": 10}, "Avg_Rating": {"count": 13, "mean": 8.91, "min": 8.6, "max": 9.5}, "Viewer_Count": {"count": 13, "mean": 16846154, "min": 9000000, "max": 30000000}}, "top_3_shows_by_viewer_count": ["Game of Thrones (30000000)", "Friends (25000000)", "Stranger Things (23000000)"], "correlations": {"Seasons_vs_Avg_Rating": 0.21, "Seasons_vs_Viewer_Count": 0.48, "Avg_Rating_vs_Viewer_Count": 0.58}}}
{"purpose": "Analyze urban taxi trip characteristics to identify data quality issues and common trip patterns.", "raw_table": "TripID,Pickup_Date,Pickup_Time,Dropoff_Time,Passenger_Count,Trip_Distance,Fare_Amount,Payment_Type\n1,2024-05-01,08:15 AM,08:40am,2,3.5,12.50,Cash\n2,2024/05/01,09:05,09:25,1,,15.00,credit\n3,05-01-2024,10:30 AM, 10:55 AM,,5.2,20.00,Cash\n4,2024-05-01,11:00 AM,11:20am,3,2.3,NaN,CARD\n5,2024-05-01,13:15,13:45,1,4.0,18.75,Cash\n6,2024-05-02,14:00,14:30,2,3.8,abc,Credit\n7,May 2 2024,15:10,15:35,4,6.0,25.50,CASH\n8,2024-05-02,16:00,16:20,,3.3,13.00,cash\n9,2024-05-02,17:30,17:55,1,2.0,10.00,Credit Card\n10,2024-05-02,18:00,18:25,2,NaN,14.00,CARD\n", "eda_steps": ["Check for missing values in each column", "Standardize and parse date and time columns", "Compute descriptive statistics for numeric columns: Passenger_Count, Trip_Distance, Fare_Amount", "Generate value counts for Payment_Type to identify inconsistent capitalization and categories", "Identify rows with non-numeric or invalid Fare_Amount values", "Calculate average trip duration in minutes", "Find correlation between Trip_Distance and Fare_Amount", "Summarize distribution skewness for Trip_Distance and Fare_Amount"], "eda_results": {"missing_values": {"Passenger_Count": 2, "Trip_Distance": 2, "Fare_Amount": 1}, "value_counts_Payment_Type": {"cash": 4, "Credit": 2, "CARD": 2, "Credit Card": 1, "CASH": 1}, "invalid_fare_amount_rows": [6], "descriptive_statistics": {"Passenger_Count": {"count": 8, "mean": 2.25, "std": 1.14, "min": 1, "max": 4}, "Trip_Distance": {"count": 8, "mean": 3.99, "std": 1.37, "min": 2.0, "max": 6.0}, "Fare_Amount": {"count": 9, "mean": 15.28, "std": 5.14, "min": 10.0, "max": 25.5}}, "average_trip_duration_minutes": 24, "correlation_TripDistance_FareAmount": 0.95, "skewness": {"Trip_Distance": 0.5, "Fare_Amount": 0.7}}}
{"purpose": "Analyze production line efficiency and identify patterns in defect types and machine downtime.", "raw_table": "Production_ID,Machine,Start_Date,End_Date,Units_Produced,Defect_Type,Downtime_Hours\n101,Lathe-1,2024/01/15,2024-01-16,500,Surface,2\n102,CNC-2,15-Jan-2024,,450,Crack,1.5\n103,lathe-1,2024-01-17,2024-01-17,NA,Surface,0.5\n104,CNC-2,2024-01-18,2024/01/19,470,,2\n105,Press-3,2024/01/20,2024-01-21,520,Bent,3\n106,press-3,20-Jan-2024,21-Jan-2024,530,Bent,NA\n107,Lathe-1,2024-01-22,2024-01-22,480,Surface,1\n108,CNC-2,2024/01/23,2024/01/24,460,crack,2\n109,CNC-2,,2024-01-25,455,Bent,1\n110,Press-3,2024-01-26,2024-01-27,515,,2", "eda_steps": ["Check and summarize missing values per column", "Standardize machine names capitalization", "Parse and unify date formats for Start_Date and End_Date", "Calculate production duration in days for each row", "Compute descriptive statistics for Units_Produced and Downtime_Hours", "Generate value counts for Defect_Type", "Identify rows with missing or anomalous Units_Produced", "Compute correlation between Units_Produced and Downtime_Hours", "Summarize average downtime by Machine"], "eda_results": {"missing_values": {"Production_ID": 0, "Machine": 0, "Start_Date": 1, "End_Date": 0, "Units_Produced": 1, "Defect_Type": 2, "Downtime_Hours": 1}, "standardized_machines": {"Lathe-1": 3, "CNC-2": 4, "Press-3": 3}, "parsed_dates": {"Start_Date": "All dates successfully parsed except 1 missing", "End_Date": "All dates successfully parsed"}, "production_duration_days": {"min": 0, "max": 1, "mean": 0.73, "median": 1}, "descriptive_stats": {"Units_Produced": {"count": 13, "mean": 489.23, "std": 28.84, "min": 450, "max": 530}, "Downtime_Hours": {"count": 9, "mean": 1.72, "std": 0.72, "min": 0.5, "max": 3}}, "defect_type_counts": {"Surface": 3, "Crack": 2, "crack": 1, "Bent": 3, "": 2}, "anomalous_units_produced_rows": [3], "correlation_units_downtime": -0.36, "avg_downtime_by_machine_hours": {"Lathe-1": 1.17, "CNC-2": 1.62, "Press-3": 2.5}}}
{"purpose": "Analyze recent monthly temperature and precipitation patterns from multiple climate stations to identify data quality issues and summarize key statistics.", "raw_table": "Station,Date,Temperature_C,Precipitation_mm,Weather_Condition\nNorthPole,2023/01/15,-30.5,5.2,Snow\nSouthPole,15-02-2023,-45.0,,Blizzard\nEquator,2023-03-01,29.3,0.0,clear\nnorthpole,2023-04-12,-25.1,1.8,Snow\nSouthPole,2023-05-10,-40.2,3.5,BlizZard\nEquator,2023/06/15,31.2,0.0,Clear\nNorthPole,,,-10.0,Snow\nSahara,2023-07-20,42.0,0,hot\nSahara,2023-08-20,41.5,,Hot\nEquator,2023-09-10,28.7,0.3,rain\nSouthPole,2023-10-05,-43.3,4.0,blizzard\nSahara,2023-11-25,40.1,,Hot\nnorthpole,2023-12-01,-28.4,2.1,Snow\n", "eda_steps": ["Check for missing values in each column", "Standardize and correct inconsistent capitalization in 'Station' and 'Weather_Condition' columns", "Parse and unify 'Date' column into ISO 8601 format (YYYY-MM-DD)", "Compute descriptive statistics for 'Temperature_C' and 'Precipitation_mm'", "Generate value counts for 'Station' and 'Weather_Condition' columns", "Identify rows with invalid or outlier temperature values", "Calculate correlation coefficient between temperature and precipitation", "Summarize the percentage of missing precipitation values per station"], "eda_results": {"missing_values": {"Station": 1, "Date": 1, "Temperature_C": 1, "Precipitation_mm": 4, "Weather_Condition": 0}, "standardized_categories": {"Station": ["NorthPole", "SouthPole", "Equator", "Sahara"], "Weather_Condition": ["Snow", "Blizzard", "Clear", "Hot", "Rain"]}, "date_formats_fixed": {"original_formats": ["2023/01/15", "15-02-2023", "2023-03-01", "", "2023/06/15", "2023-07-20", "2023-12-01"], "converted_to_iso": ["2023-01-15", "2023-02-15", "2023-03-01", null, "2023-06-15", "2023-07-20", "2023-12-01"]}, "summary_stats": {"Temperature_C": {"count": 13, "mean": 6.05, "std_dev": 32.42, "min": -45.0, "max": 42.0}, "Precipitation_mm": {"count": 9, "mean": 1.85, "std_dev": 2.06, "min": 0.0, "max": 5.2}}, "value_counts": {"Station": {"NorthPole": 4, "SouthPole": 3, "Equator": 3, "Sahara": 3}, "Weather_Condition": {"Snow": 4, "Blizzard": 3, "Clear": 2, "Hot": 3, "Rain": 1}}, "temperature_outliers": {"rows": [2, 1], "details": {"row_2": {"Station": "SouthPole", "Temperature_C": -45.0, "Date": "2023-02-15"}, "row_1": {"Station": "SouthPole", "Temperature_C": -40.2, "Date": "2023-05-10"}}}, "correlation": {"Temperature_C_vs_Precipitation_mm": -0.62}, "missing_precipitation_per_station_percent": {"NorthPole": 25, "SouthPole": 0, "Equator": 0, "Sahara": 66.7}}}
{"purpose": "Analyze user engagement patterns and content types on a social media platform during March 2024.", "raw_table": "user_id,post_date,content_type,likes,comments,shares\nU001,2024-03-01,text,15,2,0\nu002,03/02/2024,Image,45,,3\nU003,2024/03/03,video,100,20,5\nU004,March 4 2024,text,5,0,0\nU005,2024-03-05,Video,NaN,5,2\nU006,2024-3-06,IMAGE,30,1,1\nU007,,text,8,NaN,0\nu008,2024-03-08,link,0,0,0\nU009,03-09-2024,Text,12,3,\nU010,2024/03/10,video,80,15,7\nU011,2024-03-11,image,50,5,4\nU012,03/12/2024,link,3,0,0\nu013,2024-03-13,video,NaN,NaN,NaN\nU014,March 14 2024,text,7,1,0", "eda_steps": ["Check and summarize missing values in each column", "Standardize and parse post_date into a consistent date format", "Normalize content_type categories to lowercase", "Compute descriptive statistics for likes, comments, and shares", "Generate value counts for content_type", "Identify posts with zero engagement (likes, comments, shares all zero or missing)", "Calculate correlation matrix between likes, comments, and shares", "Determine top 3 posts by likes", "Summarize engagement trends over the dates"], "eda_results": {"missing_values": {"user_id": 0, "post_date": 1, "content_type": 0, "likes": 2, "comments": 2, "shares": 2}, "standardized_post_date": ["2024-03-01", "2024-03-02", "2024-03-03", "2024-03-04", "2024-03-05", "2024-03-06", null, "2024-03-08", "2024-03-09", "2024-03-10", "2024-03-11", "2024-03-12", "2024-03-13", "2024-03-14"], "normalized_content_type_counts": {"text": 5, "image": 3, "video": 4, "link": 2}, "summary_stats": {"likes": {"count": 12, "mean": 33.3, "std": 37.5, "min": 0, "25%": 7.0, "50%": 15.0, "75%": 45.0, "max": 100}, "comments": {"count": 12, "mean": 5.3, "std": 6.6, "min": 0, "25%": 0.75, "50%": 2.5, "75%": 7.5, "max": 20}, "shares": {"count": 12, "mean": 2.1, "std": 2.4, "min": 0, "25%": 0, "50%": 1, "75%": 3.5, "max": 7}}, "zero_engagement_posts": ["U008"], "correlations": {"likes_comments": 0.92, "likes_shares": 0.88, "comments_shares": 0.85}, "top_3_posts_by_likes": [{"user_id": "U003", "likes": 100}, {"user_id": "U010", "likes": 80}, {"user_id": "U011", "likes": 50}], "engagement_trends": {"peak_likes_date": "2024-03-03", "lowest_likes_date": "2024-03-08", "average_likes_week1": 26.6, "average_likes_week2": 39.5}}}
{"purpose": "Analyze recent temperature and precipitation patterns across selected climate zones.", "raw_table": "Date,Climate_Zone,Average_Temperature,Cumulative_Precipitation,Weather_Condition\n2024-01-15,TROPICAL,29.5,120.4,Sunny\n2024/01/16,tropical,30.1,115.0,Rain\n15-01-2024,SUBTROPICAL,22.0,,Cloudy\n2024-01-17,subtropical,21.5,5.5,RAIN\n2024-01-18,Temperate,10.2,0,Overcast\n2024-01-19,temperate,Not Available,2.3,Cloudy\n2024-01-20,Polar,-5.5,0,Snow\n2024-1-21,Polar,-6.0,,snow\n2024-01-22,Unknown,15.0,10.0,Foggy\n2024-01-23,TROPICAL,31.0,130.0,Sunny\n2024-01-24,SUBtropical,21.8,7.0,rain\n2024/01/25,temperate,11.0,0.0,Sunny\n2024-01-26,Polar,-4.8,0,Cloudy", "eda_steps": ["Standardize date formats to YYYY-MM-DD", "Normalize climate zone names to consistent capitalization", "Identify and count missing values per column", "Convert Average_Temperature to numeric, handle non-numeric entries", "Calculate descriptive statistics for Average_Temperature and Cumulative_Precipitation", "Generate frequency counts for Climate_Zone and Weather_Condition", "Check correlation between Average_Temperature and Cumulative_Precipitation", "Identify top 3 most common Weather_Condition entries"], "eda_results": {"missing_values": {"Date": 0, "Climate_Zone": 0, "Average_Temperature": 1, "Cumulative_Precipitation": 2, "Weather_Condition": 0}, "value_counts": {"Climate_Zone": {"Tropical": 3, "Subtropical": 3, "Temperate": 3, "Polar": 3, "Unknown": 1}, "Weather_Condition": {"Sunny": 3, "Rain": 3, "Cloudy": 2, "Overcast": 1, "Snow": 2, "Foggy": 1}}, "summary_stats": {"Average_Temperature": {"count": 12, "mean": 15.38, "std_dev": 14.08, "min": -6.0, "max": 31.0}, "Cumulative_Precipitation": {"count": 11, "mean": 29.3, "std_dev": 48.1, "min": 0.0, "max": 130.0}}, "correlations": {"Average_Temperature_vs_Precipitation": -0.28}, "top_categories": {"Weather_Condition": ["Sunny", "Rain", "Cloudy"]}}}
{"purpose": "Explore housing listing data to understand price distribution and missing data patterns.", "raw_table": "ListingID,Price,Location,Bedrooms,Bathrooms,SquareFeet,DateListed\n001,350000,Downtown,3,2,1500,2023-01-15\n002,450000,suburbs,4,3,2000,15/02/2023\n003,,Uptown,2,,1100,2023-03-01\n004,300000,downtown,3,2,NaN,2023-01-20\n005,NaN,Suburbs,5,4,2500,02-25-2023\n006,500000,uptown,4,3,2100,2023/03/10\n007,275000,DOWNTOWN,2,1,900,2023-01-22\n008,420000,Suburbs,,3,1800,2023-02-28\n009,380000,,3,2,1600,2023-01-30\n010,390000,Uptown,3,2,1700,2023-03-05", "eda_steps": ["Check and summarize missing values for each column", "Standardize the 'Location' column capitalization", "Compute descriptive statistics for numeric columns: Price, Bedrooms, Bathrooms, SquareFeet", "Generate value counts for the 'Location' column", "Analyze the distribution of 'DateListed' and identify inconsistent date formats", "Calculate the correlation matrix between numeric columns", "Identify top 2 most common bedroom counts"], "eda_results": {"missing_values": {"ListingID": 0, "Price": 2, "Location": 1, "Bedrooms": 2, "Bathrooms": 1, "SquareFeet": 1, "DateListed": 0}, "standardized_locations": {"Downtown": 3, "Suburbs": 3, "Uptown": 3, "": 1}, "summary_stats": {"Price": {"count": 8, "mean": 383125, "std": 73869, "min": 275000, "25%": 350000, "50%": 385000, "75%": 450000, "max": 500000}, "Bedrooms": {"count": 8, "mean": 3.25, "std": 1.09, "min": 2, "25%": 2.75, "50%": 3, "75%": 4, "max": 5}, "Bathrooms": {"count": 9, "mean": 2.44, "std": 0.83, "min": 1, "25%": 2, "50%": 2, "75%": 3, "max": 4}, "SquareFeet": {"count": 9, "mean": 1722, "std": 487, "min": 900, "25%": 1500, "50%": 1700, "75%": 2000, "max": 2500}}, "location_value_counts": {"Downtown": 3, "Suburbs": 3, "Uptown": 3, "Missing": 1}, "date_format_issues": {"Standard yyyy-mm-dd": 7, "DD/MM/YYYY": 1, "MM-DD-YYYY": 1, "YYYY/MM/DD": 1}, "correlations": {"Price_Bedrooms": 0.85, "Price_Bathrooms": 0.78, "Price_SquareFeet": 0.92, "Bedrooms_Bathrooms": 0.75, "Bedrooms_SquareFeet": 0.7, "Bathrooms_SquareFeet": 0.68}, "top_bedroom_counts": {"3": 4, "4": 2}}}
{"purpose": "Analyze production line efficiency and defects to identify patterns impacting output quality.", "raw_table": "Production_ID,Machine_ID,Operator,Production_Date,Shift,Units_Produced,Defects,Production_Time_Minutes\n001,MACH-A,John Doe,2024/04/01,Morning,100,5,240\n002,mach-a,Mary Jane,04-01-2024,Evening,110,3,235\n003,MACH-B,alice smith,2024-04-02,MORNING,95,,250\n004,Mach-C,Bob Lee,2024/4/02,Night,80,8,260\n005,MACH-B,ALICE SMITH,2024-04-03,morning,105,2,245\n006,Mach-a,John Doe,,Evening,90,6,230\n007,MACH-C,Bob Lee,2024/04/03,Night,85,,255\n008,Mach-C,Mary Jane,2024/04/04,morning,100,1,240\n009,MACH-A,John Doe,2024-04-04,Evening,95,4,238\n010,MACH-B,Alice Smith,April 5 2024,Morning,102,3,242\n011,MACH-B,,2024-04-05,Morning,100,2,240\n", "eda_steps": ["Check for missing values in each column", "Standardize 'Machine_ID' and 'Shift' capitalization", "Parse 'Production_Date' into a consistent date format", "Compute descriptive statistics for 'Units_Produced', 'Defects', and 'Production_Time_Minutes'", "Generate value counts for 'Operator' and 'Machine_ID'", "Calculate correlation matrix for numeric columns", "Identify rows with missing 'Operator' or 'Defects' values", "Summarize defect rates per shift"], "eda_results": {"missing_values": {"Production_ID": 0, "Machine_ID": 0, "Operator": 1, "Production_Date": 1, "Shift": 0, "Units_Produced": 0, "Defects": 2, "Production_Time_Minutes": 0}, "standardized_values": {"Machine_ID": ["MACH-A", "MACH-B", "MACH-C"], "Shift": ["Morning", "Evening", "Night"]}, "date_parsing_issues": 0, "summary_stats": {"Units_Produced": {"count": 11, "mean": 96.09, "std": 9.34, "min": 80, "25%": 90, "50%": 95, "75%": 102, "max": 110}, "Defects": {"count": 9, "mean": 3.78, "std": 2.36, "min": 1, "25%": 2, "50%": 3, "75%": 5, "max": 8}, "Production_Time_Minutes": {"count": 11, "mean": 244.5, "std": 9.91, "min": 230, "25%": 238, "50%": 242, "75%": 250, "max": 260}}, "value_counts": {"Operator": {"John Doe": 3, "Mary Jane": 2, "Alice Smith": 3, "Bob Lee": 3, "": 1}, "Machine_ID": {"MACH-A": 4, "MACH-B": 4, "MACH-C": 3}}, "correlations": {"Units_Produced_Defects": -0.63, "Units_Produced_Production_Time_Minutes": -0.42, "Defects_Production_Time_Minutes": 0.31}, "rows_with_missing_values": {"Operator": [11], "Defects": [3, 7], "Production_Date": [6]}, "defect_rates_per_shift": {"Morning": 3.0, "Evening": 4.33, "Night": 4.5}}}
{"purpose": "Analyze customer call patterns and identify common issues in telecom service usage.", "raw_table": "CustomerID,CallDate,CallDuration,CallType,IssueReported,PlanType\n001,2023-01-15,300,Voice,No Signal,Basic\n002,01/20/2023,180,voice,,premium\n003,2023-01-22,NaN,DATA,Slow Internet,Basic\n004,2023/01/23,240,Voice,Dropped Calls,PREMIUM\n005,15-01-2023,120,voice,,basic\n006,2023-01-25,60,VoIP,Audio Distortion,Premium\n007,,200,Voice,No Signal,basic\n008,2023-01-27,300,DATA,,basic\n009,2023-1-28,150,voice,Dropped calls,PREMIUM\n010,2023-01-29,100,Voice,No signal,BASIC", "eda_steps": ["Check for missing values in each column", "Standardize capitalization in CallType and PlanType columns", "Parse CallDate into consistent date format", "Compute descriptive statistics for CallDuration", "Generate value counts for CallType and IssueReported", "Identify percentage of calls with reported issues", "Analyze call duration distribution by PlanType", "Find correlations between CallDuration and reported issues"], "eda_results": {"missing_values": {"CustomerID": 0, "CallDate": 1, "CallDuration": 1, "CallType": 0, "IssueReported": 4, "PlanType": 0}, "standardized_categories": {"CallType": ["voice", "data", "voip"], "PlanType": ["basic", "premium"]}, "call_date_consistency": "All dates parsed successfully with 3 original formats (YYYY-MM-DD, MM/DD/YYYY, DD-MM-YYYY)", "summary_stats": {"CallDuration": {"count": 13, "mean": 185.38, "median": 180, "min": 60, "max": 300}}, "value_counts": {"CallType": {"voice": 7, "data": 2, "voip": 1}, "IssueReported": {"No Signal": 3, "Dropped Calls": 2, "Audio Distortion": 1, "Slow Internet": 1, "": 4}}, "reported_issue_percentage": 61.54, "call_duration_by_plan": {"basic": {"mean_duration": 220, "count": 6}, "premium": {"mean_duration": 140, "count": 4}}, "correlations": {"CallDuration_vs_IssueReported": "Calls with reported issues had 25% longer average duration compared to calls without issues"}}}
{"purpose": "Explore viewership patterns and content characteristics of streaming platform shows.", "raw_table": "Show_ID,Title,Genre,Release_Date,Avg_Viewers_Millions,Rating,Seasons,Language\n101,The Last Quest,Fantasy,2021-07-15,3.5,8.2,3,english\n102,space tales,SCI-FI,07/20/2020,5.1,7.8,2,English\n103,Love & Drama,Drama,2020-13-05,2.8, ,1,english\n104,Chef's Journey,Reality,2019-11-30,1.9,6.5,4,ENglish\n105,Haunted Nights,Horror,2022-01-25,N/A,7.2,2,English\n106,The BIG Game,Sports,2018-06-10,4.5,8.0,5,english\n107,comedy Central,COMEDY,2017-08-24,3.9,7.5,6,English\n108,Unknown lands,Fantasy,2019-09-15,3.0,7.0,3,English\n109,space tales,SCI-Fi,2020-07-20,5.3,7.9,2,english\n110,Mystery Manor,Mystery,2021/12/01,2.5,7.4,,English\n111,The Last Quest,Fantasy,2021-07-15,3.7,8.3,3,english", "eda_steps": ["Check and summarize missing values in each column", "Standardize capitalization in 'Genre' and 'Language' columns", "Identify duplicate show titles with different Show_IDs", "Compute basic statistics for 'Avg_Viewers_Millions' and 'Rating'", "Check for inconsistent or invalid date formats in 'Release_Date'", "Count the number of shows per genre after cleaning", "Identify shows with missing or zero seasons", "Find correlation between 'Avg_Viewers_Millions' and 'Rating'"], "eda_results": {"missing_values": {"Avg_Viewers_Millions": 1, "Rating": 1, "Seasons": 1}, "standardized_genres": ["Fantasy", "Sci-Fi", "Drama", "Reality", "Horror", "Sports", "Comedy", "Mystery"], "standardized_languages": ["English"], "duplicate_titles": {"The Last Quest": [101, 111], "Space Tales": [102, 109]}, "descriptive_statistics": {"Avg_Viewers_Millions": {"count": 11, "mean": 3.82, "median": 3.7, "min": 1.9, "max": 5.3}, "Rating": {"count": 10, "mean": 7.44, "median": 7.4, "min": 6.5, "max": 8.3}}, "invalid_dates": ["2020-13-05"], "shows_per_genre": {"Fantasy": 3, "Sci-Fi": 2, "Drama": 1, "Reality": 1, "Horror": 1, "Sports": 1, "Comedy": 1, "Mystery": 1}, "missing_or_zero_seasons": [{"Show_ID": 110, "Title": "Mystery Manor", "Seasons": null}], "correlation_Avg_Viewers_Rating": 0.75}}
{"purpose": "Analyze stock trading activity and price variations for selected companies over a short period.", "raw_table": "TradeID,Symbol,TradeDate,TradeTime,Price,Volume,TraderType\n1,AAPL,2023/06/10,09:30AM,149.5,1000,Institutional\n2,goog,2023-06-10,09:31,2340.7,500,retail\n3,MSFT,06-10-2023,09:32:15,259.3,NaN,Institutional\n4,AMZN,2023/06/10,0933,3345.2,200,RETAIL\n5,aapl,2023-06-10,09:34AM,149.8,800,Institutional\n6,GOOG,2023-06-10,,2341.0,600,Retail\n7,msft,2023-06-10,09:36:00,258.9,700,Institutional\n8,AMZN,2023/6/10,09:37,3344.0,100,retail\n9,,,,,400,Institutional\n10,AAPL,2023-06-10,09:39AM,NaN,900,Retail\n11,GOOG,2023-06-10,09:40AM,2342.5,abc,Institutional\n12,MSFT,2023/06/10,09:41,260.1,1200,Retail\n13,AMZN,2023-06-10,09:42,NaN,150,Institutional\n14,AAPL,2023-06-10,09:43AM,150.2,1000,Institutional", "eda_steps": ["Check and summarize missing values for each column", "Standardize the Symbol column capitalization to uppercase", "Convert TradeDate and TradeTime columns into a single datetime column", "Identify and handle non-numeric and invalid values in Price and Volume columns", "Compute descriptive statistics (mean, median, std) for Price and Volume", "Generate value counts for the TraderType column with standardized capitalization", "Calculate correlation between Price and Volume where applicable", "Identify top 2 traded symbols by total volume"], "eda_results": {"missing_values": {"TradeID": 0, "Symbol": 1, "TradeDate": 1, "TradeTime": 1, "Price": 3, "Volume": 2, "TraderType": 0}, "standardized_symbols": ["AAPL", "GOOG", "MSFT", "AMZN"], "datetime_conversion_issues": 1, "invalid_price_entries": [9, 10, 13], "invalid_volume_entries": [9, 11], "summary_stats": {"Price": {"mean": 1657.65, "median": 259.3, "std": 1213.9}, "Volume": {"mean": 729.1, "median": 700, "std": 344.7}}, "trader_type_counts": {"Institutional": 7, "Retail": 6}, "price_volume_correlation": 0.12, "top_traded_symbols_by_volume": {"AAPL": 3700, "MSFT": 2900}}}
{"purpose": "Analyze public transportation trip patterns and identify data quality issues in ridership records.", "raw_table": "TripID,Date,Route,Passenger_Count,Start_Time,End_Time,Delay_Minutes\n1,2024/04/01,Route A,25,08:00AM,08:45AM,5\n2,04-01-2024,route b,30,09:15,10:00,0\n3,2024-04-01,Route C,,10:30AM,11:10AM,NA\n4,2024/4/02,route a,20,08:05 AM,08:50 AM,3\n5,2024-04-02,Route B,27,09:20,09:55,1\n6,04/02/2024,Route D,15,11:00 AM,11:40 AM,7\n7,2024-04-02,route c,22,10:35,11:15,2\n8,2024-4-03,Route A,23,08:10AM,08:55AM,NA\n9,04.03.2024,ROUTE B,29,09:00,09:45,0\n10,2024-04-03,Route C,NA,10:25AM,11:05AM,4\n11,2024/04/03,route d,18,11:05,11:45,6\n12,2024-04-04,Route A,24,08:15 AM,09:00 AM,1\n13,04-04-2024,Route B,31,09:10,09:50,NA\n14,2024/04/04,route c,20,10:40,11:20,3", "eda_steps": ["Check the total number of rows and columns in the dataset", "Identify missing values and their percentage per column", "Standardize the 'Route' column capitalization and list unique routes", "Convert 'Date' column to a consistent date format", "Compute descriptive statistics (mean, median, std) for 'Passenger_Count' and 'Delay_Minutes'", "Count the frequency of trips per route", "Analyze the distribution of 'Delay_Minutes' including handling missing or 'NA' values", "Check data types of each column and identify inconsistencies", "Calculate the average trip duration from 'Start_Time' and 'End_Time'"], "eda_results": {"dataset_shape": {"rows": 14, "columns": 7}, "missing_values_percent": {"Passenger_Count": 14.29, "Delay_Minutes": 21.43}, "unique_routes": ["Route A", "Route B", "Route C", "Route D"], "passenger_count_stats": {"mean": 24.14, "median": 24, "std": 4.56, "count": 12}, "delay_minutes_stats": {"mean": 2.88, "median": 2, "std": 2.55, "count": 11}, "trip_frequency_by_route": {"Route A": 4, "Route B": 4, "Route C": 4, "Route D": 2}, "data_types": {"TripID": "integer", "Date": "string/mixed formats", "Route": "string (inconsistent capitalization)", "Passenger_Count": "mixed (numeric with missing values as blanks or NA)", "Start_Time": "string (mixed AM/PM and 24-hour formats)", "End_Time": "string (mixed formats)", "Delay_Minutes": "mixed (numeric with 'NA' strings)"}, "average_trip_duration_minutes": 45}}
{"purpose": "Analyze customer call data to identify usage patterns and data quality issues.", "raw_table": "CustomerID,CallDate,CallDuration,CallType,DataUsageMB,PlanType\nC001,2024-04-01,300,Voice, ,Premium\nc002,04/02/2024,180,voice,50,standard\nC003,2024/04/03, ,SMS,0,Standard\nC004,2024-04-04,240,Voice,75,PREMIUM\nc005,04-05-2024,NaN,voice,NaN,standard\nC006,2024-04-06,60,SMS,,basic\nc007,2024-4-07,120,VoIP,30,Basic\nC008,,200,Voice,45,Standard\nC009,2024-04-09,NaN,Voice,100,PREMIUM\nC010,2024-04-10,90,voice,NaN,standard\n", "eda_steps": ["Check missing value percentages for each column", "Standardize capitalization in CallType and PlanType columns", "Convert CallDuration and DataUsageMB to numeric types and handle missing values", "Parse CallDate into a consistent date format", "Generate descriptive statistics for CallDuration and DataUsageMB", "Count the frequency of each CallType", "Count the frequency of each PlanType", "Identify customers with missing CallDate or CallDuration", "Examine correlation between CallDuration and DataUsageMB"], "eda_results": {"missing_values": {"CustomerID": 0, "CallDate": 1, "CallDuration": 3, "CallType": 0, "DataUsageMB": 4, "PlanType": 0}, "standardized_categories": {"CallType": ["Voice", "SMS", "VoIP"], "PlanType": ["Premium", "Standard", "Basic"]}, "descriptive_stats": {"CallDuration": {"count": 7, "mean": 167.14, "std": 90.42, "min": 60, "25%": 90, "50%": 120, "75%": 240, "max": 300}, "DataUsageMB": {"count": 6, "mean": 50, "std": 33.56, "min": 0, "25%": 30, "50%": 47.5, "75%": 75, "max": 100}}, "value_counts": {"CallType": {"Voice": 6, "SMS": 2, "VoIP": 1}, "PlanType": {"Standard": 4, "Premium": 3, "Basic": 2}}, "customers_missing_info": {"Missing CallDate": ["C008"], "Missing CallDuration": ["C003", "C005", "C009"]}, "correlations": {"CallDuration_vs_DataUsageMB": 0.82}}}
{"purpose": "Analyze household energy consumption patterns and identify missing or inconsistent data entries.", "raw_table": "Household_ID,Date,Energy_Consumed_kWh,Energy_Source,Region\nH001,2024-01-05,12.5,solar,North\nH002,01/07/2024,15,,south\nh003,2024-01-08,9.8,Wind,East\nH004,2024/01/09,11.2,SOLAR,North\nH005,2024-01-10,,coal,West\nH006,2024-1-11,14.0,wind,South\nH007,2024-01-12,13.5,Coal,west\nH008,2024-01-13,nan,solar,East\nH009,2024-01-14,10.8,Nuclear,North\nH010,2024-01-15,8.9,solar,East\nH011,15-01-2024,7.5,Wind,South\nH012,2024-01-16,14.3,nuclear,North", "eda_steps": ["Check and standardize date formats", "Identify and count missing values in each column", "Standardize capitalization in categorical columns 'Energy_Source' and 'Region'", "Calculate descriptive statistics for 'Energy_Consumed_kWh'", "Generate value counts for 'Energy_Source' and 'Region'", "Identify rows with inconsistent or unusual energy sources", "Compute the number of unique households", "Summarize distribution skewness for 'Energy_Consumed_kWh'"], "eda_results": {"missing_values": {"Household_ID": 0, "Date": 0, "Energy_Consumed_kWh": 2, "Energy_Source": 1, "Region": 0}, "standardized_energy_source_value_counts": {"solar": 4, "wind": 3, "coal": 2, "nuclear": 2, "": 1}, "standardized_region_value_counts": {"north": 4, "south": 3, "east": 3, "west": 2}, "descriptive_stats_energy_consumed_kWh": {"count": 11, "mean": 11.5, "std": 2.3, "min": 7.5, "25%": 9.8, "50%": 11.2, "75%": 13.5, "max": 15.0}, "unique_households": 12, "inconsistent_energy_source_entries": ["", "nan"], "date_formats_found": ["YYYY-MM-DD", "YYYY/MM/DD", "DD-MM-YYYY", "MM/DD/YYYY", "YYYY-M-D"], "energy_consumed_skewness": 0.15}}
{"purpose": "Explore the characteristics and data quality of recent real estate listings to understand price distribution and data completeness.", "raw_table": "ListingID,Neighborhood,Price,Bedrooms,Bathrooms,SquareFeet,DateListed,PropertyType\n101,Downtown,450000,2,1.5,850,2023-06-15,Condo\n102,suburbs,350000,3,2,,06/20/2023,Single-Family\n103,Riverside,575000,4,3,2000,2023/06/18,Single family\n104,Suburbs,NaN,3,2,1500,2023-06-17,Townhouse\n105,downtown,480000,2,1.5,900,15-06-2023,condo\n106,Riverside,620000,4,3,2100,2023-6-19,Single-Family\n107,Midtown,300000,1,1,700,2023-06-16,Apartment\n108,MidTown,NaN,1,1,720,2023/06/20,apartment\n109,Suburbs,400000,3,2,1400,,Single-Family\n110,Downtown,455000,2,2,880,2023-06-21,Condo\n111,Suburbs,NaN,,2,1300,2023-06-22,Townhouse\n112,Riverside,590000,4,3,2050,2023-06-23,Single-Family", "eda_steps": ["Check for missing values in all columns", "Standardize the Neighborhood and PropertyType categorical columns for consistent capitalization", "Compute descriptive statistics for Price, Bedrooms, Bathrooms, and SquareFeet", "Generate value counts for Neighborhood and PropertyType", "Analyze the distribution of DateListed and standardize date formats", "Identify listings with missing Price or Bedrooms data", "Calculate the correlation between Price and SquareFeet", "Summarize the count of listings by Neighborhood"], "eda_results": {"missing_values": {"ListingID": 0, "Neighborhood": 0, "Price": 3, "Bedrooms": 1, "Bathrooms": 0, "SquareFeet": 1, "DateListed": 1, "PropertyType": 0}, "standardized_categories": {"Neighborhood": ["Downtown", "Suburbs", "Riverside", "Midtown"], "PropertyType": ["Condo", "Single-Family", "Townhouse", "Apartment"]}, "summary_stats": {"Price": {"count": 12, "mean": 485416.67, "std": 100095.78, "min": 300000, "25%": 415000, "50%": 455000, "75%": 590000, "max": 620000}, "Bedrooms": {"count": 13, "mean": 2.73, "std": 1.09, "min": 1, "25%": 2, "50%": 3, "75%": 4, "max": 4}, "Bathrooms": {"count": 14, "mean": 2.07, "std": 0.64, "min": 1, "25%": 1.5, "50%": 2, "75%": 3, "max": 3}, "SquareFeet": {"count": 13, "mean": 1377.69, "std": 534.59, "min": 700, "25%": 880, "50%": 1400, "75%": 2050, "max": 2100}}, "value_counts": {"Neighborhood": {"Downtown": 3, "Suburbs": 4, "Riverside": 4, "Midtown": 2}, "PropertyType": {"Condo": 3, "Single-Family": 5, "Townhouse": 2, "Apartment": 2}}, "date_format_issues": {"inconsistent_formats": 3, "missing_dates": 1}, "listings_missing_critical_info": {"missing_Price": [104, 108, 111], "missing_Bedrooms": [111]}, "correlations": {"Price_SquareFeet": 0.91}, "listings_per_neighborhood": {"Downtown": 3, "Suburbs": 4, "Riverside": 4, "Midtown": 2}}}
{"purpose": "Analyze crop yield patterns across different farm regions and planting dates.", "raw_table": "Farm_ID,Region,Crop_Type,Planting_Date,Yield_kg,Soil_pH,Rainfall_mm\nF001,north,Maize,2023-04-15,2500,6.5,120\nF002,South,maize,15/04/2023,2450,,110\nf003,East,Wheat,2023/04/20,1800,7.1,95\nF004,west,Rice,2023-04-18,NaN,6.8,85\nF005,North,Rice,04-22-2023,2100,6.9,100\nF006,South,Wheat,2023-04-19,1750,7.0,NaN\nF007,East,maize,2023-04-21,2600,6.6,125\nF008,WEST,Rice,2023/04/17,2000,6.7,90\nF009,North,MAIZE,2023-04-15,2550,6.4,130\nF010,South,wheat,2023-04-20,1700,7.2,105\nF011,,Rice,2023-04-18,1950,6.8,95\nF012,East,Maize,,2400,6.5,115", "eda_steps": ["Standardize capitalization in 'Region' and 'Crop_Type' columns", "Parse and unify 'Planting_Date' formats into YYYY-MM-DD", "Check and report missing values in all columns", "Compute descriptive statistics for numeric columns: Yield_kg, Soil_pH, Rainfall_mm", "Generate value counts for 'Region' and 'Crop_Type'", "Analyze correlation between Yield_kg, Soil_pH, and Rainfall_mm", "Identify rows with missing or inconsistent 'Planting_Date'", "Summarize average Yield_kg by Crop_Type", "Visualize distribution skewness of Yield_kg"], "eda_results": {"missing_values": {"Farm_ID": 0, "Region": 1, "Crop_Type": 0, "Planting_Date": 1, "Yield_kg": 1, "Soil_pH": 1, "Rainfall_mm": 1}, "value_counts": {"Region": {"North": 3, "South": 3, "East": 3, "West": 2, "": 1}, "Crop_Type": {"Maize": 5, "Wheat": 3, "Rice": 4}}, "summary_stats": {"Yield_kg": {"count": 14, "mean": 2142.86, "std": 396.87, "min": 1700, "25%": 1800, "50%": 2100, "75%": 2500, "max": 2600}, "Soil_pH": {"count": 13, "mean": 6.81, "std": 0.28, "min": 6.4, "25%": 6.5, "50%": 6.8, "75%": 7.0, "max": 7.2}, "Rainfall_mm": {"count": 13, "mean": 108.46, "std": 13.29, "min": 85, "25%": 95, "50%": 100, "75%": 120, "max": 130}}, "correlations": {"Yield_kg_Soil_pH": -0.15, "Yield_kg_Rainfall_mm": 0.45, "Soil_pH_Rainfall_mm": 0.05}, "inconsistent_dates": ["F002", "F004", "F005", "F008", "F012"], "average_yield_by_crop": {"Maize": 2500, "Wheat": 1750, "Rice": 2012.5}, "yield_skewness": 0.2}}
{"purpose": "Analyze student performance and attendance patterns in a high school semester.", "raw_table": "StudentID,Name,Grade,Subject,Score,Attendance,ExamDate\n101,alice,10,Math,85,Present,2023-03-15\n102,Bob,10,science,78,absent,15/03/2023\n103,Charlie,11,Math,,present,2023/03/16\n104,diana,11,English,90,Present,03-17-2023\n105,Edward,10,Science,82,Absent,2023-03-15\n106,Fiona,12,Math,88,present,2023-03-15\n107,george,11,english,75,PRESENT,17 Mar 2023\n108,Helen,10,Math,95,Present,\n109,Ian,12,Science,NaN,Absent,2023-03-15\n110,Jane,11,English,85,Present,2023-03-16", "eda_steps": ["Check and count missing values in each column", "Standardize the capitalization in the Name, Subject, and Attendance columns", "Convert ExamDate to a consistent date format", "Calculate basic statistics (mean, median, std) for the Score column", "Generate value counts for the Subject and Attendance columns", "Identify students with missing or invalid scores", "Analyze average scores by Grade and Subject", "Summarize attendance rates overall and by Grade", "Check correlation between Score and Attendance status"], "eda_results": {"missing_values": {"StudentID": 0, "Name": 0, "Grade": 0, "Subject": 0, "Score": 3, "Attendance": 0, "ExamDate": 1}, "standardized_categories": {"Name_sample": ["Alice", "Bob", "Charlie", "Diana"], "Subject_sample": ["Math", "Science", "English"], "Attendance_sample": ["Present", "Absent"]}, "exam_date_consistency": {"converted_dates": 14, "missing_or_invalid": 1}, "score_statistics": {"mean": 84.7, "median": 85, "std_dev": 6.9, "count": 7}, "value_counts": {"Subject": {"Math": 4, "Science": 3, "English": 3}, "Attendance": {"Present": 7, "Absent": 3}}, "students_missing_scores": ["Charlie", "Ian", "Helen"], "average_scores_by_grade_subject": {"10": {"Math": 90.0, "Science": 80.0}, "11": {"Math": null, "English": 83.3}, "12": {"Math": 88.0, "Science": null}}, "attendance_rates": {"overall_present_percent": 70, "present_by_grade": {"10": 75, "11": 66.7, "12": 50}}, "correlation_score_attendance": {"score_vs_present": 0.68}}}
{"purpose": "Explore patient demographics and lab test results to identify data quality issues and distribution patterns.", "raw_table": "Patient_ID,Age,Gender,Diagnosis,Test_Date,Glucose_Level,Blood_Pressure\n001,45,Male,diabetes,2023-01-15,110,120/80\n002,52,female,Hypertension,15-02-2023,NaN,140/90\n003,38,F,Diabetes,2023/03/10,130,135/85\n004,,Male,hypertension,2023-04-05,125,NaN\n005,29,Female,diabeteS,04-15-2023,115,118/78\n006,60,M,HYPERTENSION,2023-05-20,,145/95\n007,54,male,Diabetes,,140,130/85\n008,47,Female,Diabete,2023-07-01,128,125/82\n009,33,M,Hypertension,2023-07-15,118,122/80\n010,41,F,,2023-08-10,122,NaN\n011,50,Female,Hypertension,2023-08-20,135,138/90", "eda_steps": ["Check for missing values in each column", "Standardize the Diagnosis column values (case normalization and spelling correction)", "Parse and unify Test_Date column to ISO format", "Compute descriptive statistics for Age and Glucose_Level", "Calculate value counts for Gender and Diagnosis", "Analyze the format consistency of Blood_Pressure and identify missing values", "Identify correlations between Age and Glucose_Level", "Summarize number of unique patients and duplicates"], "eda_results": {"missing_values": {"Patient_ID": 0, "Age": 1, "Gender": 0, "Diagnosis": 1, "Test_Date": 1, "Glucose_Level": 2, "Blood_Pressure": 3}, "standardized_diagnosis": {"Diabetes": 5, "Hypertension": 5, "Missing": 1}, "parsed_test_dates": {"earliest": "2023-01-15", "latest": "2023-08-20", "missing": 1}, "summary_stats": {"Age": {"count": 10, "mean": 44.9, "std": 9.9, "min": 29, "max": 60}, "Glucose_Level": {"count": 8, "mean": 126.6, "std": 9.4, "min": 110, "max": 140}}, "value_counts": {"Gender": {"Female": 5, "Male": 4, "F": 2, "M": 2, "m": 1}, "Diagnosis": {"Diabetes": 5, "Hypertension": 5, "Missing": 1}}, "blood_pressure_issues": {"missing_values": 3, "inconsistent_formats": 0}, "correlations": {"Age_vs_Glucose_Level": 0.78}, "unique_patients": 11, "duplicates_found": 0}}
{"purpose": "Analyze user engagement patterns and content types on a social media platform.", "raw_table": "user_id,post_date,content_type,likes,comments,shares\n101,2023-05-01,Photo,25,5,2\n102,05/02/2023,video,30,10,\n103,2023/05/03,text,15,,1\n104,2023-5-04,Photo,20,3,0\n105,May 05 2023,Video,45,15,5\n106,2023-05-06,photo,NaN,7,3\n107,2023-05-07,Text,,0,0\n108,2023-05-08,Live,50,20,10\n109,2023-05-09,live,40,NaN,8\n110,05-10-2023,Poll,10,1,0\n111,2023-05-11,Text,12,2,1\n112,,video,35,12,4\n113,2023-05-13,Video,NaN,NaN,NaN", "eda_steps": ["Check for missing values in all columns", "Standardize the date format in the post_date column", "Normalize content_type to consistent capitalization", "Compute descriptive statistics for likes, comments, and shares", "Generate value counts for content_type", "Identify posts with missing engagement metrics", "Calculate correlation matrix between likes, comments, and shares", "Find top 3 most engaging posts based on total interactions (likes + comments + shares)"], "eda_results": {"missing_values": {"user_id": 0, "post_date": 1, "content_type": 0, "likes": 3, "comments": 3, "shares": 3}, "date_format_standardized": {"original_formats": ["2023-05-01", "05/02/2023", "2023/05/03", "2023-5-04", "May 05 2023", "05-10-2023", ""], "standard_format": "YYYY-MM-DD", "unparseable_dates": 1}, "content_type_normalized": {"before": ["Photo", "video", "text", "Photo", "Video", "photo", "Text", "Live", "live", "Poll", "Text", "video", "Video"], "after": ["Photo", "Video", "Text", "Photo", "Video", "Photo", "Text", "Live", "Live", "Poll", "Text", "Video", "Video"]}, "summary_stats": {"likes": {"count": 10, "mean": 28.4, "std": 14.3, "min": 10, "25%": 15.0, "50%": 25.0, "75%": 40.0, "max": 50}, "comments": {"count": 10, "mean": 7.5, "std": 6.0, "min": 0, "25%": 2.0, "50%": 5.0, "75%": 10.0, "max": 20}, "shares": {"count": 10, "mean": 3.4, "std": 3.5, "min": 0, "25%": 1.0, "50%": 2.0, "75%": 5.0, "max": 10}}, "value_counts_content_type": {"Photo": 3, "Video": 5, "Text": 3, "Live": 2, "Poll": 1}, "posts_missing_engagement": 4, "correlations": {"likes_comments": 0.87, "likes_shares": 0.78, "comments_shares": 0.75}, "top_3_engaging_posts": [{"user_id": 108, "total_interactions": 80}, {"user_id": 105, "total_interactions": 65}, {"user_id": 109, "total_interactions": 48}]}}
{"purpose": "Examine patterns in daily public transportation ridership and identify data quality issues.", "raw_table": "Date,Route,Passengers,FareCollected,DriverName\n2024-01-01,Blue Line,120,240.5,John Smith\n01/02/2024,Green line,NA,180.0,Anna lee\n2024/01/03,blue line,130,260.0,MICHAEL BROWN\n2024-01-04,Red Line,115,230.0,Jessica\n2024-01-05,Green Line,105,,Chris O'Neil\n2024-01-06,red line,NA,210.0,Anna lee\n2024-01-07,Blue Line,140,280.0,John smith\n2024-1-08,Yellow Line,50,100,David\n2024-01-09,yellow line,47,94.0,David\n2024-01-10,Green Line,NA,195.5,ANNA LEE\n2024-01-11,Blue Line,135,270.0,MICHAEL brown\n2024-01-12,Red line,110,220,Jessica\n,Green Line,100,200.0,Chris O'Neil\n2024-01-14,Blue line,125,250.0,john smith", "eda_steps": ["Check and summarize missing values per column", "Standardize and count unique categories in the Route column ignoring capitalization", "Compute descriptive statistics for Passengers and FareCollected columns", "Identify inconsistent date formats and count unique dates", "Find unique DriverName entries and standardize capitalization", "Calculate average Passengers and FareCollected per Route", "Assess correlation between Passengers and FareCollected"], "eda_results": {"missing_values": {"Date": 1, "Route": 0, "Passengers": 3, "FareCollected": 1, "DriverName": 0}, "route_value_counts": {"blue line": 5, "green line": 5, "red line": 3, "yellow line": 2}, "passengers_stats": {"count": 11, "mean": 113.18, "std": 28.93, "min": 47, "max": 140}, "fare_collected_stats": {"count": 13, "mean": 221.42, "std": 61.91, "min": 94, "max": 280}, "date_formats_detected": {"YYYY-MM-DD": 11, "MM/DD/YYYY": 1, "YYYY/MM/DD": 1, "missing": 1}, "unique_drivers_standardized": ["john smith", "anna lee", "michael brown", "jessica", "chris o'neil", "david"], "average_passengers_per_route": {"blue line": 130, "green line": 103.75, "red line": 112.5, "yellow line": 48.5}, "average_fare_per_route": {"blue line": 260, "green line": 191.38, "red line": 225, "yellow line": 97}, "passengers_fare_correlation": 0.998}}
{"purpose": "Analyze real estate listing data to understand property types, prices, and data quality issues.", "raw_table": "ListingID,DateListed,Location,PropertyType,Price,Bedrooms,Bathrooms,SquareFeet,AgentName\n1,2024/01/15,Downtown,Condo,350000,2,2,950,John Doe\n2,01-20-2024,Uptown,Single Family,450000,3,2.5,1400,Jane Smith\n3,2024-02-01,Downtown,condo,370000,2,,980,john doe\n4,02/10/2024,suburbs,Townhouse,310000,3,2,1200,Mary Johnson\n5,,Uptown,Single family,480000,4,3,1600,Jane Smith\n6,2024-03-01,Downtown,Condo,NaN,1,1,700,Jim Brown\n7,03-15-2024,Suburbs,townhouse,320000,3,2,1180,mary johnson\n8,2024/03/20,Midtown,Single Family,500000,3,2,1500,Anna Lee\n9,03/25/2024,Midtown,Single family,510000,4,3,1700,anna lee\n10,2024-04-01,Downtown,Condo,360000,2,2,950,john doe", "eda_steps": ["Check missing value percentages for each column", "Standardize and correct inconsistent capitalization in 'PropertyType' and 'Location'", "Compute descriptive statistics for 'Price', 'Bedrooms', 'Bathrooms', and 'SquareFeet'", "Generate value counts for the 'PropertyType' column", "Identify the number of unique agents and generate value counts for 'AgentName'", "Analyze date formats in 'DateListed' and convert all to a standard date format", "Compute correlation matrix for numeric variables: Price, Bedrooms, Bathrooms, SquareFeet"], "eda_results": {"missing_values": {"ListingID": "0%", "DateListed": "10%", "Location": "0%", "PropertyType": "0%", "Price": "10%", "Bedrooms": "0%", "Bathrooms": "10%", "SquareFeet": "0%", "AgentName": "0%"}, "standardized_categories": {"PropertyType": {"Condo": 4, "Single Family": 5, "Townhouse": 2}, "Location": {"Downtown": 4, "Uptown": 2, "Suburbs": 2, "Midtown": 2}}, "summary_stats": {"Price": {"count": 9, "mean": 398888.89, "std": 77275.5, "min": 310000, "max": 510000}, "Bedrooms": {"count": 10, "mean": 2.7, "std": 1.0, "min": 1, "max": 4}, "Bathrooms": {"count": 9, "mean": 2.17, "std": 0.75, "min": 1, "max": 3}, "SquareFeet": {"count": 10, "mean": 1235, "std": 312, "min": 700, "max": 1700}}, "value_counts": {"PropertyType": {"Condo": 4, "Single Family": 5, "Townhouse": 2}, "AgentName": {"John Doe": 3, "Jane Smith": 2, "Mary Johnson": 2, "Jim Brown": 1, "Anna Lee": 2}}, "unique_agents": 5, "date_formats_detected": ["YYYY/MM/DD", "MM-DD-YYYY", "YYYY-MM-DD", "MM/DD/YYYY", "missing"], "correlations": {"Price-Bedrooms": 0.87, "Price-Bathrooms": 0.82, "Price-SquareFeet": 0.91, "Bedrooms-Bathrooms": 0.9, "Bedrooms-SquareFeet": 0.88, "Bathrooms-SquareFeet": 0.85}}}
{"purpose": "Analyze public housing applications to identify common applicant demographics and application statuses.", "raw_table": "Application_ID,Applicant_Name,Age,Income,Application_Date,Status,City\n001,John Doe,34,45000,2023/02/15,approved,new york\n002,Jane Smith,28,,15-03-2023,pending,Boston\n003,Bob Johnson,45,55000,2023-03-01,Approved,CHICAGO\n004,Alice Williams,NaN,38000,03/10/2023,denied,Los Angeles\n005,Chris Davis,31,50000,2023.03.12,pending,boston\n006,NaN,40,62000,2023/3/15,approved,San Francisco\n007,Maria Garcia,29,48000,2023-03-17,Denied,new york\n008,David Martinez,50,72000,2023/03/20,approved,Chicago\n009,Sarah Lee,,53000,2023-03-21,pending,los angeles\n010,James Wilson,38,NaN,03-22-2023,approved,New york\n011,Patricia Brown,44,58000,2023/03/23,approved,BOSTON\n012,Michael Miller,36,47000,2023-03-24,Denied,San francisco\n", "eda_steps": ["Check the percentage of missing values in each column", "Standardize the 'Status' column to lowercase to unify categories", "Standardize the 'City' column capitalization to title case", "Convert 'Application_Date' column to a uniform date format", "Compute descriptive statistics for numeric columns 'Age' and 'Income'", "Generate value counts for the 'Status' and 'City' columns", "Identify number of unique applicants with missing names", "Analyze distribution of application statuses over cities", "Calculate the average income by application status"], "eda_results": {"missing_values": {"Application_ID": 0, "Applicant_Name": 1, "Age": 2, "Income": 2, "Application_Date": 0, "Status": 0, "City": 0}, "status_standardized": {"approved": 6, "pending": 3, "denied": 3}, "city_standardized_counts": {"New York": 3, "Boston": 3, "Chicago": 2, "Los Angeles": 2, "San Francisco": 2}, "application_date_uniform": {"earliest_date": "2023-02-15", "latest_date": "2023-03-24"}, "summary_stats": {"Age": {"count": 10, "mean": 37.4, "std": 6.97, "min": 28, "max": 50}, "Income": {"count": 10, "mean": 53100, "std": 10200, "min": 38000, "max": 72000}}, "unique_missing_names": 1, "status_by_city": {"New York": {"approved": 2, "pending": 0, "denied": 1}, "Boston": {"approved": 1, "pending": 2, "denied": 0}, "Chicago": {"approved": 2, "pending": 0, "denied": 0}, "Los Angeles": {"approved": 0, "pending": 1, "denied": 1}, "San Francisco": {"approved": 1, "pending": 0, "denied": 1}}, "average_income_by_status": {"approved": 57700, "pending": 49000, "denied": 47667}}}
{"purpose": "Examine student performance and attendance patterns in a middle school course.", "raw_table": "StudentID,Name,Grade,TestScore,Attendance,SubmissionDate,Participation\n1,alice,8,88,95%,2023/03/01,high\n2,Bob,8,76,88%,03-02-2023,medium\n3,CHARLIE,9,,92%,2023-03-03,Low\n4,diana,9,85,missing,2023/3/04,Medium\n5,Edward,8,90,99%,2023/03/05,High\n6, FAYE ,9,70,85%,2023/3/06, low\n7,GeOrGe,8,82,90%,2023/03/07,Medium\n8,henry,9,88,missing,2023/03/08,medium\n9,Isaac,8,missing,93%,2023-3-09,high\n10,Julia,9,91,87%,03/10/2023,High", "eda_steps": ["Check for missing values in each column", "Standardize the capitalization of the Name and Participation columns", "Convert Attendance percentages to numeric values", "Parse SubmissionDate into a consistent date format", "Compute descriptive statistics for TestScore and Attendance", "Generate value counts for Grade and Participation levels", "Identify students with missing TestScore or Attendance data", "Analyze correlation between TestScore and Attendance", "List top 3 students by TestScore"], "eda_results": {"missing_values": {"TestScore": 2, "Attendance": 2, "Name": 0, "Grade": 0, "SubmissionDate": 0, "Participation": 0}, "standardized_participation_counts": {"High": 4, "Medium": 4, "Low": 2}, "attendance_numeric": {"mean": 90.0, "min": 85, "max": 99}, "submission_date_format": "All dates converted to YYYY-MM-DD format", "testscore_stats": {"count": 8, "mean": 83.75, "min": 70, "max": 91, "std_dev": 7.15}, "grade_counts": {"8": 5, "9": 5}, "students_missing_testscore": ["CHARLIE", "Isaac"], "students_missing_attendance": ["diana", "henry"], "correlation_testscore_attendance": 0.65, "top_3_students_by_testscore": [{"StudentID": 10, "Name": "Julia", "TestScore": 91}, {"StudentID": 5, "Name": "Edward", "TestScore": 90}, {"StudentID": 1, "Name": "alice", "TestScore": 88}]}}
{"purpose": "Analyze public housing application data to understand applicant demographics and application status trends.", "raw_table": "ApplicationID,ApplicantName,Age,ApplicationDate,IncomeLevel,HouseholdSize,ApplicationStatus\n1001,John Doe,34,2023-01-15,Medium,3,Approved\n1002,jane smith,28,15/02/2023,low,2,pending\n1003,Bob Johnson,,2023/03/01,High,4,Rejected\n1004,Alice O'Neil,45,2023-03-20,MEDIUM,,Approved\n1005,Chris Lee,37,03-25-2023,medium,5,pending\n1006,Maria Garcia,29,2023-04-10,Low,3,approved\n1007,Ahmed Khan,52,2023-04-15,High,6,Rejected\n1008,,31,2023-04-18,Medium,2,Pending\n1009,Lisa Wong,41,2023/04/20,high,,approved\n1010,Tom Brown,38,2023-05-01,,4,Rejected\n1011,Emily Davis,27,05/05/2023,Low,1,Pending\n1012,George Clark,33,2023-05-10,medium,3,Approved\n1013,Sarah Johnson,36,2023-05-15,Med,2,Rejected", "eda_steps": ["Check for missing values in each column", "Standardize the ApplicationDate column to a single date format", "Normalize capitalization for IncomeLevel and ApplicationStatus columns", "Compute summary statistics for Age and HouseholdSize", "Generate value counts for IncomeLevel and ApplicationStatus", "Identify the number of unique applicants with missing names", "Analyze the distribution of application statuses by IncomeLevel"], "eda_results": {"missing_values": {"ApplicationID": 0, "ApplicantName": 1, "Age": 1, "ApplicationDate": 0, "IncomeLevel": 1, "HouseholdSize": 2, "ApplicationStatus": 0}, "date_format_standardization": "All dates converted to YYYY-MM-DD format", "normalized_categories": {"IncomeLevel": ["Low", "Medium", "High"], "ApplicationStatus": ["Approved", "Pending", "Rejected"]}, "summary_stats": {"Age": {"count": 13, "mean": 35.23, "std": 7.25, "min": 27, "max": 52}, "HouseholdSize": {"count": 11, "mean": 3.18, "std": 1.4, "min": 1, "max": 6}}, "value_counts": {"IncomeLevel": {"Medium": 5, "Low": 4, "High": 3, "Missing": 1}, "ApplicationStatus": {"Approved": 4, "Pending": 4, "Rejected": 5}}, "missing_names_count": 1, "status_by_income": {"Low": {"Approved": 1, "Pending": 3, "Rejected": 0}, "Medium": {"Approved": 3, "Pending": 1, "Rejected": 1}, "High": {"Approved": 0, "Pending": 0, "Rejected": 3}, "Missing": {"Approved": 0, "Pending": 0, "Rejected": 1}}}}
{"purpose": "Analyze citizen complaints data to identify common issues and data quality problems in city services.", "raw_table": "Complaint_ID,Date_Received,Department,Issue_Type,Status,Resolution_Time_Days,Priority\n1001,2023-01-15,Sanitation,Garbage Collection,Closed,3,High\n1002,15/01/2023,Water,Leakage,closed,5,medium\n1003,2023-01-16,Transportation,Bus Delay,In Progress,,LOW\n1004,01-17-2023,Sanitation,graffiti,Closed,2,High\n1005,2023/01/18,Water,Leakage,Closed,7,MEDIUM\n1006,,Transportation,Bus Delay,In Progress,4,low\n1007,2023-01-19,Sanitation,Illegal Dumping,Open,,High\n1008,2023-1-20,Water,Water pressure,Closed,1,Medium\n1009,2023-01-21,Transportation,Bus delay,Closed,3,Low\n1010,01/22/2023,Sanitation,Graffiti,Closed,2,High\n1011,2023-01-23,Water,Leakage,Open,,Medium\n1012,2023-01-24,Transportation,Bus delay,Closed,3,low\n", "eda_steps": ["Check and summarize missing values for each column", "Standardize 'Date_Received' to a uniform date format", "Normalize categorical variables 'Department', 'Issue_Type', 'Status', and 'Priority' for consistent capitalization", "Compute descriptive statistics for 'Resolution_Time_Days'", "Generate value counts for 'Department' and 'Issue_Type' columns", "Identify the number of complaints per 'Status'", "Find average resolution time grouped by 'Priority'", "Check for duplicate Complaint_IDs", "Analyze distribution skewness for 'Resolution_Time_Days'"], "eda_results": {"missing_values": {"Complaint_ID": 0, "Date_Received": 1, "Department": 0, "Issue_Type": 0, "Status": 0, "Resolution_Time_Days": 3, "Priority": 0}, "standardized_date_format": "All dates converted to YYYY-MM-DD except one missing value", "normalized_categories": {"Department": {"Sanitation": 4, "Water": 4, "Transportation": 4}, "Issue_Type": {"Garbage Collection": 1, "Leakage": 3, "Bus Delay": 4, "Graffiti": 2, "Illegal Dumping": 1, "Water Pressure": 1}, "Status": {"Closed": 7, "In Progress": 2, "Open": 2}, "Priority": {"High": 4, "Medium": 4, "Low": 4}}, "summary_stats_Resolution_Time_Days": {"count": 9, "mean": 3.11, "median": 3, "min": 1, "max": 7, "std_dev": 1.81, "skewness": 1.16}, "value_counts_Status": {"Closed": 7, "In Progress": 2, "Open": 2}, "average_resolution_time_by_priority": {"High": 2.33, "Medium": 4.33, "Low": 3.0}, "duplicate_complaint_ids": 0}}
{"purpose": "Analyze customer purchasing patterns and product category performance in an ecommerce store.", "raw_table": "OrderID,CustomerID,OrderDate,ProductCategory,Quantity,UnitPrice,PaymentMethod,ShippingCost\n1001,cust01,2023-01-15,Electronics,2,199.99,credit card,10\n1002,CUST02,15/01/2023,Toys,1,15.5,PayPal,5\n1003,cust03,2023/01/16,Apparel,3,29.99,credit Card,7\n1004,cust04,01-17-2023,Books,,12.0,Debit Card,3\n1005,CUST05,2023-01-18,electronics,1,199.99,credit card,10\n1006,cust06,2023-1-18,Home & Garden,4,45.0,Cash,12\n1007,cust07,2023/01/19,Toys,2,15.5,Paypal,5\n1008,CUST08,19-01-2023,apparel,1,,Credit Card,7\n1009,cust09,2023-01-20,Books,1,12,Debit card,3\n1010,CUST10,2023-01-20,Gadgets,2,99.99,Credit card,8\n1011,cust11,20/01/2023,Electronics,1,199.99,Credit Card,10\n1012,cust12,2023-01-21,Home & Garden,,45.0,Cash,12\n1013,cust13,2023-01-21,Toys,3,15.5,PayPal,5", "eda_steps": ["Standardize column names to lowercase", "Parse and standardize order date to ISO format yyyy-mm-dd", "Handle missing values in Quantity and UnitPrice by identifying their counts", "Normalize product category names to consistent capitalization", "Calculate total sales amount per order as Quantity multiplied by UnitPrice", "Generate descriptive statistics for Quantity, UnitPrice, ShippingCost, and total sales", "Count unique customers and orders per product category", "Identify top 3 product categories by total sales", "Analyze payment method frequency"], "eda_results": {"missing_values": {"Quantity": 2, "UnitPrice": 1}, "value_counts_product_category": {"Electronics": 3, "Toys": 3, "Apparel": 2, "Books": 2, "Home & Garden": 2, "Gadgets": 1}, "payment_method_counts": {"Credit Card": 6, "PayPal": 3, "Debit Card": 2, "Cash": 2}, "total_sales_per_order": {"1001": 399.98, "1002": 15.5, "1003": 89.97, "1004": null, "1005": 199.99, "1006": 180.0, "1007": 31.0, "1008": null, "1009": 12.0, "1010": 199.98, "1011": 199.99, "1012": null, "1013": 46.5}, "summary_stats": {"Quantity": {"count": 11, "mean": 1.91, "min": 1, "max": 4}, "UnitPrice": {"count": 12, "mean": 74.16, "min": 12.0, "max": 199.99}, "ShippingCost": {"count": 13, "mean": 7.69, "min": 3, "max": 12}, "TotalSales": {"count": 11, "mean": 119.41, "min": 12, "max": 399.98}}, "top_categories_by_sales": {"Electronics": 799.96, "Home & Garden": 180.0, "Gadgets": 199.98}, "unique_customers_per_category": {"Electronics": 3, "Toys": 3, "Apparel": 2, "Books": 2, "Home & Garden": 2, "Gadgets": 1}}}
{"purpose": "Analyze production line efficiency and identify inconsistencies in machine operation times.", "raw_table": "MachineID,OperationDate,Operator,RunTimeMinutes,DefectsReported,Shift\nM001,2024-01-15,alice,120,0,morning\nM002,15/01/2024,Bob,95,,Morning\nm003,2024/01/16,CHARLIE,110,2,NIGHT\nM004,2024-01-16,,105,1,night\nM001,17-01-2024,Alice,NaN,0,Morning\nM002,2024-01-17,Bob,100,1,NIGHT\nM003,2024-01-18,Charlie,115,,Night\nm004,18/01/2024,Diana,90,0,evening\nM001,2024-01-19,Alice,125,3,MORNING\nm002,2024/01/19,bob,98,0,morning\nM003,,Charlie,112,1,NIGHT\nM004,2024-01-20,Diana,NaN,1,Evening", "eda_steps": ["Check the percentage of missing values per column", "Standardize the date format in OperationDate", "Normalize the capitalization in MachineID, Operator, and Shift columns", "Compute descriptive statistics for RunTimeMinutes and DefectsReported", "Generate value counts for MachineID, Operator, and Shift", "Identify correlations between RunTimeMinutes and DefectsReported", "Summarize the distribution skewness for RunTimeMinutes", "Identify rows with missing or inconsistent data"], "eda_results": {"missing_values": {"MachineID": 0, "OperationDate": 1, "Operator": 1, "RunTimeMinutes": 2, "DefectsReported": 2, "Shift": 0}, "standardized_dates_sample": ["2024-01-15", "2024-01-15", "2024-01-16", "2024-01-16", "2024-01-17", "2024-01-17", "2024-01-18", "2024-01-18", "2024-01-19", "2024-01-19", null, "2024-01-20"], "normalized_categories": {"MachineID": ["M001", "M002", "M003", "M004"], "Operator": ["Alice", "Bob", "Charlie", "Diana", null], "Shift": ["Morning", "Night", "Evening"]}, "summary_stats": {"RunTimeMinutes": {"count": 10, "mean": 109.0, "std": 11.01, "min": 90, "25%": 98, "50%": 110, "75%": 120, "max": 125, "skewness": 0.15}, "DefectsReported": {"count": 10, "mean": 0.9, "std": 1.1, "min": 0, "max": 3}}, "value_counts": {"MachineID": {"M001": 3, "M002": 3, "M003": 3, "M004": 3}, "Operator": {"Alice": 3, "Bob": 3, "Charlie": 4, "Diana": 2, "null": 1}, "Shift": {"Morning": 5, "Night": 5, "Evening": 2}}, "correlations": {"RunTimeMinutes_vs_DefectsReported": 0.62}, "rows_with_issues": {"missing_OperationDate": [10], "missing_Operator": [3], "missing_RunTimeMinutes": [4, 11], "missing_DefectsReported": [1, 6]}}}
{"purpose": "Analyze real estate listing characteristics and identify data quality issues.", "raw_table": "ListingID,Price,Location,Bedrooms,Bathrooms,ListingDate,PropertyType\n1,350000,New York,3,2,2023-01-15,Condo\n2,450000,boston,4,3,15/02/2023,House\n3,,Los Angeles,2,1,2023-03-10,house\n4,500000,Chicago,3,,2023/04/05,Apartment\n5,285000,miami,2,2,2023-05-12,condo\n6,600000,New york,5,4,06-15-2023,House\n7,NaN,Seattle,3,2,2023-07-01,Townhouse\n8,420000,Chicago,3,2,2023-08-20,Apartment\n9,390000,Boston,2,1,2023-09-15,Condo\n10,310000,los angeles,3,2,2023/10/10,Condo", "eda_steps": ["Check for missing values in each column", "Standardize the capitalization of Location and PropertyType columns", "Convert ListingDate values to a consistent date format", "Compute descriptive statistics for Price, Bedrooms, and Bathrooms", "Generate value counts for PropertyType", "Identify listings with missing Price values", "Summarize the distribution of listings by Location", "Calculate the correlation between Price and number of Bedrooms"], "eda_results": {"missing_values": {"ListingID": 0, "Price": 2, "Location": 0, "Bedrooms": 0, "Bathrooms": 1, "ListingDate": 0, "PropertyType": 0}, "standardized_values": {"Location_unique_values": ["New York", "Boston", "Los Angeles", "Chicago", "Miami", "Seattle"], "PropertyType_unique_values": ["Condo", "House", "Apartment", "Townhouse"]}, "listing_date_formats_uniform": "All dates converted to YYYY-MM-DD format", "summary_stats": {"Price": {"count": 8, "mean": 412500, "min": 285000, "max": 600000, "std_dev": 104123.11}, "Bedrooms": {"count": 10, "mean": 3.2, "min": 2, "max": 5, "std_dev": 1.08}, "Bathrooms": {"count": 9, "mean": 2.22, "min": 1, "max": 4, "std_dev": 0.83}}, "value_counts_PropertyType": {"Condo": 4, "House": 3, "Apartment": 2, "Townhouse": 1}, "listings_missing_price": [3, 7], "listings_by_location": {"New York": 2, "Boston": 2, "Los Angeles": 2, "Chicago": 2, "Miami": 1, "Seattle": 1}, "correlation_Price_Bedrooms": 0.85}}
{"purpose": "Analyze patient demographics and lab test results to identify data quality issues and distribution patterns.", "raw_table": "Patient_ID,Age,Gender,Visit_Date,Diagnosis,Cholesterol_mg_dL,Glucose_mg_dL\n001,45,Male,2023-03-15,Diabetes,210,105\n002,37,FEMALE,15/04/2023, Hypertension ,190,NA\n003,,male,2023/05/10,Diabetes,NaN,98\n004,29,Fem,2023-06-01, asthma,180,88\n005,52,Female,06-15-2023,Hypertension,220,115\n006,40,M,2023-07-03,,205,110\n007,33,FEMALE,2023-08-07,diabetes,195,102\n008,NaN,unknown,2023-08-20,Hypertension,200,100\n009,47,Male,2023-09-10,Asthma,185,NaN\n010,55,Female,2023-09-15,Hypertension,225,120\n011,41,Female,2023-10-01,diabetes,215,108\n012,38,Male,2023-10-11,Hypertension,198,104\n013,44,Female,10/15/2023,Diabetes,NaN,NaN", "eda_steps": ["Check missing value percentages for each column", "Standardize and clean the Gender column values", "Standardize and parse the Visit_Date column into consistent datetime format", "Generate value counts for the Diagnosis column", "Compute descriptive statistics for numeric columns Age, Cholesterol_mg_dL, and Glucose_mg_dL", "Identify rows with inconsistent or unusual entries", "Calculate correlation matrix for numeric columns"], "eda_results": {"missing_values": {"Patient_ID": 0, "Age": 2, "Gender": 1, "Visit_Date": 0, "Diagnosis": 1, "Cholesterol_mg_dL": 3, "Glucose_mg_dL": 3}, "cleaned_gender_value_counts": {"Male": 5, "Female": 6, "Unknown": 1}, "standardized_visit_date_range": {"min_date": "2023-03-15", "max_date": "2023-10-15"}, "diagnosis_value_counts": {"Diabetes": 5, "Hypertension": 5, "Asthma": 2, "Missing": 1}, "summary_stats": {"Age": {"count": 11, "mean": 41.9, "std": 7.5, "min": 29, "max": 55}, "Cholesterol_mg_dL": {"count": 10, "mean": 205.8, "std": 15.3, "min": 180, "max": 225}, "Glucose_mg_dL": {"count": 9, "mean": 105.6, "std": 9.9, "min": 88, "max": 120}}, "inconsistent_entries": {"Gender": ["FEMALE", "male", "Fem", "M", "unknown"], "Diagnosis": [" Hypertension ", " asthma", ""], "Visit_Date_formats": ["2023-03-15", "15/04/2023", "2023/05/10", "06-15-2023", "10/15/2023"]}, "correlations": {"Age-Cholesterol_mg_dL": 0.68, "Age-Glucose_mg_dL": 0.55, "Cholesterol_mg_dL-Glucose_mg_dL": 0.72}}}
{"purpose": "Analyze customer purchase behavior and identify missing data patterns in recent ecommerce transactions.", "raw_table": "OrderID,CustomerID,Product,Quantity,Price,OrderDate,PaymentMethod\n1001,C123,Wireless Mouse,2,25.99,2023/01/15,Credit card\n1002,C234,USB-C Cable,1,7.5,15-01-2023,Paypal\n1003,C123,wireless mouse,1,,2023-01-17,Credit Card\n1004,C345,Laptop Stand,1,39.99,2023/01/18,Credit Card\n1005,C456,Bluetooth speaker,,49.99,01/19/2023,credit card\n1006,C234,USB-C cable,3,7.5,2023/01/20,\n1007,C567,Keyboard,1,19.99,2023-01-21,CREDIT CARD\n1008,C678,Monitor,1,129.99,01/22/2023,Paypal\n1009,C789,wireless mouse,1,25.99,2023/1/23,Credit Card\n1010,C234,USB-C Cable,2,7.5,2023/01/24,Credit card", "eda_steps": ["Check for missing values in each column", "Standardize product names by converting to lowercase and correcting inconsistent spelling", "Calculate descriptive statistics for Quantity and Price columns", "Generate value counts for PaymentMethod column", "Identify unique customers based on CustomerID", "Analyze order frequency over OrderDate", "Compute total revenue per product", "Summarize correlation between Quantity and Price"], "eda_results": {"missing_values": {"Quantity": 1, "Price": 1, "PaymentMethod": 1}, "standardized_product_names": ["wireless mouse", "usb-c cable", "wireless mouse", "laptop stand", "bluetooth speaker", "usb-c cable", "keyboard", "monitor", "wireless mouse", "usb-c cable"], "summary_stats": {"Quantity": {"count": 9, "mean": 1.56, "min": 1, "max": 3}, "Price": {"count": 9, "mean": 33.43, "min": 7.5, "max": 129.99}}, "payment_method_counts": {"credit card": 6, "paypal": 2, "": 1}, "unique_customers": 7, "order_frequency_by_date": {"2023-01-15": 2, "2023-01-17": 1, "2023-01-18": 1, "2023-01-19": 1, "2023-01-20": 1, "2023-01-21": 1, "2023-01-22": 1, "2023-01-23": 1, "2023-01-24": 1}, "total_revenue_per_product": {"wireless mouse": 77.97, "usb-c cable": 45, "laptop stand": 39.99, "bluetooth speaker": 49.99, "keyboard": 19.99, "monitor": 129.99}, "correlation_quantity_price": 0.12}}
{"purpose": "Explore property listing data to understand price distributions, missing data, and common property types in a real estate market.", "raw_table": "ListingID,Price,PropertyType,Bedrooms,Bathrooms,ListingDate,Location\n1001,350000,House,3,2,2023-01-15,downtown\n1002,NaN,condo,2,1.5,01/20/2023,Suburb\n1003,450000,House,4,,2023/02/10,Downtown\n1004,380000,townhouse,3,2,15-02-2023,suburb\n1005,299000,Condo,1,1,,suburbs\n1006,NaN,house,3,2,2023-03-01,Downtown\n1007,500000,House,5,3,2023-03-05,Downtown\n1008,275000,Apartment,2,1,03-10-2023,suburbs\n1009,320000,TownHouse,3,2,2023-03-15,Suburbs\n1010,310000,apartment,2,1,2023/03/18,Downtown\n1011,NaN,Condo,,1,2023-03-20,Suburb\n1012,400000,House,4,2,2023-04-01,downtown\n1013,360000,Apartment,3,2,2023-04-05,Suburb\n1014,NaN,townhouse,3,2,2023-04-10,Downtown", "eda_steps": ["Check for missing values in each column and calculate the percentage missing", "Standardize the PropertyType column capitalization and unify categories", "Compute descriptive statistics (mean, median, min, max) for numeric columns Price, Bedrooms, and Bathrooms", "Generate value counts for the PropertyType and Location columns", "Analyze the date formats in ListingDate and convert them to a consistent format", "Calculate correlations between numeric columns Bedrooms, Bathrooms, and Price", "Identify the top 2 most common property types and locations", "Summarize the distribution skewness for the Price column"], "eda_results": {"missing_values": {"ListingID": "0%", "Price": "29%", "PropertyType": "0%", "Bedrooms": "14%", "Bathrooms": "7%", "ListingDate": "7%", "Location": "0%"}, "property_type_standardized_counts": {"house": 5, "condo": 3, "townhouse": 3, "apartment": 3}, "location_counts": {"downtown": 6, "suburb": 5, "suburbs": 3}, "price_stats": {"mean": 357222, "median": 355000, "min": 275000, "max": 500000}, "bedrooms_stats": {"mean": 3.2, "median": 3, "min": 1, "max": 5}, "bathrooms_stats": {"mean": 1.7, "median": 2, "min": 1, "max": 3}, "listing_date_format_analysis": {"iso_format": 7, "dd-mm-yyyy": 2, "mm/dd/yyyy": 1, "yyyy/mm/dd": 2, "missing": 1}, "correlations": {"Price_Bedrooms": 0.83, "Price_Bathrooms": 0.76, "Bedrooms_Bathrooms": 0.65}, "top_categories": {"PropertyType": ["house", "condo"], "Location": ["downtown", "suburb"]}, "price_skewness": 0.45}}
{"purpose": "Explore patient demographics and clinical measurements to identify data quality issues and distribution patterns in a small medical dataset.", "raw_table": "PatientID,Age,Gender,AdmissionDate,BloodPressure,Diagnosis,HeartRate\nP001,45,Male,2023/01/15,120/80,Hypertension,72\nP002,NaN,FEMALE,15-02-2023,130/85,diabetes,85\nP003,38,Male,2023-03-05,,Asthma,NaN\nP004,60,Female,03/20/2023,140/90,Hypertension,78\nP005,29,female,2023/04/01,115/75,,65\nP006,50,M,2023-04-15,135/88,Diabetes,90\nP007,NaN,Male,2023-04-10,125/82,Asthma,80\nP008,55,F,04/05/2023,NaN,hypertension,75\nP009,40,,2023/03/28,128/84,Diabetes,82\nP010,48,Female,2023/02/20,132/85,Asthma,88", "eda_steps": ["Standardize Gender values to consistent categories", "Parse and unify AdmissionDate to ISO format yyyy-mm-dd", "Calculate missing value percentages per column", "Compute descriptive statistics for Age and HeartRate", "Extract systolic and diastolic from BloodPressure and summarize", "Generate value counts for Diagnosis categories", "Check correlation between Age and HeartRate", "Identify patients with missing critical fields"], "eda_results": {"missing_values": {"PatientID": "0%", "Age": "20%", "Gender": "10%", "AdmissionDate": "0%", "BloodPressure": "20%", "Diagnosis": "10%", "HeartRate": "10%"}, "gender_standardized": {"Male": 4, "Female": 5, "Unknown": 1}, "admission_dates_standardized": ["2023-01-15", "2023-02-15", "2023-03-05", "2023-03-20", "2023-04-01", "2023-04-15", "2023-04-10", "2023-04-05", "2023-03-28", "2023-02-20"], "age_stats": {"count": 8, "mean": 45.4, "min": 29, "max": 60, "std_dev": 10.4}, "heartrate_stats": {"count": 9, "mean": 79.4, "min": 65, "max": 90, "std_dev": 7.9}, "bloodpressure_extracted": {"systolic_mean": 127.9, "diastolic_mean": 83.9, "missing": 2}, "diagnosis_value_counts": {"Hypertension": 3, "Diabetes": 3, "Asthma": 3, "Unknown": 1}, "correlation_age_heartrate": 0.65, "patients_missing_critical": ["P002 (Age missing)", "P003 (BloodPressure, HeartRate missing)", "P007 (Age missing)", "P008 (BloodPressure missing)", "P009 (Gender missing)", "P005 (Diagnosis missing)"]}}
{"purpose": "Examine student performance and attendance patterns in a high school math class.", "raw_table": "StudentID,Name,Gender,Math_Score,Attendance,Enrollment_Date\n101,alice smith,Female,88,95%,2023-01-10\n102,BOB JONES,Male,92,91%,1/15/2023\n103,cArOl White,F,85,,2023-01-20\n104,David black,male,78,89%,2023/01/22\n105,ellen Green,Female,NaN,87%,Jan 25 2023\n106,Franklin, Male,91,93%,2023-01-28\n107,Gina   Lee,Female,84,90%,2023-02-02\n108,henry Kim,M,88%,88%,2023-02-05\n109,Ian O'neil,Male,83,missing,2023-02-07\n110,julia Brown,F,NaN,85%,2023-2-10", "eda_steps": ["Check and count missing values in each column", "Standardize the Gender column to consistent categories", "Convert Math_Score and Attendance to numeric types, handling non-numeric values", "Compute descriptive statistics for Math_Score and Attendance", "Calculate value counts for Enrollment_Date formats", "Identify students with missing Math_Score or Attendance", "Assess correlation between Math_Score and Attendance", "Find top 3 highest scoring students", "Summarize distribution skewness for Math_Score"], "eda_results": {"missing_values": {"StudentID": 0, "Name": 0, "Gender": 0, "Math_Score": 2, "Attendance": 2, "Enrollment_Date": 0}, "gender_value_counts": {"Female": 4, "Male": 5, "M": 1, "F": 2}, "math_score_stats": {"count": 8, "mean": 86.125, "std_dev": 4.99, "min": 78, "25%": 83, "50%": 87, "75%": 91.5, "max": 92, "skewness": -0.12}, "attendance_stats": {"count": 8, "mean_percent": 89.75, "std_dev": 2.77, "min": 85, "max": 95}, "enrollment_date_format_counts": {"YYYY-MM-DD": 7, "M/D/YYYY": 1, "MMM DD YYYY": 1, "YYYY/MM/DD": 1, "YYYY-M-D": 1}, "students_missing_scores_or_attendance": ["Carol White", "Ellen Green", "Henry Kim", "Ian O'neil", "Julia Brown"], "correlation_math_attendance": 0.56, "top_3_students": [{"StudentID": 102, "Name": "BOB JONES", "Math_Score": 92}, {"StudentID": 106, "Name": "Franklin", "Math_Score": 91}, {"StudentID": 101, "Name": "alice smith", "Math_Score": 88}]}}
{"purpose": "Analyze patient demographic and clinical characteristics for a cardiology study.", "raw_table": "PatientID,Age,Gender,AdmissionDate,Diagnosis,BloodPressure,HeartRate,Cholesterol\n001,65,Male,2023-01-15,Myocardial Infarction,140/90,85,200\n002,58,Female,15/02/2023,hypertension,130/85,78,180\n003,,male,2023-03-05,Stroke,125/80,90,NaN\n004,72,FEMALE,03-20-2023,Hypertension,135/88,,220\n005,60,Male,2023/04/01,Myocardial infarction,142/92,88,210\n006,55,Male,2023-04-12,Other,120/75,75,170\n007,NaN,Female,2023-04-13,Hypertension,NaN,80,195\n008,67,Male,2023-05-01,stroke,138/85,92,205\n009,59,Female,04-25-2023,Hypertension,132/82,85,NaN\n010,62,Male,2023-05-05,Myocardial infarction,145/95,90,215\n011,66,Female,2023-5-10,hypertension,130/80,83,185\n012,61,male,2023-05-12,Other,125/78,88,200", "eda_steps": ["Check and summarize missing values in each column", "Standardize and normalize the Gender column values", "Convert AdmissionDate to a consistent date format", "Separate BloodPressure into Systolic and Diastolic numeric columns", "Compute descriptive statistics for Age, Systolic, Diastolic, HeartRate, and Cholesterol", "Generate value counts for Diagnosis", "Calculate correlation matrix for numeric variables", "Identify top 3 most frequent diagnoses"], "eda_results": {"missing_values": {"PatientID": 0, "Age": 2, "Gender": 0, "AdmissionDate": 0, "Diagnosis": 0, "BloodPressure": 1, "HeartRate": 1, "Cholesterol": 2}, "gender_standardized": {"Male": 6, "Female": 6}, "diagnosis_value_counts": {"Hypertension": 5, "Myocardial Infarction": 3, "Stroke": 2, "Other": 2}, "age_stats": {"count": 10, "mean": 61.9, "min": 55, "max": 72, "std": 5.3}, "blood_pressure_stats": {"systolic": {"count": 11, "mean": 132.45, "min": 120, "max": 145, "std": 8.4}, "diastolic": {"count": 11, "mean": 84.36, "min": 75, "max": 95, "std": 7.1}}, "heart_rate_stats": {"count": 11, "mean": 85.27, "min": 75, "max": 92, "std": 5.2}, "cholesterol_stats": {"count": 10, "mean": 198.5, "min": 170, "max": 220, "std": 16.2}, "correlations": {"Age_HeartRate": -0.12, "Age_Cholesterol": 0.35, "Systolic_Diastolic": 0.78, "Systolic_HeartRate": 0.11, "Diastolic_HeartRate": 0.08}, "top_3_diagnoses": ["Hypertension", "Myocardial Infarction", "Stroke"]}}
{"purpose": "Examine patient demographics and lab test outcomes to identify data quality issues and basic distributions.", "raw_table": "Patient_ID,Age,Gender,Visit_Date,Diagnosis,Blood_Pressure,Cholesterol_mg_dl\nP001,45,Male,2023-01-12,Hypertension,130/85,200\np002,52,Female,01/15/2023,Diabetes,140/90,NaN\nP003,38,female,2023-1-17,Asthma,118/78,180\nP004,60,M,17-01-2023,hypertension,,210\np005,29,F,2023/01/20,none,120/80,170\nP006,,Female,2023-01-22,Diabetes,150/95,NaN\np007,50,male,,Asthma,135/88,195\nP008,43,F,2023-01-25,COPD,125/82,185\nP009,55,Female,01-27-2023,Hypertension,142/92,220\nP010,47,m,2023-01-28,COPD,128/85,NaN", "eda_steps": ["Check and report missing values for each column", "Standardize and count unique values in the Gender column", "Parse and standardize Visit_Date into a uniform date format", "Calculate basic statistics (mean, median, min, max) for Age and Cholesterol_mg_dl", "Generate value counts for Diagnosis categories", "Extract systolic and diastolic values from Blood_Pressure and compute their average", "Identify rows with inconsistent or missing Blood_Pressure values", "Summarize the number of unique patients and visits"], "eda_results": {"missing_values": {"Patient_ID": 0, "Age": 1, "Gender": 0, "Visit_Date": 1, "Diagnosis": 0, "Blood_Pressure": 1, "Cholesterol_mg_dl": 3}, "gender_value_counts": {"female": 4, "male": 3, "f": 2, "m": 2}, "visit_date_standardized": ["2023-01-12", "2023-01-15", "2023-01-17", "2023-01-17", "2023-01-20", "2023-01-22", null, "2023-01-25", "2023-01-27", "2023-01-28"], "age_statistics": {"mean": 46.9, "median": 47, "min": 29, "max": 60}, "cholesterol_statistics": {"mean": 193.3, "median": 192.5, "min": 170, "max": 220}, "diagnosis_counts": {"Hypertension": 3, "Diabetes": 2, "Asthma": 2, "COPD": 2, "none": 1}, "blood_pressure_extracted": {"systolic_avg": 131.4, "diastolic_avg": 85.4, "inconsistent_or_missing_count": 1}, "unique_patients_count": 10, "total_visits_count": 10}}
{"purpose": "Analyze citizen complaints data to identify common issues and data quality problems.", "raw_table": "Complaint_ID,Date,Department,Issue_Type,Status,Resolution_Time_Days,Priority\n001,2023/05/10,Sanitation,Trash Collection,Closed,2,High\n002,05-12-2023,Public Safety,Noise complaint,Open,,medium\n003,2023-05-13,Transportation,Traffic Light malfunction,Closed,5,HIGH\n004,,sanitation,trash collection,Closed,3,Low\n005,2023/05/15,Public safety,Noise Complaint,open,1,Medium\n006,15-May-2023,Transportation,Parking violation,Closed,2,Medium\n007,2023/05/16,Sanitation,Trash Collection,Closed,4,high\n008,2023/05/17,Public Safety,Noise complaint,,3,Medium\n009,2023.05.18,Transportation,traffic light Malfunction,Closed,6,Low\n010,2023/05/19,sanitation,Trash Collection,Closed,,High\n011,2023/05/20,Public safety,Noise Complaint,Closed,2,Medium\n012,2023/05/20,Transportation,,Open,,low\n013,2023/05/21,Sanitation,Trash Collection,Closed,3,High\n014,2023/05/22,Public Safety,noise complaint,Closed,2,Medium", "eda_steps": ["Check for and quantify missing values per column", "Standardize capitalization for Department, Issue_Type, Status, and Priority columns", "Parse and unify Date column formats into YYYY-MM-DD", "Compute descriptive statistics for Resolution_Time_Days", "Generate value counts for Department and Issue_Type", "Identify the number of complaints by Status", "Calculate average Resolution_Time_Days by Priority", "Detect duplicate Complaint_ID values", "Summarize the distribution of Priority levels"], "eda_results": {"missing_values": {"Date": 1, "Issue_Type": 1, "Resolution_Time_Days": 3, "Status": 1}, "standardized_values": {"Department": ["Sanitation", "Public Safety", "Transportation"], "Issue_Type": ["Trash Collection", "Noise Complaint", "Traffic Light Malfunction", "Parking Violation", "Missing"], "Status": ["Closed", "Open"], "Priority": ["High", "Medium", "Low"]}, "date_range": {"min": "2023-05-10", "max": "2023-05-22"}, "resolution_time_stats": {"count": 11, "mean": 3.09, "std_dev": 1.58, "min": 1, "max": 6}, "value_counts": {"Department": {"Sanitation": 5, "Public Safety": 6, "Transportation": 4}, "Issue_Type": {"Trash Collection": 5, "Noise Complaint": 6, "Traffic Light Malfunction": 2, "Parking Violation": 1, "Missing": 1}, "Status": {"Closed": 11, "Open": 3}}, "average_resolution_by_priority": {"High": 2.8, "Medium": 3.2, "Low": 3.7}, "duplicate_complaint_ids": 0, "priority_distribution": {"High": 5, "Medium": 6, "Low": 3}}}
{"purpose": "Analyze trip characteristics and delays in a city taxi service dataset to identify patterns and data quality issues.", "raw_table": "trip_id,trip_date,passenger_count,distance_miles,trip_duration_min,payment_type,tip_amount\nT1001,2023-01-05,2,3.5,15,Cash,1.5\nT1002,01/06/2023,1,,10,credit,2\nT1003,2023-01-07,three,5.2,20,Cash,\nT1004,2023-1-08,4,8.0,35,Card,5\nT1005,2023/01/09,1,2.3,,credit,0\nT1006,2023-01-10,,4.1,18,cash,1\nT1007,01-11-2023,2,6.7,25,CASH,3\nT1008,2023-01-12,1,3.2,14,credit,1.2\nT1009,2023-01-13,2,7.1,30,card,4\nT1010,2023-01-14,1,NaN,12,Credit,2.5\n", "eda_steps": ["Check for missing values in each column", "Standardize and convert trip_date to a uniform date format", "Convert passenger_count to numeric and handle non-numeric entries", "Compute descriptive statistics for numeric columns: distance_miles, trip_duration_min, tip_amount", "Generate value counts for payment_type to identify inconsistencies", "Calculate average trip_duration_min by passenger_count", "Identify the number of trips with missing distance_miles", "Summarize tip_amount distribution and identify missing tips"], "eda_results": {"missing_values": {"trip_id": 0, "trip_date": 0, "passenger_count": 1, "distance_miles": 3, "trip_duration_min": 1, "payment_type": 0, "tip_amount": 1}, "trip_date_format_standardized": true, "passenger_count_clean": {"converted": 9, "non_numeric_replaced": 1}, "summary_stats": {"distance_miles": {"count": 7, "mean": 5.54, "min": 2.3, "max": 8.0}, "trip_duration_min": {"count": 9, "mean": 20.78, "min": 10, "max": 35}, "tip_amount": {"count": 9, "mean": 2.3, "min": 0, "max": 5}}, "payment_type_value_counts": {"cash": 3, "credit": 4, "card": 2}, "avg_trip_duration_by_passenger_count": {"1": 15.25, "2": 23.33, "3": null, "4": 35}, "trips_missing_distance_count": 3, "tip_amount_distribution": {"missing_tips": 1, "tips_zero": 1, "average_tip": 2.3}}}
{"purpose": "Analyze viewership patterns and ratings of recent streaming platform movies.", "raw_table": "MovieID,Title,Genre,ReleaseDate,DurationMinutes,ViewerRating,Views\nM001,The Lost World,Adventure,2023-01-15,130,4.5,12000\nM002,romance in paris,Romance,2023/02/10,95,,8500\nM003,Space Odyssey,SCI-FI,15-03-2023,145,4.7,15000\nM004,the last laugh,Comedy,2023-04-05,110,4.2,NaN\nM005,ghosts of tokyo,Horror,2023-04-01,100,3.9,7800\nM006,Deep Blue,Documentary,2023-03-20,85,4.8,9000\nM007,The lost world,Adventure,2023-01-15,130,4.4,11800\nM008,Cook & Bake,Cooking,2023-02-28,50,4.0,6200\nM009,romance in paris,Romance,,95,4.1,8300\nM010,Shadow Realm,Horror,2023-04-15,102,missing,7200\nM011,Space Odyssey,SCI-FI,2023-03-15,145,4.6,15500", "eda_steps": ["Check for missing values in each column", "Standardize Genre capitalization", "Convert ReleaseDate to consistent date format", "Calculate descriptive statistics for DurationMinutes, ViewerRating, and Views", "Identify duplicate movies based on Title and Genre", "Generate value counts for Genre", "Find average ViewerRating by Genre", "Summarize missing and anomalous data entries"], "eda_results": {"missing_values": {"MovieID": 0, "Title": 0, "Genre": 0, "ReleaseDate": 1, "DurationMinutes": 0, "ViewerRating": 2, "Views": 1}, "standardized_genres": {"adventure": 2, "romance": 2, "sci-fi": 2, "comedy": 1, "horror": 2, "documentary": 1, "cooking": 1}, "release_dates_parsed": {"earliest": "2023-01-15", "latest": "2023-04-15", "missing": 1}, "descriptive_stats": {"DurationMinutes": {"count": 11, "mean": 109.5, "min": 50, "max": 145, "std": 29.0}, "ViewerRating": {"count": 9, "mean": 4.31, "min": 3.9, "max": 4.8, "std": 0.29}, "Views": {"count": 10, "mean": 10230, "min": 6200, "max": 15500, "std": 2940}}, "duplicate_movies": [{"Title": "The Lost World", "Genre": "Adventure", "count": 2}, {"Title": "Romance in Paris", "Genre": "Romance", "count": 2}, {"Title": "Space Odyssey", "Genre": "SCI-FI", "count": 2}], "top_genres_by_count": {"adventure": 2, "romance": 2, "sci-fi": 2, "horror": 2, "comedy": 1, "documentary": 1, "cooking": 1}, "average_rating_by_genre": {"adventure": 4.45, "romance": 4.1, "sci-fi": 4.65, "comedy": 4.2, "horror": 3.9, "documentary": 4.8, "cooking": 4.0}, "anomalies": {"missing_viewer_ratings": ["M002", "M010"], "missing_views": ["M004"], "inconsistent_release_date_formats": ["M003", "M009", "M011"]}}}
{"purpose": "Analyze monthly transaction patterns and customer segments in a retail banking dataset.", "raw_table": "TransactionID,CustomerID,TransactionDate,TransactionType,Amount,AccountType\nT001,cust01,2023-01-15,deposit,1000,Savings\nT002,CUST02,15-Feb-2023,Withdrawal,250,checking\nT003,cust03,2023/03/05,Deposit,,savings\nT004,cust04,2023-03-25,withdrawal,300,Checking\nT005,cust05,04-04-2023,transfer,500,Business\nT006,cust06,2023-05-01,Deposit,750,Savings\nT007,CUST07,2023-05-12,Withdrawal,abc,personal\nT008,cust08,,deposit,400,savings\nT009,cust09,2023-06-20,withdrawal,200,Checking\nT010,cust10,2023-06-25,Transfer,-100,Business\nT011,CUST11,2023-06-30,Deposit,650,Savings\nT012,cust12,07/07/2023,withdrawal,350,Checking\nT013,cust13,2023-07-10,deposit,500,Personal\nT014,cust14,2023-07-15,Withdrawal,NaN,Savings", "eda_steps": ["Check and summarize missing values for each column", "Standardize and parse the TransactionDate column into a consistent date format", "Normalize capitalization in categorical columns: TransactionType and AccountType", "Identify and summarize invalid or non-numeric values in the Amount column", "Calculate descriptive statistics (mean, median, min, max) for Amount after cleaning", "Generate value counts for TransactionType and AccountType", "Analyze distribution of transactions over months", "Compute correlation between transaction Amount and AccountType encoded as categorical variables"], "eda_results": {"missing_values": {"TransactionID": 0, "CustomerID": 0, "TransactionDate": 1, "TransactionType": 0, "Amount": 3, "AccountType": 0}, "date_parsing_summary": {"earliest_date": "2023-01-15", "latest_date": "2023-07-15", "unparsed_dates": 0}, "normalized_categories": {"TransactionType": {"deposit": 6, "withdrawal": 6, "transfer": 2}, "AccountType": {"savings": 5, "checking": 4, "business": 2, "personal": 2}}, "amount_issues": {"missing_or_nan": 3, "non_numeric": 1, "negative_values": 1}, "amount_stats": {"mean": 475, "median": 475, "min": 200, "max": 1000, "count": 10}, "transaction_counts_by_month": {"2023-01": 1, "2023-02": 1, "2023-03": 2, "2023-04": 1, "2023-05": 2, "2023-06": 3, "2023-07": 3}, "correlation_amount_accounttype": {"savings": 0.65, "checking": 0.12, "business": -0.43, "personal": 0.04}}}
{"purpose": "Analyze equipment downtime causes and durations to improve maintenance scheduling.", "raw_table": "EquipmentID,Date,Shift,Downtime_Hours,Cause,Operator\nEQ101,2023/03/15,morning,2.5,Electrical failure,John\nEQ102,15-03-2023,NIGHT,0.5,Mechanical,alice\nEQ103,2023-03-15,Evening,,OverHeating,Bob\nEQ104,2023/03/16,Morning,1.0,,Eve\nEQ101,16-03-2023,morning,three,Mechanical,JOHN\nEQ105,2023-03-16,Night,2,Electrical Failure,carol\nEQ102,2023/03/17,night,1.5,Mechanical,ALICE\nEQ106,17/03/2023,evening,1.2,Overheating,bob\nEQ103,2023/03/17,Morning,0.75,Electrical Failure,\nEQ104,18-03-2023,MORNING,0.0,Unknown,Dave\nEQ105,2023/03/18,Night,Two,Mechanical,Carol", "eda_steps": ["Check for missing values in all columns", "Standardize date formats to YYYY-MM-DD", "Clean and standardize categorical values for Shift, Cause, and Operator", "Convert Downtime_Hours to numeric, handling non-numeric entries", "Compute descriptive statistics for Downtime_Hours", "Generate value counts for Cause and Shift", "Identify number of downtime records per EquipmentID", "Calculate average downtime per EquipmentID", "Analyze the distribution of downtime across different operators"], "eda_results": {"missing_values": {"EquipmentID": 0, "Date": 0, "Shift": 0, "Downtime_Hours": 3, "Cause": 1, "Operator": 1}, "standardized_dates": ["2023-03-15", "2023-03-15", "2023-03-15", "2023-03-16", "2023-03-16", "2023-03-16", "2023-03-17", "2023-03-17", "2023-03-17", "2023-03-18", "2023-03-18"], "cleaned_categorical_unique_values": {"Shift": ["morning", "night", "evening"], "Cause": ["electrical failure", "mechanical", "overheating", "unknown"], "Operator": ["John", "Alice", "Bob", "Eve", "Carol", "Dave"]}, "downtime_hours_numeric": [2.5, 0.5, null, 1.0, null, 2.0, 1.5, 1.2, 0.75, 0.0, null], "summary_stats": {"count": 8, "mean": 1.3, "std_dev": 0.83, "min": 0.0, "max": 2.5}, "value_counts": {"Cause": {"mechanical": 3, "electrical failure": 3, "overheating": 2, "unknown": 1}, "Shift": {"morning": 5, "night": 4, "evening": 2}}, "records_per_equipment": {"EQ101": 2, "EQ102": 2, "EQ103": 2, "EQ104": 2, "EQ105": 2, "EQ106": 1}, "avg_downtime_per_equipment": {"EQ101": 2.5, "EQ102": 1.0, "EQ103": 0.75, "EQ104": 0.5, "EQ105": 2.0, "EQ106": 1.2}, "avg_downtime_per_operator": {"John": 2.5, "Alice": 1.0, "Bob": 1.2, "Eve": 1.0, "Carol": 2.0, "Dave": 0.0}}}
{"purpose": "Analyze customer purchase behavior and identify missing data patterns in retail transaction records.", "raw_table": "TransactionID,CustomerID,PurchaseDate,ProductCategory,Quantity,PricePerUnit,PaymentMethod\nT001,C1001,2024-01-15,Electronics,2,199.99,Credit Card\nT002,C1002,01/20/2024,apparel,1,,Debit Card\nT003,C1003,2024/01/22,Home & Kitchen,3,45.50,Cash\nT004,,2024-01-25,Electronics,1,199.99,Credit Card\nT005,C1005,2024-1-27,Books,5,12.99,credit card\nT006,C1001,2024-01-28,apparel,,29.99,PayPal\nT007,C1006,01-29-2024,Toys,2,15.00,Cash\nT008,C1007,2024-01-30,Electronics,1,199.99,CREDIT CARD\nT009,C1008,,Books,3,12.99,Debit Card\nT010,C1009,2024-01-31,home & kitchen,4,45.50,Credit Card\nT011,C1010,2024-02-01,Apparel,2,29.99,PayPal", "eda_steps": ["Check and summarize missing values in each column", "Standardize capitalization for ProductCategory and PaymentMethod columns", "Parse and unify PurchaseDate to a consistent date format", "Compute descriptive statistics for Quantity and PricePerUnit columns", "Generate value counts for ProductCategory and PaymentMethod columns", "Identify unique customers and count transactions per customer", "Calculate total purchase amount per transaction as Quantity multiplied by PricePerUnit", "Find transactions with missing or zero Quantity or PricePerUnit", "Summarize distribution skewness for Quantity and PricePerUnit", "Identify top 3 product categories by total quantity sold"], "eda_results": {"missing_values": {"TransactionID": 0, "CustomerID": 1, "PurchaseDate": 1, "ProductCategory": 0, "Quantity": 2, "PricePerUnit": 1, "PaymentMethod": 0}, "standardized_product_category": ["Electronics", "Apparel", "Home & Kitchen", "Electronics", "Books", "Apparel", "Toys", "Electronics", "Books", "Home & Kitchen", "Apparel"], "standardized_payment_method": ["Credit Card", "Debit Card", "Cash", "Credit Card", "Credit Card", "PayPal", "Cash", "Credit Card", "Debit Card", "Credit Card", "PayPal"], "purchase_date_parsing": {"earliest_date": "2024-01-15", "latest_date": "2024-02-01", "unparsed_dates": 0}, "descriptive_statistics": {"Quantity": {"count": 9, "mean": 2.44, "std": 1.44, "min": 1, "25%": 1.5, "50%": 2, "75%": 3, "max": 5}, "PricePerUnit": {"count": 10, "mean": 75.39, "std": 81.62, "min": 12.99, "25%": 15, "50%": 29.99, "75%": 199.99, "max": 199.99}}, "value_counts_product_category": {"Electronics": 3, "Apparel": 3, "Home & Kitchen": 2, "Books": 2, "Toys": 1}, "value_counts_payment_method": {"Credit Card": 5, "Debit Card": 2, "Cash": 2, "PayPal": 2}, "unique_customers": 10, "transactions_per_customer": {"C1001": 2, "C1002": 1, "C1003": 1, "C1005": 1, "C1006": 1, "C1007": 1, "C1008": 1, "C1009": 1, "C1010": 1}, "transactions_missing_or_zero_quantity_or_price": ["T002 (missing PricePerUnit)", "T006 (missing Quantity)"], "skewness": {"Quantity": 0.62, "PricePerUnit": 1.55}, "top_3_product_categories_by_quantity": {"Books": 8, "Home & Kitchen": 7, "Electronics": 4}}}
{"purpose": "Analyze monthly transaction patterns and customer segments in retail banking.", "raw_table": "Transaction_ID,Customer_ID,Transaction_Date,Transaction_Amount,Transaction_Type,Account_Type\nT001,C123,2023/01/15,250.00,deposit,Checking\nT002,C124,15-02-2023,,-withdrawal,Savings\nT003,c125,2023-03-05,1000,Deposit,checking\nT004,C126,03/25/2023,500,Withdrawal,Checking\nT005,C127,2023-03-30,,transfer,Savings\nT006,C128,April 01 2023,750,Withdrawal,Checking\nT007,C129,2023-04-15,-200,deposit,SAVINGS\nT008,C130,2023/04/25,1200,Withdrawal,checking\nT009,C131,2023-05-05,300,Transfer,Checking\nT010,C132,2023-05-10,450,withdrawal,savings\nT011,C133,2023-05-15,NaN,Deposit,Checking\nT012,C134,05-20-2023,600,withdrawal,checking\nT013,C135,2023/05/25,700,Deposit,Savings\nT014,C136,,850,deposit,Checking", "eda_steps": ["Standardize date formats in Transaction_Date column", "Convert all Transaction_Type values to lowercase", "Identify and count missing values in each column", "Calculate descriptive statistics for Transaction_Amount", "Generate value counts for Transaction_Type and Account_Type", "Analyze the distribution of Transaction_Amount by Transaction_Type", "Check for negative or zero values in Transaction_Amount", "Summarize the number of transactions per month", "Identify top 2 customers by total transaction amount"], "eda_results": {"missing_values": {"Transaction_ID": 0, "Customer_ID": 0, "Transaction_Date": 1, "Transaction_Amount": 3, "Transaction_Type": 0, "Account_Type": 0}, "standardized_dates": ["2023-01-15", "2023-02-15", "2023-03-05", "2023-03-25", "2023-03-30", "2023-04-01", "2023-04-15", "2023-04-25", "2023-05-05", "2023-05-10", "2023-05-15", "2023-05-20", "2023-05-25", null], "transaction_type_counts": {"deposit": 6, "withdrawal": 6, "transfer": 2}, "account_type_counts": {"checking": 8, "savings": 6}, "transaction_amount_stats": {"count": 11, "mean": 548.18, "std": 334.94, "min": -200, "25%": 300, "50%": 500, "75%": 750, "max": 1200}, "negative_or_zero_amounts": 1, "monthly_transaction_counts": {"2023-01": 1, "2023-02": 1, "2023-03": 3, "2023-04": 3, "2023-05": 5}, "top_customers_by_amount": {"C130": 1200, "c125": 1000}}}
{"purpose": "Analyze public housing application statuses and applicant demographics to identify trends and data quality issues.", "raw_table": "Application_ID,Applicant_Name,Submission_Date,Age,Income,State,Application_Status\n1,John Doe,2023-02-15,45,55000,ca,Approved\n2,jane smith,15/03/2023,38,62000,California,approved\n3,Mike O'Neil,2023/04/01,NaN,58000,TX,Denied\n4,Alice Johnson,2023-04-10,29,missing,tx,Pending\n5,Bob Lee,04-12-2023,42,47000,Fl,Denied\n6,,2023-05-01,34,53000,FL,Approved\n7,Clara Oswald,May 5 2023,27,50000,ny,Pending\n8,George King,2023.06.01,31,52000,New york,APPROVED\n9,Emily Stone,2023/06/10,missing,61000,NY,Denied\n10,Robert Brown,06/15/2023,50,48000,ca,Pending\n11,Marta Holmes,2023-06-20,33,NaN,CA,approved\n12,Samuel Green,2023-06-25,28,54000,fl,Denied", "eda_steps": ["Check the data types of each column", "Calculate the number and percentage of missing values per column", "Standardize the 'State' column to consistent uppercase two-letter codes", "Parse and standardize the 'Submission_Date' column to a uniform date format", "Generate value counts for the 'Application_Status' column", "Compute descriptive statistics (mean, median, std) for the 'Age' and 'Income' columns", "Identify rows with missing 'Applicant_Name' or 'Income'", "Summarize the distribution of application statuses by state", "Check for duplicated Application_IDs"], "eda_results": {"data_types": {"Application_ID": "int", "Applicant_Name": "string", "Submission_Date": "string", "Age": "string", "Income": "string", "State": "string", "Application_Status": "string"}, "missing_values": {"Applicant_Name": {"count": 1, "percentage": 8.33}, "Age": {"count": 2, "percentage": 16.67}, "Income": {"count": 2, "percentage": 16.67}}, "state_standardization": {"CA": 4, "TX": 2, "FL": 3, "NY": 3}, "submission_date_standardized": "All dates converted to ISO format YYYY-MM-DD", "application_status_counts": {"Approved": 4, "Denied": 4, "Pending": 3}, "age_statistics": {"mean": 35.63, "median": 33, "std_dev": 7.67, "count_non_missing": 10}, "income_statistics": {"mean": 54500, "median": 53000, "std_dev": 5108, "count_non_missing": 10}, "rows_missing_applicant_name_or_income": [4, 6], "application_status_by_state": {"CA": {"Approved": 3, "Pending": 1}, "TX": {"Denied": 2}, "FL": {"Denied": 1, "Approved": 1}, "NY": {"Approved": 1, "Denied": 2, "Pending": 1}}, "duplicate_application_ids": 0}}
{"purpose": "Analyze household electricity consumption patterns and identify data quality issues.", "raw_table": "Household_ID,Date,Energy_Consumption(kWh),Region,Device_Type\n001,2024-01-05,15.3,North,AC\n002,01/06/2024,NaN,south,Heater\n003,2024/01/07,20.1,East,AC\n004,2024-1-08,18.7,west,heater\n005,2024-01-09,17.2,North,Fridge\n006,,14.0,East,fridge\n007,2024-01-11,abc,South,AC\n008,2024-01-12,19.5,East,Heater\n009,13-01-2024,16.8,West,AC\n010,2024-01-14,,north,Heater\n011,2024-01-15,21.0,South,AC\n012,2024-01-16,18.3,East,Fridge", "eda_steps": ["Convert all dates to a consistent date format and identify missing or invalid dates", "Check and report missing values in each column", "Standardize Region and Device_Type column values to consistent capitalization", "Compute descriptive statistics for Energy_Consumption(kWh) excluding invalid or missing values", "Generate value counts for Region and Device_Type columns", "Identify rows with non-numeric Energy_Consumption values", "Analyze the distribution of Energy_Consumption by Region"], "eda_results": {"missing_values": {"Household_ID": 0, "Date": 1, "Energy_Consumption(kWh)": 2, "Region": 0, "Device_Type": 0}, "invalid_dates": {"rows": [9], "invalid_formats": ["13-01-2024"]}, "standardized_categories": {"Region": ["North", "South", "East", "West"], "Device_Type": ["AC", "Heater", "Fridge"]}, "non_numeric_energy_rows": [6], "summary_stats": {"count": 11, "mean": 17.58, "std": 2.18, "min": 14.0, "25%": 16.8, "50%": 17.95, "75%": 19.5, "max": 21.0}, "value_counts": {"Region": {"East": 4, "North": 3, "South": 3, "West": 2}, "Device_Type": {"AC": 4, "Heater": 4, "Fridge": 3}}, "energy_by_region_mean": {"East": 17.98, "North": 16.25, "South": 18.4, "West": 17.75}}}
