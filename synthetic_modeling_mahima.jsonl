{"purpose": "Build a classification model to predict whether a bus trip will be delayed by more than 5 minutes.", "raw_table": "TripID,RouteType,DayOfWeek,DepartureTime,DriverExperience,WeatherCondition,TrafficLevel,DelayOver5Min\n1,Urban,Monday,08:15,5,Clear,Low,No\n2,Suburban,FRIDAY,17:40,2,Rain,High,Yes\n3,urban,Tuesday,06:55,8,Fog,Medium,No\n4,Suburban,Wednesday,19:10,3,,High,Yes\n5,Urban,Thursday,12:00,7,Clear,Medium,No\n6,Urban,Saturday,14:30,NaN,Rain,Low,No\n7,Suburban,Sunday,09:45,4,Fog,Medium,Yes\n8,Urban,Monday,07:05,6,Clear,LOW,No\n9,Suburban,Wednesday,18:25,3,Rain,High,Yes\n10,urban,Friday,20:00,1,Fog,High,Yes\n11,Urban,Tuesday,13:15,9,Clear,Medium,No\n12,Suburban,Thursday,16:50,3,Rain,,Yes\n13,Urban,Sunday,10:30,5,clear,Low,No\n14,Suburban,Monday,18:00,2,Fog,High,Yes", "model_steps": ["Load dataset and inspect for missing and inconsistent values", "Standardize categorical values: normalize 'RouteType' and 'DayOfWeek' capitalization", "Impute missing 'DriverExperience' values with median experience", "Impute missing 'WeatherCondition' and 'TrafficLevel' with most frequent category", "Convert 'DepartureTime' to numeric feature representing minutes from midnight", "One-hot encode categorical features: 'RouteType', 'DayOfWeek', 'WeatherCondition', 'TrafficLevel'", "Split data into training (80%) and testing (20%) sets with stratification on target", "Standardize numeric features: 'DriverExperience' and 'DepartureTime'", "Train a RandomForestClassifier with 100 trees and max_depth=5", "Evaluate model on test set computing accuracy, precision, recall, and F1 score", "Generate and display the confusion matrix", "Identify and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.79, "f1": 0.81, "confusion_matrix": {"true_positive": 11, "true_negative": 10, "false_positive": 2, "false_negative": 3}, "top_feature_importances": {"TrafficLevel_High": 0.28, "DepartureTime": 0.22, "WeatherCondition_Rain": 0.18}, "hyperparameters": {"n_estimators": 100, "max_depth": 5, "random_state": 42}}}
{"purpose": "Build a regression model to predict hourly energy consumption in a residential building based on weather and occupancy data.", "raw_table": "Hour,Temperature_C,Humidity_Percent,Weather_Condition,Occupancy_Status,Energy_Consumption_kWh\n0,15.2,55,Clear,Occupied,1.2\n1,14.8,57,clear,occupied,1.1\n2,14.5,,Clear,Occupied,1.0\n3,14.0,60,Rain,Vacant,0.8\n4,13.6,62,rain,vacant,0.7\n5,13.2,64,Cloudy,Occupied,1.0\n6,14.0,63,cloudy,Occupied,1.3\n7,16.5,58,Clear,Occupied,2.0\n8,18.0,55,Clear,Occupied,2.5\n9,20.0,50,Clear,Occupied,3.0\n10,22.1,48,Clear,Occupied,3.5\n11,23.5,45,Clear,Occupied,4.0\n12,24.0,44,Clear,Occupied,4.2\n13,24.5,43,Clear,Occupied,4.3\n14,24.0,44,Clear,Occupied,4.1", "model_steps": ["Load the dataset and identify target variable as Energy_Consumption_kWh", "Handle missing values by imputing median for Humidity_Percent", "Standardize numeric features: Hour, Temperature_C, Humidity_Percent", "Normalize categorical variables Weather_Condition and Occupancy_Status by lowercasing all text", "One-hot encode Weather_Condition and Occupancy_Status", "Split the data into training (80%) and testing (20%) subsets", "Train a GradientBoostingRegressor model on the training set", "Perform hyperparameter tuning on learning_rate and n_estimators using grid search", "Evaluate the model on the test set using RMSE, MAE, and R2 metrics", "Analyze feature importances from the trained model", "Generate predicted vs actual energy consumption plot for test data"], "model_results": {"rmse": 0.22, "mae": 0.16, "r2": 0.92, "top_feature_importances": {"Hour": 0.35, "Temperature_C": 0.3, "Occupancy_Status_Occupied": 0.2, "Weather_Condition_Clear": 0.08, "Humidity_Percent": 0.07}, "best_hyperparameters": {"learning_rate": 0.1, "n_estimators": 100}}}
{"purpose": "Predict the risk of readmission within 30 days for patients discharged after heart failure hospitalization.", "raw_table": "PatientID,Age,Gender,PreviousAdmissions,BloodPressure,Cholesterol,SmokingStatus,DischargeDisposition,Readmitted\n1,68,Male,2,140,220,Yes,HOME,Yes\n2,75,Female,0,160,,no,Rehab,No\n3,82,Female,1,135,240,Yes,home,Yes\n4,55,Male,3,150,200,No,HOME,No\n5,70,Female,,145,230,No,rehab,Yes\n6,60,Male,1,130,215,yes,Home,No\n7,77,Male,2,155,225,No,HOME,Yes\n8,63,Female,1,NaN,210,No,rehab,No\n9,85,Male,4,165,235,Yes,HOME,Yes\n10,79,Female,2,142,NaN,No,Home,No\n11,67,Male,1,148,220,No,HOME,No\n12,73,Female,0,138,225,YEs,Rehab,Yes\n13,58,Male,1,132,215,no,Home,No\n14,80,Female,3,155,240,NO,rehab,Yes", "model_steps": ["Load data and identify missing and inconsistent values", "Impute missing numeric values (BloodPressure and Cholesterol) using median imputation", "Standardize numeric features: Age, PreviousAdmissions, BloodPressure, Cholesterol", "Normalize categorical variables: unify casing in Gender, SmokingStatus, and DischargeDisposition", "One-hot encode categorical variables: Gender, SmokingStatus, DischargeDisposition", "Split data into training (80%) and test (20%) sets stratified by target variable Readmitted", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search over max_depth parameter [3,5,7] using 5-fold cross-validation", "Evaluate the final model on test set using accuracy, precision, recall, and F1 score", "Generate the confusion matrix and report feature importances"], "model_results": {"accuracy": 0.79, "precision": 0.75, "recall": 0.82, "f1": 0.78, "confusion_matrix": {"true_positive": 9, "true_negative": 7, "false_positive": 3, "false_negative": 2}, "top_feature_importances": {"PreviousAdmissions": 0.28, "Age": 0.22, "DischargeDisposition_HOME": 0.15, "SmokingStatus_Yes": 0.12, "BloodPressure": 0.1}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a loan applicant will default on their loan within 12 months.", "raw_table": "ApplicantID,Age,Income,EmploymentStatus,LoanAmount,CreditScore,LoanPurpose,Defaulted\n1,45,55000,Employed,15000,680,Home Improvement,No\n2,38,72000,Self-employed,22000,720,Debt Consolidation,No\n3,29,NaN,employed,18000,690,Home Improvement,Yes\n4,50,61000,Employed,25000,710,Car,No\n5,41,48000,Unemployed,12000,650,Home improvement,Yes\n6,34,53000,Employed,,670,Debt consolidation,No\n7,28,47000,Self-employed,14000,NaN,Car,No\n8,55,80000,Employed,30000,750,Vacation,No\n9,26,43000,employed,13000,640,Debt Consolidation,Yes\n10,33,,Self-Employed,16000,685,Car,No\n11,47,62000,Employed,20000,700,Home Improvement,No\n12,NaN,59000,Employed,17000,695,Debt Consolidation,Yes\n13,39,60000,Employed,15000,680,Home Improvement,No\n14,31,52000,Unemployed,10000,630,Car,Yes", "model_steps": ["Load the dataset and identify missing and inconsistent values.", "Correct inconsistent capitalization in EmploymentStatus and LoanPurpose columns.", "Impute missing numeric values (Age, Income, LoanAmount, CreditScore) using median imputation.", "Encode categorical variables EmploymentStatus and LoanPurpose using one-hot encoding.", "Define target variable as Defaulted (binary classification: Yes=1, No=0).", "Split data into 80% training and 20% testing sets with stratification on target.", "Standardize numeric features such as Age, Income, LoanAmount, CreditScore.", "Train a RandomForestClassifier on the training set.", "Tune max_depth hyperparameter using 5-fold cross-validation grid search.", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score.", "Generate and analyze confusion matrix to understand prediction errors."], "model_results": {"accuracy": 0.79, "f1": 0.76, "precision": 0.74, "recall": 0.78, "confusion_matrix": {"true_positive": 7, "true_negative": 15, "false_positive": 5, "false_negative": 4}, "top_feature_importances": {"CreditScore": 0.32, "LoanAmount": 0.22, "Income": 0.18, "EmploymentStatus_Self-employed": 0.12, "LoanPurpose_Home Improvement": 0.08}, "best_hyperparameters": {"max_depth": 6}}}
{"purpose": "Predict whether a student will pass the final exam based on demographic and academic features.", "raw_table": "StudentID,Age,Gender,StudyHoursPerWeek,PreviousGrade,AttendanceRate,SchoolType,PassedFinal\nS01,17,Male,12,85,95,Public,Yes\nS02,18,Female,8,78,88,private,No\nS03,17,,10,82,92,Public,Yes\nS04,16,Female,5,70,80,public,No\nS05,19,Male,NaN,88,97,Private,Yes\nS06,18,Female,15,90,99,Public,Yes\nS07,17,Male,7,65,75,private,No\nS08,16,Female,9,73,85,Public,No\nS09,18,MALE,11,80,90,Public,Yes\nS10,17,Female,6,68,82,Private,No\nS11,16,Male,13,83,94,Public,Yes\nS12,17,Female,4,60,78,Private,No\nS13,18,Male,14,87,96,Public,Yes\nS14,16,female,10,77,89,public,No", "model_steps": ["Load the CSV data into a DataFrame", "Standardize capitalization in categorical columns: Gender and SchoolType", "Fill missing StudyHoursPerWeek value with the median study hours", "Impute missing Gender value with the mode", "Convert target variable 'PassedFinal' to binary labels (Yes=1, No=0)", "One-hot encode categorical variables: Gender and SchoolType", "Split data into training and test sets (80/20 split)", "Standardize numeric features: Age, StudyHoursPerWeek, PreviousGrade, AttendanceRate", "Train a Logistic Regression classifier on the training data", "Evaluate model performance using accuracy, F1 score, precision, and recall on the test set", "Extract and report the top 3 feature coefficients to interpret feature importance"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.88, "recall": 0.79, "top_feature_importances": {"PreviousGrade": 1.45, "StudyHoursPerWeek": 0.92, "AttendanceRate": 0.75}}}
{"purpose": "Predict customer churn within a telecommunications company based on usage patterns and customer demographics.", "raw_table": "CustomerID,MonthlyCharges,Contract,Tenure,PaymentMethod,InternetService,Churn\n001,70.35,Month-to-month,5,Credit card, DSL,Yes\n002,45.00,One year,24,Bank transfer, Fiber optic,No\n003,NaN,Month-to-month,3,electronic check,Fiber optic,Yes\n004,89.50,Two Year,36,Mailed check,DSL,no\n005,65.75,Month-to-month,1,Credit Card,Fiber optic,Yes\n006,55.20,One Year,18,electronic Check,DSL,No\n007,80.00,Month-to-month,7,Bank Transfer,Fiber optic,Yes\n008,42.10,one year,20,Mailed Check,DSL,No\n009,55.55,Month-to-month,2,Credit card,Fiber Optic,Yes\n010,90.10,Two year,40,Electronic check,DSL,No\n011,59.99,Month-to-month,4,credit card,Fiber optic,Yes\n012,78.25,One Year,22,Bank transfer,DSL,No\n013,48.00,Month-to-month,6,Mailed Check,Fiber optic,Yes\n014,NaN,Month-to-month,NaN,Electronic Check,Fiber optic,Yes", "model_steps": ["Load dataset and inspect for missing values and inconsistencies", "Impute missing MonthlyCharges with median and Tenure with mode", "Normalize categorical string values (e.g., standardize capitalization in Contract, PaymentMethod, InternetService, Churn)", "One-hot encode categorical variables: Contract, PaymentMethod, InternetService", "Convert target variable 'Churn' to binary (Yes=1, No=0)", "Split data into training (80%) and testing (20%) sets using stratified sampling on the target", "Standardize numeric features MonthlyCharges and Tenure", "Train a GradientBoostingClassifier with default parameters", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Extract and report top 5 feature importances", "Generate confusion matrix and predicted probabilities for the test set"], "model_results": {"accuracy": 0.86, "f1": 0.79, "precision": 0.75, "recall": 0.83, "top_feature_importances": {"Contract_Month-to-month": 0.31, "Tenure": 0.21, "MonthlyCharges": 0.18, "InternetService_Fiber optic": 0.12, "PaymentMethod_Credit card": 0.07}, "confusion_matrix": {"true_positive": 23, "true_negative": 42, "false_positive": 8, "false_negative": 5}, "hyperparameters": {"n_estimators": 100, "learning_rate": 0.1, "max_depth": 3}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features and location.", "raw_table": "HouseID,Size_sqft,Bedrooms,Bathrooms,Neighborhood,YearBuilt,Garage,Condition,SalePrice\n1,1500,3,2,Suburb,2005,Yes,good,250000\n2,850,2,1,downtown,1998,no,Fair,180000\n3,2000,4,3,Suburb,2010,Yes,Excellent,320000\n4,1200,3,2,Suburb,NaN,Yes,good,260000\n5,950,2,1,Suburb,2000,No,fair,190000\n6,1800,4,3,Downtown,2012,yes,Excellent,340000\n7,1300,3,2,suburb,2003,Yes,good,270000\n8,1100,2,1,Downtown,2001,No,Poor,210000\n9,1750,3,2,Suburb,2008,Yes,Good,300000\n10,NaN,3,2,Downtown,2005,No,Good,240000\n11,1600,3,2,Suburb,2007,Yes,Good,310000\n12,1400,3,2,Suburb,2004,YES,Good,280000", "model_steps": ["Load the dataset and inspect for missing values and inconsistent data", "Standardize capitalization in categorical columns 'Neighborhood', 'Garage', and 'Condition' to ensure consistency", "Impute missing numeric values in 'Size_sqft' and 'YearBuilt' using median values", "Convert 'Garage' and 'Condition' into categorical variables and one-hot encode them along with 'Neighborhood'", "Split data into training and testing sets with an 80/20 ratio", "Standardize numeric features: 'Size_sqft', 'Bedrooms', 'Bathrooms', 'YearBuilt'", "Train a RandomForestRegressor model on the training data", "Perform hyperparameter tuning on max_depth and n_estimators using 5-fold cross-validation", "Evaluate the model on the test set calculating RMSE, MAE, and R2 score", "Extract and report the top 5 feature importances from the trained model"], "model_results": {"rmse": 18000, "mae": 14000, "r2": 0.88, "top_feature_importances": {"Size_sqft": 0.42, "Neighborhood_Suburb": 0.18, "Condition_Good": 0.12, "YearBuilt": 0.1, "Garage_Yes": 0.08}, "hyperparameters": {"max_depth": 12, "n_estimators": 100}}}
{"purpose": "Predict whether a student will pass or fail the final exam based on study habits and demographic factors.", "raw_table": "StudentID,StudyHoursPerWeek,AttendanceRate,PreviousGrade,ParentalEducation,SchoolType,Gender,FinalPass\n1,15,0.9,85,College,Public,Male,Pass\n2,7,0.7,78,HighSchool,private,Female,fail\n3,12,0.85,88,,Public,Male,Pass\n4,5,0.6,70,College,Public,Female,Fail\n5,NaN,0.8,65,HighSchool,Public,Male,fail\n6,20,0.95,92,College,Private,female,Pass\n7,8,missing,72,Highschool,Public,Male,Fail\n8,10,0.75,80,College,Private,Female,Pass\n9,3,0.5,58,HighSchool,public,Male,fail\n10,14,0.88,83,College,Public,Female,Pass\n11,9,0.82,81,HighSchool,Private,Male,Pass\n12,6,0.65,68,,Public,Female,Fail\n13,11,0.9,87,College,public,Male,Pass\n14,4,0.55,60,HighSchool,Private,Female,fail", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Impute missing StudyHoursPerWeek with median value", "Impute missing AttendanceRate with median value", "Standardize capitalization in categorical columns: ParentalEducation, SchoolType, Gender", "Fill missing ParentalEducation values with mode 'HighSchool'", "One-hot encode categorical variables: ParentalEducation, SchoolType, Gender", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features: StudyHoursPerWeek, AttendanceRate, PreviousGrade", "Train a RandomForestClassifier model to predict FinalPass", "Perform grid search over number of trees (n_estimators) with values [50, 100]", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix and identify top 3 important features"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.81, "recall": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"StudyHoursPerWeek": 0.35, "AttendanceRate": 0.3, "PreviousGrade": 0.2}, "best_hyperparameters": {"n_estimators": 100}}}
{"purpose": "Predict monthly average surface temperature anomalies based on climate indicators and geographic factors.", "raw_table": "region,month,sea_surface_temp,co2_concentration,enso_phase,vegetation_index,temp_anomaly\nNorth Atlantic,Jan,16.2,411.2,Neutral,0.42,0.3\nSouth Pacific,Feb,20.1,412.1,El Nino,0.34,0.8\nIndian Ocean,Mar,28.5,409.9,la nina,0.47,0.1\nArctic,Apr,1.2,410.5,Neutral,missing,1.2\nNorth Atlantic,May,15.8,413.0,El Nino,0.39,0.5\nSouth Pacific,Jun,19.7,NA,Neutral,0.36,0.2\nIndian Ocean,Jul,29.1,414.2,El NiNo,0.43,0.6\nArctic,Aug,0.8,412.7,La Ni\u00f1a,0.40,1.0\nNorth Atlantic,Sep,15.3,411.6,Neutral,0.44,0.4\nSouth Pacific,Oct,20.5,413.5,El Nino,0.35,0.7\nIndian Ocean,Nov,28.9,410.3,Neutral,0.48,0.3\nArctic,Dec,1.0,411.0,El Nino,0.41,1.1", "model_steps": ["Load the dataset and identify the target variable as temp_anomaly", "Handle missing values by imputing the vegetation_index missing entry with the median", "Standardize the numeric features: sea_surface_temp and co2_concentration", "Normalize the month column by converting month names to numeric values (Jan=1, Feb=2, etc.)", "One-hot encode the categorical variables: region and enso_phase (fix inconsistent capitalization in enso_phase)", "Split data into train and test sets with 80/20 ratio", "Train a Gradient Boosting Regressor on the training data", "Perform hyperparameter tuning for learning_rate and max_depth using 5-fold cross-validation", "Evaluate the model on the test set using RMSE, MAE, and R-squared metrics", "Extract feature importances from the trained model"], "model_results": {"rmse": 0.18, "mae": 0.14, "r2": 0.85, "top_feature_importances": {"co2_concentration": 0.35, "sea_surface_temp": 0.3, "enso_phase_El Nino": 0.15, "region_Arctic": 0.1, "vegetation_index": 0.05}, "best_hyperparameters": {"learning_rate": 0.05, "max_depth": 4}}}
{"purpose": "Build a classification model to predict whether a crop field will have high or low yield based on soil and weather conditions.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Temperature_C,Previous_Crop,Yield_Category\nF001,Loam,350,22.5,Corn,High\nF002,Sandy,200,25.0,Wheat,Low\nF003,Clay,NaN,20.0,Soybean,High\nF004,Loam,300,21.5,corn,High\nF005,Silt,400,23.0,Wheat,Low\nF006,Sandy,180,27.0,soybean,Low\nF007,Clay,250,22.0,Corn,High\nF008,Loam,NaN,19.5,Wheat,Low\nF009,Silt,370,24.5,Corn,High\nF010,Sandy,210,NaN,Wheat,Low\nF011,Clay,260,22.5,Soybean,High\nF012,Loam,320,23.0,Corn,High\nF013,Silt,NaN,21.0,Wheat,Low\nF014,Sandy,190,26.0,soybean,Low\nF015,Clay,280,22.0,Corn,High", "model_steps": ["Load CSV data into a DataFrame", "Identify missing values in Rainfall_mm and Temperature_C and impute using median values", "Normalize capitalization in categorical columns Soil_Type and Previous_Crop", "One-hot encode Soil_Type and Previous_Crop columns", "Split data into training (80%) and testing (20%) sets", "Standardize numeric features Rainfall_mm and Temperature_C using training set statistics", "Train a RandomForestClassifier with default parameters to predict Yield_Category", "Perform grid search on n_estimators and max_depth hyperparameters using 5-fold cross-validation", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix for test set predictions", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.9, "f1": 0.87, "confusion_matrix": {"High": {"High": 7, "Low": 1}, "Low": {"High": 1, "Low": 6}}, "top_feature_importances": {"Rainfall_mm": 0.32, "Soil_Type_Loam": 0.25, "Temperature_C": 0.18}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a government grant application will be approved based on applicant and project information.", "raw_table": "Application_ID,Applicant_Age,Applicant_Employment_Status,Project_Type,Requested_Amount,Previous_Grants_Awarded,Region,Approval_Status\n1,45,Employed,Infrastructure,500000,2,North,Approved\n2,38,Unemployed,Research,300000,,South,Rejected\n3,29,Self-employed,Education,150000,1,East,approved\n4,52,Employed,Healthcare,400000,3,West,Approved\n5,47,,Infrastructure,600000,2,North,Rejected\n6,34,Employed,RESEARCH,250000,0,South,Rejected\n7,41,Unemployed,Education,200000,1,East,Approved\n8,36,Employed,Infrastructure,550000,1,North,\n9,28,Self-employed,Healthcare,350000,1,West,Rejected\n10,50,Employed,Education,180000,5,South,Approved\n11,33,Employed,Healthcare,400000,NaN,East,Rejected\n12,39,Unemployed,Infrastructure,480000,2,North,Approved\n13,44,Employed,Research,320000,3,West,Approved\n14,31,Self-employed,Education,210000,1,South,Rejected", "model_steps": ["Load the CSV data into a DataFrame", "Handle missing values in 'Applicant_Employment_Status' by imputing the mode", "Fill missing 'Previous_Grants_Awarded' with zero and convert to integer", "Standardize capitalization in 'Project_Type' and 'Approval_Status' columns", "Encode 'Applicant_Employment_Status', 'Project_Type', and 'Region' using one-hot encoding", "Convert 'Approval_Status' to binary target variable: Approved=1, Rejected=0", "Split data into train (80%) and test (20%) sets", "Standardize numeric features: 'Applicant_Age', 'Requested_Amount', 'Previous_Grants_Awarded'", "Train a RandomForestClassifier with default parameters on the training set", "Evaluate model on the test set computing accuracy, precision, recall, and F1 score", "Generate confusion matrix for test set predictions", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.83, "precision": 0.85, "recall": 0.79, "f1": 0.82, "confusion_matrix": {"true_positive": 5, "true_negative": 6, "false_positive": 1, "false_negative": 2}, "top_features": {"Requested_Amount": 0.32, "Project_Type_Infrastructure": 0.21, "Previous_Grants_Awarded": 0.18}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a customer will purchase a recommended product based on browsing and demographic data.", "raw_table": "CustomerID,Age,Gender,Browser,TimeOnSiteMinutes,PagesVisited,ReferralSource,PreviousPurchases,Purchase\nC001,34,Male,Chrome,15,5,Google,2,Yes\nC002,22,Female,firefox,7,3,Direct,,No\nC003,29,,Safari,12,7,Google,1,Yes\nC004,45,Female,Edge,,4,Bing,0,No\nC005,38,Male,chrome,20,10,google,3,Yes\nC006,26,Female,Firefox,5,2,Direct,0,No\nC007,31,Male,Safari,NaN,6,Bing,1,Yes\nC008,40,Female,EDGE,17,8,Google,2,Yes\nC009,35,Male,Chrome,9,3,,1,No\nC010,28,Female,chrome,14,5,Google,1,Yes\nC011,33,Male,Firefox,11,5,Direct,NaN,No\nC012,41,Female,Safari,16,7,Bing,3,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Handle missing values: impute missing Age and TimeOnSiteMinutes using median; fill missing Gender with mode; fill missing ReferralSource with 'Unknown'; impute PreviousPurchases missing with 0", "Normalize inconsistent casing in categorical columns (e.g., Browser and ReferralSource to lowercase)", "Encode categorical variables Gender, Browser, ReferralSource using one-hot encoding", "Split data into train and test sets with 80/20 ratio, stratified by the target Purchase", "Standardize numeric features Age, TimeOnSiteMinutes, PagesVisited, PreviousPurchases", "Train a RandomForestClassifier with default hyperparameters", "Perform grid search over n_estimators [50,100], max_depth [5,10,15] using 5-fold cross-validation", "Evaluate model on test set computing accuracy, precision, recall, and F1 score", "Extract feature importances from the best model"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}, "top_feature_importances": {"TimeOnSiteMinutes": 0.3, "PagesVisited": 0.25, "PreviousPurchases": 0.2, "ReferralSource_google": 0.1, "Browser_chrome": 0.08, "Age": 0.07}}}
{"purpose": "Predict whether a manufactured component will fail quality inspection based on sensor readings and production conditions.", "raw_table": "component_id,temperature,pressure,vibration_level,machine_id,shift,operator_experience,defect_status\nC001,75.2,30.1,0.005,M01,Day,5,Pass\nC002,80.5,NaN,0.007,M02,NIGHT,3,Fail\nC003,77.0,29.8,0.006,M01,day,7,Pass\nC004,NaN,31.0,0.008,m01,Night,2,Fail\nC005,79.1,30.5,0.005,M03,Day,,Pass\nC006,76.3,30.2,0.007,M02,DAY,4,Fail\nC007,78.4,29.9,NA,M03,Night,6,Pass\nC008,81.0,30.7,0.009,M01,Night,3,Fail\nC009,75.6,30.0,0.005,M02,Day,5,Pass\nC010,80.0,30.6,0.008,M03,day,1,Fail\nC011,79.5,31.1,0.007,M01,Night,4,Pass\nC012,77.8,30.3,0.006,M02,Day,3,Pass\nC013,NaN,30.4,0.007,M03,Night,2,Fail\nC014,76.9,NaN,0.005,M01,day,5,Pass\nC015,78.7,30.8,0.006,m02,Night,4,Fail", "model_steps": ["Load dataset and inspect for missing or inconsistent values", "Impute missing numeric values (temperature, pressure, vibration_level) with median values", "Standardize numeric features: temperature, pressure, vibration_level", "Normalize categorical columns: machine_id (convert to uppercase), shift (convert to lowercase)", "Impute missing operator_experience values with the median experience level", "One-hot encode categorical variables: machine_id and shift", "Encode target variable defect_status as binary: Pass=0, Fail=1", "Split data into training and testing sets with 80% for training and 20% for testing", "Train a RandomForestClassifier with 100 trees on the training data", "Tune hyperparameters max_depth and min_samples_split using grid search with 5-fold cross-validation", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and feature importance rankings"], "model_results": {"accuracy": 0.87, "precision": 0.83, "recall": 0.78, "f1": 0.8, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 2}, "top_feature_importances": {"vibration_level": 0.32, "temperature": 0.25, "machine_id_M01": 0.15, "pressure": 0.12, "shift_night": 0.1, "operator_experience": 0.06}, "best_hyperparameters": {"max_depth": 6, "min_samples_split": 4}}}
{"purpose": "Predict whether a student will pass the final exam based on study habits and demographics.", "raw_table": "StudentID,StudyHours,Attendance,Major,Gender,PreviousGrade,Passed\n1,15,90,Engineering,Male,85,Yes\n2,5,70,Biology,Female,60,No\n3,12,85,engineering,Female,78,Yes\n4,8,missing,Mathematics,Male,65,No\n5,20,95,Mathematics,FEMALE,90,Yes\n6,7,80,Biology,Male,55,No\n7,14,88,Engineering,Female,82,Yes\n8,missing,75,Mathematics,male,58,No\n9,18,92,Biology,Female,87,Yes\n10,10,85,Engineering,Female,72,Yes\n11,6,60,biology,Male,50,No\n12,16,90,Mathematics,Female,88,Yes\n13,9,80,Engineering,FemAle,70,No\n14,11,85,Biology,male,74,Yes", "model_steps": ["Load data and inspect for missing and inconsistent values", "Standardize capitalization in categorical columns Major and Gender", "Impute missing numeric values in StudyHours and Attendance with median values", "Convert target variable Passed to binary format (Yes=1, No=0)", "One-hot encode categorical variables Major and Gender", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features StudyHours, Attendance, and PreviousGrade", "Train a RandomForestClassifier on the training set", "Perform grid search over max_depth parameter with 5-fold cross-validation", "Evaluate the model on test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.85, "f1": 0.87, "precision": 0.84, "recall": 0.9, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"PreviousGrade": 0.35, "StudyHours": 0.3, "Attendance": 0.2, "Major_Engineering": 0.08, "Gender_Female": 0.07}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a patient will be readmitted to the hospital within 30 days after discharge.", "raw_table": "PatientID,Age,Gender,AdmissionType,NumPrevAdmissions,PrimaryDiagnosis,BloodPressure,Cholesterol,Smoker,Readmitted\n1,65,M,Emergency,2,Diabetes,130/85,High,Yes,Yes\n2,54,F,urgent,1,Hypertension,120/80,Normal,no,No\n3,45,Other,Elective,0,Asthma,110/70,High,Yes,No\n4,72,M,Emergency,3,diabetes,140/90,High,Yes,Yes\n5,38,F,Elective,,Asthma,115/75,normal,No,No\n6,59,m,Urgent,2,Hypertension,135/88,High,NO,Yes\n7,49,F,Emergency,1,COPD,128/82,Normal,No,No\n8,83,M,Elective,4,Diabetes,145/95,High,Yes,Yes\n9,56,F,Emergency,2,Hypertension,130/85,High,yes,Yes\n10,62,M,Urgent,2,Asthma,125/80,Normal,No,No\n11,47,F,Elective,0,COPD,118/78,Normal,No,No\n12,69,M,emergency,3,Diabetes,138/89,High,YES,Yes\n13,52,F,Urgent,1,Hypertension,122/79,Normal,No,No\n14,60,M,Emergency,2,Asthma,133/87,High,No,Yes", "model_steps": ["Parse the 'BloodPressure' column into two numeric features: SystolicBP and DiastolicBP", "Handle missing values in 'NumPrevAdmissions' by imputing with the median", "Normalize capitalization inconsistencies in categorical columns like 'Gender', 'AdmissionType', 'PrimaryDiagnosis', and 'Smoker'", "Encode categorical variables using one-hot encoding", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features including Age, SystolicBP, DiastolicBP, NumPrevAdmissions, and Cholesterol (encoded as binary)", "Train a RandomForestClassifier to predict 'Readmitted'", "Perform hyperparameter tuning on number of trees (n_estimators) and max_depth using grid search with 5-fold cross-validation", "Evaluate the final model on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Identify and report the top 3 most important features"], "model_results": {"accuracy": 0.79, "precision": 0.81, "recall": 0.75, "f1": 0.78, "confusion_matrix": {"true_positive": 6, "true_negative": 7, "false_positive": 2, "false_negative": 2}, "top_feature_importances": {"NumPrevAdmissions": 0.32, "SystolicBP": 0.25, "PrimaryDiagnosis_Diabetes": 0.18}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a student will pass the final exam based on their attendance, homework scores, and study habits.", "raw_table": "StudentID,AttendancePercentage,HomeworkScore,StudyHours,ParticipationLevel,PreviousGrade,PassedFinal\n1,95,88,10,High,A,Yes\n2,80,75,5,medium,B,Yes\n3,60,65,2,low,C,No\n4,85,90,8,HIGH,B,Yes\n5,NaN,70,4,Medium,C,No\n6,70,Na,3,low,D,No\n7,90,85,9,High,B,Yes\n8,55,60,1,Low,D,No\n9,100,95,12,High,A,Yes\n10,65,68,3,medium,C,No\n11,78,72,5,Medium,B,Yes\n12,88,80,7,high,B,Yes\n13,82,78,6,Medium,C,Yes\n14,50,55,1,low,D,No", "model_steps": ["Load the dataset and identify target variable 'PassedFinal'.", "Handle missing values by imputing AttendancePercentage and HomeworkScore with their respective means.", "Normalize inconsistent capitalization in 'ParticipationLevel' and convert it into a categorical variable.", "Convert categorical features 'ParticipationLevel' and 'PreviousGrade' into one-hot encoded variables.", "Split the data into train and test sets with an 80/20 ratio.", "Scale numeric features AttendancePercentage, HomeworkScore, and StudyHours using standardization.", "Train a RandomForestClassifier with default hyperparameters on the training set.", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score.", "Extract and rank feature importances from the trained RandomForest model.", "Generate a confusion matrix for the test set predictions."], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": [[7, 1], [2, 4]], "top_feature_importances": {"AttendancePercentage": 0.35, "StudyHours": 0.25, "HomeworkScore": 0.2, "ParticipationLevel_High": 0.1, "PreviousGrade_B": 0.05, "ParticipationLevel_Low": 0.05}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a movie will be a box office hit based on its attributes.", "raw_table": "MovieID,Genre,DirectorExperience,BudgetMillion,LeadActorPopularity,ReleaseMonth,HasSequel,BoxOfficeHit\n1,Action,5,150,8,July,Yes,Yes\n2,Comedy,2,30,6,December,No,No\n3,Drama,7,50,,march,No,No\n4,Action,3,120,7,August,Yes,Yes\n5,Comedy,10,40,5,November,No,No\n6,Drama,NaN,60,7,May,No,No\n7,Action,4,200,9,July,Yes,Yes\n8,Comedy,6,25,6,December,No,No\n9,Drama,8,70,8,April,No,Yes\n10,Action,3,110,6,August,Yes,Yes\n11,Comedy,5,35,5,December,No,No\n12,Drama,7,55,7,MARCH,No,No\n13,Action,2,130,8,July,Yes,Yes\n14,Comedy,1,20,4,november,No,No\n15,Drama,9,80,9,May,No,Yes", "model_steps": ["Load the CSV data into a dataframe", "Fix inconsistent capitalization in 'ReleaseMonth' column", "Impute missing values in 'DirectorExperience' and 'LeadActorPopularity' using median", "Convert 'HasSequel' and 'BoxOfficeHit' to binary variables", "One-hot encode 'Genre' and 'ReleaseMonth' categorical variables", "Split data into training (80%) and test sets (20%)", "Standardize numeric features: 'DirectorExperience', 'BudgetMillion', 'LeadActorPopularity'", "Train a RandomForestClassifier to predict 'BoxOfficeHit'", "Perform grid search over number of trees (n_estimators) and max_depth", "Evaluate model on test set with accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.9, "f1": 0.87, "confusion_matrix": [[6, 1], [2, 6]], "top_feature_importances": {"BudgetMillion": 0.32, "HasSequel": 0.25, "LeadActorPopularity": 0.18}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Build a classification model to predict whether a given crop field is likely to experience pest infestation during the growing season.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Avg_Temp_C,Previous_Crop,Pesticide_Used,Infestation\n1,Loamy,250,24.5,Wheat,Yes,No\n2,clay,180,22.1,Corn,no,Yes\n3,Sandy,,27.0,Rice,Yes,No\n4,Loamy,300,25.3,Wheat,,No\n5,Clay,210,23.4,Rice,Yes,Yes\n6,Sandy,190,26.8,Corn,No,No\n7,Loamy,220,24.9,Wheat,Yes,Yes\n8,sandy,260,27.1,Rice,No,No\n9,Clay,205,23.0,Corn,yes,Yes\n10,Loamy,NaN,25.7,Wheat,No,No\n11,Sandy,230,26.0,Rice,No,No\n12,Clay,200,22.5,Corn,No,Yes", "model_steps": ["Inspect data and identify missing values and inconsistent capitalization in categorical columns", "Fill missing numeric values (Rainfall_mm) using median rainfall of the dataset", "Normalize inconsistent capitalization in Soil_Type and Pesticide_Used columns", "Encode categorical variables Soil_Type, Previous_Crop, and Pesticide_Used using one-hot encoding", "Split dataset into training (80%) and testing (20%) sets with stratification on target variable Infestation", "Standardize numeric features Rainfall_mm and Avg_Temp_C using StandardScaler fitted on training data", "Train a RandomForestClassifier with 100 trees on the training set", "Tune hyperparameter max_depth using 5-fold cross-validation over [5, 10, 15]", "Evaluate model performance on the test set calculating accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.83, "precision": 0.8, "recall": 0.86, "f1": 0.83, "confusion_matrix": {"true_positive": 6, "true_negative": 8, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Pesticide_Used_Yes": 0.32, "Soil_Type_Clay": 0.21, "Rainfall_mm": 0.18}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict the likelihood of extreme heat days in a region based on weather and geographic features.", "raw_table": "Region,Avg_Temp_C,Humidity_Percent,Land_Use,Elevation_m,Prev_Year_Heat_Days,Extreme_Heat_Day\nNorth,29.5,65,Urban,200,5,Yes\nSouth,33.1,70,Rural,150,8,yes\nEast,28.2,NaN,Urban,100,6,No\nWest,31.0,60,Industrial,250,7,Yes\ncentral,30.5,68,Urban,300,7,Yes\nNorth,27.0,62,Rural,180,4,no\nSouth,NaN,75,Rural,140,9,Yes\nEast,29.9,63,Urban,110,5,No\nWest,32.8,59,INDUSTRIAL,260,8,Yes\nCentral,30.3,67,Urban,310,6,Yes\nNorth,28.8,,Rural,190,5,No\nSouth,33.0,72,Rural,145,8,Yes\nEast,28.5,64,urban,120,6,No\nWest,31.5,61,Industrial,255,7,Yes", "model_steps": ["Load data and identify target variable as Extreme_Heat_Day", "Handle missing numeric values in Humidity_Percent and Avg_Temp_C by imputing median values", "Standardize numeric features: Avg_Temp_C, Humidity_Percent, Elevation_m, Prev_Year_Heat_Days", "Normalize inconsistent capitalization in categorical fields Region and Land_Use", "One-hot encode categorical variables Region and Land_Use", "Convert target variable Extreme_Heat_Day to binary label (Yes=1, No=0)", "Split dataset into training (80%) and testing (20%) sets with stratification on target", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search over max_depth parameters [5, 10, 15]", "Evaluate model on test set computing accuracy, precision, recall, and F1 score", "Extract feature importances from the Random Forest model", "Generate predicted probabilities for the test set"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "top_feature_importances": {"Prev_Year_Heat_Days": 0.32, "Avg_Temp_C": 0.25, "Land_Use_Urban": 0.15, "Humidity_Percent": 0.12, "Elevation_m": 0.1, "Region_South": 0.06}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Build a classification model to predict whether a taxi trip will exceed 20 minutes based on trip and passenger details.", "raw_table": "trip_id,passenger_count,pickup_location,dropoff_location,trip_distance,trip_start_hour,weather_condition,exceeded_20min\n1,2,Downtown,Airport,8.5,14,Clear,Yes\n2,1,Suburb,downtown,4.2,09,rain,No\n3,3,Airport,Suburb,10.1,20,Clear,Yes\n4,,Downtown,Suburb,7.5,18,Fog,Yes\n5,2,Suburb,Downtown,3.8,07,clear,No\n6,1,Downtown,Airport,9.0,22,Snow,Yes\n7,2,Airport,Downtown,8.7,15,Snow,Yes\n8,4,Suburb,Airport,6.3,11,Clear,No\n9,1,Downtown,Suburb,5.5,13,,No\n10,2,Suburb,Downtown,4.0,19,Fog,Yes\n11,3,Airport,Suburb,9.8,08,Rain,Yes\n12,1,Downtown,Airport,7.2,16,Clear,Yes\n13,2,Suburb,Downtown,3.6,21,Rain,No\n14,1,Downtown,Suburb,6.8,10,Fog,No", "model_steps": ["Load data and identify missing values in passenger_count and weather_condition columns", "Impute missing passenger_count with median value", "Impute missing weather_condition with mode value", "Standardize capitalization of categorical values in pickup_location, dropoff_location, and weather_condition", "One-hot encode categorical variables: pickup_location, dropoff_location, weather_condition", "Split data into 80% train and 20% test sets with stratification on exceeded_20min", "Train a RandomForestClassifier to predict exceeded_20min", "Perform grid search on number of trees (n_estimators) and max_depth hyperparameters using 5-fold cross-validation", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Generate and display confusion matrix"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.81, "recall": 0.88, "confusion_matrix": {"true_positive": 7, "false_positive": 2, "true_negative": 8, "false_negative": 1}, "top_feature_importances": {"trip_distance": 0.32, "trip_start_hour": 0.21, "pickup_location_Airport": 0.15, "weather_condition_Rain": 0.1, "passenger_count": 0.07}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Predict whether a citizen will default on a government loan based on demographic and financial attributes.", "raw_table": "Citizen_ID,Age,Income,Employment_Status,Loan_Amount,Marital_Status,Previous_Default,Defaulted\n1,34,54000,Employed,15000,Married,No,No\n2,58,72000,Self-Employed,25000,Single,Yes,Yes\n3,45,48000,employed,13000,Married,,No\n4,23,NaN,Unemployed,5000,Single,No,No\n5,39,61000,Employed,20000,Divorced,No,No\n6,52,85000,Self-employed,27000,Married,No,Yes\n7,29,42000,Employed,9000,Single,No,No\n8,47,73000,Employed,22000,Widowed,Yes,Yes\n9,33,58000,Unemployed,11000,SINGLE,No,No\n10,41,NaN,Employed,17000,Married,No,No\n11,36,49000,Employed,14000,Divorced,No,No\n12,55,79000,Self-Employed,26000,Married,Yes,Yes\n13,30,51000,EMPLOYED,12000,Single,No,No\n14,44,67000,Employed,21000,Married,No,Yes\n15,38,60000,Employed,18000,Single,No,No", "model_steps": ["Load the CSV data into a DataFrame", "Identify and fix inconsistent capitalization in Employment_Status and Marital_Status columns", "Impute missing Income values using median income", "Convert categorical variables Employment_Status, Marital_Status, and Previous_Default into one-hot encoded features", "Encode the target variable Defaulted as binary (Yes=1, No=0)", "Split data into train and test sets with 80% training and 20% testing", "Standardize numeric features: Age, Income, and Loan_Amount", "Train a RandomForestClassifier on the training data", "Perform grid search to optimize max_depth parameter between 3 and 7", "Evaluate the model on the test set calculating accuracy, F1 score, precision, and recall", "Generate a confusion matrix to understand classification errors"], "model_results": {"accuracy": 0.86, "f1": 0.83, "precision": 0.81, "recall": 0.85, "confusion_matrix": {"true_positive": 6, "false_positive": 2, "true_negative": 7, "false_negative": 1}, "top_feature_importances": {"Previous_Default_Yes": 0.32, "Loan_Amount": 0.2, "Employment_Status_Self-Employed": 0.15, "Income": 0.12, "Age": 0.1}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a movie will be a box office hit based on production and genre features.", "raw_table": "MovieID,Genre,DirectorExperience,ProductionBudgetMillions,LeadActorPopularity,ReleaseSeason,BoxOfficeHit\n1,Action,5,150,High,Summer,Yes\n2,Comedy,2,40,Medium,Winter,No\n3,Drama,,70,Low,Spring,No\n4,Action,7,200,HIGH,Fall,Yes\n5,Comedy,3,35,Medium,Summer,No\n6,Drama,4,80,Low,Winter,No\n7,Action,8,180,Very High,Summer,Yes\n8,Comedy,1,50,,Spring,No\n9,Drama,5,90,Low,Fall,No\n10,Action,6,160,High,Summer,Yes\n11,Comedy,2,45,Medium,Winter,No\n12,Drama,3,,Medium,Spring,No\n13,Action,7,170,High,Summer,YES\n14,Comedy,4,60,Medium,Fall,No", "model_steps": ["Load the CSV dataset into a DataFrame", "Identify and impute missing numeric values in DirectorExperience and ProductionBudgetMillions columns using median", "Normalize capitalization inconsistencies in LeadActorPopularity and BoxOfficeHit columns (e.g., 'HIGH' to 'High', 'YES' to 'Yes')", "Encode categorical features: Genre, LeadActorPopularity, ReleaseSeason, and target BoxOfficeHit using label encoding or one-hot encoding as appropriate", "Split data into training (80%) and test (20%) sets with stratification on BoxOfficeHit", "Standardize numeric features: DirectorExperience and ProductionBudgetMillions", "Train a RandomForestClassifier to predict BoxOfficeHit", "Perform grid search to tune max_depth and n_estimators parameters", "Evaluate model performance using accuracy, F1 score, precision, and recall on the test set", "Generate and analyze the confusion matrix", "Identify and report top feature importances from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.88, "recall": 0.79, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 1, "false_negative": 2}, "top_feature_importances": {"ProductionBudgetMillions": 0.42, "LeadActorPopularity": 0.3, "DirectorExperience": 0.15, "Genre_Action": 0.08, "ReleaseSeason_Summer": 0.05}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict extreme heat event occurrence based on atmospheric and land conditions to assist climate risk management.", "raw_table": "Day,Temperature_C,Humidity_Percent,Wind_Speed_kmh,Land_Cover_Type,Region,Extreme_Heat_Event\n1,35.2,45.0,12.3,Forest,North,Yes\n2,38.5,40.2,15.0,Urban,South,yes\n3,30.0,55.5,8.1,agriculture,East,No\n4,INVALID,50.0,10.0,Urban,West,No\n5,33.1,,20.5,Forest,North,No\n6,36.7,48.6,18.2,Urban,South,Yes\n7,34.0,47.5,13.3,Forest,East,No\n8,39.0,42.0,17.8,Urban,West,Yes\n9,31.5,50.0,9.9,Agriculture,North,No\n10,37.8,44.0,16.1,forest,South,yes\n11,32.0,53.0,11.0,Urban,East,No\n12,35.5,46.5,14.0,Forest,West,Yes\n13,29.9,60.0,7.5,Urban,North,No\n14,38.2,41.0,19.0,Agriculture,South,Yes", "model_steps": ["Load dataset from CSV string and parse columns", "Identify and correct inconsistent capitalization in 'Land_Cover_Type' and 'Extreme_Heat_Event' columns", "Handle missing values by imputing numeric columns with median and dropping rows with invalid numeric entries", "Convert 'Extreme_Heat_Event' target variable to binary encoding (Yes=1, No=0)", "Split data into training (80%) and test (20%) sets", "One-hot encode categorical variables 'Land_Cover_Type' and 'Region'", "Standardize numeric features: Temperature_C, Humidity_Percent, Wind_Speed_kmh", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search over max_depth parameter with values [5, 10, 15]", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Extract top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.81, "recall": 0.88, "top_feature_importances": {"Temperature_C": 0.4, "Wind_Speed_kmh": 0.25, "Land_Cover_Type_Urban": 0.15}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a citizen will complete their tax filing on time based on demographic and prior behavior data.", "raw_table": "Age,Income,Employment_Status,Prior_Filing_Compliance,Region,Marital_Status,Tax_Filing_Completed\n34,55000,Employed,Yes,North,Single,Yes\n45,72000,Self-employed,No,South,Married,No\n27,45000,employed,yes,East,Single,Yes\n,62000,Unemployed,No,west,Married,No\n52,NaN,Employed,No,North,Divorced,No\n39,58000,Employed,Yes,East,Single,Yes\n41,64000,Self-employed,Yes,South,Married,Yes\n29,47000,unemployed,no,North,Single,No\n36,53000,Employed,YES,East,Married,Yes\n48,70000,Self-employed,No,South,Divorced,No\n33,51000,Employed,Yes,North,,Yes\n57,80000,Self-employed,No,South,Married,No\n31,48000,Employed,Yes,East,Single,Yes\n44,68000,Employed,No,West,Married,No", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Standardize capitalization in categorical columns (e.g., Employment_Status, Prior_Filing_Compliance, Region)", "Impute missing Age and Income values using median values", "Fill missing Marital_Status entries with the mode", "Encode categorical variables using one-hot encoding", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features (Age, Income)", "Train a Logistic Regression classifier to predict Tax_Filing_Completed", "Perform 5-fold cross-validation to tune regularization strength", "Evaluate the model on test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.89, "f1": 0.86, "confusion_matrix": {"true_positive": 38, "true_negative": 22, "false_positive": 5, "false_negative": 4}, "top_feature_importances": {"Prior_Filing_Compliance_Yes": 0.45, "Employment_Status_Employed": 0.22, "Income": 0.15, "Region_North": 0.1, "Marital_Status_Married": 0.08}, "best_hyperparameter_C": 1.0}}
{"purpose": "Build a regression model to estimate house sale prices based on property features and location.", "raw_table": "property_id,num_bedrooms,num_bathrooms,sqft_living,neighborhood,year_built,has_garage,condition,price\n1,3,2,1500,Downtown,1998,Yes,Good,350000\n2,2,,1200,Uptown,2005,No,FAIR,280000\n3,4,3,2500,suburb,2010,yes,Excellent,450000\n4,3,2,1800,Suburb,1995,No,Fair,365000\n5,5,4,3200,Downtown,2018,YES,GOOD,620000\n6,3,2,NaN,Uptown,2000,No,good,410000\n7,2,1,1100,Downtown,1980,No,Poor,200000\n8,4,3,2400,Suburb,2015,Yes,Excellent,480000\n9,3,2,1700,UPTOWN,1999,No,Fair,360000\n10,3,2,1600,Downtown,2003,Yes,Good,390000\n11,3,2,1500,Suburb,1997,no,Poor,340000\n12,4,3,2600,uptown,2012,Yes,Excellent,470000\n13,2,1,1300,Downtown,1985,No,GOOD,310000\n14,5,4,3300,Suburb,2019,YES,Excellent,650000", "model_steps": ["Load CSV data and parse columns, identifying numeric and categorical features.", "Clean data by fixing inconsistent capitalization in 'neighborhood', 'condition', and 'has_garage' columns.", "Impute missing 'num_bathrooms' and 'sqft_living' values with median values.", "Encode categorical variables ('neighborhood', 'condition', 'has_garage') using one-hot encoding.", "Split the dataset into training (80%) and test (20%) sets randomly.", "Standardize numeric features: 'num_bedrooms', 'num_bathrooms', 'sqft_living', 'year_built'.", "Train a RandomForestRegressor model on the training data to predict 'price'.", "Perform grid search over 'max_depth' parameter with values [5, 10, 15] to optimize performance.", "Evaluate the final model on the test set using RMSE, MAE, and R2 metrics.", "Extract and report the top 3 feature importances from the trained model."], "model_results": {"rmse": 24000, "mae": 18000, "r2": 0.85, "top_feature_importances": {"sqft_living": 0.42, "neighborhood_Suburb": 0.18, "num_bedrooms": 0.12}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict if a manufactured part will pass quality inspection based on sensor readings and production conditions.", "raw_table": "PartID,Temperature,Pressure,OperatorShift,MaterialBatch,DefectDetected\n1,85.2,30.1,day,BatchA,No\n2,90.0,29.8,Night,batchB,yes\n3,87.5,missing,Day,BatchA,No\n4,88.0,30.3,day,BatchC,No\n5,89.1,31.0,NIGHT,BatchB,Yes\n6,85.7,30.2,day,BatchA,No\n7,missing,29.9,Day,BatchC,No\n8,91.3,30.5,night,BatchC,Yes\n9,88.8,30.0,Night,BatchA,No\n10,90.5,missing,DAY,BatchB,Yes\n11,86.9,30.1,day,BatchB,No\n12,87.2,30.4,Night,BatchC,No\n13,88.3,29.7,Day,BatchA,No\n14,missing,30.3,day,batchB,Yes\n15,89.9,30.8,Night,BatchC,Yes", "model_steps": ["Impute missing numeric values in Temperature and Pressure columns using median values", "Standardize numeric features Temperature and Pressure", "Normalize inconsistent capitalization in OperatorShift and MaterialBatch columns (e.g., convert all to lowercase)", "Encode categorical variables OperatorShift and MaterialBatch using one-hot encoding", "Convert target variable DefectDetected to binary labels (Yes=1, No=0)", "Split data into train and test sets with 80% training data and 20% testing data", "Train a RandomForestClassifier on the training data", "Perform grid search to tune max_depth parameter with values [3, 5, 7]", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix for test set predictions", "Identify and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.87, "precision": 0.83, "recall": 0.79, "f1": 0.81, "confusion_matrix": {"true_positive": 11, "true_negative": 20, "false_positive": 3, "false_negative": 4}, "top_feature_importances": {"Pressure": 0.32, "Temperature": 0.28, "OperatorShift_night": 0.15}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict if a customer will make a purchase during a website visit based on session and demographic data.", "raw_table": "session_id,customer_age,device_type,browser,country,pages_visited,time_on_site_seconds,referral_source,purchase_made\n1,34,Mobile,Chrome,USA,5,300,Social,Yes\n2,27,Desktop,firefox,Canada,3,150,Direct,No\n3,45,Tablet,Safari,UK,8,420,Email,Yes\n4,NaN,Mobile,Chrome,usa,2,90,Social,No\n5,22,desktop,Edge,Germany,4,200,Direct,No\n6,39,Mobile,Chrome,USA,7,350,Social,Yes\n7,31,Tablet,SAFARI,Canada,5,280,Email,No\n8,50,Desktop,Firefox,UK,9,480,Direct,Yes\n9,28,Mobile,chrome,Germany,3,120,Social,No\n10,37,Desktop,Edge,USA,6,330,Email,Yes\n11,40,Mobile,Chrome,Canada,5,310,Direct,No\n12,26,Tablet,Safari,UK,NaN,400,Email,Yes\n13,35,desktop,Chrome,USA,7,360,Social,Yes\n14,29,Mobile,Firefox,USA,4,190,Direct,No", "model_steps": ["Load the dataset and identify the target variable as 'purchase_made'.", "Clean the 'country' and 'device_type' columns to fix inconsistent capitalization.", "Fill missing numeric values in 'customer_age' and 'pages_visited' with the median.", "Convert 'purchase_made' from Yes/No to binary 1/0.", "One-hot encode the categorical features: device_type, browser, country, referral_source.", "Scale numeric features 'customer_age', 'pages_visited', and 'time_on_site_seconds' using standardization.", "Split the data into train and test sets with an 80/20 ratio, stratified by the target variable.", "Train a RandomForestClassifier with default parameters on the training set.", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score.", "Extract and rank feature importances from the trained RandomForest model."], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.78, "f1": 0.8, "top_feature_importances": {"time_on_site_seconds": 0.24, "pages_visited": 0.19, "device_type_Mobile": 0.15, "referral_source_Social": 0.12, "browser_Chrome": 0.1}}}
{"purpose": "Build a classification model to predict the likelihood that a citizen will comply with tax filing deadlines.", "raw_table": "CitizenID,Age,Income,EmploymentStatus,PreviousCompliance,State,TaxFilingCompleted\n001,45,55000,Full-time,Yes,CA,Yes\n002,32,NaN,Self-employed,No,tx,No\n003,28,43000,PART-TIME,Yes,NY,Yes\n004,54,72000,Full-time,Yes,CA,Yes\n005,NaN,31000,Unemployed,No,FL,No\n006,40,NaN,Full-time,No,TX,No\n007,23,27000,Part-Time,Yes,NY,Yes\n008,38,NaN,Full-time,No,ca,No\n009,31,49000,Self-Employed,No,FL,No\n010,47,67000,Full-time,Yes,NY,Yes\n011,29,35000,Unemployed,No,FL,No\n012,36,46000,Full-time,Yes,CA,Yes\n013,41,58000,Self-employed,Yes,TX,Yes\n014,39,52000,Full-time,Yes,ny,Yes", "model_steps": ["Load the dataset and identify missing and inconsistent values", "Standardize categorical variables by normalizing capitalization (e.g., 'Full-time', 'full-time', 'Full-Time' all to 'Full-time')", "Impute missing numeric values (Age, Income) using median imputation", "Encode categorical features using one-hot encoding (EmploymentStatus, State, PreviousCompliance)", "Split the dataset into training and test sets with an 80/20 ratio", "Train a RandomForestClassifier to predict TaxFilingCompleted", "Perform grid search cross-validation over number of trees (n_estimators) and maximum depth (max_depth)", "Evaluate model performance using accuracy, precision, recall, and F1 score on the test set", "Generate the confusion matrix and identify top 3 feature importances", "Save the trained model and preprocessing pipeline for deployment"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.855, "confusion_matrix": {"true_positive": 9, "true_negative": 5, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"PreviousCompliance_Yes": 0.34, "Income": 0.22, "EmploymentStatus_Full-time": 0.18}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Predict whether a movie will be a box office hit based on production and genre features.", "raw_table": "MovieID,Genre,DirectorExperienceYears,BudgetMillions,LeadActorFame,ReleaseMonth,RuntimeMinutes,BoxOfficeHit\n1,Action,10,150,High,July,130,Yes\n2,romance,5,30,Medium,February,95,No\n3,Comedy,,20,Low,december,105,No\n4,Drama,12,50,High,july,120,Yes\n5,Horror,3,15,Medium,October,,No\n6,Action,8,200,High,July,140,Yes\n7,Comedy,7,25,low,May,100,No\n8,Drama,15,60,High,August,125,Yes\n9,Action,20,180,High,july,135,Yes\n10,Romance,6,35,Medium,February,90,No\n11,Horror,2,10,Low,October,85,No\n12,Comedy,5,22,Medium,May,110,No\n13,Drama,11,55,High,August,130,Yes\n14,Action,9,170,High,July,138,Yes", "model_steps": ["Clean 'Genre' and 'LeadActorFame' columns by standardizing capitalization", "Fill missing values in 'DirectorExperienceYears' and 'RuntimeMinutes' with median values", "One-hot encode 'Genre' and 'ReleaseMonth' categorical variables", "Encode 'LeadActorFame' ordinally as Low=0, Medium=1, High=2", "Convert target variable 'BoxOfficeHit' to binary (Yes=1, No=0)", "Split data into train and test sets (80/20)", "Standardize numeric features: 'DirectorExperienceYears', 'BudgetMillions', 'RuntimeMinutes', 'LeadActorFame'", "Train a RandomForestClassifier with 100 trees", "Perform grid search over max_depth values [3, 5, 7]", "Evaluate model accuracy, precision, recall, and F1 score on test set", "Generate confusion matrix for test predictions"], "model_results": {"accuracy": 0.86, "precision": 0.89, "recall": 0.83, "f1": 0.86, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 2}, "top_feature_importances": {"BudgetMillions": 0.32, "LeadActorFame": 0.25, "Genre_Action": 0.15, "DirectorExperienceYears": 0.12, "ReleaseMonth_July": 0.08}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Build a regression model to estimate daily average surface temperature based on atmospheric and geographic variables.", "raw_table": "station_id,region,altitude_m,humidity_pct,wind_speed_kmh,cloud_cover,precipitation_mm,temp_celsius\nS01,North,350,82,15,Overcast,2.5,18.3\nS02,South,120,75,20,Clear,0.0,22.1\nS03,East,500,missing,5,Partly cloudy,1.2,16.5\nS04,West,230,80,15,overcast,3.1,19.0\nS05,North,400,85,18,Clear,0.0,17.8\nS06,South,200,78,22,Partly Cloudy,0.5,21.0\nS07,East,600,88,10,clear,0.0,15.7\nS08,West,100,74,20,Overcast,2.0,20.5\nS09,North,450,90,12,Overcast,,17.2\nS10,South,180,77,25,PARTLY CLOUDY,0.0,22.5\nS11,East,520,83,8,Clear,0,16.2\nS12,West,210,nan,17,Clear,0.0,19.8\nS13,North,370,84,14,overcast,3.0,18.0", "model_steps": ["Load CSV data and identify target variable as temp_celsius", "Handle missing and inconsistent values: impute missing humidity_pct with median, standardize cloud_cover labels to consistent capitalization", "Convert cloud_cover categorical variable into one-hot encoded features", "Split data into training (80%) and testing (20%) sets randomly", "Standardize numeric features: altitude_m, humidity_pct, wind_speed_kmh, precipitation_mm", "Train a Gradient Boosting Regressor model", "Perform hyperparameter tuning over learning_rate and n_estimators using 5-fold cross-validation", "Evaluate model performance on test set using RMSE, MAE, and R2 score", "Analyze feature importances from the trained model"], "model_results": {"rmse": 1.15, "mae": 0.85, "r2": 0.87, "best_hyperparameters": {"learning_rate": 0.1, "n_estimators": 100}, "top_feature_importances": {"humidity_pct": 0.32, "altitude_m": 0.28, "cloud_cover_Clear": 0.15, "cloud_cover_Overcast": 0.13, "wind_speed_kmh": 0.07, "precipitation_mm": 0.05}}}
{"purpose": "Predict whether a student will pass the final exam based on study habits and demographics.", "raw_table": "StudentID,Hours_Studied,Attendance_Rate,Previous_Grade,Study_Mode,Parental_Education,Passed_Final\n1,12,0.95,B,online,College,Yes\n2,8,,C,InPerson,High School,No\n3,15,0.87,B+,Online,College,Yes\n4,5,0.60,D,online,high school,No\n5,14,0.98,A-,InPerson,Graduate,Yes\n6,7,0.75,C,Inperson,College,No\n7,10,0.80,B,Online,High school,Yes\n8,NaN,0.90,B,online,College,Yes\n9,9,0.85,C,InPerson,College,No\n10,11,0.92,B+,inperson,Graduate,Yes\n11,4,0.65,D+,Online,High school,No\n12,13,0.99,A,Online,Graduate,Yes\n13,6,0.70,C-,InPerson,high School,No\n14,16,0.96,A,online,College,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values in Hours_Studied and Attendance_Rate by imputing with median values", "Normalize categorical columns 'Study_Mode' and 'Parental_Education' to consistent capitalization", "Convert categorical variables 'Study_Mode' and 'Parental_Education' to one-hot encoded features", "Encode target variable 'Passed_Final' as binary (Yes=1, No=0)", "Split data into 80% training and 20% testing sets with stratification on the target", "Standardize numeric features 'Hours_Studied' and 'Attendance_Rate' using training set statistics", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search to tune max_depth parameter over values [3, 5, 7]", "Evaluate model on the test set computing accuracy, F1 score, precision, and recall", "Generate and analyze the confusion matrix", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.8, "recall": 0.87, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Hours_Studied": 0.35, "Attendance_Rate": 0.28, "Study_Mode_Online": 0.15}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a student will pass or fail the final exam based on demographic and study-related features.", "raw_table": "StudentID,Age,Gender,StudyHours,AttendanceRate,PreviousGrade,ParentalEducation,FinalResult\n1,17,Male,15,0.95,88,Bachelor,Pass\n2,18,Female,8,0.80,76,High School,fail\n3,17,Male,NaN,0.60,65,unknown,Fail\n4,19,Female,12,0.90,NaN,Bachelor,Pass\n5,18,Female,7,0.70,70,Master,Fail\n6,17,male,20,0.98,92,Bachelor,Pass\n7,18,Female,5,0.50,55,High school,Fail\n8,19,Male,11,,78,Bachelor,Pass\n9,18,Female,NaN,0.85,80,Master,Pass\n10,17,Male,9,0.88,72,High School,fail\n11,18,Female,16,0.95,90,Bachelor,Pass\n12,19,Male,14,0.92,85,bachelor,Pass\n13,17,Female,6,0.65,67,Master,Fail\n14,18,Male,10,0.75,NaN,High School,Fail", "model_steps": ["Load the data and identify target and feature columns", "Handle missing values by imputing StudyHours and AttendanceRate with median values and PreviousGrade with mean", "Normalize inconsistent capitalization in 'Gender' and 'ParentalEducation' columns", "One-hot encode the categorical variables Gender and ParentalEducation", "Split data into train (80%) and test (20%) sets with stratification on FinalResult", "Standardize numeric features: Age, StudyHours, AttendanceRate, PreviousGrade", "Train a RandomForestClassifier on the training set", "Perform hyperparameter tuning with grid search over number of trees (n_estimators) and max_depth", "Evaluate model on the test set calculating accuracy, F1 score, precision, and recall", "Generate confusion matrix and identify top 3 important features"], "model_results": {"accuracy": 0.86, "f1": 0.85, "precision": 0.88, "recall": 0.83, "confusion_matrix": {"TP": 6, "FP": 1, "FN": 2, "TN": 5}, "top_features": {"StudyHours": 0.35, "AttendanceRate": 0.27, "PreviousGrade": 0.22}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a government grant application will be approved based on applicant and project details.", "raw_table": "ApplicationID,ApplicantAge,ApplicantIncome,ProjectType,RequestedAmount,State,PreviousGrants,Approved\n001,45,55000,Infrastructure,100000,CA,2,Yes\n002,38,48000,Education,75000,ny,1,No\n003,50,62000,Healthcare,120000,TX,missing,Yes\n004,29,35000,Infrastructure,85000,CA,0,No\n005,40,NaN,Education,70000,FL,1,Yes\n006,33,47000,Healthcare,110000,TX,2,No\n007,55,68000,INFRASTRUCTURE,95000,CA,3,Yes\n008,47,53000,Education,missing,fl,0,No\n009,44,60000,Healthcare,115000,TX,1,Yes\n010,36,49000,Education,80000,NY,null,No\n011,30,40000,Infrastructure,87000,CA,1,No\n012,50,61000,Healthcare,125000,tx,2,Yes\n013,42,58000,Education,78000,FL,1,No\n014,39,52000,Infrastructure,90000,CA,missing,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Normalize 'State' and 'ProjectType' columns to consistent capitalization", "Impute missing numeric values in 'ApplicantIncome' and 'RequestedAmount' with median values", "Impute missing categorical values in 'PreviousGrants' with mode and convert to numeric", "Encode 'ProjectType' and 'State' categorical variables using one-hot encoding", "Convert target column 'Approved' to binary labels (Yes=1, No=0)", "Split data into training (80%) and testing (20%) sets", "Standardize numeric features: 'ApplicantAge', 'ApplicantIncome', 'RequestedAmount', 'PreviousGrants'", "Train a RandomForestClassifier with default parameters on the training set", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Compute and display the confusion matrix", "Identify and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.79, "precision": 0.82, "recall": 0.75, "f1": 0.78, "confusion_matrix": [[7, 2], [3, 8]], "top_feature_importances": {"RequestedAmount": 0.31, "PreviousGrants": 0.24, "ProjectType_Healthcare": 0.15}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a loan application will be approved based on applicant financial and demographic data.", "raw_table": "ApplicantID,Income,EmploymentStatus,CreditScore,LoanAmount,LoanPurpose,Approved\n1,55000,Full-time,720,15000,Home Improvement,Yes\n2,43000,part-time,680,8000,Debt Consolidation,no\n3,NaN,Unemployed,610,5000,Medical,No\n4,72000,Full-time,740,20000,Home Improvement,Yes\n5,29000,Full-Time,,7000,education,No\n6,61000,Full-time,690,13000,Debt consolidation,Yes\n7,48000,Contract,650,9000,Home Improvement,No\n8,52000,full-Time,700,11000,Medical,Yes\n9,40000,Part-Time,620,6000,education,No\n10,67000,Full-time,710,14000,Debt Consolidation,Yes\n11,36000,Unemployed,580,4000,Medical,No\n12,59000,Full-time,690,12000,Home Improvement,Yes\n13,NaN,Contract,640,7000,Debt Consolidation,no", "model_steps": ["Load the CSV data into a DataFrame", "Standardize capitalization in EmploymentStatus and LoanPurpose columns", "Impute missing Income and CreditScore values using median imputation", "Convert Approved target variable to binary (Yes=1, No=0)", "One-hot encode EmploymentStatus and LoanPurpose categorical variables", "Split data into training (80%) and test (20%) sets", "Standardize numeric features: Income, CreditScore, LoanAmount", "Train a RandomForestClassifier on the training set", "Perform grid search tuning over max_depth parameter with 5-fold cross-validation", "Evaluate model on test set computing accuracy, F1 score, precision, and recall", "Compute and display the confusion matrix", "Identify top 3 most important features from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.8, "recall": 0.86, "confusion_matrix": {"true_positive": 14, "true_negative": 12, "false_positive": 3, "false_negative": 2}, "top_feature_importances": {"CreditScore": 0.35, "Income": 0.28, "LoanAmount": 0.15}, "best_hyperparameters": {"max_depth": 6}}}
{"purpose": "Predict whether a student will pass or fail the final exam based on their demographic and academic performance data.", "raw_table": "StudentID,Age,Gender,AttendanceRate,PreviousGrade,StudyHoursPerWeek,ParentalEducation,PassedFinal\n1,17,M,0.95,A,10,High School,Pass\n2,18,F,0.80,B,8,College,Fail\n3,17,m,0.90,A,12,College,Pass\n4,16,F,missing,C,5,high school,Fail\n5,19,M,0.85,B,7,College,Pass\n6,18,F,0.60,C,3,College,Fail\n7,17,M,0.75,B,9,HS,Pass\n8,16,F,0.88,A,11,College,Pass\n9,17,M,0.70,C,missing,High school,Fail\n10,18,F,0.92,B,10,College,Pass\n11,16,F,0.65,D,4,College,Fail\n12,19,M,0.78,B,6,high school,Pass\n13,17,F,0.83,A,9,College,Pass\n14,18,M,0.90,b,8,College,Pass", "model_steps": ["Load data from CSV and inspect for missing or inconsistent values", "Impute missing AttendanceRate values using median attendance", "Impute missing StudyHoursPerWeek values using median study hours", "Normalize inconsistent capitalization in categorical columns (Gender, ParentalEducation, PreviousGrade)", "Encode categorical variables: Gender (binary), ParentalEducation (ordinal: High School < College), PreviousGrade (ordinal: A > B > C > D)", "Split data into training (80%) and test (20%) sets", "Standardize numeric features: Age, AttendanceRate, StudyHoursPerWeek", "Train a RandomForestClassifier to predict PassedFinal", "Perform grid search to tune max_depth and n_estimators parameters", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix for test set predictions"], "model_results": {"accuracy": 0.86, "f1": 0.88, "precision": 0.9, "recall": 0.85, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"AttendanceRate": 0.32, "PreviousGrade": 0.27, "StudyHoursPerWeek": 0.18, "ParentalEducation": 0.14, "Age": 0.09}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict customer churn probability for a telecom provider based on customer usage and demographics.", "raw_table": "CustomerID,MonthlyCharges,Contract,InternetService,TechSupport,TenureMonths,PaymentMethod,Churn\n001,70.5,Month-to-month,Fiber optic,Yes,5,Electronic check,Yes\n002,89.3,One year,Fiber optic,No,12,Mailed Check,No\n003,29.9,Month-to-month,DSL,No,,Bank transfer, Yes\n004,53.2,Two year,DSL,No,24,Credit Card,No\n005,99.0,Month-to-month,Fiber optic,No,3,electronic check,YES\n006,60.7,One year,DSL,Yes,18,Electronic Check,No\n007,40.1,Month-to-month,none,No,2,Mailed Check,No\n008,85.6,Month-to-month,Fiber optic,No,6,Electronic check,Yes\n009,75.3,Two year,Fiber optic,Yes,36,Credit Card,No\n010,45.0,Month-to-month,DSL,No,8,Electronic check,Yes\n011,25.8,One year,DSL,No,15,Bank transfer,No\n012,88.9,Month-to-month,Fiber optic,No,4,Electronic check,YES\n013,35.5,Month-to-month,DSL,No,7,Mailed Check,No\n014,52.0,Month-to-month,Fiber optic,Yes,6,Electronic check,Yes\n", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing values and inconsistent entries.", "Correct inconsistent capitalization and trim whitespace in categorical columns (e.g., 'electronic check', 'Electronic Check').", "Impute missing numeric values in TenureMonths using median imputation.", "Encode target variable 'Churn' as binary (Yes=1, No=0).", "Split dataset into train (80%) and test (20%) sets using stratified sampling on the target.", "One-hot encode categorical features: Contract, InternetService, TechSupport, PaymentMethod.", "Standardize numeric features: MonthlyCharges, TenureMonths.", "Train a RandomForestClassifier with 100 trees on the training data.", "Perform grid search for hyperparameter tuning over max_depth values [5, 10, 15].", "Evaluate model on test set by computing accuracy, F1 score, precision, and recall.", "Generate and analyze the confusion matrix to understand false positives and false negatives."], "model_results": {"accuracy": 0.85, "f1": 0.81, "precision": 0.78, "recall": 0.85, "top_feature_importances": {"TenureMonths": 0.28, "Contract_Month-to-month": 0.24, "InternetService_Fiber optic": 0.15, "TechSupport_No": 0.12, "MonthlyCharges": 0.1}, "confusion_matrix": {"true_positive": 22, "true_negative": 30, "false_positive": 6, "false_negative": 4}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a social media post will go viral based on user and post characteristics.", "raw_table": "post_id,user_followers,user_age,post_length,post_type,post_time,hashtags_count,viral\n1,1500,25,120,text,Morning,3,Yes\n2,5000,30,45,Image,Evening,2,No\n3,300,,300,Video,NIGHT,5,Yes\n4,2500,22,80,text,Morning,,No\n5,10000,35,200,video,Afternoon,4,Yes\n6,800,28,60,Text,Evening,1,No\n7,4000,40,150,image,Morning,3,Yes\n8,NaN,27,100,Text,Night,2,No\n9,6000,33,90,IMAGE,Afternoon,3,Yes\n10,1200,26,110,text,Morning,NaN,No\n11,700,23,40,Video,Evening,1,No\n12,3500,29,130,video,Night,4,Yes\n13,4500,31,55,Text,Afternoon,2,No\n14,1000,24,100,image,Morning,3,No\n15,500,22,70,text,NIGHT,1,No", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Normalize capitalization in 'post_type' and 'post_time' columns", "Impute missing numeric values in 'user_age' and 'hashtags_count' with median values", "Convert 'viral' target variable to binary (Yes=1, No=0)", "Split data into training and test sets (80% train, 20% test)", "One-hot encode the categorical features 'post_type' and 'post_time'", "Standardize numeric features: 'user_followers', 'user_age', 'post_length', 'hashtags_count'", "Train a RandomForestClassifier to predict 'viral'", "Tune hyperparameters 'max_depth' and 'n_estimators' using 5-fold cross-validation", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze prediction errors"], "model_results": {"accuracy": 0.87, "precision": 0.83, "recall": 0.79, "f1": 0.81, "best_hyperparameters": {"max_depth": 8, "n_estimators": 100}, "top_feature_importances": {"user_followers": 0.35, "post_length": 0.22, "post_type_video": 0.15, "hashtags_count": 0.12, "post_time_morning": 0.08, "user_age": 0.08}, "confusion_matrix": {"true_positive": 19, "true_negative": 28, "false_positive": 6, "false_negative": 5}}}
{"purpose": "Predict whether a social media post will go viral based on its attributes and posting context.", "raw_table": "post_id,num_words,post_hour,media_type,user_followers,user_verified,post_language,target_viral\n1,120,14,image,1500,Yes,English,Yes\n2,75,9,video,3000,No,english,No\n3,200,20,text,5000,yes,Spanish,Yes\n4,50,15,Image,800,No,French,No\n5,110,23,video,12000,Yes,,Yes\n6,90,12,text,4000,no,German,No\n7,140,8,video,1000,Yes,English,Yes\n8,85,18,text,250,no,english,No\n9,55,14,image,600,No,French,No\n10,170,19,video,9000,Yes,Spanish,Yes\n11,30,11,text,3500,No,english,No\n12,130,21,image,7000,YES,English,Yes\n13,95,13,video,1500,No,english,No\n14,105,22,text,4600,Yes,German,Yes\n15,80,10,image,3000,No,English,No", "model_steps": ["Load the CSV data into a DataFrame", "Identify and clean messy values: standardize 'user_verified' to uppercase Yes/No, fill missing 'post_language' with mode", "Convert categorical variables ('media_type', 'user_verified', 'post_language') to one-hot encoding", "Convert target variable 'target_viral' to binary (Yes=1, No=0)", "Split data into training (80%) and testing (20%) sets, stratified by target", "Scale numeric features 'num_words', 'post_hour', 'user_followers' using StandardScaler", "Train a RandomForestClassifier with 100 trees on the training set", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Extract and report feature importances from the RandomForest model", "Generate confusion matrix and analyze false positives and false negatives"], "model_results": {"accuracy": 0.87, "precision": 0.83, "recall": 0.79, "f1": 0.81, "feature_importances": {"user_followers": 0.32, "media_type_video": 0.25, "num_words": 0.18, "user_verified_Yes": 0.12, "post_language_English": 0.08, "post_hour": 0.05}, "confusion_matrix": {"true_positives": 11, "true_negatives": 15, "false_positives": 3, "false_negatives": 4}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Build a classification model to predict whether a manufactured part will fail quality inspection based on sensor readings and production parameters.", "raw_table": "Part_ID,Machine_ID,Operator_Shift,Temperature_C,Pressure_PSI,Humidity_pct,Material_Batch,Surface_Finish,Part_Failed\n001,M01,Day,75.4,101.2,45.0,BatchA,smooth,No\n002,M02,Night,80.1,102.5,50.2,BatchB,ROUGH,Yes\n003,m01,Day,NaN,100.8,47.5,BatchA,smooth,No\n004,M03,Night,78.3,103.0,missing,BatchC,rough,Yes\n005,M02,Day,74.9,99.9,44.8,BatchB,smooth,No\n006,M01,Night,79.0,NaN,46.1,BatchA,SMOOTH,No\n007,M03,Day,77.5,101.0,48.0,BatchC,rough,Yes\n008,M02,Night,82.0,104.3,51.0,BatchB,rough,Yes\n009,M01,Day,73.8,100.5,,BatchA,rough,No\n010,M03,Night,78.1,102.8,49.2,BatchC,smooth,Yes\n011,M02,Day,75.0,101.5,45.5,BatchB,smooth,No\n012,M01,Night,missing,100.7,46.2,BatchA,rough,No\n013,M03,Day,76.7,102.0,48.7,BatchC,smooth,Yes\n014,M02,Night,81.3,103.5,50.8,BatchB,rough,Yes", "model_steps": ["Load data from CSV and identify missing and inconsistent values", "Standardize categorical variables by lowercasing and trimming whitespace (e.g., 'ROUGH', 'rough', 'SMOOTH', 'smooth')", "Impute missing numeric values using median imputation for Temperature_C, Pressure_PSI, and Humidity_pct", "Encode categorical variables (Machine_ID, Operator_Shift, Material_Batch, Surface_Finish) using one-hot encoding", "Split data into train and test sets with 80/20 ratio, stratifying by Part_Failed label", "Standardize numeric features to zero mean and unit variance", "Train a RandomForestClassifier with default parameters to predict Part_Failed", "Perform grid search cross-validation tuning max_depth and n_estimators hyperparameters", "Evaluate the best model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Temperature_C": 0.32, "Pressure_PSI": 0.27, "Surface_Finish_rough": 0.18}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict if a social media post will go viral based on post attributes and user engagement features.", "raw_table": "post_id,user_type,post_length,has_image,day_posted,followers,avg_likes,target_viral\n1,Influencer,120,Yes,Monday,15000,350,Yes\n2,regular,80,No,Tuesday,2000,15,No\n3,Influencer,200,yes,Friday,23000,480,Yes\n4,Regular,60,No,Wednesday,3000,30,No\n5,INFLUENCER,150,Yes,Sunday,18000,400,Yes\n6,regular,missing,No,Thursday,2500,25,No\n7,Regular,90,No,monday,4000,50,No\n8,Influencer,130,YES,Saturday,21000,420,Yes\n9,regular,70,No,Tuesday,3500,18,No\n10,Influencer,180,Yes,Friday,22000,460,Yes\n11,regular,85,no,Wednesday,2800,22,No\n12,Regular,100,No,Sunday,3200,40,No\n13,Influencer,160,Yes,Friday,19500,390,Yes\n14,regular,95,No,Thursday,2700,28,No", "model_steps": ["Replace missing values in post_length with the median value", "Standardize the post_length and followers numeric columns", "Normalize avg_likes using min-max scaling", "Correct inconsistent capitalization in user_type and has_image columns", "One-hot encode user_type, has_image, and day_posted categorical variables", "Split data into train and test sets (75/25 split)", "Train a RandomForestClassifier to predict target_viral", "Perform grid search over n_estimators and max_depth hyperparameters", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate the confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.93, "f1": 0.91, "precision": 0.89, "recall": 0.93, "confusion_matrix": [[6, 1], [1, 6]], "top_feature_importances": {"avg_likes": 0.35, "followers": 0.25, "has_image_Yes": 0.15, "user_type_Influencer": 0.12, "post_length": 0.08, "day_posted_Friday": 0.05}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 8}}}
{"purpose": "Build a classification model to predict whether wheat crops have high or low yield based on soil and environmental factors.", "raw_table": "Field_ID,Soil_Type,Avg_Temperature_C,Rainfall_mm,Previous_Crop,Crop_Yield_Status\n1,Loam,22.5,130,Corn,High\n2,Clay,18.2,85,Wheat,Low\n3,SAND,25.0,110,Corn,High\n4,loam,20.1,NaN,Soybean,Low\n5,Clay,19.5,95,Corn,Low\n6,Loam,21.3,120,Corn,High\n7,Clay,NaN,100,Soybean,Low\n8,Sand,24.8,115,Corn,High\n9,Loam,23.0,140,Wheat,High\n10,Clay,17.9,88,Soybean,Low\n11,Sand,26,130,Corn,High\n12,Clay,19.2,NaN,Corn,Low\n13,loam,22,125,Corn,High\n14,Loam,21.7,128,Soybean,High", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Standardize capitalization in the Soil_Type categorical column to ensure consistency", "Impute missing numeric values (Avg_Temperature_C, Rainfall_mm) using median of respective columns", "One-hot encode categorical variables: Soil_Type and Previous_Crop", "Split data into training (80%) and testing (20%) sets using stratified sampling on Crop_Yield_Status", "Standardize numeric features Avg_Temperature_C and Rainfall_mm using z-score normalization", "Train a RandomForestClassifier on the training set to predict Crop_Yield_Status", "Perform grid search cross-validation over max_depth and n_estimators parameters", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix and examine top 3 feature importances"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "confusion_matrix": {"True_Positive": 6, "True_Negative": 5, "False_Positive": 1, "False_Negative": 2}, "top_feature_importances": {"Rainfall_mm": 0.32, "Soil_Type_Loam": 0.25, "Avg_Temperature_C": 0.18}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a citizen will comply with a new tax filing deadline based on demographic and filing history.", "raw_table": "CitizenID,Age,Income,EmploymentStatus,PreviousFilingStatus,Region,Compliance\n001,34,55000,Employed,On time,North,Yes\n002,47,NaN,Self-employed,LATE,South,No\n003,29,43000,employed,On Time,East,Yes\n004,52,72000,Unemployed,,West,No\n005,40,62000,Employed,LATE,North,No\n006,NaN,58000,Employed,On time,South,Yes\n007,38,49000,Self-Employed,On time,East,Yes\n008,45,53000,Employed,Late,South,No\n009,33,NaN,Employed,On time,West,Yes\n010,50,67000,Unemployed,LATE,North,No\n011,41,56000,Employed,On time,East,Yes\n012,28,48000,Self-employed,Late,South,No\n013,36,60000,Employed,On Time,West,Yes\n014,39,59000,employed,On time,North,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Clean and standardize categorical variables (e.g., unify 'Employed' capitalization, fix 'Self-Employed' vs 'Self-employed')", "Impute missing numeric values (Age, Income) using median imputation", "Impute missing categorical values (PreviousFilingStatus) with the mode", "Encode categorical variables using one-hot encoding", "Split the data into training (80%) and testing (20%) sets using stratified split on Compliance", "Standardize numeric features (Age, Income) using z-score normalization", "Train a RandomForestClassifier on the training set", "Perform grid search over number of trees (n_estimators) and max_depth", "Evaluate model performance on the test set with accuracy, precision, recall, and F1 score", "Generate a confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"True_Positive": 7, "True_Negative": 6, "False_Positive": 2, "False_Negative": 1}, "top_feature_importances": {"PreviousFilingStatus_On time": 0.31, "Income": 0.25, "EmploymentStatus_Employed": 0.18}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Classify whether a field is suitable for high-yield wheat production based on soil and environmental factors.", "raw_table": "Field_ID,Soil_pH,Soil_Type,Rainfall_mm,Avg_Temp_C,Previous_Crop,Crop_Suitability\n1,6.5,Loamy,400,18,Corn,Yes\n2,7.0,Sandy,350,20,Wheat,No\n3,5.8,Clay,420,17,Rice,Yes\n4,6.9,loamy,390,,Corn,Yes\n5,NaN,Sandy,300,21,Cotton,No\n6,7.2,Clay,450,16,WHEAT,No\n7,6.3,Loamy,410,19,Corn,Yes\n8,5.9,Sandy,380,20,Corn,Yes\n9,6.8,Clay,430,18,Rice,Yes\n10,7.1,Sandy,,22,Corn,No\n11,6.7,Loamy,400,18,Corn,Yes\n12,6.4,Clay,415,17,Wheat,Yes\n13,6.0,Sandy,360,20,Corn,No\n14,6.5,LoAmy,405,19,Rice,Yes", "model_steps": ["Load CSV data into a DataFrame and inspect for missing or inconsistent values", "Standardize capitalization in categorical variables 'Soil_Type' and 'Previous_Crop' to ensure consistency", "Impute missing numeric values in 'Soil_pH', 'Rainfall_mm', and 'Avg_Temp_C' using median values", "Encode categorical features 'Soil_Type' and 'Previous_Crop' with one-hot encoding", "Convert target variable 'Crop_Suitability' to binary labels (Yes=1, No=0)", "Split dataset into training and test sets with an 80/20 ratio", "Standardize numeric features 'Soil_pH', 'Rainfall_mm', and 'Avg_Temp_C' using training set statistics", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search to tune max_depth parameter over values [5, 10, None]", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate and analyze confusion matrix", "Extract and report top 3 feature importances"], "model_results": {"accuracy": 0.86, "precision": 0.89, "recall": 0.83, "f1": 0.86, "confusion_matrix": [[10, 2], [3, 9]], "top_feature_importances": {"Rainfall_mm": 0.32, "Soil_pH": 0.28, "Soil_Type_Loamy": 0.15}, "best_max_depth": 10}}
{"purpose": "Predict hourly energy consumption category (Low, Medium, High) for commercial buildings based on environmental and operational features.", "raw_table": "Hour,Temperature_C,Humidity_Percent,Building_Type,Day_of_Week,Occupancy_Status,Energy_Consumption_Category\n0,22.5,45,Office,Monday,Occupied,Medium\n1,21.0,50,office,Monday,Occupied,Low\n2,19.8,55,Retail,Monday,Unoccupied,LOW\n3,18.7,60,Retail,Tuesday,Unoccupied,Low\n4,17.6,,Warehouse,Tuesday,Occupied,Medium\n5,16.5,65,warehouse,Tuesday,Occupied,Medium\n6,15.7,70,Warehouse,Wednesday,Occupied,High\n7,20.3,48,Office,Wednesday,Occupied,Medium\n8,23.1,44,Office,Wednesday,Occupied,High\n9,25.0,40,Office,Thursday,Occupied,High\n10,26.5,38,Retail,Thursday,Occupied,High\n11,27.1,35,Retail,Thursday,Occupied,High\n12,28.0,33,office,Friday,,High\n13,29.2,30,Warehouse,Friday,Unoccupied,Medium", "model_steps": ["Load dataset and inspect for missing and inconsistent values", "Standardize capitalization in categorical columns such as Building_Type and Energy_Consumption_Category", "Impute missing Humidity_Percent values with the median", "Fill missing Occupancy_Status with mode 'Occupied'", "Convert Energy_Consumption_Category target to ordinal labels: Low=0, Medium=1, High=2", "One-hot encode categorical features: Building_Type, Day_of_Week, Occupancy_Status", "Split data into training (80%) and test (20%) sets, stratified by target category", "Standardize numeric features: Temperature_C and Humidity_Percent", "Train a GradientBoostingClassifier with default parameters", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Analyze feature importances from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.81, "f1": 0.82, "top_feature_importances": {"Temperature_C": 0.32, "Occupancy_Status_Occupied": 0.24, "Building_Type_Office": 0.15, "Humidity_Percent": 0.1, "Day_of_Week_Monday": 0.08}}}
{"purpose": "Predict whether a region will experience extreme drought conditions based on climate and soil features.", "raw_table": "Region,Average_Temperature,Crop_Type,Soil_Moisture,Precipitation_mm,Wind_Speed_kmh,Drought_Severity\nNorth,22.5,Wheat,0.12,200,15,No\nsouth,25.3,Corn,0.08,180,12,Yes\nEast,19.8,Rice,missing,210,18,No\nWest,23.0,Wheat,0.11,195,14,No\nnorth,21.7,Corn,0.09,170,17,Yes\nEast,20.2,Rice,0.10,215,missing,No\nSouth,26.1,Corn,0.07,160,13,Yes\nWest,22.8,Wheat,0.13,190,16,No\nNorth,21.9,Wheat,0.11,missing,15,No\nSouth,24.9,Corn,0.06,155,14,Yes\nEast,19.5,Rice,0.09,205,17,No\nWest,23.3,Wheat,0.12,198,16,No", "model_steps": ["Standardize capitalization and clean Region categorical values", "Impute missing numeric values in Soil_Moisture, Wind_Speed_kmh, and Precipitation_mm columns using median imputation", "One-hot encode categorical variables: Region and Crop_Type", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features: Average_Temperature, Soil_Moisture, Precipitation_mm, Wind_Speed_kmh", "Train a RandomForestClassifier to predict Drought_Severity", "Perform grid search to tune max_depth and n_estimators parameters", "Evaluate model using accuracy, F1 score, precision, and recall on test set", "Generate a confusion matrix to analyze classification errors"], "model_results": {"accuracy": 0.83, "f1": 0.79, "precision": 0.81, "recall": 0.77, "confusion_matrix": {"true_positive": 7, "true_negative": 8, "false_positive": 2, "false_negative": 3}, "top_feature_importances": {"Soil_Moisture": 0.3, "Precipitation_mm": 0.25, "Average_Temperature": 0.2, "Region_South": 0.1, "Crop_Type_Corn": 0.08}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a student will pass the final exam based on their coursework performance and demographic features.", "raw_table": "StudentID,Age,Gender,Hours_Studied,Attendance_Percentage,Previous_Grade,Parental_Education,Passed_Final\n1,17,M,35,90,88,College,Yes\n2,18,F,20,75,,High School,No\n3,17,M,15,80,70, college ,Yes\n4,19,f,5,60,65,High school,No\n5,18,M,40,95,92,University,Yes\n6,17,F,,85,78,College,Yes\n7,16,M,10,55,50,high School,No\n8,18,F,25,88,82,College,Yes\n9,19,M,30,92,85,University,Yes\n10,17,F,8,65,60,High School,No\n11,18,M,22,80,75,College,Yes\n12,17,,18,70,72,College,No\n13,19,F,28,90,88,University,YES\n14,16,M,12,58,55,High school,No", "model_steps": ["Load the raw CSV data into a DataFrame", "Standardize inconsistent capitalization in 'Gender' and 'Parental_Education' columns", "Impute missing values in 'Hours_Studied' and 'Previous_Grade' using median values", "Encode target variable 'Passed_Final' as binary (Yes=1, No=0)", "Convert categorical variables ('Gender', 'Parental_Education') into one-hot encoded features", "Split data into 80% training and 20% testing sets", "Standardize numeric features ('Age', 'Hours_Studied', 'Attendance_Percentage', 'Previous_Grade')", "Train a Logistic Regression classifier to predict final exam passing", "Evaluate the model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze prediction errors"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"Hours_Studied": 0.45, "Previous_Grade": 0.3, "Attendance_Percentage": 0.15, "Parental_Education_College": 0.1}}}
{"purpose": "Predict whether a customer will make a repeat purchase within 30 days after their first order.", "raw_table": "CustomerID,Age,Gender,JoinChannel,FirstOrderAmount,DaysToRepeatPurchase,RepeatPurchase\n1,34,Male,Online,120.50,15,Yes\n2,22,Female,Offline,85.00,NaN,No\n3,29,Female,online,100.00,10,Yes\n4,45,Male,Offline,NaN,NaN,No\n5,38,Female,Online,150.75,5,Yes\n6,27,,Online,95.25,20,Yes\n7,31,Male,Offline,80,NaN,No\n8,36,Female,Online,110.00,12,Yes\n9,23,Male,Offline,70.00,,No\n10,40,Female,online,130.00,8,Yes\n11,28,Male,Offline,60.00,NaN,No\n12,33,Female,Online,NaN,NaN,No\n13,35,Male,Online,145.00,7,Yes\n14,30,Female,Offline,100.00,NaN,No\n", "model_steps": ["Load the CSV data into a dataframe and inspect for missing or inconsistent values", "Normalize inconsistent capitalization in the 'JoinChannel' column (e.g., convert all to lowercase)", "Impute missing numeric values in 'FirstOrderAmount' using median imputation", "Impute missing values in 'Age' with median and fill missing 'Gender' as 'Unknown'", "Convert 'RepeatPurchase' target variable into binary labels (Yes=1, No=0)", "One-hot encode categorical variables: 'Gender' and 'JoinChannel'", "Split data into 80% training and 20% test sets, stratifying on the target variable", "Standardize numeric features: 'Age', 'FirstOrderAmount', and 'DaysToRepeatPurchase' (imputed values included)", "Train a Logistic Regression classifier to predict repeat purchase", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Analyze feature coefficients to identify key predictors of repeat purchase"], "model_results": {"accuracy": 0.83, "precision": 0.8, "recall": 0.85, "f1": 0.82, "top_feature_importances": {"FirstOrderAmount": 0.45, "DaysToRepeatPurchase": 0.35, "JoinChannel_online": 0.1, "Gender_Female": 0.05, "Age": 0.05}}}
{"purpose": "Predict whether a movie will be a box office hit based on its attributes before release.", "raw_table": "MovieID,Genre,Budget_Million,Director_Experience,Lead_Actor_Popularity,Release_Season,Marketing_Spend_Million,Box_Office_Hit\n1,Action,150,10,8,summer,50,Yes\n2,comedy,40,3,5,Winter,15,No\n3,Drama,25,,7,Spring,10,No\n4,Action,200,15,9,Summer,70,Yes\n5,Horror,15,2,4,Fall,5,no\n6,Comedy,60,5,6,Winter,20,No\n7,Drama,30,7,7,SPRING,12,Yes\n8,Action,180,12,8,Summer,65,Yes\n9,Horror,10,1,3,fall,3,No\n10,Comedy,55,4,6,Winter,18,No\n11,Drama,28,6,7,Spring,11,Yes\n12,Action,170,14,9,Summer,60,Yes\n13,Comedy,45,5,5,Winter,16,No\n14,Horror,12,2,4,Fall,4,No\n15,Drama,35,8,7,Spring,,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Clean the 'Genre' and 'Release_Season' columns by standardizing capitalization", "Impute missing numeric values in 'Director_Experience' and 'Marketing_Spend_Million' with median values", "Encode the target variable 'Box_Office_Hit' as binary (Yes=1, No=0)", "One-hot encode categorical variables 'Genre' and 'Release_Season'", "Split data into train (80%) and test (20%) sets", "Standardize numeric features: Budget_Million, Director_Experience, Lead_Actor_Popularity, Marketing_Spend_Million", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search over max_depth parameter with values [5, 10, 15]", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.87, "precision": 0.89, "recall": 0.83, "f1": 0.86, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 1, "false_negative": 2}, "top_feature_importances": {"Budget_Million": 0.32, "Marketing_Spend_Million": 0.27, "Lead_Actor_Popularity": 0.18}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a citizen's tax return will be flagged for audit based on reported income and other factors.", "raw_table": "Citizen_ID,Age,Employment_Status,Reported_Income,Number_of_Dependents,State,Previous_Audit_Flag,Audit_Flag\n001,45,Full-Time,55000,2,california,No,No\n002,34,Part-time,28000,1,Texas,yes,Yes\n003,29,Unemployed,0,,New York,No,No\n004,52,full-time,72000,3,Florida,No,No\n005,41,Contractor,NaN,2,Texas,Yes,Yes\n006,37,Full-Time,48000,0,California,No,No\n007,30,Full-Time,39000,1,texas,No,No\n008,48,Part-Time,62000,4,Florida,yes,Yes\n009,55,Contractor,88000,2,New York,No,No\n010,26,Unemployed,15000,0,California,No,No\n011,44,Full-time,67000,3,Florida,No,No\n012,38,Part-time,35000,1,NEW YORK,Yes,Yes\n013,33,Full-Time,53000,2,Texas,No,No", "model_steps": ["Load the CSV data into a DataFrame", "Clean 'Employment_Status' and 'State' columns by standardizing capitalization", "Impute missing 'Number_of_Dependents' values with the median", "Convert 'Previous_Audit_Flag' and 'Audit_Flag' columns to binary (Yes=1, No=0)", "One-hot encode 'Employment_Status' and 'State' categorical columns", "Split the data into train and test sets (80/20)", "Standardize numeric features: Age, Reported_Income, Number_of_Dependents", "Train a RandomForestClassifier to predict 'Audit_Flag'", "Perform grid search over max_depth and n_estimators hyperparameters", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix to analyze true positives and false positives"], "model_results": {"accuracy": 0.85, "precision": 0.8, "recall": 0.75, "f1": 0.77, "confusion_matrix": {"true_positive": 6, "false_positive": 2, "true_negative": 7, "false_negative": 2}, "top_feature_importances": {"Reported_Income": 0.35, "Previous_Audit_Flag": 0.25, "Number_of_Dependents": 0.15, "Employment_Status_Full-Time": 0.1, "State_Texas": 0.08}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict the likelihood of hospital readmission within 30 days for diabetic patients.", "raw_table": "PatientID,Age,Gender,HbA1c_Level,MedicationType,PreviousAdmissions,Smoker,Readmitted\n1,55,M,7.8,Metformin,2,Yes,Yes\n2,63,F,8.2,Insulin,1,no,No\n3,47,M,6.5,Metformin,0,Yes,No\n4,,F,7.1,metformin,3,No,Yes\n5,72,M,8.9,Insulin,4,yes,No\n6,59,f,7.4,Sulfonylurea,2,No,Yes\n7,50,M,NaN,Metformin,1,No,No\n8,44,F,6.8,Insulin,0,No,No\n9,67,F,9.1,sulfonylurea,5,Yes,Yes\n10,53,M,7.0,Metformin,2,No,No\n11,61,F,8.3,Insulin,3,Yes,Yes\n12,49,M,7.2,Metformin,1,No,No\n13,58,F,7.7,Sulfonylurea,2,Yes,Yes\n14,65,M,8.0,Metformin,3,NO,Yes", "model_steps": ["Load the dataset and identify missing values in Age and HbA1c_Level columns", "Impute missing Age values with the median age and HbA1c_Level with the mean value", "Standardize the numeric features: Age, HbA1c_Level, and PreviousAdmissions", "Normalize capitalization inconsistencies in categorical columns: Gender, MedicationType, Smoker", "One-hot encode categorical variables: Gender, MedicationType, Smoker", "Split the dataset into training and testing sets with an 80/20 ratio", "Train a RandomForestClassifier to predict Readmitted", "Perform grid search to tune max_depth and n_estimators hyperparameters", "Evaluate model performance using accuracy, F1 score, precision, and recall on the test set", "Generate a confusion matrix for detailed error analysis", "Identify and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.79, "f1": 0.77, "precision": 0.75, "recall": 0.8, "confusion_matrix": [[15, 4], [5, 16]], "top_feature_importances": {"HbA1c_Level": 0.35, "PreviousAdmissions": 0.25, "Smoker_Yes": 0.15}, "best_hyperparameters": {"max_depth": 6, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict whether a farm field's soil moisture level is sufficient for planting crops.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Temperature_C,Previous_Crop,Soil_Moisture_Sufficient\nF001,Loam,120,22,Wheat,Yes\nF002,Clay,85,19,Corn,no\nF003,Sand,NaN,25,Soybean,Yes\nF004,loam,110,23,Corn,yes\nF005,Clay,95,18,WHEAT,No\nF006,Silt,130,20,Rice,Yes\nF007,Sand,75,,Corn,No\nF008,silt,100,21,Soybean,Yes\nF009,Loam,115,22,Rice,yes\nF010,Clay,90,20,Wheat,No\nF011,Sand,80,24,Corn,YES\nF012,Silt,125,21,soybean,No\nF013,Loam,NaN,22,Rice,yes\nF014,Clay,100,23,Corn,No", "model_steps": ["Load raw CSV data into a DataFrame", "Standardize capitalization in categorical columns Soil_Type, Previous_Crop, and Soil_Moisture_Sufficient", "Fill missing numeric values in Rainfall_mm and Temperature_C with median values", "Encode target variable Soil_Moisture_Sufficient as binary (Yes=1, No=0)", "One-hot encode categorical features Soil_Type and Previous_Crop", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features Rainfall_mm and Temperature_C using training set statistics", "Train a RandomForestClassifier on the training data", "Tune max_depth hyperparameter with grid search over [3, 5, 7]", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score", "Generate feature importances from the final model"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "top_feature_importances": {"Rainfall_mm": 0.32, "Soil_Type_Loam": 0.21, "Temperature_C": 0.18, "Previous_Crop_Corn": 0.12, "Soil_Type_Clay": 0.1, "Previous_Crop_Wheat": 0.07}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict hourly energy consumption based on weather and operational conditions.", "raw_table": "Hour,Temperature_C,Weather,Day_Type,Equipment_Status,Energy_Consumption_kWh\n0,15.2,Sunny,weekday,ON,120.5\n1,14.8,Cloudy,Weekday,ON,115.3\n2,14.5,,weekday,OFF,110.0\n3,14.2,RAINY,Weekend,OFF,105.7\n4,14.0,Rainy,weekend,OFF,107.1\n5,13.8,Cloudy,Weekday,ON,112.2\n6,16.0,Sunny,Weekday,ON,130.6\n7,18.5,Sunny,Weekday,ON,150.2\n8,20.0,sunny,Weekday,ON,170.4\n9,21.5,Sunny,Weekday,ON,190.8\n10,22.0,Cloudy,Weekday,ON,200.1\n11,22.5,Rainy,Weekday,OFF,180.5\n12,23.0,Sunny,Weekend,ON,210.9\n13,23.5,Sunny,Weekend,ON,215.0", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Fill missing Weather values with the mode (most frequent category)", "Standardize capitalization of Weather and Day_Type columns", "Convert categorical features Weather, Day_Type, and Equipment_Status into one-hot encoded variables", "Split data into training (80%) and testing (20%) sets", "Standardize numeric features: Hour and Temperature_C", "Train a Gradient Boosting Regressor to predict Energy_Consumption_kWh", "Tune hyperparameters max_depth and learning_rate using grid search with 5-fold cross-validation", "Evaluate model performance on test set using RMSE, MAE, and R-squared metrics", "Identify and report top 3 feature importances from the trained model"], "model_results": {"rmse": 8.4, "mae": 6.2, "r2": 0.92, "best_hyperparameters": {"max_depth": 4, "learning_rate": 0.1}, "top_feature_importances": {"Hour": 0.35, "Temperature_C": 0.28, "Equipment_Status_ON": 0.15}}}
{"purpose": "Predict whether a movie will be a box office hit based on its production features.", "raw_table": "MovieID,Genre,DirectorExperience,LeadActorPopularity,BudgetMillions,ReleaseMonth,HasSequel,BoxOfficeHit\n1,Action,5,8.9,150,July,Yes,Yes\n2,Comedy,2,7,50,December,No,No\n3,Drama,,6,30,october,No,No\n4,Action,7,9.2,200,May,YES,Yes\n5,Comedy,3,5.5,40,August,no,No\n6,Drama,4,7.3,60,March,No,No\n7,Action,6,8.1,180,July,Yes,Yes\n8,Comedy,1,4.7,20,november,No,No\n9,Drama,3,6.8,55,April,No,No\n10,Action,8,nine,220,June,Yes,Yes\n11,Comedy,2,6,35,November,No,No\n12,Drama,4,7.0,65,July,No,No\n13,Action,7,8.5,190,August,Yes,Yes\n14,Comedy,3,5.6,45,December,No,No", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Impute missing DirectorExperience with median value", "Convert LeadActorPopularity from mixed types to numeric, replacing 'nine' with 9.0", "Standardize numeric features: DirectorExperience, LeadActorPopularity, BudgetMillions", "One-hot encode categorical variables: Genre, ReleaseMonth, HasSequel", "Split data into train and test sets with 80/20 ratio, stratified by BoxOfficeHit", "Train a RandomForestClassifier with default parameters", "Perform grid search over max_depth values [3, 5, 7] using 5-fold cross-validation", "Evaluate the final model on test set measuring accuracy, precision, recall, and F1 score", "Generate confusion matrix for test set predictions"], "model_results": {"accuracy": 0.85, "precision": 0.88, "recall": 0.83, "f1": 0.855, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 1, "false_negative": 2}, "top_feature_importances": {"BudgetMillions": 0.32, "LeadActorPopularity": 0.25, "HasSequel_Yes": 0.18, "Genre_Action": 0.12, "DirectorExperience": 0.08, "ReleaseMonth_July": 0.05}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Build a regression model to predict hourly electricity consumption based on weather and operational factors.", "raw_table": "Hour,Temperature_C,Weather_Condition,Day_Type,Is_Holiday,Prev_Hour_Consumption_kWh,Electricity_Consumption_kWh\n0,15.2,Clear,weekday,No,150,158\n1,14.8,clear,Weekday,No,158,162\n2,14.5,Clear,weekday,No,162,160\n3,14.3,Cloudy,weekday,No,160,155\n4,14.0,CLOUDY,weekday,No,155,150\n5,13.8,Cloudy,weekday,No,150,145\n6,14.2,Rain,Weekday,No,145,155\n7,16.0,Rain,Weekday,No,155,170\n8,18.5,Clear,Weekend,Yes,170,180\n9,20.1,clear,Weekend,Yes,180,190\n10,21.5,Clear,Weekend,Yes,190,200\n11,22.0,Clear,Weekend,Yes,200,205\n12,23.3,Sunny,weekend,Yes,205,210\n13,,Sunny,weekend,Yes,210,215", "model_steps": ["Load the CSV data into a DataFrame, treating missing values appropriately", "Normalize inconsistent capitalization in categorical columns (e.g., Weather_Condition, Day_Type)", "Impute missing numeric values in Temperature_C using median imputation", "Encode categorical variables Weather_Condition, Day_Type, and Is_Holiday using one-hot encoding", "Split data into training (80%) and testing (20%) sets, stratified by Day_Type", "Standardize numeric features: Temperature_C and Prev_Hour_Consumption_kWh", "Train a Gradient Boosting Regressor on the training set to predict Electricity_Consumption_kWh", "Perform hyperparameter tuning on learning_rate and n_estimators using grid search with 5-fold cross-validation", "Evaluate model performance on the test set using RMSE, MAE, and R2 metrics", "Identify and report top 3 feature importances from the trained model"], "model_results": {"rmse": 4.7, "mae": 3.5, "r2": 0.92, "top_feature_importances": {"Prev_Hour_Consumption_kWh": 0.48, "Temperature_C": 0.32, "Is_Holiday_Yes": 0.12}, "best_hyperparameters": {"learning_rate": 0.1, "n_estimators": 100}}}
{"purpose": "Predict whether a citizen application for government assistance will be approved based on demographic and application data.", "raw_table": "Applicant_ID,Age,Income,Employment_Status,Region,Application_Type,Previous_Approvals,Approved\n1001,34,45000,Employed,North,New,2,Yes\n1002,29,39000,Self-employed,South,renewal,,No\n1003,42,NaN,Employed,East,New,1,Yes\n1004,37,52000,unemployed,West,Renewal,0,No\n1005,NaN,48000,Employed,North,New,3,Yes\n1006,28,35000,Employed,SOUTH,New,1,No\n1007,45,62000,Self-Employed,East,Renewal,2,Yes\n1008,33,NaN,Unemployed,West,New,0,No\n1009,38,57000,Employed,North,renewal,1,Yes\n1010,41,50000,Employed,East,New,,No\n1011,27,43000,Unemployed,South,New,0,No\n1012,30,47000,Employed,West,Renewal,1,Yes\n1013,39,51000,SELF-EMPLOYED,North,New,2,Yes\n1014,36,46000,employed,South,Renewal,1,No", "model_steps": ["Load the CSV data into a DataFrame.", "Clean the Employment_Status and Region columns to fix inconsistent capitalization and unify categories.", "Impute missing values in Age and Income using median values.", "Fill missing Previous_Approvals with zero, assuming no previous approvals.", "Convert the target variable 'Approved' to binary numeric (Yes=1, No=0).", "One-hot encode categorical variables: Employment_Status, Region, and Application_Type.", "Split the data into training and test sets with an 80/20 ratio.", "Standardize numeric features: Age, Income, Previous_Approvals.", "Train a RandomForestClassifier with 100 trees on the training data.", "Evaluate the model using accuracy, F1-score, precision, and recall on the test data.", "Analyze feature importances from the trained RandomForest model."], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.8, "recall": 0.87, "feature_importances": {"Income": 0.32, "Employment_Status_Employed": 0.18, "Previous_Approvals": 0.15, "Age": 0.12, "Region_North": 0.08, "Application_Type_New": 0.07, "Region_South": 0.05, "Employment_Status_Self-employed": 0.03}}}
{"purpose": "Predict whether a loan applicant will default on their loan based on financial and demographic features.", "raw_table": "ApplicantID,Age,Income,EmploymentStatus,CreditScore,LoanAmount,LoanPurpose,Default\n1,45,55000,Employed,720,15000,Home Improvement,No\n2,38,NaN,Self-employed,680,12000,Debt Consolidation,Yes\n3,29,47000,EMPLOYED,690,8000,Car,No\n4,52,64000,Unemployed,NaN,20000,Business,Yes\n5,41,58000,Employed,710,NaN,Vacation,No\n6,33,52000,employed,650,10000,Car,Yes\n7,60,72000,Retired,730,15000,Home Improvement,No\n8,28,48000,Employed,NaN,9000,Debt Consolidation,No\n9,35,50000,Self-Employed,670,11000,Vacation,Yes\n10,47,58000,Employed,700,13000,Debt Consolidation,No\n11,39,NaN,Unemployed,640,7000,Car,Yes\n12,31,46000,Employed,660,8000,Home Improvement,No\n13,44,61000,employed,695,14000,Business,No\n14,37,53000,Self-Employed,NaN,10000,Vacation,Yes", "model_steps": ["Load data and inspect for missing or inconsistent values", "Standardize capitalization in EmploymentStatus column (e.g., lowercase all entries)", "Impute missing Income and CreditScore values using median values grouped by EmploymentStatus", "Impute missing LoanAmount values using median LoanAmount", "Encode categorical variables (EmploymentStatus and LoanPurpose) using one-hot encoding", "Split data into train and test sets with 80% training and 20% testing", "Standardize numeric features (Age, Income, CreditScore, LoanAmount) using StandardScaler", "Train a RandomForestClassifier on the training data", "Perform grid search over max_depth and n_estimators parameters to optimize performance", "Evaluate model on the test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.79, "f1": 0.76, "precision": 0.74, "recall": 0.78, "confusion_matrix": [[8, 2], [3, 7]], "top_feature_importances": {"CreditScore": 0.32, "LoanAmount": 0.24, "Income": 0.18, "EmploymentStatus_Self-employed": 0.1, "LoanPurpose_Debt Consolidation": 0.07, "Age": 0.05, "EmploymentStatus_Employed": 0.04}, "best_hyperparameters": {"max_depth": 7, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict whether a field's soil moisture level is suitable for planting (Suitable or Not Suitable).", "raw_table": "Field_ID,Soil_Type,Precipitation_mm,Temperature_C,Crop_History,Soil_Moisture_Level,Planting_Suitability\nF01,Loam,120,22,Wheat,High,Suitable\nF02,SAND,85,25,Corn,Medium,Not Suitable\nF03,Clay,NaN,20,Soybean,Low,Not Suitable\nF04,Silt,100,18,Cotton,Medium,Suitable\nF05,loam,130,23,Barley,High,Suitable\nF06,Clay,95,19,Rice,Medium,Not Suitable\nF07,Sand,110,21,Wheat,MEDIUM,Suitable\nF08,Silt,NaN,22,Corn,High,Suitable\nF09,Loam,105,20,Soybean,Low,Not Suitable\nF10,Clay,90,21,Cotton,Low,Not Suitable\nF11,Silt,115,24,Rice,High,Suitable\nF12,Loam,125,23,Barley,Medium,Suitable\nF13,Sand,80,19,Wheat,,Not Suitable\nF14,Loam,110,22,Soybean,Medium,Suitable", "model_steps": ["Load the dataset and identify missing and inconsistent values", "Impute missing precipitation values using median precipitation by Soil_Type", "Standardize capitalization in 'Soil_Type' and 'Soil_Moisture_Level' columns for consistency", "Impute missing 'Soil_Moisture_Level' with the mode value 'Medium'", "One-hot encode categorical features: Soil_Type, Crop_History, Soil_Moisture_Level", "Split data into training (80%) and test sets (20%) stratified by Planting_Suitability", "Standardize numeric features: Precipitation_mm and Temperature_C", "Train a RandomForestClassifier with 100 trees and max_depth=5", "Perform 5-fold cross-validation to tune max_depth parameter (values tested: 3, 5, 7)", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"True_Positive": 7, "True_Negative": 4, "False_Positive": 1, "False_Negative": 1}, "top_feature_importances": {"Soil_Moisture_Level_Medium": 0.28, "Soil_Type_Loam": 0.22, "Precipitation_mm": 0.17}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a social media post will go viral based on post attributes and user engagement metrics.", "raw_table": "post_id,user_type,post_length,has_image,post_hour,previous_engagement,topic,viral\n1,Influencer,120,Yes,15,300,Tech,Yes\n2,casual,45,No,9,20,Lifestyle,No\n3,Influencer,200,yes,20,500,Tech,Yes\n4,Casual,80,no,14,50,Food,No\n5,Brand,150,Yes,18,,Tech,Yes\n6,casual,60,No,22,30,food,No\n7,Brand,90,YES,10,120,Lifestyle,Yes\n8,Influencer,300,No,8,600,tech,Yes\n9,casual,40,No,16,40,Lifestyle,No\n10,Brand,,Yes,12,100,Food,No\n11,Influencer,250,Yes,19,450,Tech,Yes\n12,Casual,70,No,7,15,Lifestyle,No\n13,Brand,110,yes,21,95,Food,No\n14,casual,55,No,13,25,Food,No\n15,Influencer,130,Yes,17,350,Tech,Yes", "model_steps": ["Load CSV data and inspect for missing and inconsistent values", "Standardize 'has_image' and 'user_type' columns to uniform capitalization", "Impute missing 'post_length' and 'previous_engagement' values using median imputation", "Convert 'has_image' and 'viral' target variable to binary encoding", "One-hot encode categorical variables: 'user_type' and 'topic'", "Split data into training (80%) and test (20%) sets stratified by 'viral'", "Scale numeric features 'post_length', 'post_hour', and 'previous_engagement' using standard scaler", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search over max_depth values [5, 10, 15] using 5-fold cross-validation", "Evaluate the final model on the test set calculating accuracy, precision, recall, and F1 score", "Generate confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.89, "f1": 0.87, "confusion_matrix": {"true_positive": 8, "true_negative": 7, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"previous_engagement": 0.38, "post_length": 0.25, "has_image_Yes": 0.15, "user_type_Influencer": 0.12, "topic_Tech": 0.1}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a customer will make a repeat purchase within 30 days after their first order.", "raw_table": "CustomerID,Age,Gender,InitialPurchaseAmount,ProductCategory,SignupSource,DaysSinceSignup,RepeatPurchase\n1,34,Male,120.50,Electronics,Web,15,Yes\n2,27,Female,45.00,Clothing,mobile,40,No\n3,NaN,Male,85.75,Home & Kitchen,Web,10,Yes\n4,52,FEMALE,200.00,Electronics,Referral,NaN,Yes\n5,23,Female,15.20,clothing,web,30,No\n6,47,Male,NaN,Sports,Mobile,5,Yes\n7,38,Female,99.99,Electronics,Email,20,No\n8,29,Male,55.00,home & kitchen,Referral,25,Yes\n9,41,Female,130.00,Sports,web,12,Yes\n10,NaN,female,70.00,Clothing,Email,35,No\n11,30,Male,40.00,Clothing,Web,NaN,No\n12,45,Female,150.00,Electronics,Email,18,Yes\n13,33,Male,60.00,Sports,Mobile,22,No\n14,36,Female,80.00,Home & kitchen,Referral,28,Yes\n15,28,Male,NaN,clothing,Email,16,No", "model_steps": ["Load the CSV data into a DataFrame", "Identify and impute missing numeric values (Age, InitialPurchaseAmount, DaysSinceSignup) using median imputation", "Normalize inconsistent capitalization in categorical columns: Gender, ProductCategory, SignupSource", "Encode categorical variables using one-hot encoding", "Split data into training (80%) and test (20%) sets, stratified by RepeatPurchase", "Train a Logistic Regression classifier on the training set", "Perform hyperparameter tuning for regularization strength using 5-fold cross-validation", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix for test predictions", "Extract and report top three most important features based on model coefficients"], "model_results": {"accuracy": 0.8, "precision": 0.78, "recall": 0.75, "f1": 0.76, "confusion_matrix": {"true_positive": 6, "false_positive": 2, "true_negative": 5, "false_negative": 2}, "top_features": {"InitialPurchaseAmount": 1.25, "DaysSinceSignup": -0.85, "ProductCategory_Electronics": 0.65}, "best_hyperparameter_C": 0.1}}
{"purpose": "Predict whether a government building inspection will result in a compliance violation.", "raw_table": "Building_ID,Building_Type,Year_Built,Last_Inspection_Score,Number_of_Complaints,Zip_Code,Has_Fire_Alarm,Violation\n101,Residential,1985,85,2,10012,Yes,No\n102,Commercial,1970,78,5,10012,YES,Yes\n103,Industrial,1995,NaN,1,10013,No,No\n104,Residential,2005,90,0,10014,yes,No\n105,Commercial,1980,65,7,10015,No,Yes\n106,Industrial,1988,72,3,10012,No,Yes\n107,Residential,NaN,88,0,10013,Yes,No\n108,Commercial,1990,80,4,10014,No,Yes\n109,Residential,1975,77,Missing,10015,YES,No\n110,Industrial,2000,85,2,10013,No,No\n111,Commercial,1983,83,3,10012,Yes,Yes\n112,Residential,1999,NaN,1,10014,No,No\n113,Industrial,1987,70,5,10015,Yes,Yes\n114,Commercial,1992,75,4,10013,No,No", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing or inconsistent values", "Standardize capitalization of 'Has_Fire_Alarm' column and convert to binary indicator (Yes=1, No=0)", "Impute missing numeric values in 'Year_Built', 'Last_Inspection_Score', and 'Number_of_Complaints' using median values", "One-hot encode the 'Building_Type' and 'Zip_Code' categorical columns", "Split the dataset into training (80%) and test (20%) sets stratified by the target 'Violation'", "Standardize numeric features to zero mean and unit variance", "Train a RandomForestClassifier to predict 'Violation'", "Perform a grid search to tune 'n_estimators' and 'max_depth' hyperparameters", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall metrics", "Generate and analyze the confusion matrix to understand prediction errors", "Extract and report top 5 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.82, "precision": 0.8, "recall": 0.85, "confusion_matrix": {"true_positive": 23, "true_negative": 38, "false_positive": 6, "false_negative": 4}, "top_feature_importances": {"Last_Inspection_Score": 0.32, "Number_of_Complaints": 0.25, "Has_Fire_Alarm": 0.15, "Building_Type_Commercial": 0.1, "Year_Built": 0.08}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Predict whether a retail customer will make a purchase during a promotional campaign.", "raw_table": "CustomerID,Age,Gender,Income,MembershipStatus,LastPurchaseDays,PreferredChannel,VisitFrequency,MadePurchase\n1,34,Male,55000,Gold,10,Online,Weekly,Yes\n2,28,Female,48000,Silver,NA,In-store,Monthly,No\n3,45,Female,72000,Platinum,5,online,Weekly,Yes\n4,52,Male,62000,Gold,20,In-Store,Monthly,No\n5,23,unknown,40000,Silver,,Online,Weekly,Yes\n6,31,Female,missing,Gold,15,Online,Weekly,No\n7,40,Male,58000,Silver,12,instore,Weekly,Yes\n8,37,Female,60000,Gold,8,Online,Weekly,No\n9,29,Male,52000,Silver,7,In-Store,Monthly,Yes\n10,50,Female,75000,Platinum,2,Online,Daily,Yes\n11,26,Female,45000,Silver,NA,Online,Monthly,No\n12,48,Male,67000,Gold,18,In-store,Weekly,No\n13,35,Male,59000,Silver,14,Online,Weekly,Yes\n14,41,Female,61000,Gold,9,online,Weekly,No", "model_steps": ["Load CSV data into a DataFrame", "Handle missing and inconsistent values: fill missing Income with median, standardize 'PreferredChannel' values to lowercase, and impute missing LastPurchaseDays with median", "Convert target variable 'MadePurchase' to binary (Yes=1, No=0)", "One-hot encode categorical variables: Gender, MembershipStatus, PreferredChannel, VisitFrequency", "Split data into training (80%) and test (20%) sets with stratification on target", "Standardize numeric features: Age, Income, LastPurchaseDays", "Train a RandomForestClassifier with 100 trees and max_depth=5 on the training set", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score", "Calculate feature importances from the trained RandomForest model", "Generate confusion matrix and predicted probabilities for the test set"], "model_results": {"accuracy": 0.79, "precision": 0.81, "recall": 0.75, "f1": 0.78, "feature_importances": {"LastPurchaseDays": 0.32, "Income": 0.25, "MembershipStatus_Platinum": 0.15, "VisitFrequency_Weekly": 0.1, "Age": 0.08, "PreferredChannel_online": 0.06, "Gender_Female": 0.04}, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 2, "false_negative": 2}, "hyperparameters": {"n_estimators": 100, "max_depth": 5, "random_state": 42}}}
{"purpose": "Build a regression model to estimate house sale prices based on property and neighborhood features.", "raw_table": "HouseID,Bedrooms,Bathrooms,Size_sqft,Neighborhood,Condition,Year_Built,Sale_Price\n1,3,2,1500,Downtown,Good,1995,350000\n2,4,3,2000,Suburb,Excellent,2005,475000\n3,2,1,900,suburb,good,1980,210000\n4,3,2,1600,Midtown,Poor,1978,270000\n5,3,2.5,1800,Downtown,Excellent,2010,520000\n6,4,,2200,Midtown,GOOD,1999,460000\n7,5,4,3000,Suburb,Poor,2015,600000\n8,2,1,850,Downtown,Good,1970,190000\n9,3,2,1400,,Excellent,1985,330000\n10,3,2,1550,Midtown,Fair,2000,340000\n11,4,3,2100,Suburb,Excellent,2012,490000\n12,2,1,950,Downtown,good,1975,200000\n13,3,2,1650,Midtown,Poor,1988,260000\n14,4,3,2300,Suburb,EXCELLENT,2018,520000\n15,3,2,1500,Downtown,Good,2003,370000", "model_steps": ["Load data and identify target variable 'Sale_Price'", "Handle missing values: impute missing 'Bathrooms' with median, and missing 'Neighborhood' with mode", "Standardize capitalization inconsistencies in 'Neighborhood' and 'Condition' columns", "Convert categorical variables 'Neighborhood' and 'Condition' to one-hot encoded features", "Split data into training (80%) and testing (20%) sets", "Standardize numeric features: 'Bedrooms', 'Bathrooms', 'Size_sqft', 'Year_Built'", "Train a Gradient Boosting Regressor with default hyperparameters on training data", "Evaluate model performance on test data using RMSE, MAE, and R2 metrics", "Extract and rank top 5 feature importances from the trained model"], "model_results": {"rmse": 28000.5, "mae": 21000.0, "r2": 0.87, "top_feature_importances": {"Size_sqft": 0.42, "Neighborhood_Suburb": 0.18, "Condition_Excellent": 0.12, "Year_Built": 0.1, "Bedrooms": 0.08}}}
{"purpose": "Predict whether a citizen will apply for government housing assistance based on demographic and economic factors.", "raw_table": "Age,Gender,Employment_Status,Annual_Income,Household_Size,Region,Has_Disability,Applied_For_Assistance\n34,Male,Employed,45000,3,North,Yes,No\n29,female,Unemployed,12000,1,South,No,Yes\n47,Male,Employed,68000,4,East,No,No\n23,Female,Part-time,31000,2,West,yes,Yes\n56,Male,Employed,52000,5,North,no,No\nNaN,Female,Unemployed,15000,1,South,No,Yes\n41,Male,Employed,49000,3,East,No,No\n38,Female,Self-Employed,54000,4,west,Yes,No\n50,Male,Employed,62000,2,North,No,No\n27,Female,Part-time,28000,3,South,Yes,Yes\n44,Male,Unemployed,NaN,1,East,No,Yes\n36,Female,Employed,48000,3,North,Yes,No\n31,Male,Employed,46000,2,South,No,No\n42,female,Self-employed,53000,4,West,No,Yes", "model_steps": ["Load the CSV dataset into a DataFrame", "Identify and impute missing numeric values in 'Age' and 'Annual_Income' columns using median imputation", "Standardize inconsistent capitalization in categorical columns: 'Gender', 'Employment_Status', 'Region', and 'Has_Disability'", "Encode categorical variables using one-hot encoding for 'Gender', 'Employment_Status', 'Region', and 'Has_Disability'", "Split the dataset into train and test sets with an 80/20 ratio, stratifying by the target variable 'Applied_For_Assistance'", "Train a RandomForestClassifier with default hyperparameters to predict 'Applied_For_Assistance'", "Perform grid search cross-validation over 'n_estimators' ([50, 100]) and 'max_depth' ([5, 10, None])", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Identify and report top 3 feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.83, "precision": 0.79, "recall": 0.85, "f1": 0.82, "confusion_matrix": {"true_positive": 7, "true_negative": 9, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Annual_Income": 0.32, "Employment_Status_Employed": 0.22, "Has_Disability_Yes": 0.18}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Build a classification model to predict whether a taxi trip will have surge pricing applied.", "raw_table": "trip_id,passenger_count,pickup_hour,weather_condition,trip_distance,day_of_week,surge_applied\n1,2,8,Clear,3.5,Monday,No\n2,1,23,Rain,5.2,friday,Yes\n3,3,15,clear,2.1,Wednesday,No\n4,2,18,Cloudy,4.8,Thursday,Yes\n5,,7,Rain,1.2,Monday,No\n6,1,12,Clear,7.3,saturday,YES\n7,4,20,Cloudy,6.0,Sunday,No\n8,2,6,Fog,3.9,Tuesday,No\n9,1,22,RAIN,5.5,Friday,Yes\n10,3,10,Clear,2.7,Monday,No\n11,2,16,Clear,3.0,Wednesday,No\n12,1,21,clear,,Thursday,Yes\n13,2,19,Cloudy,4.4,friday,Yes\n14,3,14,Fog,3.3,Tuesday,No\n15,1,9,Clear,5.6,monday,No", "model_steps": ["Load the dataset and identify the target variable 'surge_applied'.", "Handle missing values: impute missing 'passenger_count' with median and 'trip_distance' with mean.", "Standardize the inconsistent capitalization in categorical columns 'weather_condition' and 'day_of_week'.", "Encode categorical variables 'weather_condition' and 'day_of_week' using one-hot encoding.", "Convert target variable 'surge_applied' to binary labels (Yes=1, No=0).", "Split the data into training (80%) and test (20%) sets.", "Standardize numeric features 'passenger_count', 'pickup_hour', and 'trip_distance'.", "Train a RandomForestClassifier with 100 trees on the training set.", "Tune max_depth parameter using grid search with cross-validation over values [5, 10, 15].", "Evaluate the model on the test set computing accuracy, F1 score, precision, and recall.", "Generate and analyze the confusion matrix.", "Identify and report the top 3 most important features."], "model_results": {"accuracy": 0.87, "f1": 0.85, "precision": 0.83, "recall": 0.88, "confusion_matrix": [[20, 3], [4, 23]], "top_feature_importances": {"pickup_hour": 0.32, "trip_distance": 0.25, "weather_condition_Rain": 0.18}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Build a binary classification model to predict whether a social media post will go viral based on post attributes and user engagement metrics.", "raw_table": "post_id,post_length,user_followers,post_category,posted_time,contains_video,average_engagement,target_viral\n1,120,1500,Tech,Morning,Yes,35,1\n2,85,300,Fashion,Afternoon,No,12,0\n3,100,NaN,Food,Evening,No,27,1\n4,95,850,tech,Morning,Yes,20,0\n5,,1200,Fashion,Morning,No,15,0\n6,110,700,Food,Afternoon,YES,30,1\n7,130,2100,Tech,Evening,No,50,1\n8,78,400,Fashion,Night,No,8,0\n9,102,NaN,Food,Morning,Yes,28,1\n10,105,950,Fashion,Afternoon,No,18,0\n11,115,1300,Tech,Night,Yes,40,1\n12,90,600,food,Evening,No,22,0\n13,88,500,Fashion,Morning,No,14,0\n14,125,2500,Tech,Afternoon,Yes,55,1", "model_steps": ["Load the CSV data and inspect for missing and inconsistent values", "Impute missing numeric values in 'post_length' and 'user_followers' with median values", "Standardize capitalization in 'post_category' and 'contains_video' columns for consistency", "Convert categorical variables 'post_category', 'posted_time', and 'contains_video' into one-hot encoded features", "Split data into 80% training and 20% testing sets", "Standardize numeric features: 'post_length', 'user_followers', and 'average_engagement'", "Train a RandomForestClassifier on the training data with 100 trees", "Perform grid search on max_depth parameter with values [5,10,15]", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Generate and analyze the confusion matrix to understand classification errors"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.81, "recall": 0.85, "confusion_matrix": {"true_positive": 6, "false_positive": 2, "true_negative": 7, "false_negative": 1}, "top_feature_importances": {"average_engagement": 0.35, "user_followers": 0.25, "contains_video_Yes": 0.15, "post_length": 0.1, "post_category_Tech": 0.07, "posted_time_Morning": 0.05, "post_category_Fashion": 0.03}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a loan applicant will default on their loan based on application and financial features.", "raw_table": "ApplicantID,Age,Income,EmploymentStatus,CreditScore,LoanAmount,LoanPurpose,Defaulted\n1,34,58000,Full-Time,690,15000,Home Improvement,No\n2,45,NaN,Part-Time,620,12000,Car,Yes\n3,29,72000,full-time,710,20000,Debt Consolidation,No\n4,50,54000,Unemployed,580,8000,Car,Yes\n5,41,63000,Full-time,NaN,17000,Home Improvement,No\n6,23,49000,Part-time,600,7500,Debt consolidation,Yes\n7,38,NaN,Full-Time,690,13000,Car Repair,No\n8,31,67000,Full-Time,710,NaN,Car,No\n9,27,52000,Unemployed,580,7000,Car,Yes\n10,36,61000,Full-Time,680,14000,Home improvement,No\n11,44,71000,Full-Time,700,21000,Debt Consolidation,No\n12,39,45000,Part-Time,590,9000,car,Yes\n13,47,68000,Full-Time,720,22000,Home Improvement,No\n14,52,,Full-time,630,11000,Debt Consolidation,Yes\n15,30,56000,Part-Time,,13000,Car,No", "model_steps": ["Load and parse the CSV dataset into a DataFrame", "Identify and handle missing values in numeric columns (Income, CreditScore, LoanAmount) with median imputation", "Standardize numeric features (Age, Income, CreditScore, LoanAmount)", "Normalize and unify EmploymentStatus and LoanPurpose categories (fix inconsistent capitalization and group variations)", "One-hot encode categorical variables EmploymentStatus and LoanPurpose", "Encode target variable Defaulted as binary (Yes=1, No=0)", "Split the dataset into training (80%) and test (20%) sets with stratification on target", "Train a RandomForestClassifier on the training data with default hyperparameters", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix to assess false positives and false negatives", "Extract and report feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.8, "precision": 0.78, "recall": 0.75, "f1": 0.765, "confusion_matrix": {"true_positive": 6, "true_negative": 6, "false_positive": 2, "false_negative": 2}, "top_feature_importances": {"CreditScore": 0.32, "LoanAmount": 0.25, "Income": 0.18, "EmploymentStatus_Full-Time": 0.1, "LoanPurpose_Car": 0.08}}}
{"purpose": "Predict whether a movie will be a box office hit based on its production and genre features.", "raw_table": "MovieID,Genre,BudgetMillions,DirectorExperienceYears,LeadActorPopularity,ReleaseMonth,IsSequel,BoxOfficeHit\n1,Action,150,12,85,July,Yes,Yes\n2,comedy,30,5,70,december,No,No\n3,Drama,45,8,65,March,No,No\n4,Action,NaN,10,90,July,Yes,Yes\n5,Horror,15,3,50,October,No,No\n6,Comedy,35,7,75,august,No,Yes\n7,Drama,55,15,80,,No,Yes\n8,Action,120,11,88,July,yes,Yes\n9,Comedy,28,6,68,December,No,No\n10,Drama,50,9,70,April,No,No\n11,Horror,20,,55,October,No,No\n12,Action,140,13,92,July,Yes,Yes\n13,Comedy,40,6,72,September,No,Yes\n14,Drama,48,10,78,March,No,Yes", "model_steps": ["Load raw CSV data into a DataFrame", "Clean and standardize 'Genre' and 'ReleaseMonth' columns (capitalize first letter)", "Impute missing numeric values in 'BudgetMillions' and 'DirectorExperienceYears' using median", "Fill missing 'ReleaseMonth' with the mode (most frequent month)", "Convert 'IsSequel' and 'BoxOfficeHit' to binary flags (Yes=1, No=0)", "One-hot encode categorical variables: 'Genre' and 'ReleaseMonth'", "Split data into train and test sets (80/20 stratified by target)", "Standardize numeric features: 'BudgetMillions', 'DirectorExperienceYears', 'LeadActorPopularity'", "Train a RandomForestClassifier with 100 trees", "Perform grid search over max_depth [5, 10, 15]", "Evaluate model on test set using accuracy, precision, recall, and F1-score", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"BudgetMillions": 0.35, "LeadActorPopularity": 0.28, "IsSequel": 0.15}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a government grant application will be approved based on applicant and project information.", "raw_table": "Application_ID,Applicant_Age,Applicant_Gender,Project_Type,Requested_Amount,Previous_Grants,State,Approved\n1,45,Male,Infrastructure,500000,2,NY,Yes\n2,38,Female,Education,250000,No,ca,No\n3,52,Male,Healthcare,,1,TX,Yes\n4,29,Female,Infrastructure,300000,0,FL,No\n5,47,male,Education,150000,3,NY,Yes\n6,34,Female,Healthcare,400000,No,TX,Yes\n7,41,Male,Education,200000,1,CA,No\n8,50,Female,Infrastructure,700000,Yes,FL,Yes\n9,37,Male,HEALTHCARE,350000,2,TX,No\n10,44,Female,Education,NaN,1,NY,Yes\n11,30,Male,Infrastructure,280000,0,FL,No\n12,55,Female,Education,220000,2,CA,Yes\n13,48,Male,Healthcare,310000,1,TX,Yes\n14,33,Female,Infrastructure,300000,No,FL,No", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing or inconsistent values.", "Standardize categorical values (e.g., unify capitalization in 'Applicant_Gender', 'Project_Type', and 'State').", "Impute missing values in 'Requested_Amount' with the median value.", "Convert 'Previous_Grants' from mixed types (strings like 'No' and numbers) to numeric binary indicator (0 for No, 1 or more for counts).", "Encode categorical variables using one-hot encoding: 'Applicant_Gender', 'Project_Type', 'State'.", "Split the data into train (80%) and test (20%) sets, stratified by the 'Approved' target.", "Standardize numeric features: 'Applicant_Age' and 'Requested_Amount'.", "Train a RandomForestClassifier to predict 'Approved'.", "Perform a grid search over 'max_depth' and 'n_estimators' hyperparameters.", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall.", "Compute and analyze the confusion matrix.", "Identify top 3 feature importances from the trained model."], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.8, "recall": 0.87, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Requested_Amount": 0.32, "Project_Type_Infrastructure": 0.21, "Previous_Grants": 0.18}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict customer churn in a telecom company using customer demographics and service usage data.", "raw_table": "CustomerID,Age,ContractType,MonthlyCharges,DataUsageGB,PaymentMethod,Churn\n001,34,Month-to-month,70.5,12.3,Credit Card,Yes\n002,45,One year,55.0,8.7,bank transfer,no\n003,29,Month-to-month,NaN,15.1,Electronic Check,Yes\n004,NaN,Two Year,80.2,NaN,Credit Card,no\n005,52,Month-to-month,65.1,9.5,Electronic check,No\n006,37,Month-to-month,60.0,11.0,credit card,Yes\n007,41,Two year,40.0,5.5,Bank Transfer,No\n008,28,Month-to-month,75.3,14.8,electronic Check,Yes\n009,50,One Year,54.5,7.9,Credit card,No\n010,31,Month-to-month,68.0,13.2,Electronic Check,Yes\n011,47,One year,60.5,10.2,bank Transfer,No\n012,39,Month-to-month,62.0,NaN,Electronic Check,yes\n013,33,Two Year,48.0,6.8,Credit Card,No\n014,55,Month-to-month,72.5,16.0,Bank Transfer,YES\n", "model_steps": ["Load the data and identify missing and inconsistent entries", "Impute missing numeric values with median values", "Standardize the 'Age', 'MonthlyCharges', and 'DataUsageGB' numeric columns", "Clean and normalize categorical columns 'ContractType' and 'PaymentMethod' to consistent capitalization", "One-hot encode categorical variables 'ContractType' and 'PaymentMethod'", "Convert target variable 'Churn' to binary (Yes=1, No=0), handling inconsistent capitalization", "Split dataset into training (80%) and testing (20%) sets with stratification by target", "Train a RandomForestClassifier with default hyperparameters on the training data", "Evaluate the model using accuracy, F1 score, precision, and recall on the test set", "Generate the confusion matrix for test set predictions", "Identify and report the top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.8, "recall": 0.86, "confusion_matrix": {"true_negative": 18, "false_positive": 3, "false_negative": 4, "true_positive": 19}, "top_feature_importances": {"MonthlyCharges": 0.32, "ContractType_Month-to-month": 0.27, "DataUsageGB": 0.15}}}
{"purpose": "Predict whether a customer will make a purchase during a browsing session based on session attributes and customer demographics.", "raw_table": "session_id,session_length_minutes,page_views,device_type,referrer,customer_age,customer_gender,purchase_made\nS001,12,5,Mobile,Google,34,Male,Yes\nS002,7,3,Desktop,facebook,28,Female,No\nS003,15,8,Tablet,Direct,45,Female,Yes\nS004,NaN,4,mobile,Google,38,Male,No\nS005,10,6,Desktop,Direct,NaN,Female,Yes\nS006,5,2,Mobile,Twitter,23,Male,No\nS007,20,10,tablet,Google,30,,Yes\nS008,8,4,Desktop,Direct,41,Female,No\nS009,3,1,Mobile,Facebook,29,Male,No\nS010,17,9,Desktop,google,36,Male,Yes\nS011,11,5,Mobile,Direct,27,Female,No\nS012,6,3,Desktop,Twitter,33,Male,No\nS013,14,7,Tablet,Facebook,39,Female,Yes\nS014,9,5,mobile,Direct,31,Male,No", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values: impute numeric columns with median and categorical with mode", "Normalize inconsistent capitalization in categorical columns (e.g., 'mobile', 'Mobile', 'tablet', 'Tablet', 'google', 'Google')", "Convert target variable 'purchase_made' to binary labels (Yes=1, No=0)", "One-hot encode categorical variables: device_type, referrer, customer_gender", "Split data into training (80%) and test (20%) sets", "Standardize numeric features: session_length_minutes, page_views, customer_age", "Train a RandomForestClassifier with 100 estimators on the training set", "Perform grid search over max_depth parameter (values: 3, 5, 7)", "Evaluate model performance on test set calculating accuracy, F1 score, precision, and recall", "Generate predictions and confusion matrix on test data"], "model_results": {"accuracy": 0.85, "f1": 0.82, "precision": 0.8, "recall": 0.85, "confusion_matrix": {"true_positive": 5, "true_negative": 8, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"session_length_minutes": 0.28, "page_views": 0.22, "device_type_Mobile": 0.15, "referrer_Google": 0.12, "customer_age": 0.1, "device_type_Tablet": 0.08, "customer_gender_Female": 0.05}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Build a classification model to predict if a taxi trip will have a high tip (>= 20% of fare).", "raw_table": "trip_id,passenger_count,trip_distance,payment_type,day_of_week,tip_percentage,high_tip\n1,1,2.5,Card,Monday,22,Yes\n2,2,5.1,Cash,Tuesday,15,No\n3,1,3.2,CARD,Wednesday,20,Yes\n4,3,,Card,thursday,18,No\n5,2,7.8,Cash,Friday,25,Yes\n6,1,1.0,card,Saturday,5,No\n7,1,4.3,Cash,Sunday,30,Yes\n8,,2.2,card,Monday,12,No\n9,2,3.6,Cash,Tuesday,17,No\n10,1,2.8,Card,Wednesday,21,Yes\n11,4,10.5,Cash,Thursday,35,Yes\n12,3,6.0,Card,Friday,19,No\n13,2,3.9,Cash,Saturday,23,Yes\n14,1,2.0,card,Sunday,16,No", "model_steps": ["Load CSV data into a DataFrame", "Fill missing passenger_count and trip_distance values with median", "Standardize inconsistent capitalization in payment_type and day_of_week columns", "Encode payment_type and day_of_week using one-hot encoding", "Convert target variable 'high_tip' to binary (Yes=1, No=0)", "Split data into train and test sets (80% train, 20% test)", "Standardize numeric features: passenger_count, trip_distance, tip_percentage", "Train a RandomForestClassifier on the training set", "Perform hyperparameter tuning over max_depth and n_estimators using 5-fold cross-validation", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.81, "recall": 0.88, "confusion_matrix": [[8, 2], [1, 7]], "top_feature_importances": {"tip_percentage": 0.45, "trip_distance": 0.25, "payment_type_Card": 0.15, "day_of_week_Friday": 0.08, "passenger_count": 0.07}, "best_hyperparameters": {"max_depth": 6, "n_estimators": 100}}}
{"purpose": "Predict whether a given day will experience extreme heat based on weather and environmental conditions.", "raw_table": "Date,AvgTemperature,Humidity,WindSpeed,Region,WeatherCondition,IsExtremeHeat\n2024-06-01,35.2,45,12,North,Sunny,Yes\n2024-06-02,28.5,55,8,South,Partly cloudy,No\n2024-06-03,NaN,60,15,East,Rainy,No\n2024-06-04,33.0,,10,West,sunny,Yes\n2024-06-05,29.8,50,7,North,Cloudy,no\n2024-06-06,31.5,48,11,South,Rainy,Yes\n2024-06-07,27.0,52,9,East,Sunny,No\n2024-06-08,36.1,43,13,West,Partly Cloudy,YES\n2024-06-09,30.2,49,NaN,North,Cloudy,No\n2024-06-10,34.7,44,14,South,Sunny,Yes\n2024-06-11,28.0,55,10,East,Partly cloudy,No\n2024-06-12,35.5,46,12,West,SUNNY,Yes\n2024-06-13,29.4,54,8,North,Rainy,No\n2024-06-14,31.0,50,11,South,Cloudy,Yes", "model_steps": ["Load data and parse Date column as datetime", "Impute missing AvgTemperature and Humidity using median values", "Fill missing WindSpeed with mean WindSpeed", "Standardize casing in WeatherCondition column and fill missing values as 'Unknown'", "Convert target variable IsExtremeHeat to binary (Yes=1, No=0) with case-insensitivity", "One-hot encode categorical features: Region and WeatherCondition", "Split data into train and test sets (80/20) stratified on target", "Standardize numeric features AvgTemperature, Humidity, WindSpeed using training set statistics", "Train a RandomForestClassifier with 100 trees and max_depth=5", "Evaluate model performance with accuracy, precision, recall, and F1 score on test set", "Calculate and display feature importances from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "top_feature_importances": {"AvgTemperature": 0.4, "Humidity": 0.25, "WeatherCondition_Sunny": 0.15, "WindSpeed": 0.1, "Region_West": 0.05, "WeatherCondition_Rainy": 0.05}}}
{"purpose": "Build a regression model to predict hourly electricity consumption based on weather and operational factors.", "raw_table": "Hour,Temperature_C,Weather_Condition,Day_Type,Operational_Status,Previous_Hour_Consumption_kWh,Electricity_Consumption_kWh\n0,15.2,Clear,weekday,ON,230.5,245.3\n1,14.8,clear,Weekday,ON,245.3,252.7\n2,14.5,Cloudy,Weekday,OFF,252.7,198.4\n3,14.3,Rainy,weekend,OFF,198.4,175.2\n4,13.9,Rainy,Weekend,OFF,175.2,169.8\n5,13.7,Clear,Weekend,ON,169.8,190.4\n6,14.0,Clear,weekday,ON,190.4,220.7\n7,16.5,Cloudy,Weekday,ON,220.7,240.0\n8,18.2,Clear,Weekday,ON,240.0,265.5\n9,20.1,clear,Weekday,ON,265.5,280.2\n10,22.5,Cloudy,Weekend,ON,280.2,295.1\n11,24.0,Rainy,weekend,ON,295.1,210.3\n12,25.5,Clear,Weekday,OFF,210.3,190.6\n13,26.1,CLEAR,Weekday,ON,190.6,260.4\n14,,Cloudy,Weekday,ON,260.4,275.8", "model_steps": ["Load dataset and inspect for missing values and inconsistencies", "Impute missing Temperature_C value using median temperature", "Standardize capitalization in categorical columns (Weather_Condition, Day_Type, Operational_Status)", "One-hot encode Weather_Condition, Day_Type, and Operational_Status", "Split data into training (80%) and test (20%) sets based on the Hour feature", "Standardize numeric features: Temperature_C and Previous_Hour_Consumption_kWh", "Train a GradientBoostingRegressor on the training set to predict Electricity_Consumption_kWh", "Tune hyperparameters max_depth and learning_rate via grid search with 5-fold cross-validation", "Evaluate the model on the test set using RMSE, MAE, and R-squared metrics", "Analyze feature importances to understand key drivers of electricity consumption"], "model_results": {"rmse": 12.5, "mae": 9.3, "r2": 0.87, "best_hyperparameters": {"max_depth": 4, "learning_rate": 0.1}, "top_feature_importances": {"Previous_Hour_Consumption_kWh": 0.45, "Temperature_C": 0.3, "Operational_Status_ON": 0.15, "Weather_Condition_Clear": 0.07, "Day_Type_Weekday": 0.03}}}
{"purpose": "Predict the likelihood of hospital readmission within 30 days for patients discharged with heart failure.", "raw_table": "PatientID,Age,Gender,AdmissionType,NumPriorAdmissions,BloodPressure,Cholesterol,DischargeDisposition,Readmitted\n001,68,Male,Emergency,2,130/85,high,Home,Yes\n002,74,Female,Elective,0,120/80,Normal,Rehab,No\n003,59,Female,Emergency,1,140/90,HIGH,home,Yes\n004,82,Male,Emergency,,150/95,High,,Yes\n005,70,Female,Elective,3,110/70,normal,Home,No\n006,65,Male,ELECTIVE,2,125/85,Normal,Rehab,No\n007,77,Male,Emergency,1,135/88,High,Home,Yes\n008,60,Female,emergency,0,,Normal,home,No\n009,73,Female,Emergency,1,145/92,high,Rehab,Yes\n010,69,Male,Elective,2,130/82,normal,Home,No\n011,75,Female,Emergency,1,140/90,High,Home,Yes\n012,80,Male,Elective,4,128/84,Normal,Home,No\n013,67,Female,Emergency,1,138/89,HIGH,Home,Yes\n014,64,Male,Elective,0,115/75,normal,Home,No", "model_steps": ["Parse the BloodPressure column to separate systolic and diastolic values into numeric columns", "Standardize inconsistent capitalization and missing values in categorical columns (AdmissionType, Cholesterol, DischargeDisposition)", "Fill missing values in NumPriorAdmissions and BloodPressure with median values", "Convert categorical variables (Gender, AdmissionType, Cholesterol, DischargeDisposition) using one-hot encoding", "Split the dataset into training (80%) and test sets (20%) with stratification on the target variable 'Readmitted'", "Standardize numeric features: Age, NumPriorAdmissions, SystolicBP, DiastolicBP", "Train a RandomForestClassifier to predict Readmitted", "Perform grid search over n_estimators (50,100) and max_depth (3,5) using 5-fold cross-validation on training data", "Evaluate model performance on the test set computing accuracy, F1 score, precision, and recall", "Generate and analyze the confusion matrix", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.79, "recall": 0.83, "confusion_matrix": {"True_Positive": 5, "True_Negative": 6, "False_Positive": 2, "False_Negative": 1}, "top_feature_importances": {"NumPriorAdmissions": 0.28, "SystolicBP": 0.22, "AdmissionType_Emergency": 0.18}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a social media user will engage with a sponsored post based on user and post features.", "raw_table": "user_id,age,gender,account_type,posts_last_week,avg_daily_time_min,post_category,target_engaged\n101,25,Male,standard,5,120,Tech,Yes\n102,,Female,Premium,7,150,Lifestyle,No\n103,32,male,Standard,3,NaN,TECH,Yes\n104,28,Female,premium,8,180,Gaming,No\n105,40,Male,Standard,0,95,Lifestyle,No\n106,35,Female,Standard,6,130,Gaming,Yes\n107,29,Female,Standard,4,abc,Tech,Yes\n108,22,Male,Premium,10,200,Lifestyle,Yes\n109,38,Female,standard,2,110,Gaming,No\n110,27,Male,Premium,,140,Tech,Yes\n111,31,Female,Standard,5,125,lifestyle,No\n112,26,Male,Standard,7,160,Gaming,Yes\n113,30,Female,standard,3,100,Tech,No\n114,29,Male,Premium,6,155,Lifestyle,Yes\n115,33,Female,standard,4,115,Gaming,No", "model_steps": ["Load the CSV data into a DataFrame", "Clean and standardize categorical columns: unify 'gender', 'account_type', and 'post_category' casing", "Impute missing numeric values in 'age', 'posts_last_week', and 'avg_daily_time_min' using median", "Convert the target variable 'target_engaged' to binary (Yes=1, No=0)", "One-hot encode categorical variables: 'gender', 'account_type', and 'post_category'", "Split the data into train and test sets with 80% training and 20% testing", "Standardize numeric features such as 'age', 'posts_last_week', and 'avg_daily_time_min'", "Train a GradientBoostingClassifier on the training set", "Perform hyperparameter tuning over learning rate and number of estimators using 5-fold cross-validation", "Evaluate the final model on the test set with accuracy, precision, recall, and F1 score", "Compute and display the confusion matrix", "Identify and report the top 3 most important features"], "model_results": {"accuracy": 0.82, "precision": 0.79, "recall": 0.85, "f1": 0.82, "confusion_matrix": {"true_positive": 22, "true_negative": 18, "false_positive": 5, "false_negative": 4}, "top_feature_importances": {"avg_daily_time_min": 0.32, "posts_last_week": 0.26, "post_category_Tech": 0.15}, "best_hyperparameters": {"learning_rate": 0.1, "n_estimators": 100}}}
{"purpose": "Predict hourly energy consumption of residential buildings based on weather and building characteristics.", "raw_table": "Hour,Temperature_C,Humidity,Building_Type,Heating_System,Energy_Consumption_kWh\n0,15.2,55,Detached,Gas,3.5\n1,14.8,58,Detached,Electric,3.8\n2,13.5,60,Townhouse,electric,3.1\n3,NaN,62,Apartment,Gas,2.7\n4,12.8,,Apartment,Gas,2.9\n5,12,59,Townhouse,Electric,3.0\n6,13,57,Detached,Gas,4.1\n7,15.5,53,Detached,GAS,5.0\n8,18.0,50,Apartment,electric,5.5\n9,20.1,48,Townhouse,Electric,6.1\n10,22.3,45,Detached,Gas,6.5\n11,24.0,40,Apartment,Electric,7.0\n12,25.6,35,Townhouse,Gas,7.3\n13,27.0,33,Apartment,Electric,7.5", "model_steps": ["Impute missing numeric values in Temperature_C and Humidity with median values", "Standardize numeric features Temperature_C and Humidity", "Normalize target variable Energy_Consumption_kWh using log transformation", "Clean and unify categorical values in Heating_System to lowercase", "One-hot encode categorical variables Building_Type and Heating_System", "Split data into training (80%) and testing sets (20%) randomly", "Train a Gradient Boosting Regressor on training data", "Tune hyperparameters: number of estimators and learning rate using 5-fold cross-validation", "Evaluate model performance on test set using RMSE, MAE, and R2 metrics", "Analyze feature importances from the trained model"], "model_results": {"rmse": 0.48, "mae": 0.35, "r2": 0.87, "hyperparameters": {"n_estimators": 100, "learning_rate": 0.1}, "top_feature_importances": {"Temperature_C": 0.42, "Hour": 0.25, "Building_Type_Detached": 0.12, "Heating_System_gas": 0.1, "Humidity": 0.08}}}
{"purpose": "Predict whether a manufactured part will pass quality inspection based on sensor readings and production parameters.", "raw_table": "Part_ID,Machine_ID,Operator_Shift,Temperature_C,Pressure_psi,Material_Type,Defect_Flag\n1,M01,Morning,350,85,TypeA,Pass\n2,m02,Evening,355,90,typeb,Fail\n3,M01,Night,NaN,88,TypeA,Pass\n4,M03,Morning,345,missing,TypeC,Fail\n5,M02,Evening,360,92,TypeB,Fail\n6,M01,Morning,348,87,TypeA,Pass\n7,M03,Night,355,89,TypeC,Pass\n8,M02,evening,362,91,TYPEB,Fail\n9,M01,Morning,349,86,TypeA,Pass\n10,M03,Night,351,88,TypeC,Fail\n11,M02,Evening,358,93,typeb,Fail\n12,M01,Morning,347,85,TypeA,Pass\n13,M03,Morning,NaN,90,TypeC,Fail", "model_steps": ["Load CSV data and identify target variable 'Defect_Flag'", "Standardize inconsistent capitalization in categorical columns (Machine_ID, Operator_Shift, Material_Type)", "Impute missing numeric values in 'Temperature_C' and 'Pressure_psi' using median imputation", "Encode categorical variables using one-hot encoding", "Split data into training (80%) and testing (20%) sets with stratification on target", "Standardize numeric features 'Temperature_C' and 'Pressure_psi'", "Train a RandomForestClassifier on training data", "Perform grid search on number of estimators (50, 100) and max_depth (None, 10)", "Evaluate model on test set with accuracy, precision, recall, and F1 score", "Generate and analyze confusion matrix"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.81, "recall": 0.85, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 2}, "top_feature_importances": {"Pressure_psi": 0.35, "Temperature_C": 0.3, "Material_Type_TypeB": 0.15, "Operator_Shift_Morning": 0.1, "Machine_ID_M01": 0.1}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Predict whether a retail customer will make a purchase during a promotional campaign.", "raw_table": "CustomerID,Age,Gender,AnnualIncome,MembershipStatus,LastPurchaseDaysAgo,PreferredStore,MadePurchase\nC001,34,Male,45000,Gold,12,Downtown,Yes\nC002,45,Female,54000,Silver,45,Suburb,no\nC003,29,Female,NaN,Platinum,7,Downtown,YES\nC004,NaN,Male,38000,Bronze,30,Online,No\nC005,39,female,47000,Gold,5,Online,Yes\nC006,50,Male,52000,Silver,NaN,suburb,No\nC007,41,Female,56000,Gold,18,Downtown,yes\nC008,36,Male,48000,bronze,60,Online,No\nC009,28,Female,43000,Silver,25,Downtown,Yes\nC010,34,Male,46000,Gold,15,Suburb,No\nC011,31,Female,47000,Gold,9,Downtown,Yes\nC012,NaN,Male,49000,Silver,20,Online,No\nC013,47,Female,53000,Gold,14,Downtown,YES\nC014,35,Male,42000,bronze,37,Suburb,No", "model_steps": ["Load the dataset and inspect for missing values and inconsistencies.", "Standardize capitalization in categorical columns: Gender, MembershipStatus, PreferredStore, and MadePurchase.", "Impute missing numeric values (Age, AnnualIncome, LastPurchaseDaysAgo) using median values.", "Encode target variable 'MadePurchase' as binary (Yes=1, No=0).", "One-hot encode categorical features: Gender, MembershipStatus, PreferredStore.", "Split data into train (80%) and test (20%) sets with stratification on the target variable.", "Standardize numeric features: Age, AnnualIncome, LastPurchaseDaysAgo.", "Train a RandomForestClassifier with default parameters on the training set.", "Perform grid search over max_depth parameter with values [3, 5, 7] using 5-fold cross-validation.", "Evaluate the best model on the test set calculating accuracy, precision, recall, and F1 score.", "Generate and analyze the confusion matrix to understand prediction errors.", "Identify top 3 feature importances from the trained RandomForest model."], "model_results": {"accuracy": 0.83, "precision": 0.79, "recall": 0.85, "f1": 0.82, "confusion_matrix": {"true_positive": 7, "false_positive": 2, "true_negative": 8, "false_negative": 1}, "top_feature_importances": {"MembershipStatus_Gold": 0.28, "LastPurchaseDaysAgo": 0.22, "AnnualIncome": 0.18}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a social media post will go viral based on post characteristics and user engagement features.", "raw_table": "post_id,user_type,post_length,num_hashtags,time_posted,platform,avg_engagement,last_post_engagement,viral\n1,Influencer,120,3,18,Instagram,250.5,300,Yes\n2,regular,45,1,9,facebook,50.2,40,No\n3,Regular,78,0,14,TWITTER,70.0,65,No\n4,Influencer,200,5,20,instagram,400.0,420,Yes\n5,guest,60,,22,Facebook,30.1,25,No\n6,Regular,90,2,8,Twitter,95.0,100,No\n7,INFLUENCER,150,4,19,Instagram,320.0,310,Yes\n8,regular,30,1,7,facebook,20.0,18,No\n9,Guest,85,3,21,TWITTER,110.0,115,Yes\n10,Regular,55,NaN,15,Instagram,80.0,75,No\n11,Influencer,170,6,16,Instagram,360.0,350,Yes\n12,guest,40,0,12,facebook,25.0,20,No\n13,Regular,95,2,10,Twitter,105.0,110,No\n14,INFLUENCER,130,3,13,Instagram,270.0,280,Yes\n15,Regular,50,1,6,facebook,45.0,50,No", "model_steps": ["Load the data and inspect for missing and inconsistent values", "Standardize capitalization in 'user_type' and 'platform' columns", "Impute missing values in 'num_hashtags' with the median", "Convert categorical variables 'user_type' and 'platform' to one-hot encoded features", "Convert target variable 'viral' to binary labels (Yes=1, No=0)", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features: post_length, num_hashtags, time_posted, avg_engagement, last_post_engagement", "Train a RandomForestClassifier to predict viral posts", "Perform grid search over max_depth and n_estimators hyperparameters", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Compute confusion matrix to analyze false positives and false negatives", "Identify top 3 important features based on feature importances from the model"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.83, "f1": 0.84, "confusion_matrix": {"true_positive": 5, "true_negative": 7, "false_positive": 1, "false_negative": 2}, "top_feature_importances": {"avg_engagement": 0.35, "user_type_Influencer": 0.25, "num_hashtags": 0.18}, "best_hyperparameters": {"max_depth": 7, "n_estimators": 100}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features and location.", "raw_table": "HouseID,Neighborhood,Bedrooms,Bathrooms,SquareFeet,YearBuilt,Garage,Price\n1,Downtown,3,2,1500,2005,Yes,350000\n2,Suburb,4,3,2500,2010,yes,480000\n3,Rural,2,1,900,1990,No,120000\n4,Suburb,3,2,1800,2015,Yes,390000\n5,Downtown,2,1.5,1300,2000,NO,330000\n6,rural,4,2.5,2100,2003,Yes,230000\n7,Suburb,3,,1700,2008,Yes,400000\n8,Downtown,3,2,1600,NaN,No,355000\n9,Rural,5,3,2600,1995,YES,250000\n10,Suburb,3,2,NaN,2012,No,420000\n11,Downtown,4,3,2200,2007,Yes,460000\n12,Suburb,3,2,1900,2013,Yes,\n13,Rural,3,2,1400,1985,No,180000\n14,Suburb,2,1,1200,2000,No,310000", "model_steps": ["Load data from CSV and inspect for missing or inconsistent values", "Normalize capitalization in categorical columns (Neighborhood, Garage)", "Impute missing numeric values (Bathrooms, SquareFeet, YearBuilt) using median values", "Fill missing target variable Price rows by removing those records from the dataset", "One-hot encode categorical features Neighborhood and Garage", "Split data into training (80%) and testing (20%) sets", "Standardize numeric features: Bedrooms, Bathrooms, SquareFeet, YearBuilt", "Train a Gradient Boosting Regressor model on the training data", "Tune hyperparameters max_depth and learning_rate with grid search using 5-fold cross-validation", "Evaluate model performance on test set using RMSE, MAE, and R2 metrics"], "model_results": {"rmse": 23000, "mae": 18000, "r2": 0.82, "best_hyperparameters": {"max_depth": 4, "learning_rate": 0.1}, "top_feature_importances": {"SquareFeet": 0.38, "Neighborhood_Suburb": 0.22, "YearBuilt": 0.15, "Bedrooms": 0.1, "Garage_Yes": 0.08, "Bathrooms": 0.07}}}
{"purpose": "Predict the likelihood of hospital readmission within 30 days for diabetic patients based on clinical and demographic data.", "raw_table": "PatientID,Age,Gender,HbA1c_Level,Blood_Pressure,Medication_Adherence,Previous_Readmissions,Smoking_Status,Readmitted_30days\n1,58,M,7.8,130/85,High,2,Yes,Yes\n2,67,f,8.1,125/80,medium,1,No,No\n3,45,M,9.2,140/90,Low,3,yes,Yes\n4,72,F,NA,135/88,High,0,No,No\n5,39,m,6.5,120/80,High,0,No,No\n6,55,F,7.0,128/82,Low,,No,No\n7,60,M,8.4,142/95,Medium,2,Yes,Yes\n8,48,F,7.9,130/87,high,1,No,No\n9,69,M,7.3,135/85,Low,2,Yes,Yes\n10,52,F,8.0,NA,Medium,1,no,No\n11,61,M,7.6,138/88,High,2,Yes,Yes\n12,50,f,6.8,125/80,Medium,0,no,No", "model_steps": ["Load the raw CSV data into a DataFrame", "Clean and standardize categorical columns: normalize 'Gender', 'Medication_Adherence', and 'Smoking_Status' values to consistent capitalization", "Impute missing numeric values for 'HbA1c_Level' and 'Blood_Pressure' using median and mode respectively", "Convert 'Blood_Pressure' from string format (e.g., '130/85') into two separate numeric features: systolic and diastolic pressure", "Encode categorical variables ('Gender', 'Medication_Adherence', 'Smoking_Status') using one-hot encoding", "Split the dataset into training (80%) and test sets (20%) stratified on the target variable 'Readmitted_30days'", "Standardize numeric features such as 'Age', 'HbA1c_Level', 'Systolic_BP', 'Diastolic_BP', 'Previous_Readmissions'", "Train a RandomForestClassifier to predict 'Readmitted_30days'", "Perform hyperparameter tuning using grid search over 'max_depth' and 'n_estimators'", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Identify and report the top 3 most important features"], "model_results": {"accuracy": 0.83, "precision": 0.8, "recall": 0.78, "f1": 0.79, "confusion_matrix": {"true_positive": 18, "true_negative": 25, "false_positive": 6, "false_negative": 5}, "top_feature_importances": {"Previous_Readmissions": 0.3, "HbA1c_Level": 0.25, "Medication_Adherence_High": 0.15}, "best_hyperparameters": {"max_depth": 7, "n_estimators": 100}}}
{"purpose": "Predict whether a customer will purchase a recommended product based on browsing and demographic data.", "raw_table": "customer_id,age,gender,browse_time_minutes,device_type,recommendation_clicked,target\n1,34,Male,12.5,Desktop,Yes,1\n2,28,Female,,Mobile,No,0\n3,45,Male,8.0,tablet,yes,1\n4,22,Female,5.5,Mobile,No,0\n5,,Male,15.0,Desktop,YES,1\n6,37,Female,10.0,Desktop,No,0\n7,29,Unknown,13.0,Mobile,Yes,1\n8,41,Male,7.5,Mobile,no,0\n9,33,Female,9.0,Tablet,No,0\n10,26,Female,11.0,desktop,Yes,1\n11,38,Male,missing,Tablet,No,0\n12,31,Female,14.0,Mobile,No,0", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Impute missing numeric values (age, browse_time_minutes) using median imputation", "Standardize column casing in categorical variables (e.g., device_type, recommendation_clicked)", "Encode categorical variables: one-hot encode device_type and gender, binary encode recommendation_clicked", "Split data into train and test sets with 80/20 ratio, stratifying by the target variable", "Standardize numeric features (age, browse_time_minutes) using StandardScaler", "Train a RandomForestClassifier with default hyperparameters on the training set", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Extract and analyze feature importances from the trained RandomForest model", "Generate confusion matrix to examine false positives and false negatives"], "model_results": {"accuracy": 0.83, "precision": 0.79, "recall": 0.85, "f1": 0.82, "feature_importances": {"recommendation_clicked_Yes": 0.42, "browse_time_minutes": 0.25, "device_type_Mobile": 0.1, "age": 0.12, "gender_Female": 0.06, "device_type_Desktop": 0.05}, "confusion_matrix": {"true_negatives": 5, "false_positives": 1, "false_negatives": 2, "true_positives": 7}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a student will pass or fail a final exam based on study habits and demographic information.", "raw_table": "StudentID,Hours_Studied,Attendance_Rate,Gender,Major,Previous_Grade,Participates_in_Clubs,Final_Result\n1,15,0.9,Male,Engineering,85,Yes,Pass\n2,8,0.75,Female,Biology,,No,Fail\n3,12,0.85,female,Engineering,78,yes,Pass\n4,5,0.60,Male,History,65,No,Fail\n5,10,0.80,Male,biology,72,Yes,Pass\n6,NaN,0.95,Female,Engineering,88,No,Pass\n7,7,missing,FEMALE,History,70,yes,Fail\n8,13,0.88,Male,Engineering,83,No,Pass\n9,4,0.55,Female,History,60,No,Fail\n10,9,0.78,Male,Biology,75,Yes,Pass\n11,11,0.82,Male,Engineering,80,Yes,Pass\n12,6,0.65,female,Biology,68,No,Fail\n13,14,0.92,Female,Engineering,90,YES,Pass\n14,3,0.50,Male,History,58,No,Fail", "model_steps": ["Load the dataset into a dataframe and inspect for missing and inconsistent values", "Clean the 'Hours_Studied' and 'Attendance_Rate' columns by imputing missing values with the median", "Standardize capitalization and values in categorical columns such as 'Gender' and 'Major'", "Convert categorical variables ('Gender', 'Major', 'Participates_in_Clubs') into one-hot encoded features", "Split the dataset into training (80%) and testing (20%) subsets, stratifying on the 'Final_Result' target", "Standardize numeric features 'Hours_Studied', 'Attendance_Rate', and 'Previous_Grade' using StandardScaler", "Train a RandomForestClassifier to predict 'Final_Result' (Pass/Fail)", "Perform grid search over 'n_estimators' and 'max_depth' hyperparameters to optimize model performance", "Evaluate the final model on the test set calculating accuracy, F1 score, precision, and recall", "Compute and analyze the confusion matrix to understand error types", "Identify the top 3 most important features influencing the prediction"], "model_results": {"accuracy": 0.86, "f1": 0.87, "precision": 0.89, "recall": 0.85, "confusion_matrix": [[8, 2], [2, 12]], "top_feature_importances": {"Hours_Studied": 0.38, "Attendance_Rate": 0.33, "Previous_Grade": 0.18}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 8}}}
{"purpose": "Predict customer churn probability for a telecom provider to improve retention strategies.", "raw_table": "CustomerID,Tenure,MonthlyCharges,Contract,PaymentMethod,InternetService,TechSupport,Churn\n001,12,70.35,Month-to-month,Electronic check,Fiber optic,Yes,Yes\n002,24,89.10,One year,Mailed check,Fiber optic,No,No\n003,,55.50,Month-to-month,Bank transfer,DSL,No,Yes\n004,6,45.00,Month-to-month,Electronic Check,DSL,yes,Yes\n005,36,99.90,Two Year,Credit card,Fiber optic,No,No\n006,48,80.75,One year,Electronic check,Fiber optic,Yes,No\n007,3,NaN,Month-to-month,Mailed check,DSL,No,Yes\n008,18,65.20,Month-to-month,bank transfer,Fiber optic,No,Yes\n009,30,85.10,Two year,Credit card,DSL,Yes,No\n010,15,59.99,Month-to-month,Electronic Check,Fiber optic,No,Yes\n011,9,60.50,Month-to-month,Electronic check,Fiber Optic,no,Yes\n012,60,105.20,Two Year,Credit Card,Fiber optic,Yes,No\n013,20,70.00,One year,Electronic check,DSL,Yes,No\n014,5,50.75,Month-to-month,Mailed Check,DSL,No,Yes", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Impute missing 'Tenure' and 'MonthlyCharges' values using median of the respective columns", "Standardize capitalization and spelling inconsistencies in categorical columns such as 'PaymentMethod' and 'TechSupport'", "Encode categorical variables using one-hot encoding for Contract, PaymentMethod, InternetService, and TechSupport", "Split data into training (80%) and test sets (20%) with stratification on the Churn target", "Standardize numeric features 'Tenure' and 'MonthlyCharges' using StandardScaler", "Train a RandomForestClassifier with default parameters on the training set", "Perform grid search cross-validation over 'max_depth' values [5, 10, 15] to optimize model performance", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Compute and analyze the confusion matrix", "Extract and rank top 5 feature importances from the trained model"], "model_results": {"accuracy": 0.83, "f1": 0.79, "precision": 0.77, "recall": 0.81, "confusion_matrix": {"true_negatives": 7, "false_positives": 2, "false_negatives": 3, "true_positives": 8}, "top_feature_importances": {"Contract_Month-to-month": 0.26, "InternetService_Fiber optic": 0.21, "TechSupport_No": 0.15, "MonthlyCharges": 0.13, "PaymentMethod_Electronic check": 0.08}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict hourly electricity consumption for residential customers based on weather and household characteristics.", "raw_table": "hour,temperature_c,humidity_percent,day_type,household_type,appliance_usage_kw,electricity_consumption_kw\n0,15.2,55,weekday,Apartment,1.2,1.5\n1,14.8,60,Weekday,apartment,1.1,1.4\n2,14.5,58,weekday,Detached,1.3,1.7\n3,14.0,57,Weekday,Detached,1.5,1.8\n4,13.8,missing,weekday,Townhouse,1.4,1.6\n5,13.5,59,weekday,Townhouse,1.6,1.9\n6,14.0,61,Weekend,Apartment,2.0,2.3\n7,16.5,65,weekend,Detached,2.5,2.9\n8,18.0,70,Weekend,Townhouse,2.7,3.1\n9,20.0,72,Weekday,apartment,3.0,3.5\n10,21.5,68,Weekday,Detached,3.1,3.6\n11,22.0,67,Weekday,Townhouse,3.2,3.7\n12,23.0,66,Weekend,Apartment,2.9,3.4\n13,22.5,65,weekend,Detached,3.3,3.8", "model_steps": ["Load the dataset from CSV string into a DataFrame", "Normalize inconsistent capitalization and fix 'missing' humidity values by imputing median humidity", "Convert 'day_type' and 'household_type' categorical columns to lowercase for consistency", "One-hot encode 'day_type' and 'household_type' categorical variables", "Split data into training (80%) and testing (20%) sets based on the hour index", "Standardize numeric features: 'temperature_c', 'humidity_percent', and 'appliance_usage_kw'", "Train a RandomForestRegressor to predict 'electricity_consumption_kw'", "Perform grid search to tune 'n_estimators' (50, 100) and 'max_depth' (5, 10)", "Evaluate model performance using RMSE, MAE, and R-squared on test set", "Extract and report top 3 feature importances", "Generate predicted vs actual consumption values for test data"], "model_results": {"rmse": 0.15, "mae": 0.12, "r2": 0.92, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}, "top_feature_importances": {"appliance_usage_kw": 0.45, "temperature_c": 0.3, "household_type_Detached": 0.1}, "predicted_vs_actual_sample": [{"actual": 1.8, "predicted": 1.75}, {"actual": 2.9, "predicted": 2.95}, {"actual": 3.7, "predicted": 3.65}]}}
{"purpose": "Predict whether a movie will be a box office hit based on its production and genre features.", "raw_table": "MovieID,Genre,DirectorExperienceYears,BudgetMillions,LeadActorPopularity,ReleaseSeason,IsBoxOfficeHit\n1,Action,10,150,High,Summer,Yes\n2,Comedy,5,30,medium,winter,No\n3,Drama,12,,Low,Fall,Yes\n4,Action,8,100,High,Summer,Yes\n5,Comedy,3,20,Medium,Spring,No\n6,Drama,6,50,low,Fall,No\n7,Thriller,7,80,Medium,Summer,Yes\n8,action,4,90,High,Summer,No\n9,Comedy,9,40,Medium,Spring,Yes\n10,Drama,11,70,Medium,Fall,Yes\n11,Thriller,,60,Medium,Winter,No\n12,Action,15,200,High,Summer,Yes\n13,Comedy,5,35,,Spring,No\n14,Drama,10,55,Low,Fall,Yes\n15,Thriller,8,85,Medium,Summer,Yes", "model_steps": ["Load the CSV data into a dataframe", "Handle missing values: impute missing BudgetMillions with median, and DirectorExperienceYears with median", "Normalize inconsistent capitalization in 'Genre' and 'LeadActorPopularity' columns", "Encode categorical variables 'Genre', 'LeadActorPopularity', and 'ReleaseSeason' using one-hot encoding", "Convert target variable 'IsBoxOfficeHit' to binary labels (Yes=1, No=0)", "Split the dataset into training (80%) and test sets (20%) randomly", "Standardize numeric features: DirectorExperienceYears and BudgetMillions", "Train a RandomForestClassifier with 100 trees on the training data", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Output feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.8, "precision": 0.78, "recall": 0.82, "f1": 0.8, "feature_importances": {"BudgetMillions": 0.35, "LeadActorPopularity_High": 0.2, "DirectorExperienceYears": 0.15, "Genre_Action": 0.1, "ReleaseSeason_Summer": 0.08, "LeadActorPopularity_Medium": 0.07, "Genre_Drama": 0.05}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features and location.", "raw_table": "PropertyID,Location,Size_sqft,Bedrooms,Bathrooms,YearBuilt,HasGarage,Condition,SalePrice\n1,Downtown,850,2,1,1998,Yes,good,320000\n2,Suburb,1250,3,2,2010,No,Excellent,450000\n3,suburb,900,2,1,2005,yes,Fair,350000\n4,Industrial Area,700,1,1,1980,No,Poor,200000\n5,Downtown,1100,3,2,2015,Yes,GOOD,480000\n6,Suburb,NaN,3,2,2012,Yes,Excellent,460000\n7,Rural,1300,4,3,1995,No,Fair,370000\n8,downtown,950,2,2,2000,Yes,Good,400000\n9,Suburb,1150,3,2,NaN,No,Excellent,430000\n10,Industrial Area,800,2,1,1985,No,poor,210000\n11,Rural,1400,4,3,1990,Yes,Fair,390000\n12,Suburb,1000,3,mixed,2008,Yes,excellent,440000\n13,Downtown,1050,3,2,2011,Yes,Good,460000\n14,Rural,1250,4,2,1997,No,Fair,380000", "model_steps": ["Load raw CSV data and inspect for inconsistencies", "Clean 'Bathrooms' column by converting non-numeric/mixed values to NaN and imputing with median", "Standardize capitalization inconsistencies in 'Location' and 'Condition' columns", "Impute missing numeric values ('Size_sqft', 'YearBuilt') with median values", "Encode categorical variables 'Location', 'Condition', and 'HasGarage' using one-hot encoding", "Split dataset into training (80%) and testing (20%) sets", "Standardize numeric features 'Size_sqft', 'Bedrooms', 'Bathrooms', and 'YearBuilt'", "Train a Gradient Boosting Regressor to predict SalePrice", "Perform hyperparameter tuning for number of estimators and learning rate using grid search", "Evaluate model performance on test set with RMSE, MAE, and R2 metrics", "Identify and report top 3 most important features"], "model_results": {"rmse": 23000, "mae": 18000, "r2": 0.87, "top_feature_importances": {"Size_sqft": 0.42, "Location_Suburb": 0.27, "Condition_Excellent": 0.15}, "best_hyperparameters": {"n_estimators": 150, "learning_rate": 0.05}}}
{"purpose": "Build a classification model to predict whether a taxi trip will experience heavy traffic delay.", "raw_table": "trip_id,driver_id,trip_distance_km,pickup_hour,day_of_week,weather_condition,passenger_count,traffic_delay\n1,D102,5.4,08,Monday,Clear,2,No\n2,d107,12.1,18,Friday,Rain,1,Yes\n3,D103,3.2,09,Tuesday,clear,3,No\n4,D105,7.0,17,Wednesday,Fog,1,Yes\n5,d106,15.5,22,Thursday,Rain,2,Yes\n6,D104,8.8,07,Monday,Clear,NaN,No\n7,D108,6.3,14,Saturday,Clear,1,No\n8,D101,10.0,20,Sunday,Snow,2,Yes\n9,D109,4.5,12,Friday,Clear,2,No\n10,D110,9.7,16,Thursday,Fog,1,Yes\n11,D111,5.0,06,Monday,Clear,2,No\n12,D112,,19,Friday,Rain,3,Yes\n13,D113,11.2,21,Saturday,Fog,1,Yes\n14,D114,7.5,13,Wednesday,clear,2,No", "model_steps": ["Load the CSV data and handle missing values by imputing median for numeric and mode for categorical columns", "Normalize 'trip_distance_km' and 'pickup_hour' numeric features", "Standardize 'passenger_count' after imputation", "One-hot encode categorical features: 'day_of_week' and 'weather_condition' after normalizing capitalization", "Convert target variable 'traffic_delay' to binary label (Yes=1, No=0)", "Split the data into training (80%) and testing (20%) sets with stratification on the target", "Train a GradientBoostingClassifier on the training set with default hyperparameters", "Evaluate the model on the test set calculating accuracy, precision, recall, and F1-score", "Extract and rank feature importances from the trained model", "Generate and analyze the confusion matrix on the test data"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.78, "f1": 0.8, "confusion_matrix": [[18, 4], [3, 14]], "top_feature_importances": {"trip_distance_km": 0.35, "pickup_hour": 0.22, "weather_condition_Rain": 0.15, "day_of_week_Friday": 0.1, "passenger_count": 0.08}, "hyperparameters": {"n_estimators": 100, "learning_rate": 0.1, "max_depth": 3}}}
{"purpose": "Predict whether a customer will make a purchase during a website visit based on their behavior and demographics.", "raw_table": "CustomerID,Age,Gender,Device,TimeOnSite,PagesVisited,PreviousPurchases,Purchase\n1,25,Male,Mobile,5.2,8,2,Yes\n2,34,Female,Desktop,3.5,5,,No\n3,47,Male,mobile,7.1,12,5,Yes\n4,29,Female,Tablet,2.8,3,1,No\n5,,Female,Mobile,4.6,7,0,No\n6,40,Male,Desktop,6.0,10,3,Yes\n7,23,Female,Desktop,1.5,2,0,No\n8,37,Male,Tablet,5.9,9,2,Yes\n9,31,MALE,Mobile,,6,1,No\n10,45,Female,Desktop,4.0,8,4,Yes\n11,28,Female,tablet,3.3,4,1,No\n12,50,Male,Mobile,7.5,11,6,Yes\n13,38,Female,Desktop,5.1,7,2,No\n14,36,,Mobile,4.7,6,1,No", "model_steps": ["Load the CSV data into a DataFrame", "Handle missing values by imputing median for numeric columns and mode for categorical columns", "Standardize inconsistent capitalizations in 'Gender' and 'Device' columns", "Convert categorical variables ('Gender' and 'Device') into one-hot encoded features", "Split the dataset into training (80%) and testing (20%) sets, stratified on the target 'Purchase'", "Standardize numeric features: Age, TimeOnSite, PagesVisited, PreviousPurchases", "Train a RandomForestClassifier to predict 'Purchase' status", "Perform grid search to tune max_depth parameter with values [3, 5, 7]", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix to analyze false positives and false negatives"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.88, "f1": 0.85, "best_hyperparameters": {"max_depth": 5}, "feature_importances": {"TimeOnSite": 0.32, "PagesVisited": 0.28, "PreviousPurchases": 0.2, "Age": 0.1, "Gender_Female": 0.05, "Gender_Male": 0.03, "Device_Desktop": 0.015, "Device_Mobile": 0.01, "Device_Tablet": 0.005}, "confusion_matrix": {"true_positives": 15, "true_negatives": 10, "false_positives": 3, "false_negatives": 2}}}
{"purpose": "Predict whether a customer will make a repeat purchase within 30 days after their first order.", "raw_table": "customer_id,age,gender,avg_order_value,last_order_days_ago,membership_status,preferred_device,repeat_purchase\n1,34,Male,120.50,15,Gold,Mobile,Yes\n2,22,Female,85.00,45,Silver,Desktop,No\n3,45,Male,NaN,5,Gold,Mobile,Yes\n4,29,Female,60.75,,Bronze,tablet,No\n5,38,male,110.00,20,Gold,Mobile,YES\n6,31,Female,95.20,35,Silver,Desktop,No\n7,27,,70.00,50,Bronze,Laptop,No\n8,42,Female,130,10,Gold,Mobile,Yes\n9,23,Male,55.50,60,bronze,Tablet,No\n10,36,Female,100.00,25,Silver,mobile,Yes\n11,30,Male,75.25,40,Silver,Desktop,No\n12,40,Female,NaN,8,Gold,Mobile,Yes\n13,33,Female,90.00,30,Silver,Desktop,No\n14,35,Male,115.00,12,Gold,Mobile,Yes", "model_steps": ["Impute missing values in avg_order_value with the median value", "Impute missing last_order_days_ago values with the mean value", "Standardize numeric columns: age, avg_order_value, last_order_days_ago", "Normalize and unify categorical columns: gender (treat missing as 'Unknown'), membership_status (normalize capitalization), preferred_device (normalize capitalization)", "One-hot encode categorical variables: gender, membership_status, preferred_device", "Convert target variable repeat_purchase to binary (Yes=1, No=0)", "Split data into train and test sets with an 80/20 ratio, stratifying on repeat_purchase", "Train a Logistic Regression classifier with L2 regularization", "Perform hyperparameter tuning over regularization strength (C) using 5-fold cross-validation", "Evaluate final model on test set computing accuracy, precision, recall, and F1 score", "Generate confusion matrix and analyze feature coefficients for interpretation"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "confusion_matrix": {"true_positive": 6, "true_negative": 7, "false_positive": 1, "false_negative": 2}, "top_features": {"membership_status_Gold": 1.25, "avg_order_value": 0.95, "last_order_days_ago": -0.85, "preferred_device_Mobile": 0.65, "gender_Female": 0.3}, "hyperparameters": {"C": 1.0, "penalty": "l2", "solver": "liblinear"}}}
{"purpose": "Predict whether a loan applicant will default on their loan based on financial and demographic features.", "raw_table": "ApplicantID,Income,EmploymentStatus,CreditScore,LoanAmount,LoanPurpose,Defaulted\n1,55000,Employed,720,20000,Home improvement,No\n2,NaN,Self-employed,650,15000,Debt consolidation,Yes\n3,47000,employed,700,18000,Home Improvement,No\n4,62000,Unemployed,580,22000,Car,Yes\n5,85000,Employed,800,25000,Debt Consolidation,No\n6,43000,Self-Employed,NaN,13000,Medical,Yes\n7,39000,Employed,600,9000,Medical,No\n8,,Unemployed,580,12000,car,Yes\n9,67000,Employed,700,21000,Home improvement,No\n10,72000,Employed,730,NaN,Debt consolidation,No\n11,51000,Self-employed,680,17000,Car,Yes\n12,48000,Employed,640,15000,medical,No\n13,53000,Employed,690,16000,Debt Consolidation,No\n14,60000,Unemployed,620,14000,Car,Yes\n15,58000,Self-employed,710,20000,Home Improvement,No", "model_steps": ["Load the CSV dataset into a DataFrame", "Clean and impute missing numeric values with median", "Standardize capitalization in categorical columns like EmploymentStatus and LoanPurpose", "Convert 'Defaulted' target variable to binary (Yes=1, No=0)", "One-hot encode categorical variables EmploymentStatus and LoanPurpose", "Split data into training and test sets (80/20 split with stratification on Defaulted)", "Standardize numeric features Income, CreditScore, LoanAmount using training set statistics", "Train a RandomForestClassifier with 100 trees", "Perform grid search over max_depth (values 5, 10, 15) using 5-fold cross-validation", "Evaluate model on test set computing accuracy, F1 score, precision, and recall", "Extract feature importances from the best model"], "model_results": {"accuracy": 0.8, "f1": 0.75, "precision": 0.78, "recall": 0.72, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}, "top_feature_importances": {"CreditScore": 0.32, "LoanAmount": 0.22, "EmploymentStatus_Self-employed": 0.15, "Income": 0.12, "LoanPurpose_Debt Consolidation": 0.1}, "confusion_matrix": {"True Negative": 30, "False Positive": 8, "False Negative": 7, "True Positive": 25}}}
{"purpose": "Predict whether a student will pass the final exam based on demographic and academic performance features.", "raw_table": "StudentID,Age,Gender,Hours_Studied,Attendance_Rate,Previous_Grade,Parental_Education,Passed_Final\nS001,17,Female,35,0.90,A,College,Yes\nS002,18,male,12,0.75,B,Highschool,No\nS003,17,Female,25,0.85,C,College,Yes\nS004,16,Male,15,,B,highschool,No\nS005,17,Female,40,0.95,A,College,Yes\nS006,18,Male,5,0.60,D,HighSchool,No\nS007,17,Female,,0.80,B,College,Yes\nS008,16,Male,20,0.70,C,Highschool,No\nS009,18,Female,30,0.88,B,College,Yes\nS010,17,Male,10,0.65,C,College,No\nS011,16,Female,22,0.92,B,College,Yes\nS012,17,Male,18,missing,D,Highschool,No\nS013,18,Female,38,0.98,A,College,Yes\nS014,16,Male,8,0.55,C,highschool,No", "model_steps": ["Load dataset and observe data quality issues such as missing values and inconsistent capitalization in categorical columns", "Impute missing numeric values in Hours_Studied and Attendance_Rate with median values", "Standardize capitalization and unify categories in Parental_Education and Gender columns", "Convert categorical variables Gender and Parental_Education into one-hot encoded features", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features Hours_Studied, Age, and Attendance_Rate", "Train a RandomForestClassifier to predict Passed_Final", "Perform grid search to tune max_depth between 3 and 10", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Identify and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.89, "recall": 0.84, "f1": 0.86, "confusion_matrix": {"true_positive": 6, "false_positive": 1, "true_negative": 5, "false_negative": 1}, "top_feature_importances": {"Hours_Studied": 0.43, "Attendance_Rate": 0.29, "Previous_Grade_B": 0.15}, "best_max_depth": 7}}
{"purpose": "Predict whether a student will pass the final exam based on demographic and study habit data.", "raw_table": "StudentID,Age,Gender,StudyHoursPerWeek,AttendanceRate,PreviousGrade,ParticipationLevel,PassedFinal\n1,19,Male,12,0.95,A,High,Yes\n2,20,Female,8,0.85,B,medium,Yes\n3,18,female,5,0.70,C,Low,No\n4,21,Male,15,0.90,B,High,YES\n5,22,Female,NaN,0.80,C,Medium,No\n6,20,Male,7,missing,B,Medium,No\n7,19,Male,10,0.88,D,low,No\n8,18,Female,9,0.92,A,HIGH,Yes\n9,21,Male,6,0.75,C,Medium,No\n10,22,Female,11,0.97,B,High,Yes\n11,20,Male,14,0.85,B,Medium,Yes\n12,19,Female,4,0.60,D,low,No\n13,21,Male,13,N/A,B,High,Yes\n14,20,Female,7,0.80,C,Medium,No", "model_steps": ["Load the data and identify the target variable 'PassedFinal' as binary classification", "Clean and standardize categorical values for 'Gender' and 'ParticipationLevel', fixing capitalization inconsistencies", "Impute missing numeric values in 'StudyHoursPerWeek' and 'AttendanceRate' using median imputation", "Encode categorical variables 'Gender' and 'ParticipationLevel' using one-hot encoding", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features 'Age', 'StudyHoursPerWeek', and 'AttendanceRate'", "Train a RandomForestClassifier on the training data", "Perform grid search for hyperparameters 'n_estimators' and 'max_depth' using cross-validation", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix to analyze classification errors", "Identify and report top 3 most important features contributing to the prediction"], "model_results": {"accuracy": 0.86, "f1": 0.85, "precision": 0.88, "recall": 0.82, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 2}, "top_features": {"StudyHoursPerWeek": 0.34, "AttendanceRate": 0.29, "ParticipationLevel_High": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a retail customer will make a purchase during their visit based on demographics and browsing behavior.", "raw_table": "CustomerID,Age,Gender,MembershipLevel,TimeOnSiteMinutes,PagesViewed,DeviceType,PreviousPurchases,MadePurchase\n1,25,Male,Gold,15,7,Mobile,3,Yes\n2,34,FEMALE,Silver,8,3,Desktop,1,No\n3,28,Male,,12,5,Mobile,0,Yes\n4,47,Female,Gold,20,12,Tablet,8,Yes\n5,,Male,Silver,5,2,Desktop,2,No\n6,31,Female,bronze,10,4,MOBILE,0,No\n7,22,Male,Gold,17,9,Tablet,5,Yes\n8,39,Female,Silver,11,6,Desktop,4,No\n9,30,Male,Gold,14,8,Mobile,6,Yes\n10,45,,Silver,7,3,Desktop,1,No\n11,35,Female,Gold,18,11,Tablet,7,Yes\n12,27,Male,Bronze,9,5,mobile,0,No\n13,40,Female,Silver,13,7,Desktop,3,No\n14,29,Male,Gold,16,10,Mobile,4,Yes\n15,33,FEMALE,Silver,10,5,Tablet,1,No", "model_steps": ["Load the raw data into a DataFrame", "Clean messy values: standardize Gender and DeviceType to consistent capitalization, impute missing Age values with median age, and fill missing MembershipLevel with mode", "Convert categorical variables Gender, MembershipLevel, and DeviceType into one-hot encoded features", "Split the data into training (80%) and test (20%) sets", "Standardize numeric features Age, TimeOnSiteMinutes, PagesViewed, and PreviousPurchases", "Train a LogisticRegression classifier to predict MadePurchase (Yes/No)", "Perform hyperparameter tuning for regularization strength using grid search with 5-fold cross-validation", "Evaluate the final model on the test set calculating accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 important features from model coefficients"], "model_results": {"accuracy": 0.87, "precision": 0.84, "recall": 0.9, "f1": 0.87, "confusion_matrix": [[10, 2], [3, 15]], "top_features": {"TimeOnSiteMinutes": 1.25, "MembershipLevel_Gold": 0.95, "PagesViewed": 0.8}, "best_hyperparameter_C": 1.0}}
{"purpose": "Predict whether a customer will make a purchase in their next session based on browsing behavior and demographics.", "raw_table": "customer_id,age,gender,browser,session_duration,page_views,purchase\nC001,25,Male,Chrome,300,15,Yes\nC002,NaN,Female,firefox,120,5,No\nC003,32,Female,Safari,230,8,Yes\nC004,45,Male,Edge,NaN,12,No\nC005,28,Female,chrome,180,NaN,Yes\nC006,37,Male,Firefox,210,10,No\nC007,23,Female,Safari,150,7,yes\nC008,40,Male,EDGE,400,20,No\nC009,35,Female,Chrome,270,11,No\nC010,29,Male,fireFox,190,9,Yes\nC011,31,Other,Chrome,200,10,No\nC012,NaN,Female,Safari,160,6,No\nC013,50,Male,Edge,350,15,Yes\nC014,27,Female,Chrome,,8,No", "model_steps": ["Load the data from CSV into a DataFrame", "Clean target variable 'purchase' to standardize values to binary (Yes/No)", "Impute missing numeric values with median (for 'age', 'session_duration', 'page_views')", "Standardize capitalization and unify categories in 'browser' and 'gender' columns", "One-hot encode categorical variables 'gender' and 'browser'", "Split data into training (80%) and test (20%) sets with stratification on target", "Standardize numeric features 'age', 'session_duration', and 'page_views' using training set statistics", "Train a Logistic Regression classifier to predict purchase", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze false positives and false negatives"], "model_results": {"accuracy": 0.79, "precision": 0.76, "recall": 0.82, "f1": 0.79, "confusion_matrix": {"true_positive": 9, "false_positive": 3, "true_negative": 13, "false_negative": 2}, "top_feature_importances": {"session_duration": 0.45, "page_views": 0.3, "browser_Chrome": 0.12, "age": 0.08, "gender_Female": 0.05}}}
{"purpose": "Predict whether a field's soil moisture level is sufficient for optimal crop growth based on environmental and soil features.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Temperature_C,Soil_pH,Previous_Crop,Soil_Moisture_Sufficient\nF001,Loam,120,22.5,6.8,Corn,Yes\nF002,Clay,85,19.3,7.1,wheat,No\nF003,SAND,100,23.0,6.5,Corn,Yes\nF004,Silt,missing,20.4,6.9,soybean,Yes\nF005,Loam,95,21.7,7.0,Wheat,No\nF006,Clay,110,22.1,6.7,corn,Yes\nF007,Loam,88,21.0,6.6,Soybean,No\nF008,Silt,105,24.3,6.8,corn,Yes\nF009,Sand,90,19.5,missing,Wheat,No\nF010,Clay,115,22.8,6.9,soybean,Yes\nF011,Loam,102,20.0,7.2,Corn,Yes\nF012,Silt,97,21.5,6.7,soybean,No\nF013,Loam,80,18.9,6.9,soybean,No\nF014,Sand,108,23.2,6.5,Wheat,Yes\nF015,Clay,99,22.0,missing,corn,No", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing and inconsistent values", "Standardize the capitalization in categorical columns Soil_Type and Previous_Crop to ensure consistency", "Impute missing numeric values in Rainfall_mm and Soil_pH using median values grouped by Soil_Type", "Encode categorical variables Soil_Type and Previous_Crop using one-hot encoding", "Split the dataset into training (80%) and test (20%) sets with stratification on the target Soil_Moisture_Sufficient", "Standardize numeric features Rainfall_mm, Temperature_C, and Soil_pH using z-score normalization", "Train a RandomForestClassifier to predict Soil_Moisture_Sufficient", "Perform grid search over hyperparameters n_estimators=[50,100], max_depth=[5,10] using 5-fold cross-validation on training data", "Evaluate the best model on the test set and compute accuracy, F1 score, precision, and recall", "Generate and analyze the confusion matrix to assess model performance on both classes", "Identify and report the top 3 most important features influencing the prediction"], "model_results": {"accuracy": 0.87, "f1": 0.85, "precision": 0.83, "recall": 0.88, "confusion_matrix": {"True_Positive": 7, "True_Negative": 6, "False_Positive": 2, "False_Negative": 1}, "top_feature_importances": {"Rainfall_mm": 0.34, "Soil_Type_Loam": 0.25, "Soil_pH": 0.18}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Predict customer churn probability for a telecommunications provider.", "raw_table": "customer_id,tenure_months,contract_type,monthly_charges,internet_service,tech_support,churn\nC001,12,Month-to-month,70.35,Fiber optic,Yes,No\nC002,48,One year,55.20,DSL,No,No\nC003,3,Month-to-month,85.10,Fiber Optic,No,YES\nC004,24,Two year,40.75,DSL,Yes,no\nC005,,Month-to-month,NaN,Fiber optic,no,Yes\nC006,36,One Year,60.00,No,No,No\nC007,5,month-to-month,78.99,Fiber optic,Yes,yes\nC008,60,Two Year,45.50,DSL,Yes,No\nC009,8,Month-to-month,90.20,Fiber optic,No,Yes\nC010,18,One year,55.60,DSL,No,No\nC011,15,Month-to-month,72.30,Fiber optic,NO,Yes\nC012,30,Two year,40.00,DSL,Yes,No\nC013,2,month-To-month,95.00,Fiber optic,No,Yes", "model_steps": ["Load the data and inspect for missing and inconsistent values", "Impute missing tenure_months with median value", "Standardize capitalization of categorical variables (e.g., contract_type, internet_service, tech_support, churn)", "Convert churn column to binary (Yes=1, No=0)", "One-hot encode categorical variables: contract_type, internet_service, tech_support", "Standardize numeric features: tenure_months, monthly_charges", "Split data into training (80%) and testing (20%) sets", "Train a RandomForestClassifier on training data", "Perform grid search over n_estimators and max_depth parameters", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix for test predictions", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.82, "precision": 0.79, "recall": 0.86, "confusion_matrix": [[7, 2], [1, 7]], "top_feature_importances": {"contract_type_Month-to-month": 0.3, "internet_service_Fiber optic": 0.25, "monthly_charges": 0.2}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 8}}}
{"purpose": "Predict whether a movie will be a box office hit based on its attributes before release.", "raw_table": "MovieID,Genre,Director,BudgetMillion,RuntimeMinutes,LeadActor,ReleaseMonth,BoxOfficeHit\n1,Action,Smith,150,130,John Doe,July,Yes\n2,Comedy,Johnson,50,95,Jane Roe,december,No\n3,Drama,Adams,30,110,Richard Roe,March,No\n4,Action,Smith,200,140,John Doe,July,Yes\n5,Comedy,Lee,40,100,Anne Ray,November,No\n6,Drama,Adams,45,,Richard Roe,March,No\n7,Action,Smith,180,135,John Doe,July,Yes\n8,Comedy,lee,60,90,Anne Ray,December,No\n9,Drama,Adams,35,115,Richard Roe,April,No\n10,Action,Smith,170,128,John Doe,August,Yes\n11,Comedy,Johnson,55,97,Jane Roe,December,No\n12,Drama,adams,50,112,Richard Roe,March,No\n13,Action,Smith,160,132,John Doe,July,Yes\n14,Comedy,Lee,58,92,Anne Ray,November,No", "model_steps": ["Load dataset and identify target variable 'BoxOfficeHit' as binary classification target", "Correct inconsistent capitalization in 'Director' and 'ReleaseMonth' columns", "Impute missing values in 'RuntimeMinutes' column with median runtime", "One-hot encode categorical variables: 'Genre', 'Director', 'LeadActor', 'ReleaseMonth'", "Standardize numeric features: 'BudgetMillion', 'RuntimeMinutes'", "Split data into training (80%) and testing (20%) sets stratified by target", "Train a RandomForestClassifier with 100 trees and max_depth=5", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Extract feature importances from the trained Random Forest", "Generate confusion matrix for test set predictions"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.88, "recall": 0.81, "top_feature_importances": {"Genre_Action": 0.28, "BudgetMillion": 0.22, "ReleaseMonth_July": 0.18, "Director_Smith": 0.15, "RuntimeMinutes": 0.1}, "confusion_matrix": {"true_positive": 6, "true_negative": 7, "false_positive": 1, "false_negative": 2}}}
{"purpose": "Build a classification model to predict whether a wheat crop yield will be high or low based on environmental and soil features.", "raw_table": "temperature_c,soil_type,rainfall_mm,pesticide_use,crop_variety,yield_category\n22.5,Loam,120,Yes,VarA,High\n19.0,Clay,85,no,VarB,Low\n25.1,SAND,110,Yes,VarA,High\n20.0,Loam,?,yes,VarC,Low\n23.3,Clay,95,Yes,varB,High\n21.7,Loam,105,No,VarA,High\n18.5,Clay,90,No,VarC,Low\n24.0,Sand,115,yes,VarA,High\n20.8,Clay,100,No,VarB,Low\n22.2,Loam,?,Yes,VarA,High\n21.0,Loam,98,yes,VarC,Low\n19.5,Clay,88,No,varB,Low\n23.7,Sand,112,Yes,VarA,High", "model_steps": ["Load the dataset and identify missing values and inconsistent capitalization", "Standardize categorical columns by lowercasing and correcting 'yes'/'no' values", "Impute missing values in 'rainfall_mm' with median rainfall", "One-hot encode categorical variables: 'soil_type', 'pesticide_use', and 'crop_variety'", "Split data into train and test sets (80/20 split)", "Standardize numeric features: 'temperature_c' and 'rainfall_mm'", "Train a RandomForestClassifier to predict 'yield_category'", "Perform grid search over number of trees (n_estimators) and max_depth", "Evaluate model performance using accuracy, F1 score, precision, and recall on test set", "Generate a confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.85, "f1": 0.86, "precision": 0.83, "recall": 0.89, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"rainfall_mm": 0.32, "pesticide_use_yes": 0.25, "temperature_c": 0.2}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict the likelihood of a machine part failure within the next production cycle based on sensor readings and operational conditions.", "raw_table": "Part_ID,Temperature,Pressure,Operator_Shift,Material_Batch,Failure\nP001,75.3,101.2,Morning,Batch_A,no\nP002,79.1,NaN,Night,Batch_B,Yes\nP003,80.0,99.8,morning,Batch_A,No\nP004,85.5,105.1,Night,batch_c,Yes\nP005,NaN,102.3,Evening,Batch_B,No\nP006,77.0,100.5,Evening,Batch_B,No\nP007,82.1,103.2,Morning,Batch_C,YES\nP008,74.5,98.7,Night,Batch_A,No\nP009,79.8,101.8,evening,Batch_A,No\nP010,83.3,104.0,Morning,batch_C,Yes\nP011,76.4,NaN,Evening,Batch_B,No\nP012,81.0,102.7,Night,Batch_C,Yes\nP013,78.5,100.0,Morning,Batch_A,No\nP014,NaN,99.5,Night,Batch_B,No", "model_steps": ["Load dataset and identify target variable 'Failure' as binary classification (Yes/No).", "Clean 'Failure' column by standardizing all entries to lower case and mapping 'yes'/'Yes'/'YES' to 1 and 'no'/'No'/'NO' to 0.", "Handle missing values in numerical columns 'Temperature' and 'Pressure' using median imputation.", "Normalize 'Temperature' and 'Pressure' features using standard scaling.", "Standardize categorical columns 'Operator_Shift' and 'Material_Batch' by converting to lowercase and correcting inconsistent capitalization.", "One-hot encode the categorical features 'Operator_Shift' and 'Material_Batch'.", "Split data into training and test sets with 80% training and 20% testing, stratifying by the target variable to maintain class balance.", "Train a RandomForestClassifier with default hyperparameters on the training set.", "Perform grid search over 'max_depth' parameter with values [3, 5, 7] using 5-fold cross-validation to optimize F1 score.", "Retrain RandomForestClassifier with the best max_depth on the full training data.", "Evaluate model performance on the test set, computing accuracy, precision, recall, and F1 score.", "Compute and report the confusion matrix and the top 3 feature importances from the trained model."], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.78, "f1": 0.8, "confusion_matrix": {"true_negative": 7, "false_positive": 2, "false_negative": 3, "true_positive": 6}, "top_feature_importances": {"Temperature": 0.35, "Pressure": 0.3, "Operator_Shift_night": 0.15}, "best_max_depth": 5}}
{"purpose": "Predict whether a loan applicant will default on their loan based on financial and demographic features.", "raw_table": "ApplicantID,Age,Income,EmploymentStatus,MaritalStatus,LoanAmount,CreditScore,Defaulted\n1,34,58000,Full-time,married,15000,720,No\n2,45,NaN,Part-time,Single,8000,690,Yes\n3,29,45000,Self-employed,Married,12000,NaN,No\n4,NaN,52000,full-time,Divorced,10000,710,No\n5,50,67000,Full-time,Single,20000,680,Yes\n6,38,59000,Part-Time,Married,15000,700,No\n7,41,61000,Full-time,,17000,695,Yes\n8,36,58000,Self-employed,Single,16000,720,No\n9,27,43000,Full-Time,Single,NaN,680,Yes\n10,44,66000,Full-time,Married,21000,715,No\n11,31,47000,part-time,Single,13000,690,No\n12,39,60000,Full-time,Married,18000,705,Yes\n13,NaN,50000,Self-employed,Single,14000,710,No\n14,35,55000,full-time,Married,15000,NaN,No", "model_steps": ["Load data and identify missing values and inconsistent capitalization in EmploymentStatus and MaritalStatus columns", "Impute missing numeric values (Age, Income, LoanAmount, CreditScore) using median imputation", "Fill missing categorical values (MaritalStatus) with the mode", "Standardize EmploymentStatus values to consistent lowercase categories and encode categorical variables using one-hot encoding", "Split data into training (80%) and testing (20%) sets", "Standardize numeric features (Age, Income, LoanAmount, CreditScore) using StandardScaler fitted on training data", "Train a RandomForestClassifier to predict Defaulted using training data", "Tune max_depth hyperparameter using 5-fold cross-validation on training set", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Compute confusion matrix on test predictions", "Identify top 3 feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.79, "precision": 0.75, "recall": 0.68, "f1": 0.71, "confusion_matrix": [[8, 2], [3, 7]], "top_feature_importances": {"CreditScore": 0.32, "LoanAmount": 0.21, "Income": 0.18}, "hyperparameters": {"max_depth": 7, "n_estimators": 100, "random_state": 42}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features and location.", "raw_table": "ID,Neighborhood,HouseStyle,OverallQual,YearBuilt,TotalBsmtSF,GrLivArea,GarageCars,SaleCondition,SalePrice\n1,CollgCr,2Story,7,2003,856,1710,2,Normal,208500\n2,Veenker,1Story,6,1976,1262,1262,2,Abnorml,181500\n3,CollgCr,2Story,7,2001,920,1786,2,Normal,223500\n4,Crawfor,2Story,7,1915,756,1717,3,Normal,140000\n5,NoRidge,1Story,8,2000,1145,2198,3,Normal,250000\n6,Mitchel,1Story,5,1993,796,1362,2,Abnorml,143000\n7,Somerst,1Story,8,2004,1686,1694,3,Normal,307000\n8,OldTown,1Story,5,1931,486,1114,1,Normal,200000\n9,BrkSide,2Story,5,1939,600,1314,1,Abnorml,129900\n10,NAmes,2Story,7,1965,852,1494,2,Normal,118000\n11,Edwards,1Story,5,1962,850,1253,1,Partial,129500\n12,BrkSide,2Story,6,1995,978,1717,2,Normal,345000\n13,CollgCr,2Story,7,2006,1030,1800,2,Normal,279500\n14,OldTown,1Story,6,1940,520,1140,1,abNOrml,157000\n15,NAmes,1Story,7,1958,748,1420,2,Normal,129500", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing or inconsistent categorical values (e.g., fix 'abNOrml' to 'Abnorml')", "One-hot encode categorical variables: Neighborhood, HouseStyle, SaleCondition", "Impute any missing numeric values (if present) using median values", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features: OverallQual, YearBuilt, TotalBsmtSF, GrLivArea, GarageCars", "Train a RandomForestRegressor on the training data", "Perform grid search over number of trees (n_estimators) and max_depth parameters", "Evaluate the model on the test set calculating RMSE, MAE, and R2", "Identify top 5 feature importances from the trained model", "Generate predicted vs actual sale price plot for test data"], "model_results": {"rmse": 22000, "mae": 17000, "r2": 0.85, "top_feature_importances": {"OverallQual": 0.35, "GrLivArea": 0.25, "Neighborhood_NoRidge": 0.1, "YearBuilt": 0.08, "TotalBsmtSF": 0.07}, "best_hyperparameters": {"n_estimators": 150, "max_depth": 12}}}
{"purpose": "Build a model to predict wheat yield (tons per hectare) based on soil properties, weather conditions, and farming practices.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Avg_Temperature_C,Sowing_Method,Fertilizer_Used_kg,Yield_tph\nF001,Loam,250,18.5,Broadcast,100,3.2\nF002,Clay,300,20.1,Drill,NaN,3.8\nF003,SAND,180,17.7,Broadcast,80,2.5\nF004,Loam,missing,19.0,Drill,90,3.1\nF005,Clay,270,21.3,broadcast,110,3.5\nF006,Loam,290,20.0,Drill,105,3.7\nF007,Clay,260,19.5,Broadcast,95,3.3\nF008,Sand,200,18.2,Drill,85,2.8\nF009,Loam,310,21.0,broadcast,100,3.9\nF010,Sand,220,missing,Broadcast,75,2.6\nF011,Clay,280,20.5,Drill,100,3.6\nF012,Loam,265,19.8,Drill,NaN,3.4\nF013,Clay,240,18.9,Broadcast,90,3.0\nF014,Sand,210,17.5,Drill,80,2.7\nF015,Loam,295,20.2,Broadcast,95,3.5", "model_steps": ["Load the dataset and identify target variable as Yield_tph", "Clean data by imputing missing numeric values with median rainfall and temperature", "Standardize capitalization in Soil_Type and Sowing_Method columns", "One-hot encode categorical variables Soil_Type and Sowing_Method", "Split data into training (80%) and test (20%) sets", "Standardize numeric features Rainfall_mm, Avg_Temperature_C, and Fertilizer_Used_kg", "Train a RandomForestRegressor with 100 trees on training data", "Perform hyperparameter tuning on max_depth using 5-fold cross-validation", "Evaluate final model on test data computing RMSE, MAE, and R2 score", "Extract and rank feature importances from the trained model"], "model_results": {"rmse": 0.22, "mae": 0.17, "r2": 0.87, "top_feature_importances": {"Fertilizer_Used_kg": 0.35, "Rainfall_mm": 0.3, "Soil_Type_Loam": 0.15, "Avg_Temperature_C": 0.12, "Sowing_Method_Drill": 0.08}, "best_hyperparameters": {"max_depth": 8, "n_estimators": 100}}}
{"purpose": "Classify soil moisture levels to optimize irrigation scheduling.", "raw_table": "SampleID,SoilType,TemperatureC,HumidityPercent,RainfallLastWeek_mm,SoilMoisturePercent,IrrigationNeeded\n1,Loam,22.5,45,5,30,Yes\n2,SAND,25.0,40,0,12,No\n3,Clay,21.0,50,2,NA,Yes\n4,loam,23.5,43,7,28,Yes\n5,Silt,26.2,38,0,15,No\n6,Sand,24.0,,1,14,No\n7,Clay,20.0,55,3,35,Yes\n8,silt,27.0,37,0,10,No\n9,Loam,22.0,48,6,32,Yes\n10,Clay,19.5,52,4,34,Yes\n11,Sand,25.5,40,1,13,No\n12,silt,26.0,39,0,11,No\n13,Loam,23.0,44,5,29,Yes\n14,Clay,20.5,53,2,33,Yes", "model_steps": ["Load the dataset from CSV string into a DataFrame", "Standardize SoilType categorical values to lowercase for consistency", "Impute missing SoilMoisturePercent values using median of each SoilType group", "Impute missing HumidityPercent values using overall median", "One-hot encode SoilType categorical variable", "Split data into train (80%) and test (20%) sets with stratification on target variable IrrigationNeeded", "Standardize numeric features: TemperatureC, HumidityPercent, RainfallLastWeek_mm, SoilMoisturePercent", "Train a RandomForestClassifier to predict IrrigationNeeded", "Perform grid search over max_depth parameter with 5-fold cross-validation", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score", "Generate confusion matrix and feature importance plot"], "model_results": {"accuracy": 0.93, "precision": 0.91, "recall": 0.95, "f1": 0.93, "confusion_matrix": {"True_Positive": 7, "True_Negative": 5, "False_Positive": 1, "False_Negative": 1}, "top_feature_importances": {"soilmoisturepercent": 0.38, "humiditypercent": 0.25, "rainfalllastweek_mm": 0.18, "soiltype_loam": 0.1, "temperaturec": 0.09}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Build a classification model to predict whether a taxi trip will have a tip above 20%.", "raw_table": "trip_id,passenger_count,pickup_neighborhood,trip_distance,fare_amount,payment_type,tip_percentage,high_tip\n1,2,Downtown,3.5,12.5,CARD,25,Yes\n2,1,Uptown,1.2,6.0,cash,18,No\n3,3,midtown,5.1,18.0,CARD,22,Yes\n4,2,Downtown,NaN,15.0,CARD,15,No\n5,1,Suburb,8.0,30.0,CARD,,No\n6,4,Uptown,4.4,20.0,Cash,30,Yes\n7,2,Midtown,2.6,10.0,CARD,19,No\n8,1,downtown,3.3,13.0,CARD,21,Yes\n9,3,Suburb,7.5,28.0,CARD,17,No\n10,2,Uptown,3.8,14.5,CARD,24,Yes\n11,1,Midtown,NaN,11.0,cash,20,No\n12,3,Downtown,4.0,16.0,CARD,26,Yes\n13,2,Uptown,5.5,22.0,CARD,12,No\n14,1,Suburb,6.7,25.0,CARD,21,Yes", "model_steps": ["Load CSV data into a DataFrame", "Identify and impute missing values in 'trip_distance' and 'tip_percentage' using median imputation", "Normalize capitalization of categorical values in 'pickup_neighborhood' and 'payment_type'", "Convert 'pickup_neighborhood' and 'payment_type' into one-hot encoded features", "Define 'high_tip' as the target variable with classes Yes/No", "Split dataset into train (80%) and test (20%) sets with stratification on the target", "Standardize numeric features: 'passenger_count', 'trip_distance', 'fare_amount', and 'tip_percentage'", "Train a RandomForestClassifier with 100 trees on the training set", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score", "Extract feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.84, "recall": 0.88, "f1": 0.86, "top_feature_importances": {"tip_percentage": 0.47, "fare_amount": 0.21, "trip_distance": 0.15, "pickup_neighborhood_Suburb": 0.06, "payment_type_CARD": 0.05, "passenger_count": 0.03, "pickup_neighborhood_Downtown": 0.03}}}
{"purpose": "Predict whether a customer will make a repeat purchase within 30 days after their first order.", "raw_table": "CustomerID,Age,Gender,FirstOrderAmount,MembershipStatus,DaysToRepeatPurchase,RepeatPurchase\n101,25,Male,120.50,Silver,12,Yes\n102,37,Female,85.00,gold,NA,No\n103,,Female,50.75,Silver,40,No\n104,29,Male,NaN,Bronze,7,Yes\n105,43,Male,200.00,Gold,3,Yes\n106,35,Female,NaN,Silver,NA,No\n107,30,mAle,75.00,bronze,35,No\n108,28,Female,65.25,Silver,15,Yes\n109,22,,90.00,Gold,NA,No\n110,40,Female,110.00,Silver,5,Yes\n111,38,Male,130.00,Gold,10,Yes\n112,27,Female,95.00,Bronze,NA,No\n113,31,Male,NaN,Silver,20,Yes\n114,33,Female,160.00,GOLD,2,Yes", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Impute missing Age and FirstOrderAmount values using median values grouped by MembershipStatus", "Standardize capitalization of MembershipStatus and Gender columns to ensure consistency", "Convert 'RepeatPurchase' target variable to binary labels (Yes=1, No=0)", "One-hot encode categorical variables: Gender and MembershipStatus", "Split the data into training (80%) and testing (20%) sets, stratified by the target variable", "Scale numeric features (Age, FirstOrderAmount)", "Train a Logistic Regression classifier with L2 regularization", "Perform 5-fold cross-validation to tune regularization strength", "Evaluate the final model on the test set computing accuracy, precision, recall, and F1 score", "Generate a confusion matrix for the test set predictions", "Analyze feature coefficients to identify top predictors"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 14, "true_negative": 9, "false_positive": 3, "false_negative": 2}, "top_features": {"MembershipStatus_Gold": 1.45, "DaysToRepeatPurchase": -0.85, "FirstOrderAmount": 0.73, "Gender_Female": 0.42}, "hyperparameters": {"regularization": "L2", "C": 1.0}}}
{"purpose": "Build a classification model to predict whether a delivery will be delayed by more than 30 minutes.", "raw_table": "DeliveryID,Distance_km,VehicleType,WeatherCondition,PickupHour,TrafficLevel,Delayed\n1,12.5,Truck,sunny,9,Low,No\n2,7.8,van,rainy,15,High,yes\n3,5.0,Van,Sunny,8,medium,No\n4,20.1,Truck,Cloudy,13,Low,No\n5,3.6,bike,,17,High,Yes\n6,15.0,Truck,Rainy,11,Medium,No\n7,9.2,Van,cloudy,7,Low,No\n8,11.7,bike,Sunny,19,high,Yes\n9,8.0,Van,sunny,14,Medium,No\n10,13.4,truck,Rainy,6,Medium,Yes\n11,10.0,Van,RAINY,18,High,Yes\n12,6.5,Bike,Cloudy,12,Medium,No\n13,4.8,Truck,sunny,20,Low,No", "model_steps": ["Load data and inspect for missing and inconsistent values", "Normalize categorical column 'VehicleType' values to consistent capitalization", "Impute missing 'WeatherCondition' values with the mode", "Convert target variable 'Delayed' to binary (Yes=1, No=0)", "Split data into train and test sets (80/20)", "One-hot encode categorical variables: VehicleType, WeatherCondition, TrafficLevel", "Standardize numeric features: Distance_km, PickupHour", "Train a RandomForestClassifier on the training set", "Perform grid search over number of trees [50, 100] and max_depth [3, 5]", "Evaluate accuracy, precision, recall, and F1 score on the test set", "Generate confusion matrix and plot feature importances"], "model_results": {"accuracy": 0.85, "f1": 0.82, "precision": 0.79, "recall": 0.86, "confusion_matrix": [[7, 1], [2, 5]], "top_feature_importances": {"TrafficLevel_High": 0.24, "Distance_km": 0.21, "PickupHour": 0.15, "WeatherCondition_Rainy": 0.12, "VehicleType_Truck": 0.1}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a movie will be a box-office hit based on its production and genre features.", "raw_table": "MovieID,Genre,DirectorExperience,ProductionBudgetMillions,LeadActorPopularity,ReleaseSeason,BoxOfficeHit\n1,Action,5,150,85,summer,Yes\n2,Comedy,3,,65,Winter,No\n3,Drama,7,30,abc,Fall,No\n4,Action,2,80,70,Summer,YES\n5,comedy,6,50,55,Spring,No\n6,Drama,4,40,60,Fall,No\n7,Action,8,200,90,Summer,Yes\n8,Comedy,,45,60,summer,No\n9,Drama,5,35,50,Winter,No\n10,Action,3,100,80,Fall,yes\n11,Comedy,7,55,75,Spring,No\n12,Drama,6,25,40,Winter,No\n13,Action,4,120,78,Summer,Yes\n14,Comedy,3,60,58,fall,No", "model_steps": ["Load CSV data into dataframe", "Standardize column names and values (e.g., unify 'Yes'/'YES'/'yes' to 'Yes', unify genre capitalization)", "Handle missing numeric values in ProductionBudgetMillions and DirectorExperience by median imputation", "Convert LeadActorPopularity to numeric, replace non-numeric entries with median value", "One-hot encode categorical variables: Genre and ReleaseSeason", "Convert target variable BoxOfficeHit to binary (Yes=1, No=0)", "Split data into training (80%) and testing (20%) sets with stratification on target", "Standardize numeric features: DirectorExperience, ProductionBudgetMillions, LeadActorPopularity", "Train a RandomForestClassifier with 100 trees on the training data", "Tune max_depth hyperparameter using grid search over [3,5,7]", "Evaluate model performance on test set with accuracy, precision, recall, and F1-score", "Generate confusion matrix for test predictions"], "model_results": {"accuracy": 0.85, "precision": 0.8, "recall": 0.89, "f1": 0.84, "confusion_matrix": [[6, 1], [2, 5]], "top_feature_importances": {"ProductionBudgetMillions": 0.38, "LeadActorPopularity": 0.27, "Genre_Action": 0.15, "DirectorExperience": 0.1, "ReleaseSeason_Summer": 0.05, "Genre_Comedy": 0.05}, "best_max_depth": 5}}
{"purpose": "Build a classification model to predict whether a fruit batch is ripe based on quality and environmental factors.", "raw_table": "BatchID,FruitType,ColorScore,Size_cm,Treatment,StorageTemp_C,DaysSinceHarvest,Ripe\n1,Apple,0.75,7.2,Organic,4,12,Yes\n2,apple,0.82,7.8,conventional,6,10,Yes\n3,Banana,0.65,15.1,Organic,13,5,No\n4,Banana,0.60,14.8,,12,7,No\n5,Orange,0.70,6.4,Conventional,5,11,Yes\n6,Orange,Missing,6.5,organic,5,9,Yes\n7,Apple,0.78,7.5,Conventional,5,8,yes\n8,Banana,0.58,15.0,organic,14,6,No\n9,Orange,0.68,6.3,Organic,Missing,10,Yes\n10,apple,0.80,7.3,Conventional,5,11,Yes\n11,banana,0.62,15.2,CONVENTIONAL,13,4,No\n12,Orange,0.69,6.6,Organic,5,10,Yes\n13,Apple,0.77,7.4,Organic,4,7,Yes\n14,Banana,0.55,14.9,Organic,12,6,No", "model_steps": ["Load data and inspect for missing values and inconsistent capitalization", "Standardize categorical columns FruitType and Treatment to consistent lowercase", "Impute missing numeric values in ColorScore and StorageTemp_C using median values per FruitType", "Encode categorical variables FruitType and Treatment using one-hot encoding", "Convert target variable Ripe to binary (Yes=1, No=0)", "Split data into train and test sets (80/20 stratified by Ripe)", "Standardize numeric features ColorScore, Size_cm, StorageTemp_C, DaysSinceHarvest", "Train a RandomForestClassifier with 100 trees", "Perform grid search on max_depth with values [3, 5, 7]", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Analyze feature importances"], "model_results": {"accuracy": 0.92, "f1": 0.91, "precision": 0.93, "recall": 0.9, "best_hyperparameters": {"max_depth": 5}, "top_feature_importances": {"ColorScore": 0.35, "DaysSinceHarvest": 0.25, "StorageTemp_C": 0.15, "FruitType_apple": 0.1, "Treatment_organic": 0.08, "Size_cm": 0.07}}}
{"purpose": "Build a classification model to predict whether a crop yield will be high or low based on environmental and soil features.", "raw_table": "soil_type,rainfall_mm,temperature_c,sunlight_hours,crop_variety,previous_crop,yield_category\nLoam,120.5,22.1,8.5,VarA,Wheat,High\nClay,85,19.7,7.8,VarB,Corn,Low\nSAND,100,21.4,missing,VarA,Soy,High\nLoam,missing,22.0,8.2,VarC,Wheat,Low\nSilt,110,25.3,9.1,varB,Corn,High\nClay,95,20.0,7.5,VarA,soy,Low\nLoam,130,23.5,8.7,VarC,Wheat,High\nSilt,105,missing,8.8,VarA,Corn,Low\nSAND,115,24.1,9.0,VarB,Wheat,High\nLoam,90,19.8,7.9,VarC,Barley,Low\nClay,100,21.0,8.0,VarA,Corn,High\nSilt,missing,22.5,8.3,VarB,Barley,Low\nLoam,125,23.0,8.6,VarA,Wheat,High\nClay,88,20.2,7.7,VarC,Soy,Low\nSilt,112,24.4,9.2,VarB,Corn,High", "model_steps": ["Load the dataset and identify missing values and inconsistent capitalization", "Impute missing numeric values with the median per column", "Standardize capitalization in categorical columns (e.g., 'soil_type', 'crop_variety', 'previous_crop')", "One-hot encode categorical variables", "Split data into train and test sets (80/20)", "Standardize numeric features (rainfall_mm, temperature_c, sunlight_hours)", "Train a RandomForestClassifier on the training set", "Perform grid search over n_estimators and max_depth hyperparameters", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix", "Identify and report top 5 feature importances"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.89, "f1": 0.87, "confusion_matrix": {"True_Positive": 7, "True_Negative": 5, "False_Positive": 1, "False_Negative": 1}, "top_feature_importances": {"rainfall_mm": 0.28, "soil_type_Loam": 0.22, "sunlight_hours": 0.18, "temperature_c": 0.15, "crop_variety_VarA": 0.1}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 6}}}
{"purpose": "Predict the likelihood of hospital readmission within 30 days for patients with chronic diseases.", "raw_table": "PatientID,Age,Gender,PrimaryDiagnosis,BloodPressure,Cholesterol,MedicationAdherence,Smoker,Readmitted\n1,65,Male,Diabetes,140/90,High,Good,Yes,Yes\n2,58,Female,Hypertension,130/85,Moderate,poor,No,No\n3,72,Female,Diabetes,150/95,High,Good,yes,Yes\n4,45,Male,Asthma,120/80,Low,,No,No\n5,60,female,Hypertension,135/88,Moderate,Good,No,No\n6,68,Male,Diabetes,,High,Good,Yes,Yes\n7,55,Female,Asthma,125/82,Low,Moderate,No,No\n8,62,male,Hypertension,138/89,Moderate,Poor,Yes,Yes\n9,59,Female,Diabetes,142/91,High,Good,No,Yes\n10,70,Male,Hypertension,145/92,High,Good,Yes,Yes\n11,48,Female,Asthma,118/78,Low,Moderate,no,No\n12,66,Male,Diabetes,148/94,High,Good,Yes,Yes", "model_steps": ["Parse and clean data, standardize 'Gender' and 'Smoker' categories to consistent capitalization", "Impute missing values in 'BloodPressure' and 'MedicationAdherence' with median and mode respectively", "Convert 'BloodPressure' from systolic/diastolic string to two separate numeric features: 'SystolicBP' and 'DiastolicBP'", "One-hot encode categorical variables: 'Gender', 'PrimaryDiagnosis', 'MedicationAdherence', and 'Smoker'", "Split data into training (80%) and test (20%) sets stratified by 'Readmitted'", "Standardize numeric features: 'Age', 'SystolicBP', 'DiastolicBP', and 'Cholesterol' (encoded ordinally as Low=0, Moderate=1, High=2)", "Train a RandomForestClassifier to predict 'Readmitted' status", "Perform grid search over 'n_estimators' and 'max_depth' hyperparameters with 5-fold cross-validation", "Evaluate final model on test set computing accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.83, "precision": 0.85, "recall": 0.78, "f1": 0.81, "confusion_matrix": [[4, 1], [2, 7]], "top_feature_importances": {"MedicationAdherence_Good": 0.25, "PrimaryDiagnosis_Diabetes": 0.22, "SystolicBP": 0.18}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a retail customer will make a repeat purchase within 30 days based on their first purchase behavior and demographics.", "raw_table": "CustomerID,Age,Gender,FirstPurchaseAmount,StoreRegion,DaysSinceFirstPurchase,PreferredCategory,RepeatPurchase\n1,34,Male,120.5,North,15,Electronics,Yes\n2,29,Female,89.2,south,25,Clothing,No\n3,45,Female,45.0,East,5,home & garden,Yes\n4,NaN,Male,NaN,West,12,Clothing,No\n5,22,male,75.0,North,30,Electronics,No\n6,38,FEMALE,110.0,South,8,Electronics,Yes\n7,41,Female,95.7,East,20,,No\n8,36,Male,88.9,West,18,Clothing,Yes\n9,28,Female,60.3,North,NaN,Home & Garden,No\n10,50,Male,130.0,East,3,Electronics,Yes\n11,31,Female,70.2,South,14,Clothing,No\n12,27,male,85.4,West,22,Electronics,Yes\n13,33,Female,92.1,North,7,Clothing,Yes\n14,40,Female,100.0,south,NaN,Home & garden,No", "model_steps": ["Load the CSV data into a DataFrame", "Identify and fix messy values: standardize 'Gender' values to lowercase, fill missing 'Age' and 'FirstPurchaseAmount' with median, fill missing 'PreferredCategory' with mode, fill missing 'DaysSinceFirstPurchase' with median", "Convert 'RepeatPurchase' to binary target variable (Yes=1, No=0)", "One-hot encode categorical features: Gender, StoreRegion, PreferredCategory", "Split data into train and test sets with 80/20 ratio, stratified by target", "Standardize numeric features: Age, FirstPurchaseAmount, DaysSinceFirstPurchase", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search over max_depth values [5, 10, 15]", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Identify and report top 3 feature importances"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.82, "recall": 0.87, "top_features": {"DaysSinceFirstPurchase": 0.32, "FirstPurchaseAmount": 0.25, "PreferredCategory_Electronics": 0.18}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict whether a government loan application will be approved.", "raw_table": "Applicant_ID,Age,Income,Employment_Status,Credit_Score,Loan_Amount,State,Application_Status\n1,45,55000,Employed,720,15000,CA,Approved\n2,37,NaN,Self-Employed,680,12000,TX,Denied\n3,29,48000,employed,700,13000,NY,Approved\n4,54,61000,Unemployed,NaN,16000,ca,Denied\n5,39,59000,Employed,710,14000,TX,Approved\n6,NaN,52000,Employed,690,NaN,NY,Denied\n7,31,47000,Self-Employed,720,11000,CA,Approved\n8,43,62000,Employed,730,15000,TX,Approved\n9,35,60000,Unemployed,650,13500,ny,Denied\n10,40,58000,Employed,705,NaN,CA,Approved\n11,28,53000,Employed,690,12500,TX,Denied\n12,47,61500,self-employed,710,14500,NY,Approved\n13,50,60000,Employed,720,NaN,CA,Approved\n14,33,49000,Unemployed,670,12000,TX,Denied", "model_steps": ["Load the raw CSV data into a dataframe", "Clean and standardize the 'Employment_Status' and 'State' categorical columns by fixing capitalization inconsistencies", "Impute missing values in 'Age' and 'Income' with median values", "Impute missing 'Credit_Score' and 'Loan_Amount' with mean values", "One-hot encode categorical variables 'Employment_Status' and 'State'", "Split data into train and test sets (80/20 stratified by 'Application_Status')", "Standardize numeric features: Age, Income, Credit_Score, Loan_Amount", "Train a RandomForestClassifier to predict 'Application_Status'", "Perform grid search over 'max_depth' and 'n_estimators' to optimize model", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix and feature importance plot"], "model_results": {"accuracy": 0.86, "f1": 0.85, "precision": 0.88, "recall": 0.82, "confusion_matrix": [[24, 4], [5, 22]], "top_feature_importances": {"Credit_Score": 0.32, "Income": 0.25, "Loan_Amount": 0.15, "Employment_Status_Employed": 0.1, "State_CA": 0.08, "Age": 0.05, "Employment_Status_Self-Employed": 0.05}, "best_hyperparameters": {"max_depth": 8, "n_estimators": 100}}}
{"purpose": "Predict whether a customer will make a purchase during a browsing session on an ecommerce site.", "raw_table": "session_id,device_type,browser,time_on_site_minutes,pages_viewed,referral_source,cart_value,target_purchase\n1,Mobile,Chrome,12,5,Email,45.75,Yes\n2,Desktop,firefox,8,3,Direct,0,No\n3,Tablet,Chrome,NaN,7,Social Media,89.99,Yes\n4,desktop,Edge,15,10,Email,120.00,Yes\n5,Mobile,Safari,5,2,,0,No\n6,Mobile,Chrome,20,15,Direct,200.25,Yes\n7,Tablet,chrome,3,1,Social Media,0,No\n8,DESKTOP,Firefox,10,6,Email,30,No\n9,Mobile,Safari,7,4,Direct,55.5,Yes\n10,Tablet,Edge,NaN,NaN,Email,0,No\n11,Desktop,Chrome,9,5,Social media,75.0,Yes\n12,Mobile,safari,6,3,Direct,15,No\n13,Tablet,firefox,11,8,Email,100,Yes\n14,desktop,Safari,4,2,Direct,0,No", "model_steps": ["Load the raw CSV data into a DataFrame", "Normalize inconsistent capitalization in categorical columns device_type, browser, and referral_source", "Impute missing numeric values in time_on_site_minutes and pages_viewed using median values", "Fill missing referral_source values with 'Unknown'", "Convert target_purchase from Yes/No to binary 1/0", "One-hot encode categorical variables: device_type, browser, referral_source", "Standardize numeric features: time_on_site_minutes, pages_viewed, cart_value", "Split data into train (80%) and test (20%) subsets with stratification on target_purchase", "Train a RandomForestClassifier on the training data", "Perform grid search over n_estimators (50, 100) and max_depth (5, 10) using cross-validation", "Evaluate the best model on test set calculating accuracy, precision, recall, and F1 score", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.89, "recall": 0.83, "f1": 0.86, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}, "top_features": {"cart_value": 0.35, "time_on_site_minutes": 0.25, "pages_viewed": 0.15}, "confusion_matrix": [[7, 1], [2, 9]]}}
{"purpose": "Predict whether a government grant application will be approved based on application and applicant characteristics.", "raw_table": "ApplicationID,ApplicantAge,ApplicantGender,PreviousGrants,RequestedAmount,Department,ApplicationStatus\n1,34,Male,2,50000,Health,Approved\n2,45,Female,1,70000,Education,Rejected\n3,29,MALE,0,30000,Environment,Approved\n4,,Female,3,45000,Health,Approved\n5,52,Female,One,60000,Education,Rejected\n6,41,Male,2,55000,Infrastructure,Approved\n7,37,Female,1,48000,Health,Rejected\n8,30,Male,0,NaN,Environment,Rejected\n9,28,Male,0,35000,Education,Approved\n10,50,Female,4,80000,Infrastructure,Approved\n11,47,Male,3,75000,Health,Rejected\n12,33,Female,2,52000,Environment,approved\n13,40,male,1,50000,Education,Rejected\n14,39,Female,2,58000,Infrastructure,Approved\n15,44,,2,60000,Health,Rejected", "model_steps": ["Load raw CSV data into pandas DataFrame", "Clean data by standardizing 'ApplicantGender' values to lowercase and fixing inconsistent 'ApplicationStatus' capitalization", "Impute missing ApplicantAge with median age and missing RequestedAmount with median requested amount", "Convert 'PreviousGrants' column to numeric, replacing 'One' with 1 and imputing other invalid entries with median", "One-hot encode categorical variables: ApplicantGender and Department", "Convert target variable 'ApplicationStatus' to binary: Approved=1, Rejected=0", "Split data into training (80%) and test sets (20%) with stratification on target variable", "Standardize numeric features: ApplicantAge, PreviousGrants, RequestedAmount", "Train a RandomForestClassifier with default parameters on training data", "Perform hyperparameter tuning on max_depth and n_estimators using 5-fold cross-validation", "Evaluate final model on test set measuring accuracy, precision, recall, and F1 score", "Generate confusion matrix and feature importance rankings from the trained model"], "model_results": {"accuracy": 0.8, "precision": 0.78, "recall": 0.82, "f1": 0.8, "confusion_matrix": [[12, 3], [4, 21]], "feature_importances": {"RequestedAmount": 0.32, "PreviousGrants": 0.25, "ApplicantAge": 0.18, "Department_Health": 0.1, "Department_Education": 0.08, "ApplicantGender_male": 0.07}, "hyperparameters": {"max_depth": 6, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict if a crop field will have high or low yield based on soil and weather conditions.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Avg_Temp_C,Planting_Method,Fertilizer_Used,Yield_Class\nF001,Loam,300,22.5,Direct,Yes,High\nF002,Sandy,NA,25.0,Transplant,No,Low\nF003,Clay,280,21.0,Direct,Yes,High\nF004,Loam,290,21.5,Direct,YES,High\nF005,SANDY,310,23.0,Direct,No,Low\nF006,Clay,275,20.0,Transplant,No,Low\nF007,Loam,305,22.0,Direct,Yes,High\nF008,Clay,270,19.5,Direct,No,Low\nF009,Loam,NA,23.5,Transplant,Yes,High\nF010,Sandy,295,24.0,Direct,No,Low\nF011,Sandy,300,NA,Transplant,Yes,Low\nF012,clay,285,21.0,Direct,No,High\nF013,Loam,310,22.8,Direct,Yes,High\nF014,Sandy,290,23.3,Transplant,No,Low", "model_steps": ["Load CSV data and identify columns with missing or inconsistent values", "Standardize categorical values in 'Soil_Type' and 'Fertilizer_Used' columns (e.g., unify capitalization)", "Impute missing numeric values in 'Rainfall_mm' and 'Avg_Temp_C' using median values", "Encode categorical variables 'Soil_Type', 'Planting_Method', and 'Fertilizer_Used' using one-hot encoding", "Split data into training (80%) and testing (20%) sets", "Train a RandomForestClassifier on the training data", "Perform grid search over max_depth [3,5,7] to optimize model performance", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix to analyze prediction errors", "Identify and rank top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "f1": 0.88, "precision": 0.9, "recall": 0.87, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"Rainfall_mm": 0.35, "Soil_Type_Loam": 0.25, "Fertilizer_Used_Yes": 0.2}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict hourly electricity consumption category (Low, Medium, High) based on weather and time features.", "raw_table": "Hour,Temperature_C,Weather_Condition,Day_Type,Previous_Hour_Consumption_kWh,Consumption_Category\n0,15.6,Clear,Weekday,120,Low\n1,14.8,clear,Weekday,118,Low\n2,14.3,Clear,weekend,NaN,Low\n3,13.9,Rain,Weekday,110,Low\n4,13.5,Rain,Weekend,105,Low\n5,14.0,Cloudy,Weekday,130,Medium\n6,16.1,cloudy,Weekday,160,Medium\n7,18.3,Clear,Weekday,220,High\n8,20.2,Clear,Weekday,250,High\n9,22.5,Clear,Weekday,NaN,High\n10,24.0,Clear,Weekday,280,High\n11,25.2,Rain,Weekday,260,High\n12,26.1,Cloudy,Weekend,210,Medium\n13,25.8,Clear,WEekend,195,Medium", "model_steps": ["Load the CSV data into a DataFrame", "Identify and fill missing values in Previous_Hour_Consumption_kWh using forward fill", "Standardize the 'Hour' and 'Temperature_C' numeric features", "Normalize 'Previous_Hour_Consumption_kWh' using min-max scaling", "Convert 'Weather_Condition' and 'Day_Type' to lowercase and perform one-hot encoding", "Encode target variable 'Consumption_Category' into ordinal labels: Low=0, Medium=1, High=2", "Split data into train and test sets with 80% training, 20% testing", "Train a Gradient Boosting Classifier on the training data", "Perform hyperparameter tuning over learning_rate and n_estimators using 5-fold cross-validation", "Evaluate the final model on the test set computing accuracy, F1 score, precision, and recall", "Generate a confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.85, "f1": 0.84, "precision": 0.83, "recall": 0.85, "confusion_matrix": {"Low": {"Low": 5, "Medium": 0, "High": 0}, "Medium": {"Low": 1, "Medium": 4, "High": 1}, "High": {"Low": 0, "Medium": 1, "High": 5}}, "top_feature_importances": {"Previous_Hour_Consumption_kWh": 0.42, "Hour": 0.3, "Temperature_C": 0.18}, "best_hyperparameters": {"learning_rate": 0.1, "n_estimators": 100}}}
{"purpose": "Build a regression model to estimate hourly energy consumption in residential buildings based on weather and occupancy data.", "raw_table": "BuildingID,WeatherCondition,TemperatureC,Humidity,OccupancyStatus,DayOfWeek,Hour,EnergyConsumption_kWh\nB001,sunny,23.5,45,Occupied,Monday,8,3.2\nB002,Rainy,18.0,85,unoccupied,Monday,8,1.5\nB003,Cloudy,20.0,65,Occupied,Tuesday,14,4.1\nB004,Sunny,not available,40,Occupied,Wednesday,20,5.3\nB005,Cloudy,21.5,70,,Thursday,6,2.7\nB006,Sunny,25.0,50,Occupied,Friday,19,6.0\nB007,Rainy,17,90,Unoccupied,Saturday,23,1.2\nB008,Sunny,26.3,55,Occupied,Sunday,12,4.9\nB009,Cloudy,22.1,68,Occupied,Sunday,15,4.5\nB010,Rainy,19.5,80,occupied,Monday,9,3.0\nB011,Sunny,24.0,44,Occupied,Tuesday,7,3.1\nB012,Cloudy,21.0,75,Unoccupied,Wednesday,18,2.0\nB013,Rainy,16.5,88,Occupied,Thursday,22,2.8\nB014,Sunny,27.0,53,Occupied,Friday,13,5.6\nB015,Cloudy,20.5,70,unoccupied,Saturday,5,1.9", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values: replace 'not available' in TemperatureC with the column median", "Normalize inconsistent capitalization in OccupancyStatus and WeatherCondition columns", "Fill missing OccupancyStatus with 'Unoccupied' as the most frequent category", "Convert categorical variables (WeatherCondition, OccupancyStatus, DayOfWeek) using one-hot encoding", "Split the dataset into train (80%) and test (20%) sets ensuring temporal ordering by Hour is maintained", "Standardize numeric features: TemperatureC, Humidity, Hour", "Train a Gradient Boosting Regressor to predict EnergyConsumption_kWh", "Perform hyperparameter tuning on learning rate and number of estimators using 5-fold cross-validation", "Evaluate model performance on the test set using RMSE, MAE, and R2 metrics", "Identify and report top 3 feature importances from the trained model"], "model_results": {"rmse": 0.52, "mae": 0.38, "r2": 0.87, "top_feature_importances": {"OccupancyStatus_Occupied": 0.28, "Hour": 0.22, "TemperatureC": 0.18}, "hyperparameters": {"learning_rate": 0.05, "n_estimators": 150}}}
{"purpose": "Build a classification model to predict if a taxi ride will have a tip above 20%.", "raw_table": "ride_id,passenger_count,trip_distance,store_and_fwd_flag,payment_type,day_of_week,tip_percentage,tip_above_20\n1,2,3.5,N,N,Monday,15,FALSE\n2,1,0.9,Y,Credit,Tuesday,22,TRUE\n3,4,12.3,no,Cash,WEDNESDAY,25,TRUE\n4,3,7.8,N,credit,Thursday,,FALSE\n5,2,2.4,N,Cash,Friday,18,FALSE\n6,1,1.5,N,Cash,Saturday,20,FALSE\n7,3,5.0,Yes,Credit,Sunday,30,TRUE\n8,2,3.0,n,no,Monday,12,FALSE\n9,1,0.7,N,Credit,Tuesday,10,FALSE\n10,5,15.2,N,Cash,Wednesday,28,TRUE\n11,2,3.6,N,credit,,19,FALSE\n12,1,1.2,N,Cash,Friday,21,TRUE\n13,4,6.7,N,Credit,Saturday,16,FALSE\n14,3,4.5,N,Cash,Sunday,24,TRUE", "model_steps": ["Load the CSV data into a DataFrame", "Clean and standardize 'store_and_fwd_flag' values to binary (Y/Yes as 1, N/No/n/no as 0), filling missing with 0", "Fill missing 'tip_percentage' with median value", "Fill missing 'day_of_week' with mode", "Standardize 'payment_type' capitalization and fill missing with 'Credit'", "Encode categorical variables: 'store_and_fwd_flag', 'payment_type', and 'day_of_week' using one-hot encoding", "Split data into train and test sets (80/20 stratified by 'tip_above_20')", "Standardize numeric features ('passenger_count', 'trip_distance', 'tip_percentage')", "Train a RandomForestClassifier to predict 'tip_above_20'", "Perform grid search over 'n_estimators' (50, 100) and 'max_depth' (5, 10)", "Evaluate model using accuracy, F1 score, precision, and recall on test set", "Generate and analyze the confusion matrix"], "model_results": {"accuracy": 0.85, "f1": 0.82, "precision": 0.8, "recall": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 2, "false_negative": 1}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}, "top_feature_importances": {"tip_percentage": 0.35, "trip_distance": 0.2, "store_and_fwd_flag_1": 0.15, "payment_type_Credit": 0.1, "day_of_week_Friday": 0.08}}}
{"purpose": "Build a classification model to predict hospital readmission within 30 days for diabetic patients.", "raw_table": "PatientID,Age,Gender,BMI,HbA1c_Level,Medication_Type,Previous_Admissions,Readmitted\n1,54,Male,28.5,7.2,Insulin,2,Yes\n2,63,Female,NaN,8.1,oral,0,No\n3,45,female,31.0,6.9,Oral,1,No\n4,78,Male,29.4,NaN,Insulin,5,Yes\n5,50,Male,27.8,7.5,Oral,1,No\n6,,Female,33.2,8.8,insulin,3,Yes\n7,67,Male,35.0,7.1,Oral,2,No\n8,59,Female,NaN,7.9,oral,,Yes\n9,49,Male,26.4,6.8,Insulin,0,No\n10,55,Female,30.1,7.3,Oral,2,Yes\n11,62,Male,28.9,7.7,Insulin,3,Yes\n12,58,Female,32.5,7.0,Oral,1,No\n13,46,Male,27.5,NaN,oral,0,No\n14,70,Female,29.0,8.3,insulin,4,Yes", "model_steps": ["Load the CSV data into a dataframe.", "Handle missing numeric values in Age, BMI, and HbA1c_Level columns by imputing median values.", "Standardize the capitalization and spelling of categorical variables Gender and Medication_Type.", "Fill missing categorical values in Medication_Type and Previous_Admissions with the mode and zero respectively.", "Convert categorical variables Gender and Medication_Type into one-hot encoded features.", "Split the dataset into training (80%) and test (20%) sets with stratification on the target Readmitted.", "Standardize numeric features: Age, BMI, HbA1c_Level, and Previous_Admissions using z-score scaling based on training data.", "Train a RandomForestClassifier with 100 trees to predict Readmitted.", "Perform grid search over max_depth in {5, 10, 20} and min_samples_split in {2, 5} using 5-fold cross-validation on train set.", "Evaluate the best model on the test set and compute accuracy, precision, recall, and F1 score.", "Generate and display the confusion matrix and top 3 feature importances."], "model_results": {"accuracy": 0.79, "precision": 0.82, "recall": 0.75, "f1": 0.78, "confusion_matrix": {"True_Positive": 9, "False_Positive": 2, "True_Negative": 8, "False_Negative": 3}, "top_feature_importances": {"HbA1c_Level": 0.32, "Previous_Admissions": 0.25, "Medication_Type_Insulin": 0.17}, "best_hyperparameters": {"max_depth": 10, "min_samples_split": 2}}}
{"purpose": "Predict whether a client will default on a loan based on their financial and demographic information.", "raw_table": "Client_ID,Age,Income,Employment_Status,Credit_Score,Loan_Amount,Loan_Term,Marital_Status,Default\n1,45,55000,Full-time,720,15000,36,Married,No\n2,29,NaN,part-time,680,8000,24,Single,Yes\n3,62,72000,Self-employed,780,20000,48,married,No\n4,35,45000,FULL-TIME,690,,36,Single,No\n5,41,50000,Unemployed,NaN,12000,36,Divorced,Yes\n6,53,82000,Full-time,800,25000,60,Married,No\n7,27,40000,Part-time,650,7000,24,Single,Yes\n8,30,47000,Full-Time,710,NaN,36,Single,No\n9,38,60000,Self-Employed,730,18000,48,Married,No\n10,46,58000,Full-time,NaN,15000,36,Divorced,Yes\n11,51,69000,Full-Time,760,22000,60,Married,No\n12,33,43000,part-time,670,9000,24,Single,Yes\n13,59,75000,Full-time,790,21000,48,Married,No", "model_steps": ["Load data and identify missing values and inconsistent capitalization in Employment_Status and Marital_Status", "Impute missing numeric values using median for Income, Credit_Score, and Loan_Amount", "Standardize numeric features: Age, Income, Credit_Score, Loan_Amount, Loan_Term", "Normalize capitalization of Employment_Status and Marital_Status and treat them as categorical variables", "One-hot encode Employment_Status and Marital_Status", "Split data into training (80%) and test sets (20%) with stratification on the target Default", "Train a RandomForestClassifier to predict Default", "Perform grid search over n_estimators [50, 100] and max_depth [5, 10]", "Evaluate model on test set computing accuracy, precision, recall, and F1 score", "Generate confusion matrix", "Extract and report top 3 feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.78, "f1": 0.8, "confusion_matrix": {"true_positive": 7, "true_negative": 13, "false_positive": 3, "false_negative": 2}, "top_feature_importances": {"Credit_Score": 0.35, "Loan_Amount": 0.25, "Employment_Status_Full-time": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Classify soil fertility levels based on soil properties and location features to assist in crop planning.", "raw_table": "Soil_pH,Organic_Matter,Texture,Region,Irrigation_Type,Fertility_Level\n6.5,3.2,Loam,North,Drip,High\n7.1,2.8,SAND,South,Sprinkler,Medium\n5.8,NaN,Clay,East,Drip,Low\n6.9,3.5,loam,West,Flood,High\n6.0,2.1,Silt,North,Sprinkler,Medium\n7.3,3.0,Loam,South,,High\n5.5,1.8,clay,East,Flood,Low\n6.7,2.9,Silt,West,Drip,Medium\n6.2,2.5,Sand,north,Sprinkler,Medium\nNaN,3.1,Loam,South,Flood,High\n5.9,NaN,Clay,East,Drip,Low\n7.0,3.3,Loam,West,Sprinkler,High\n6.4,2.7,Silt,North,Drip,Medium\n6.8,3.4,Clay,South,Flood,Medium", "model_steps": ["Load the CSV data into a DataFrame", "Handle missing values: impute Soil_pH and Organic_Matter with median values", "Normalize inconsistent capitalization in Texture and Region columns (e.g., 'SAND' to 'Sand')", "Fill missing Irrigation_Type with the mode value", "Encode categorical variables (Texture, Region, Irrigation_Type) using one-hot encoding", "Split data into training (80%) and testing (20%) sets stratified by Fertility_Level", "Standardize numeric features Soil_pH and Organic_Matter", "Train a RandomForestClassifier on the training data", "Perform grid search cross-validation to tune max_depth parameter", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Generate and analyze the confusion matrix", "Identify top 3 important features from the trained model"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.79, "recall": 0.83, "confusion_matrix": {"High": [5, 1, 0], "Medium": [1, 6, 1], "Low": [0, 1, 3]}, "top_feature_importances": {"Organic_Matter": 0.32, "Soil_pH": 0.28, "Texture_Loam": 0.15}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Build a classification model to predict whether a field's crop yield will be high or low based on soil and weather features.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Temperature_C,Nitrogen_Content,Previous_Crop,Yield_Category\nF001,Loam,120,22,High,Corn,High\nF002,Clay,85,19,Medium,Wheat,Low\nF003,Sand,NaN,25,low,Corn,High\nF004,Loam,110,21,,Soybean,High\nF005,Clay,95,20,Medium,Wheat,Low\nF006,Sand,80,18,Low,Corn,Low\nF007,loam,115,22,High,Soybean,High\nF008,Clay,100,20,Medium,Wheat,Low\nF009,Loam,112,21,High,,High\nF010,SAND,75,19,Low,Corn,Low\nF011,Clay,90,20,Medium,Wheat,Low\nF012,Loam,108,22,High,Corn,High\nF013,Clay,NaN,20,Medium,Wheat,Low\nF014,Sand,78,18,Low,Corn,Low", "model_steps": ["Load dataset and inspect for missing and inconsistent values", "Normalize inconsistent capitalization in 'Soil_Type' and 'Nitrogen_Content' columns", "Impute missing numeric values in 'Rainfall_mm' and 'Nitrogen_Content' with median and mode respectively", "One-hot encode categorical variables: 'Soil_Type' and 'Previous_Crop'", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features: 'Rainfall_mm' and 'Temperature_C'", "Train a RandomForestClassifier to predict 'Yield_Category'", "Perform grid search over number of trees (n_estimators) and max_depth", "Evaluate model performance using accuracy, precision, recall, and F1 score on test set", "Generate and analyze confusion matrix", "Identify and rank top 3 feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.88, "recall": 0.82, "f1": 0.85, "confusion_matrix": {"True_Positive": 7, "True_Negative": 5, "False_Positive": 1, "False_Negative": 2}, "top_feature_importances": {"Rainfall_mm": 0.35, "Nitrogen_Content_High": 0.3, "Soil_Type_Loam": 0.2}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether daily air quality index (AQI) will be classified as 'Good' or 'Poor' based on meteorological and pollution data.", "raw_table": "Date,Temperature_C,Humidity_pct,Wind_Direction,NO2_ppb,O3_ppb,Day_Type,AQI_Category\n2024-04-01,15.2,55,North,34,20,weekday,Good\n2024-04-02,18.7,60,south,45,25,Weekend,Poor\n2024-04-03,NaN,58,East,39,22,Weekday,Good\n2024-04-04,20.1,62,west,50,30,weekday,Poor\n2024-04-05,19.5,55,Northwest,41,28,Weekday,Poor\n2024-04-06,16.0,59,East,37,NaN,Weekday,Good\n2024-04-07,14.3,51,Southwest,33,18,Weekend,Good\n2024-04-08,17.2,63,north,48,26,weekday,Poor\n2024-04-09,18.0,57,South,44,24,Weekday,Poor\n2024-04-10,15.8,,East,36,21,Weekend,Good\n2024-04-11,16.5,54,west,40,23,weekday,Good\n2024-04-12,19.0,60,Southwest,47,29,Weekday,Poor\n2024-04-13,21.0,65,North,52,31,Weekend,Poor\n2024-04-14,20.5,59,East,NaN,27,weekday,Good\n", "model_steps": ["Load the dataset from CSV into a DataFrame", "Identify and impute missing values in Temperature_C, Humidity_pct, NO2_ppb, and O3_ppb using median imputation", "Normalize capitalization inconsistencies in categorical columns Day_Type and Wind_Direction", "One-hot encode categorical variables: Wind_Direction and Day_Type", "Split data into training (80%) and test (20%) sets with stratification on AQI_Category", "Standardize numeric features: Temperature_C, Humidity_pct, NO2_ppb, O3_ppb", "Train a RandomForestClassifier to predict AQI_Category", "Perform hyperparameter tuning using grid search over number of trees (n_estimators) and max_depth", "Evaluate model performance on the test set calculating accuracy, F1 score, precision, and recall", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.79, "recall": 0.84, "top_feature_importances": {"NO2_ppb": 0.32, "O3_ppb": 0.27, "Temperature_C": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Predict whether a social media post will go viral based on its attributes and posting time.", "raw_table": "post_id,post_length,num_hashtags,post_type,user_followers,time_of_day,day_of_week,viral\n1,120,3,Image,1500,Morning,Monday,Yes\n2,85,0,video,2000,Afternoon,Tuesday,No\n3,200,5,Text,5000,NIGHT,Wednesday,Yes\n4,NaN,2,Image,3500,Morning,Thursday,No\n5,95,1,Video,1000,evening,Friday,No\n6,150,4,text,800,Afternoon,Saturday,Yes\n7,130,NaN,Image,1200,Morning,Sunday,Yes\n8,110,3,Video,2300,Night,Monday,No\n9,140,2,Text,1700,MORNING,Tuesday,No\n10,100,1,Image,NaN,Evening,Wednesday,Yes\n11,90,0,Video,900,Afternoon,Thursday,No\n12,160,4,Text,2100,Night,Friday,Yes\n13,115,3,Image,1800,evening,Saturday,No\n14,NaN,2,Video,2700,Morning,Sunday,Yes", "model_steps": ["Load the CSV data into a dataframe and inspect for missing and inconsistent values", "Standardize categorical column values for 'post_type' and 'time_of_day' to consistent capitalization", "Impute missing numeric values ('post_length', 'num_hashtags', 'user_followers') with median values", "Convert target variable 'viral' to binary labels (Yes=1, No=0)", "One-hot encode categorical variables 'post_type', 'time_of_day', and 'day_of_week'", "Split the dataset into training (80%) and testing (20%) sets using stratified sampling on the target", "Standardize numeric features ('post_length', 'num_hashtags', 'user_followers') using z-score normalization", "Train a RandomForestClassifier with 100 trees on the training data", "Perform 5-fold cross-validation to tune max_depth parameter with values [5, 10, 15]", "Evaluate the final model on the test set calculating accuracy, F1 score, precision, and recall", "Generate a confusion matrix to analyze false positives and false negatives", "Identify and report top 3 feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.86, "f1": 0.83, "precision": 0.81, "recall": 0.85, "confusion_matrix": {"true_positive": 18, "true_negative": 25, "false_positive": 5, "false_negative": 3}, "top_feature_importances": {"user_followers": 0.32, "post_length": 0.25, "num_hashtags": 0.15}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a movie will be a box office hit based on its production and genre features.", "raw_table": "MovieID,Genre,DirectorPopularity,BudgetMillions,LeadActorExperience,ReleaseMonth,IsHit\n1,Action,High,150,10,July,Yes\n2,comedy,Medium,35,5,December,No\n3,Drama,low,20,8,April,No\n4,Action,High,NaN,12,August,Yes\n5,Thriller,Medium,50,7,November,Yes\n6,Comedy,Medium,30,4,december,No\n7,Drama,High,45,6,March,No\n8,Action,Low,120,15,July,Yes\n9,Thriller,Medium,55,NaN,October,Yes\n10,comedy,Low,25,3,January,No\n11,Drama,Medium,40,10,April,No\n12,Thriller,High,80,13,September,Yes\n13,Action,Medium,NaN,9,June,Yes\n14,Comedy,Low,20,2,February,No\n15,Drama,Medium,35,7,May,No", "model_steps": ["Load the CSV data into a DataFrame.", "Standardize the capitalization in categorical columns such as 'Genre' and 'ReleaseMonth'.", "Handle missing numeric values in 'BudgetMillions' and 'LeadActorExperience' by imputing with median values.", "Encode the target variable 'IsHit' as binary (Yes=1, No=0).", "One-hot encode categorical features: 'Genre' and 'ReleaseMonth'.", "Split the dataset into training and test sets with an 80/20 ratio.", "Standardize numeric features 'BudgetMillions', 'DirectorPopularity' (encoded ordinally as High=3, Medium=2, Low=1), and 'LeadActorExperience'.", "Train a RandomForestClassifier on the training set.", "Perform grid search over 'max_depth' with values [5, 10, 15] to optimize the model.", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score.", "Generate a confusion matrix to analyze prediction errors."], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.9, "f1": 0.87, "confusion_matrix": [[7, 1], [2, 10]], "top_feature_importances": {"BudgetMillions": 0.35, "DirectorPopularity": 0.25, "Genre_Action": 0.15, "LeadActorExperience": 0.12, "ReleaseMonth_July": 0.08, "Genre_Comedy": 0.05}, "best_max_depth": 10}}
{"purpose": "Build a classification model to predict whether a taxi trip will experience a delay longer than 10 minutes.", "raw_table": "trip_id,passenger_count,pickup_hour,pickup_location,weather_condition,trip_distance_miles,is_delayed\n1,2,08,Midtown,Clear,3.5,No\n2,1,22,Downtown,Rain,5.2,Yes\n3,3,14,Uptown,Cloudy,2.7,No\n4,,07,Downtown,clear,4.1,No\n5,2,18,midtown,Snow,6.0,Yes\n6,1,09,Uptown,fog,3.0,No\n7,4,13,Downtown,Rain,abc,Yes\n8,1,20,Midtown,Clear,5.5,Yes\n9,2,25,Uptown,Cloudy,3.8,No\n10,3,17,Downtown,Clear,4.6,No\n11,2,11,Midtown,Fog,3.3,No\n12,1,15,Downtown,Rain,5.0,Yes\n13,3,21,Midtown,Clear,4.8,Yes", "model_steps": ["Load the CSV data and identify missing or inconsistent values.", "Impute missing passenger_count with median value.", "Correct trip_distance_miles entries by replacing non-numeric values with median distance.", "Normalize pickup_hour values to be within 0-23 by removing invalid entries or capping.", "Standardize text capitalization in categorical columns (pickup_location, weather_condition).", "One-hot encode pickup_location and weather_condition.", "Split data into training (80%) and test (20%) sets randomly.", "Standardize numeric features: passenger_count, pickup_hour, trip_distance_miles.", "Train a RandomForestClassifier to predict is_delayed.", "Perform grid search over number of estimators (50, 100) and max_depth (5, 10).", "Evaluate model on test set using accuracy, precision, recall, and F1 score.", "Generate confusion matrix and extract feature importances."], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.8, "f1": 0.815, "confusion_matrix": {"true_positive": 8, "true_negative": 10, "false_positive": 2, "false_negative": 3}, "top_feature_importances": {"trip_distance_miles": 0.35, "pickup_hour": 0.25, "weather_condition_Rain": 0.15, "pickup_location_Downtown": 0.1, "passenger_count": 0.08}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Predict hourly energy consumption (kWh) for residential households based on weather and household characteristics.", "raw_table": "Household_ID,Region,Household_Type,Avg_Temperature_C,Day_of_Week,Holiday,Energy_Consumption_kWh\n001,North,Detached,15.2,Monday,No,23.5\n002,South,Apartment,20.1,Tuesday,No,18.3\n003,East,Townhouse,NaN,Wednesday,Yes,21.7\n004,West,detached,14.5,Thursday,No,25.1\n005,North,Apartment,16.0,Friday,No,19.6\n006,South,Townhouse,19.4,Saturday,NO,22.8\n007,East,Detached,17.3,Sunday,No,24.3\n008,West,Apartment,15.9,Monday,No,18.9\n009,North,Apartment,16.5,Tuesday,No,20.1\n010,South,Townhouse,21.0,Wednesday,Yes,23.0\n011,East,Detached,18.7,Thursday,No,26.2\n012,West,Townhouse,14.2,Friday,No,22.5\n013,North,Detached,15.7,Saturday,No,23.9\n014,South,Apartment,19.5,Sunday,No,19.0", "model_steps": ["Load the CSV data into a DataFrame", "Normalize capitalization in categorical columns (e.g., 'detached' to 'Detached', 'NO' to 'No')", "Impute missing Avg_Temperature_C value with the mean temperature", "Convert categorical variables (Region, Household_Type, Day_of_Week, Holiday) using one-hot encoding", "Split data into train (80%) and test (20%) sets", "Standardize numeric features: Avg_Temperature_C", "Train a RandomForestRegressor to predict Energy_Consumption_kWh", "Perform hyperparameter tuning on max_depth and n_estimators using 5-fold cross-validation", "Evaluate model performance on the test set using RMSE, MAE, and R2 metrics", "Analyze feature importances from the trained model"], "model_results": {"rmse": 1.45, "mae": 1.12, "r2": 0.87, "top_feature_importances": {"Avg_Temperature_C": 0.38, "Household_Type_Detached": 0.22, "Holiday_Yes": 0.15, "Day_of_Week_Saturday": 0.08, "Region_South": 0.07}, "best_hyperparameters": {"max_depth": 7, "n_estimators": 100}}}
{"purpose": "Predict whether a customer will default on a loan based on their financial and demographic information.", "raw_table": "CustomerID,Age,Income,EmploymentStatus,CreditScore,LoanAmount,LoanPurpose,Defaulted\n1,34,58000,Full-time,720,15000,Home Improvement,No\n2,28,NaN,part-time,690,12000,Debt Consolidation,Yes\n3,45,84000,Full-Time,650,20000,car purchase,No\n4,52,100000,Self-employed,710,25000,Home improvement,No\n5,37,62000,Full-time,missing,18000,Debt Consolidation,Yes\n6,29,40000,Unemployed,580,7000,car purchase,Yes\n7,41,72000,Full-time,720,NaN,Redevelopment,No\n8,48,90000,Part-Time,690,22000,Car Purchase,No\n9,55,110000,Full-time,730,30000,Debt consolidation,No\n10,38,65000,Full-Time,680,16000,home Improvement,Yes\n11,26,48000,Unemployed,600,9000,Debt Consolidation,Yes\n12,33,53000,Full-Time,675,13000,Car Purchase,No\n13,44,77000,Full-Time,710,21000,Home improvement,No\n14,31,59000,Part-time,660,14000,Debt Consolidation,Yes", "model_steps": ["Load the dataset and identify missing values in numeric and categorical columns", "Impute missing numeric values with median values and fill missing categorical values with the mode", "Standardize numeric features such as Age, Income, CreditScore, and LoanAmount", "Normalize inconsistent capitalization and standardize categories in EmploymentStatus and LoanPurpose columns", "One-hot encode categorical variables EmploymentStatus and LoanPurpose", "Split data into training and test sets with an 80/20 ratio", "Train a RandomForestClassifier to predict Defaulted", "Perform hyperparameter tuning over max_depth and number of estimators using cross-validation", "Evaluate the model on the test set calculating accuracy, precision, recall, and F1 score", "Generate a confusion matrix and extract feature importances for interpretation"], "model_results": {"accuracy": 0.79, "precision": 0.75, "recall": 0.7, "f1": 0.72, "confusion_matrix": {"true_negatives": 7, "false_positives": 2, "false_negatives": 3, "true_positives": 6}, "top_feature_importances": {"CreditScore": 0.33, "LoanAmount": 0.22, "Income": 0.15, "EmploymentStatus_Full-time": 0.1, "LoanPurpose_Debt Consolidation": 0.08}, "best_hyperparameters": {"max_depth": 6, "n_estimators": 100}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features and location.", "raw_table": "property_id,area_sqft,num_bedrooms,num_bathrooms,neighborhood,year_built,has_garage,sale_price\nP001,1500,3,2,Downtown,1995,Yes,350000\nP002,NaN,2,1,suburb,2001,No,220000\nP003,1200,,1,Suburb,1990,yes,210000\nP004,1800,4,3,Downtown,2010,No,450000\nP005,2000,3,2,Uptown,2015,YES,480000\nP006,1600,3,2,uptown,,No,440000\nP007,1400,2,2,Suburb,2005,No,300000\nP008,NaN,3,2,Downtown,2000,Yes,390000\nP009,1700,4,3,uPtown,2018,Yes,520000\nP010,1300,2,1,suburb,1998,No,250000", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values: impute missing 'area_sqft' with median, 'num_bedrooms' with mode, and 'year_built' with median", "Normalize capitalization inconsistencies in 'neighborhood' and 'has_garage' columns and convert 'has_garage' to binary (Yes=1, No=0)", "One-hot encode the 'neighborhood' categorical variable", "Split data into training and testing sets using an 80/20 split", "Standardize numeric features 'area_sqft', 'year_built', 'num_bedrooms', and 'num_bathrooms'", "Train a Gradient Boosting Regressor on the training data", "Tune hyperparameters 'n_estimators' and 'max_depth' using grid search with 5-fold cross-validation", "Evaluate model performance on test set using RMSE, MAE, and R2 metrics", "Extract and report top 3 feature importances from the trained model"], "model_results": {"rmse": 22000, "mae": 17000, "r2": 0.87, "best_hyperparameters": {"n_estimators": 100, "max_depth": 4}, "top_feature_importances": {"area_sqft": 0.42, "neighborhood_Uptown": 0.25, "num_bedrooms": 0.15}}}
{"purpose": "Predict loan default risk based on customer financial and demographic information.", "raw_table": "CustomerID,Age,Income,EmploymentStatus,CreditScore,LoanAmount,LoanPurpose,Defaulted\n1,35,56000,Full-Time,720,15000,Debt Consolidation,No\n2,42,NaN,Part-time,680,20000,Home Improvement,Yes\n3,29,48000,full-time,NaN,12000,Debt consolidation,No\n4,50,82000,Self-employed,710,25000,Education,No\n5,31,53000,Unemployed,690,15000,Car Purchase,Yes\n6,,60000,Full-Time,730,18000,Debt Consolidation,No\n7,45,72000,part-Time,650,20000,Home improvement,Yes\n8,38,58000,Full-Time,705,NaN,Car Purchase,No\n9,54,90000,Self-employed,740,30000,Home Improvement,No\n10,27,45000,Full-time,670,13000,Debt Consolidation,Yes\n11,33,52000,,685,14000,Debt Consolidation,No\n12,41,70000,Full-Time,710,22000,Education,No\n13,36,59000,Self-employed,NaN,16000,Car purchase,Yes\n14,48,67000,Full-Time,695,21000,Debt Consolidation,No", "model_steps": ["Load dataset and identify missing values", "Impute missing numeric values using median imputation", "Standardize numeric columns: Age, Income, CreditScore, LoanAmount", "Normalize inconsistent categorical values in EmploymentStatus and LoanPurpose (e.g., capitalization)", "Impute missing EmploymentStatus with mode", "One-hot encode categorical variables: EmploymentStatus, LoanPurpose", "Encode target variable 'Defaulted' as binary (Yes=1, No=0)", "Split data into train (80%) and test (20%) sets with stratification on target", "Train a Gradient Boosting Classifier with early stopping", "Tune max_depth and learning_rate using 5-fold cross-validation", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate feature importance plot"], "model_results": {"accuracy": 0.82, "precision": 0.78, "recall": 0.74, "f1": 0.76, "best_hyperparameters": {"max_depth": 4, "learning_rate": 0.1}, "top_feature_importances": {"CreditScore": 0.34, "Income": 0.22, "LoanAmount": 0.15, "EmploymentStatus_Self-employed": 0.08, "LoanPurpose_Debt Consolidation": 0.07}, "confusion_matrix": {"true_positive": 14, "true_negative": 18, "false_positive": 5, "false_negative": 6}}}
{"purpose": "Predict the likelihood of hospital readmission within 30 days for diabetic patients.", "raw_table": "PatientID,Age,Gender,AdmissionType,NumPrevAdmissions,HbA1c,Smoker,Readmitted\n1,54,Male,Emergency,2,7.8,Yes,Yes\n2,67,female,Elective,0,6.2,No,No\n3,45,Male,Emergency,1,8.5,yes,Yes\n4,73,Female,urgent,3,7.1,No,No\n5,60,Male,Emergency,2,missing,No,Yes\n6,58,Female,Elective,0,6.8,no,No\n7,49,male,Emergency,1,7.4,Yes,No\n8,70,Female,Emergency,4,8.0,N/A,Yes\n9,65,Male,Elective,2,7.3,No,No\n10,55,Female,Emergency,1,7.9,Yes,Yes\n11,62,Male,urgent,3,7.0,No,No\n12,50,Female,Emergency,1,7.2,Yes,Yes\n13,59,Male,Elective,0,6.9,No,No\n14,53,Female,Emergency,2,7.5,Yes,Yes", "model_steps": ["Load data and handle missing values in HbA1c by imputing the median value", "Normalize capitalization inconsistencies in Gender, AdmissionType, and Smoker columns", "Convert 'Smoker' N/A values to 'No' assuming non-smoking unless indicated", "One-hot encode categorical variables: Gender, AdmissionType, Smoker", "Split data into training (80%) and testing (20%) sets, stratified by Readmitted", "Standardize numeric features: Age, NumPrevAdmissions, HbA1c", "Train RandomForestClassifier to predict Readmitted", "Perform grid search on number of estimators (50, 100) and max_depth (3, 5)", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze classification errors", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.79, "precision": 0.77, "recall": 0.82, "f1": 0.79, "confusion_matrix": [[7, 2], [3, 8]], "top_feature_importances": {"NumPrevAdmissions": 0.34, "HbA1c": 0.28, "AdmissionType_Emergency": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a social media post will go viral based on post characteristics and user engagement features.", "raw_table": "post_id,user_type,post_length,num_hashtags,time_posted,contains_media,avg_user_followers,likes_first_hour,shares_first_hour,viral\n1,Influencer,120,3,Evening,Yes,15000,230,45,Yes\n2,regular,45,0,Morning,No,300,12,1,No\n3,Regular,200,5,evening,yes,2500,180,40,Yes\n4,INFLUENCER,75,1,Night,No,12000,90,15,No\n5,regular,NaN,2,Afternoon,No,800,30,3,No\n6,Regular,150,4,Morning,Yes,5000,250,60,Yes\n7,Influencer,100,3,Morning,Yes,17000,300,70,Yes\n8,regular,60,,Night,No,400,25,2,No\n9,regular,85,2,afternoon,No,950,40,5,No\n10,Influencer,130,4,Evening,Yes,14000,210,50,Yes\n11,regular,55,1,Morning,No,600,20,1,No\n12,Regular,110,3,NIGHT,Yes,7000,150,35,Yes\n13,regular,90,2,Afternoon,No,bad_data,50,7,No\n14,Influencer,140,5,Evening,Yes,16000,280,65,Yes", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Clean and standardize categorical variables (e.g., normalize 'user_type' and 'time_posted' capitalization)", "Impute missing numeric values in 'post_length' and 'num_hashtags' using median imputation", "Handle invalid entries in 'avg_user_followers' by replacing non-numeric values with median", "Convert 'contains_media' and 'viral' columns to binary numeric format", "One-hot encode categorical features 'user_type' and 'time_posted'", "Split data into training (80%) and test (20%) sets stratified by target 'viral'", "Standardize numeric features such as 'post_length', 'num_hashtags', 'avg_user_followers', 'likes_first_hour', and 'shares_first_hour'", "Train a RandomForestClassifier to predict viral posts", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.79, "f1": 0.81, "top_features": {"likes_first_hour": 0.32, "shares_first_hour": 0.28, "avg_user_followers": 0.15}, "hyperparameters": {"n_estimators": 100, "max_depth": 10, "random_state": 42}}}
{"purpose": "Predict whether a social media post will go viral based on user and post features.", "raw_table": "post_id,user_followers,user_verified,post_length,post_type,post_time,engagement_rate,viral\n1,1200,True,250,text,Morning,0.03,No\n2,4500,false,500,Image,Evening,0.15,Yes\n3,300,True,NaN,video,Night,0.05,No\n4,NaN,false,150,text,Morning,0.01,No\n5,9000,TRUE,400,image,Afternoon,0.25,Yes\n6,1500,False,600,Video,Night,0.10,Yes\n7,200,False,50,Text,morning,0.00,No\n8,5000,True,350,Image,Afternoon,,Yes\n9,700,False,200,text,evening,0.02,No\n10,10000,True,700,video,NIGHT,0.30,Yes\n11,1200,FALSE,100,text,Morning,0.01,No\n12,8000,True,450,image,Afternoon,0.20,Yes\n13,NaN,false,300,Video,Evening,0.08,No", "model_steps": ["Load data and identify missing values", "Impute missing numeric values with median (user_followers, post_length, engagement_rate)", "Standardize the 'post_length' and 'engagement_rate' numeric features", "Normalize 'user_followers' using log transformation due to skewness", "Clean and unify categorical variables: fix capitalization inconsistencies in 'user_verified', 'post_type', and 'post_time'", "One-hot encode categorical variables: 'user_verified', 'post_type', and 'post_time'", "Split data into train and test sets with 80/20 ratio, stratified by the target variable 'viral'", "Train a RandomForestClassifier with default parameters on the training set", "Perform grid search over 'max_depth' parameter with values [3, 5, 10]", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix to analyze prediction errors"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.81, "recall": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"engagement_rate": 0.35, "user_followers_log": 0.25, "post_type_Image": 0.15, "post_time_Evening": 0.1, "user_verified_True": 0.08}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a customer will make a repeat purchase within 30 days based on their first purchase behavior and demographics.", "raw_table": "CustomerID,Age,Gender,FirstPurchaseCategory,PurchaseAmount,DaysSinceSignup,DeviceType,RepeatPurchase\n1001,34,Male,Electronics,199.99,15,Mobile,Yes\n1002,22,Female,Clothing,49.50,8,desktop,No\n1003,45,Male,Home,NaN,20,Mobile,Yes\n1004,29,,Beauty,25.00,5,Tablet,No\n1005,38,Female,Electronics,150.00,30,mobile,Yes\n1006,31,Female,Toys,75.25,NaN,Desktop,No\n1007,27,Male,Toys,60,10,Tablet,Yes\n1008,40,Female,Clothing,85.00,12,Desktop,No\n1009,33,Male,beauty,45.00,7,Mobile,Yes\n1010,25,Female,Electronics,NaN,14,Mobile,No\n1011,36,Male,Clothing,55.00,19,tablet,Yes\n1012,28,Female,Home,120.00,3,Mobile,No\n1013,,Male,Electronics,200,25,Mobile,Yes\n1014,30,Female,Clothing,65.00,NaN,Desktop,No", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Fill missing numeric values (Age, PurchaseAmount, DaysSinceSignup) using median imputation", "Standardize capitalization in categorical columns (Gender, FirstPurchaseCategory, DeviceType)", "Impute missing categorical values (Gender) with mode", "One-hot encode categorical variables: Gender, FirstPurchaseCategory, DeviceType", "Split data into training and testing sets with 80/20 ratio", "Train a GradientBoostingClassifier to predict RepeatPurchase", "Use 5-fold cross-validation to tune number of estimators and learning rate", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Extract and analyze feature importances from the trained model"], "model_results": {"accuracy": 0.82, "precision": 0.79, "recall": 0.85, "f1": 0.82, "top_feature_importances": {"DaysSinceSignup": 0.28, "PurchaseAmount": 0.22, "FirstPurchaseCategory_Electronics": 0.15, "Age": 0.1, "DeviceType_Mobile": 0.09, "Gender_Female": 0.05}, "best_hyperparameters": {"n_estimators": 100, "learning_rate": 0.1}}}
{"purpose": "Predict the likelihood of hospital readmission within 30 days for diabetic patients based on clinical and demographic data.", "raw_table": "PatientID,Age,Gender,HbA1c_Level,Num_Prior_Admissions,Diabetes_Type,Smoker,Readmitted\n1,55,Male,7.8,2,Type 2,No,Yes\n2,63,Female,9.1,1,Type 1,Yes,No\n3,47,F,8.4,3,type 2,No,Yes\n4,,Male,6.9,0,Type 2,No,No\n5,70,Female,10.2,5,Type 1,yes,Yes\n6,59,Male,NaN,2,Type 2,No,No\n7,52,Female,7.5,1,Type 2,nO,No\n8,45,Male,8.0,2,Type 1,No,Yes\n9,61,Female,8.7,4,Type 2,Yes,Yes\n10,54,Male,7.2,2,type 1,No,No\n11,50,Female,8.8,3,Type 2,No,Yes\n12,58,Male,7.9,2,Type 2,No,No\n13,65,Female,9.5,,Type 1,No,Yes\n14,49,Male,8.1,1,Type 2,No,No\n15,53,Female,7.6,2,Type 2,No,Yes", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Standardize column names and fix inconsistent capitalization in categorical columns", "Impute missing numeric values (Age, HbA1c_Level, Num_Prior_Admissions) using median imputation", "Convert Smoker and Diabetes_Type columns to consistent categorical formats", "Encode categorical variables (Gender, Diabetes_Type, Smoker) using one-hot encoding", "Split data into training and testing sets with 80/20 stratified by the target variable Readmitted", "Standardize numeric features (Age, HbA1c_Level, Num_Prior_Admissions) using z-score normalization", "Train a RandomForestClassifier to predict Readmitted", "Perform grid search over n_estimators and max_depth hyperparameters with 5-fold cross-validation", "Evaluate the final model on the test set using accuracy, precision, recall, and F1 score", "Generate and inspect the confusion matrix", "Identify and report feature importances from the trained model"], "model_results": {"accuracy": 0.8, "precision": 0.78, "recall": 0.82, "f1": 0.8, "confusion_matrix": {"true_positive": 9, "true_negative": 7, "false_positive": 3, "false_negative": 2}, "top_feature_importances": {"HbA1c_Level": 0.32, "Num_Prior_Admissions": 0.25, "Diabetes_Type_Type 2": 0.15, "Age": 0.12, "Smoker_Yes": 0.08, "Gender_Female": 0.08}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a loan applicant will default on their loan within 12 months.", "raw_table": "Loan_ID,Applicant_Income,Coapplicant_Income,Loan_Amount,Loan_Term,Credit_History,Property_Area,Loan_Status\nL001,5000,0,200,360,1,Rural,Y\nL002,3000,1500,100,360,1,Semiurban,N\nL003,4000,,150,360,0,Urban,Y\nL004,6000,0,250,360,1,rural,Y\nL005,NaN,1800,120,180,1,Urban,N\nL006,3500,0,100,360,No,Urban,N\nL007,8000,2000,300,360,1,Semiurban,Y\nL008,2500,0,90,360,1,RURAL,Y\nL009,4000,1000,NaN,360,1,Semiurban,N\nL010,4500,500,130,360,0,Urban,Y\nL011,3000,0,110,360,1,urban,N\nL012,7000,0,250,360,1,Semiurban,Y\nL013,6500,1500,220,360,1,Rural,N\nL014,4800,1200,140,360,0,Semiurban,Y", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values in Applicant_Income, Coapplicant_Income, and Loan_Amount by median imputation", "Correct inconsistent capitalization in Property_Area column", "Convert Credit_History to numeric, mapping 'No' to 0 and handling missing as 1", "Encode categorical variables: Property_Area and Loan_Status (target) as binary labels", "Create a feature Loan_Total_Income by summing Applicant_Income and Coapplicant_Income", "Drop Loan_ID column as it is an identifier", "Split data into 80% training and 20% testing sets stratified by Loan_Status", "Standardize numeric features: Applicant_Income, Coapplicant_Income, Loan_Amount, Loan_Term, and Loan_Total_Income", "Train a RandomForestClassifier with 100 trees and max_depth=5", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and extract top 3 feature importances"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.79, "f1": 0.81, "confusion_matrix": {"true_negative": 6, "false_positive": 1, "false_negative": 2, "true_positive": 5}, "top_feature_importances": {"Credit_History": 0.42, "Loan_Total_Income": 0.25, "Loan_Amount": 0.18}, "hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a citizen will apply for government housing assistance based on demographic and economic factors.", "raw_table": "Age,Income,Employment_Status,Marital_Status,Number_of_Dependants,Region,Previous_Applications,Applied_For_Assistance\n34,45000,Employed,Single,2,North,0,No\n29,NaN,unemployed,Married,3,south,1,Yes\n42,72000,Employed,Divorced,1,East,0,No\n37,52000,Employed,Married,4,West,2,Yes\n,38000,Unemployed,Single,0,north,0,No\n45,61000,Employed,Married,2,East,1,No\n31,48000,Employed,Married,,South,1,Yes\n54,83000,Employed,Married,3,West,0,No\n28,39000,Unemployed,Single,1,East,3,Yes\n39,58000,EMPLOYED,Divorced,2,West,0,No\n33,46000,employed,Single,1,North,2,Yes\n50,67000,Employed,Married,2,South,0,No\n41,71000,Employed,Divorced,3,East,1,No", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Impute missing numeric values in Age and Number_of_Dependants with median values", "Standardize the Income and Age columns", "Normalize Employment_Status and Marital_Status categorical variables by lowercasing and fixing inconsistent capitalization", "One-hot encode Employment_Status, Marital_Status, and Region columns", "Split the data into train and test sets with an 80/20 ratio", "Train a RandomForestClassifier on the training data", "Perform hyperparameter tuning on n_estimators and max_depth using grid search with 5-fold cross-validation", "Evaluate the model on the test set calculating accuracy, precision, recall, and F1 score", "Generate a confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.8, "recall": 0.86, "confusion_matrix": {"true_negative": 18, "false_positive": 4, "false_negative": 3, "true_positive": 19}, "top_feature_importances": {"Income": 0.27, "Previous_Applications": 0.22, "Employment_Status_Employed": 0.15, "Age": 0.13, "Region_North": 0.1}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 8}}}
{"purpose": "Predict the likelihood of hospital readmission within 30 days for diabetic patients based on clinical and demographic data.", "raw_table": "PatientID,Age,Gender,BMI,HbA1c,MedicationType,PreviousAdmissions,Smoker,Readmitted\n001,58,Male,31.2,8.1,Metformin,2,Yes,Yes\n002,72,female,27.8,7.4,sulfonylurea,1,No,No\n003,65,Male,missing,8.7,Metformin,3,yes,Yes\n004,54,Female,29.5,7.9,Insulin,0,No,No\n005,60,Male,33.1,9.2,Metformin,1,No,Yes\n006,47,Female,28.4,missing,Sulfonylurea,1,NO,No\n007,80,male,26.7,7.5,Insulin,4,Yes,Yes\n008,70,Female,31.0,8.3,Metformin,2,No,No\n009,62,Female,29.1,8.0,metformin,2,Yes,Yes\n010,55,Male,30.5,7.8,Sulfonylurea,1,No,No\n011,68,Male,27.9,8.4,Insulin,3,yes,Yes\n012,59,Female,32.1,7.6,Metformin,missing,No,No\n013,64,Male,28.8,8.2,Metformin,2,Yes,Yes\n014,53,Female,29.7,7.7,Insulin,1,No,No", "model_steps": ["Load the dataset and inspect for missing or inconsistent data", "Impute missing numeric values in BMI and HbA1c columns using median values", "Standardize capitalization and unify medication types and smoker status categories", "Encode categorical variables: Gender, MedicationType, and Smoker using one-hot encoding", "Split data into training and testing sets using an 80/20 ratio", "Standardize numeric features: Age, BMI, HbA1c, and PreviousAdmissions", "Train a RandomForestClassifier to predict Readmitted (Yes/No)", "Tune max_depth and n_estimators hyperparameters using 5-fold cross-validation", "Evaluate model performance on the test set calculating accuracy, precision, recall, and F1 score", "Generate a confusion matrix to analyze false positives and false negatives", "Identify and rank top feature importances from the trained model"], "model_results": {"accuracy": 0.79, "precision": 0.81, "recall": 0.76, "f1": 0.78, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 2, "false_negative": 3}, "top_feature_importances": {"HbA1c": 0.31, "PreviousAdmissions": 0.22, "MedicationType_Insulin": 0.15, "BMI": 0.12, "Smoker_Yes": 0.1, "Age": 0.1}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict the likelihood of hospital readmission within 30 days for diabetic patients.", "raw_table": "PatientID,Age,Gender,HbA1c_Level,Insulin_Treatment,Num_Prior_Admissions,Smoker_Status,Readmitted_30Days\n1,54,M,7.8,Yes,2,No,Yes\n2,60,F,8.1,No,1,yes,No\n3,45,F,6.9,YES,0,No,No\n4,70,M,NaN,No,3,YEs,Yes\n5,39,F,7.2,No,0,No,No\n6,50,M,8.4,Yes,2,No,Yes\n7,58,F,7.9,No,1,No,No\n8,62,M,8.0,Yes,2,YES,Yes\n9,48,F,7.5,No,0,no,No\n10,55,M,7.3,Yes,1,No,Yes\n11,53,F,6.8,No,1,No,No\n12,67,M,8.6,Yes,3,Yes,Yes\n13,47,F,7.1,No,0,No,No\n14,59,M,7.7,Yes,2,No,Yes", "model_steps": ["Load the dataset and inspect for missing or inconsistent values.", "Impute missing HbA1c_Level values using median imputation.", "Standardize numeric features: Age, HbA1c_Level, Num_Prior_Admissions.", "Normalize categorical variables: standardize values for Gender, Insulin_Treatment, Smoker_Status, and target Readmitted_30Days by lowercasing and consistent mapping.", "One-hot encode categorical variables Gender, Insulin_Treatment, Smoker_Status.", "Split data into training (80%) and test (20%) sets with stratification on the target variable.", "Train a RandomForestClassifier to predict Readmitted_30Days.", "Perform grid search over hyperparameters max_depth [3,5,7] and n_estimators [50,100].", "Evaluate the final model on the test set using accuracy, precision, recall, and F1 score.", "Generate the confusion matrix and identify top feature importances."], "model_results": {"accuracy": 0.79, "precision": 0.81, "recall": 0.75, "f1": 0.78, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 2, "false_negative": 2}, "top_feature_importances": {"HbA1c_Level": 0.35, "Num_Prior_Admissions": 0.25, "Insulin_Treatment_Yes": 0.2, "Smoker_Status_Yes": 0.12, "Age": 0.08}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a retail customer will make a repeat purchase within 30 days based on their first purchase behavior and demographics.", "raw_table": "CustomerID,Age,Gender,Region,FirstPurchaseAmount,NumItemsPurchased,PaymentMethod,RepeatPurchase\nC001,34,Male,North,120.50,3,Credit Card,Yes\nC002,45,Female,South,85.00,2,Debit Card,No\nC003,,female,East,95.75,1,CASH,Yes\nC004,29,Female,West,50,1,credit card,No\nC005,53,Male,north,200.00,5,Debit Card,Yes\nC006,40,Male,South,NA,3,Cash,No\nC007,37,Female,East,150.00,4,Credit Card,Yes\nC008,41,Female,West,80.25,2,Debit card,No\nC009,28,Male,South,110.00,3,Credit Card,Yes\nC010,33,Female,East,60.00,1,Cash,No\nC011,47,male,North,130.50,4,Credit Card,Yes\nC012,38,Female,West,90.75,2,Debit Card,No\nC013,35,Male,South,75.00,2,Credit Card,No", "model_steps": ["Load the dataset and identify missing and inconsistent values", "Impute missing Age values with median age", "Standardize capitalization in categorical columns (Gender, Region, PaymentMethod)", "One-hot encode categorical variables Gender, Region, and PaymentMethod", "Convert target variable RepeatPurchase to binary label (Yes=1, No=0)", "Split data into training (80%) and test (20%) sets", "Standardize numeric features FirstPurchaseAmount and NumItemsPurchased", "Train a RandomForestClassifier on the training set", "Tune max_depth parameter using 5-fold cross-validation", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 important features"], "model_results": {"accuracy": 0.85, "f1": 0.82, "precision": 0.8, "recall": 0.85, "confusion_matrix": {"true_positive": 7, "false_positive": 2, "true_negative": 8, "false_negative": 1}, "top_feature_importances": {"FirstPurchaseAmount": 0.35, "NumItemsPurchased": 0.25, "PaymentMethod_Credit Card": 0.15}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Build a classification model to predict whether wheat crops will have high or low yield based on environmental and management factors.", "raw_table": "soil_type,avg_temp_c,rainfall_mm,fertilizer_kg_per_ha,previous_crop,yield_category\nLoam,22,300,50,Wheat,High\nClay,18,250,40,Corn,Low\nSAND,25,310,55,soybean,High\nLoam,20,NaN,45,Corn,Low\nClay,21,280,forty,Wheat,High\nSilt,23,290,50,Barley,High\nLoam,19,270,35,Wheat,Low\nSilt,22,300,60,Soybean,High\nClay,17,260,45,corn,Low\nLoam,21,280,50,Wheat,High\nSand,24,310,55,Barley,High\nSilt,20,275,42,Wheat,Low\nLoam,21,285,50,Corn,High\nClay,NaN,265,40,Barley,Low", "model_steps": ["Load dataset and identify features and target variable", "Clean data by standardizing categorical values (e.g., soil_type and previous_crop) to lowercase", "Impute missing numeric values using median imputation for rainfall_mm and avg_temp_c", "Convert fertilizer_kg_per_ha entries to numeric, handling non-numeric values by imputation", "One-hot encode categorical features soil_type and previous_crop", "Split data into 80% training and 20% testing sets with stratification on yield_category", "Standardize numeric features (avg_temp_c, rainfall_mm, fertilizer_kg_per_ha)", "Train a RandomForestClassifier with 100 trees using training data", "Perform grid search to tune max_depth and min_samples_split hyperparameters", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Generate and analyze confusion matrix to assess class-specific errors"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.79, "recall": 0.84, "confusion_matrix": [[7, 2], [1, 8]], "top_feature_importances": {"fertilizer_kg_per_ha": 0.28, "rainfall_mm": 0.24, "soil_type_loam": 0.15, "avg_temp_c": 0.13, "previous_crop_wheat": 0.1}, "best_hyperparameters": {"max_depth": 6, "min_samples_split": 4}}}
{"purpose": "Build a classification model to predict whether a movie will be a box office hit based on initial production and genre features.", "raw_table": "MovieID,Genre,Director,ProductionBudget,RuntimeMinutes,LeadActorPopularity,ReleaseMonth,BoxOfficeHit\n1,Action,Smith,150000000,130,8.5,July,Yes\n2,comedy,Jones,30000000,95,7.0,December,No\n3,Drama,Lee,NaN,120,6.5,March,No\n4,Action,smith,100000000,140,9.0,July,Yes\n5,Horror,White,20000000,85,5.5,October,No\n6,Comedy,Nguyen,40000000,100,,June,No\n7,Drama,Lee,50000000,110,7.8,April,Yes\n8,Action,Smith,175000000,135,9.2,July,Yes\n9,Comedy,Nguyen,32000000,98,6.2,August,No\n10,Horror,White,25000000,90,5.9,October,No\n11,Drama,Lee,48000000,115,7.5,March,No\n12,Comedy,Jones,35000000,102,6.8,December,No\n13,Action,Smith,160000000,132,8.8,July,Yes\n14,Drama,Khan,45000000,118,7.1,April,No\n15,Comedy,jones,38000000,100,7.0,November,No", "model_steps": ["Load dataset and handle missing values by imputing numeric columns with median and categorical with mode", "Normalize 'ProductionBudget' and 'RuntimeMinutes' using standard scaling", "Convert categorical columns 'Genre', 'Director', and 'ReleaseMonth' to lowercase and one-hot encode them", "Convert target variable 'BoxOfficeHit' to binary labels (Yes=1, No=0)", "Split data into train and test sets with 80/20 ratio, stratifying on the target", "Train a RandomForestClassifier with 100 trees on the training set", "Perform hyperparameter tuning over max_depth values [5, 10, 15] using 5-fold cross-validation", "Evaluate the best model on the test set computing accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.9, "f1": 0.87, "confusion_matrix": [[8, 2], [1, 9]], "top_feature_importances": {"productionbudget": 0.4, "releasemonth_july": 0.25, "leadactorpopularity": 0.15}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a loan applicant will default on their loan within one year.", "raw_table": "Applicant_ID,Age,Income,Employment_Status,Credit_Score,Loan_Amount,Loan_Purpose,Defaulted\n1,34,55000,Full-time,710,15000,Home,No\n2,22,42000,part-time, 680 ,12000,Car,Yes\n3,45,,Self-employed,720,20000,Education,No\n4,38,60000,Full-Time,690,18000,Home,No\n5,29,48000,Full-time,NA,13000,car,Yes\n6,50,80000,Full-time,750,25000,Home,No\n7,31,54000,full-time,710,16000,Car,No\n8,27,47000,Unemployed,640,11000,Education,Yes\n9,41,62000,Full-time,700,19000,Home,No\n10,36,58000,Part-Time,685,17000,Education,No\n11,30,50000,Full-time,705,14000,Car,No\n12,26,45000,Self-employed,675,12000,Car,Yes", "model_steps": ["Load dataset and identify target variable 'Defaulted'.", "Handle missing values in 'Income' and 'Credit_Score' by imputing median for numeric columns.", "Standardize inconsistent capitalization in categorical columns 'Employment_Status' and 'Loan_Purpose'.", "Convert the target variable 'Defaulted' to binary (Yes=1, No=0).", "One-hot encode categorical variables 'Employment_Status' and 'Loan_Purpose'.", "Split data into training (80%) and testing (20%) sets with stratification on the target variable.", "Standardize numeric features 'Age', 'Income', 'Credit_Score', and 'Loan_Amount'.", "Train a RandomForestClassifier with 100 trees on the training set.", "Perform grid search for max_depth parameter over [5, 10, 15] using 5-fold cross-validation.", "Evaluate model using accuracy, precision, recall, and F1 score on the test set.", "Generate and analyze confusion matrix to understand prediction errors."], "model_results": {"accuracy": 0.83, "f1": 0.79, "precision": 0.75, "recall": 0.84, "confusion_matrix": {"true_negative": 8, "false_positive": 2, "false_negative": 1, "true_positive": 4}, "top_feature_importances": {"Credit_Score": 0.35, "Income": 0.22, "Loan_Amount": 0.18, "Employment_Status_Self-employed": 0.1, "Age": 0.08, "Loan_Purpose_Car": 0.07}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a bank customer will default on their loan within the next year.", "raw_table": "CustomerID,Age,Income,LoanAmount,LoanPurpose,EmploymentStatus,CreditScore,Defaulted\n1,34,58000,15000,Home,Employed,720,No\n2,45,NaN,20000,car,Self-employed,680,Yes\n3,29,45000,NaN,home,Retired,710,No\n4,52,72000,25000,Car,employed,650,Yes\n5,41,67000,18000,Education,Unemployed,NaN,No\n6,38,60000,17000,Home,Employed,690,No\n7,50,80000,22000,Car,Employed,730,Yes\n8,47,NaN,21000,education,Self-Employed,675,No\n9,36,54000,16000,Home,Employed,700,No\n10,30,48000,14000,Car,Unemployed,NaN,No\n11,43,62000,19000,Home,Employed,710,No\n12,39,58000,16500,education,Self-employed,705,No\n13,55,75000,24000,Car,Employed,680,Yes\n14,33,50000,15500,Home,,695,No", "model_steps": ["Load data and identify target variable 'Defaulted'", "Handle missing numeric values by imputing median for Income, LoanAmount, and CreditScore", "Standardize numeric features: Age, Income, LoanAmount, CreditScore", "Normalize inconsistent capitalization and typos in categorical columns LoanPurpose and EmploymentStatus", "Impute missing categorical EmploymentStatus with mode 'Employed'", "One-hot encode categorical variables LoanPurpose and EmploymentStatus", "Split data into training (80%) and test set (20%) with stratification on target", "Train a RandomForestClassifier with 100 trees on the training data", "Tune max_depth hyperparameter using 5-fold cross-validation", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze prediction errors"], "model_results": {"accuracy": 0.86, "f1": 0.78, "precision": 0.75, "recall": 0.82, "confusion_matrix": {"true_positive": 9, "true_negative": 18, "false_positive": 3, "false_negative": 2}, "top_feature_importances": {"CreditScore": 0.32, "LoanAmount": 0.25, "EmploymentStatus_Employed": 0.15, "Income": 0.12, "LoanPurpose_Car": 0.08, "Age": 0.08}, "best_hyperparameters": {"max_depth": 7}}}
{"purpose": "Predict customer churn within the next month for a telecommunications company.", "raw_table": "CustomerID,MonthlyCharge,ContractType,TenureMonths,PaymentMethod,TotalCalls,InternetService,Churn\n1001,75.50,Month-to-month,3,Credit card,50,DSL,Yes\n1002,89.99,One year,24,Electronic check,120,Fiber optic,No\n1003,NaN,Month-to-month,1,Mailed check,30,No,yes\n1004,60.00,Two year,36,CREDIT CARD,70,DSL,No\n1005,95.20,Month-to-month,2,Electronic check,55,Fiber optic,Yes\n1006,55.10,One Year,15,Mailed check,80,No,No\n1007,82.45,month-to-month,5,Credit card,65,Fiber optic,Yes\n1008,70.00,Two year,40,Electronic check,90,DSL,No\n1009,65.00,Month-to-month,4,Mailed check,45,Fiber Optic,Yes\n1010,NaN,One year,18,Credit card,85,DSL,No\n1011,78.90,Month-to-month,,Electronic check,52,No,Yes\n1012,88.75,Two year,30,Mailed check,100,Fiber optic,No\n1013,68.00,MONTH-TO-MONTH,2,Credit Card,60,DSL,Yes\n1014,73.60,One year,20,Credit card,95,Fiber optic,No", "model_steps": ["Load the CSV data and identify missing and inconsistent values", "Standardize categorical values for consistency (e.g., unify 'Month-to-month' capitalization)", "Impute missing numeric values in MonthlyCharge with median", "Impute missing TenureMonths values with median tenure", "Encode target variable Churn as binary (Yes=1, No=0)", "One-hot encode categorical features: ContractType, PaymentMethod, InternetService", "Scale numeric features MonthlyCharge, TenureMonths, and TotalCalls using StandardScaler", "Split dataset into training (80%) and test set (20%) with stratification on Churn", "Train a RandomForestClassifier with 100 trees on the training set", "Tune max_depth hyperparameter using a simple grid search over values [5, 10, 15]", "Evaluate the final model on the test set computing accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix and extract top 3 feature importances"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.8, "f1": 0.815, "confusion_matrix": [[12, 3], [4, 15]], "top_feature_importances": {"ContractType_Month-to-month": 0.31, "TenureMonths": 0.27, "MonthlyCharge": 0.18}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict daily electricity consumption (kWh) for residential households based on weather and household characteristics.", "raw_table": "household_id,region,temperature_c,day_of_week,household_size,heating_type,is_weekend,daily_consumption_kwh\n1,North,5,Monday,4,Electric,No,32.5\n2,South,15,Tuesday,3,gas,Yes,28.1\n3,East,NaN,Wednesday,2,Electric,No,25.7\n4,west,20,Thursday,5,Gas,No,40.3\n5,North,10,Friday,3,Electric,No,30.0\n6,East,8,saturday,2,Electric,Yes,27.8\n7,South,16,Sunday,4,gas,Yes,33.2\n8,West,12,Monday,NaN,Electric,No,29.9\n9,North,7,Tuesday,3,Electric,No,31.1\n10,East,NaN,Wednesday,4,Gas,No,35.0\n11,South,19,Thursday,5,electric,No,42.5\n12,West,14,Friday,3,Gas,Yes,30.7\n13,North,9,Saturday,2,Electric,Yes,26.8\n14,East,11,Sunday,3,Gas,Yes,29.3\n15,South,13,Monday,4,Electric,No,33.9", "model_steps": ["Load the dataset and identify missing values.", "Impute missing temperature and household_size values using median imputation.", "Standardize numeric features: temperature_c and household_size.", "Normalize day_of_week and is_weekend into a binary weekend feature.", "One-hot encode categorical features: region (North, South, East, West) and heating_type (Electric, Gas), normalizing capitalization inconsistencies.", "Split data into training (80%) and test (20%) sets.", "Train a Gradient Boosting Regressor to predict daily_consumption_kwh.", "Perform grid search cross-validation to tune the number of estimators and max_depth.", "Evaluate the model on the test set using RMSE, MAE, and R2 metrics.", "Extract and rank feature importances from the trained model."], "model_results": {"rmse": 2.1, "mae": 1.6, "r2": 0.85, "best_hyperparameters": {"n_estimators": 100, "max_depth": 4}, "top_feature_importances": {"temperature_c": 0.35, "household_size": 0.25, "heating_type_Electric": 0.15, "region_North": 0.1, "weekend": 0.08, "region_South": 0.07}}}
{"purpose": "Predict whether a loan applicant will default based on their financial and demographic information.", "raw_table": "Applicant_ID,Age,Income,Employment_Status,Credit_Score,Loan_Amount,Loan_Purpose,Default\n1,45,85000,Full-time,720,25000,Home Improvement,No\n2,38,NaN,Self-employed,680,18000,Debt Consolidation,Yes\n3,29,54000,full-Time,690,12000,Car,No\n4,,72000,Part-Time,NaN,15000,Education,No\n5,52,104000,Unemployed,710,30000,Home Improvement,Yes\n6,41,62000,Full-time,640,9000,Car,No\n7,33,58000,Full-Time,690,NaN,Debt Consolidation,Yes\n8,27,43000,Self-Employed,650,7000,education,No\n9,60,99000,Part-time,730,20000,Home Improvement,No\n10,35,67000,Full-time,,25000,Car,Yes\n11,48,88000,Full-Time,700,22000,Debt Consolidation,No\n12,44,75000,Full-time,710,25000,Car,No\n13,31,53000,Part-time,690,12000,Home Improvement,Yes\n14,39,60000,,680,15000,Education,No", "model_steps": ["Load raw CSV data into a DataFrame", "Identify and handle missing values: impute numeric columns with median, categorical with mode", "Standardize capitalization in Employment_Status and Loan_Purpose columns to ensure consistency", "Convert Employment_Status and Loan_Purpose into one-hot encoded features", "Split data into training (80%) and test (20%) sets stratified by Default label", "Standardize numeric features: Age, Income, Credit_Score, Loan_Amount", "Train a RandomForestClassifier with default hyperparameters on training data", "Perform grid search over n_estimators (50, 100) and max_depth (5, 10, None) using 5-fold cross-validation", "Select best model from grid search and retrain on full training set", "Evaluate model on test set with accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze prediction errors", "Extract and report top 5 feature importances from trained model"], "model_results": {"accuracy": 0.79, "precision": 0.75, "recall": 0.68, "f1": 0.71, "confusion_matrix": {"True_Positive": 13, "True_Negative": 40, "False_Positive": 8, "False_Negative": 6}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}, "top_feature_importances": {"Credit_Score": 0.32, "Income": 0.21, "Loan_Amount": 0.18, "Employment_Status_Full-time": 0.12, "Loan_Purpose_Home Improvement": 0.08}}}
{"purpose": "Predict whether a student will pass the final exam based on their study habits, attendance, and demographic factors.", "raw_table": "StudentID,StudyHours,AttendanceRate,Gender,ParentalEducation,PreviousGrade,SchoolType,Passed\n1,12,0.85,Male,Bachelor,78,public,Yes\n2,8,0.90,Female,Highschool,85,Public,YES\n3,5,0.60,Female,Master,52,private,No\n4,14,0.95,Male,master,88,Private,Yes\n5,7,missing,Female,Bachelor,65,public,No\n6,10,0.80,Male,Highschool,70,public,Yes\n7,NaN,0.70,Female,Highschool,55,private,no\n8,11,0.88,Male,Bachelor,82,Public,Yes\n9,3,0.50,Male,highschool,40,Public,No\n10,9,0.92,Female,Bachelor,75,Private,Yes\n11,6,0.65,Female,Master,60,private,No\n12,13,0.89,Male,Highschool,85,Public,Yes\n13,4,0.55,Female,Bachelor,45,public,No\n14,NaN,0.77,Male,Master,70,private,Yes", "model_steps": ["Load dataset and identify target variable 'Passed' for binary classification", "Fix inconsistent capitalization in 'Passed' and convert to binary labels (Yes=1, No=0)", "Impute missing numeric values in 'StudyHours' and 'AttendanceRate' using median values", "Standardize numeric features: 'StudyHours', 'AttendanceRate', and 'PreviousGrade'", "One-hot encode categorical variables: 'Gender', 'ParentalEducation', 'SchoolType'", "Split data into training (80%) and testing (20%) sets", "Train a RandomForestClassifier with 100 trees on the training set", "Optimize max_depth using grid search with 5-fold cross-validation", "Evaluate model on test set computing accuracy, F1 score, precision, and recall", "Generate and analyze confusion matrix", "Extract and report top 5 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "f1": 0.87, "precision": 0.89, "recall": 0.85, "confusion_matrix": {"true_positive": 7, "false_positive": 1, "true_negative": 5, "false_negative": 2}, "top_feature_importances": {"AttendanceRate": 0.32, "PreviousGrade": 0.25, "StudyHours": 0.2, "ParentalEducation_Master": 0.08, "SchoolType_Private": 0.05}, "best_hyperparameters": {"max_depth": 7}}}
{"purpose": "Predict whether a customer will make a repeat purchase within 30 days after their first order.", "raw_table": "CustomerID,Age,Gender,LastPurchaseAmount,PreferredCategory,DaysSinceLastPurchase,RepeatPurchase\n1001,34,Male,120.50,Electronics,15,Yes\n1002,29,Female,85.00,Home,45,No\n1003,45,male,NaN,Books,5,Yes\n1004,23,Female,60,beauty,60,No\n1005,38,female,95.20,Electronics,NaN,Yes\n1006,NaN,Male,45.00,Toys,30,No\n1007,31,Male,110.00,Clothing,10,Yes\n1008,27,,80.00,Books,20,No\n1009,40,Female,130.75,Electronics,2,Yes\n1010,35,Male,NaN,Home,25,No\n1011,50,Female,150.00,beauty,1,Yes\n1012,22,female,55.25,Clothing,65,No\n1013,30,Male,100.00,Toys,NaN,Yes\n1014,28,Female,85.00,Books,35,No", "model_steps": ["Load data and inspect for missing or inconsistent values", "Normalize inconsistent capitalization in categorical columns (e.g., Gender, PreferredCategory)", "Impute missing numeric values with median (Age, LastPurchaseAmount, DaysSinceLastPurchase)", "Impute missing categorical values with the mode (Gender)", "Encode categorical variables using one-hot encoding", "Split data into training (80%) and testing (20%) sets stratified by target variable RepeatPurchase", "Train a RandomForestClassifier on training data", "Perform hyperparameter tuning on number of trees and max_depth using 5-fold cross-validation", "Evaluate model on test set reporting accuracy, precision, recall, and F1 score", "Analyze feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.89, "f1": 0.86, "top_feature_importances": {"DaysSinceLastPurchase": 0.32, "LastPurchaseAmount": 0.25, "PreferredCategory_Electronics": 0.14, "Age": 0.12, "Gender_Male": 0.08, "PreferredCategory_Books": 0.05, "PreferredCategory_Beauty": 0.04}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Predict the likelihood of citizens submitting tax returns on time based on demographic and economic factors.", "raw_table": "Citizen_ID,Age,Income,Employment_Status,State,Previous_Filing_Status,Tax_Return_Submitted_On_Time\n1,34,55000,Employed,California,Yes,Yes\n2,45,NaN,Self-employed,texas,No,No\n3,29,48000,Employed,New York,Yes,Yes\n4,,72000,Unemployed,Florida,yes,No\n5,53,62000,Employed,California,No,No\n6,41,58000,Employed,New york,NO,Yes\n7,38,NaN,Self-employed,Texas,Yes,Yes\n8,47,67000,Employed,Florida,No,No\n9,50,59000,Employed,California,Yes,Yes\n10,31,51000,Unemployed,Florida,,No\n11,44,60000,Self-employed,Texas,Yes,Yes\n12,28,53000,employed,New York,No,No\n13,36,56000,Employed,California,Yes,Yes\n14,42,58000,Employed,Texas,No,No", "model_steps": ["Load dataset and identify target variable as Tax_Return_Submitted_On_Time", "Handle missing values in Age and Income by imputing median values", "Normalize capitalization and spelling inconsistencies in categorical columns: Employment_Status, State, Previous_Filing_Status", "Encode categorical variables using one-hot encoding", "Split dataset into training (80%) and test (20%) sets", "Standardize numeric features Age and Income", "Train a RandomForestClassifier on training data", "Perform grid search to optimize max_depth and n_estimators hyperparameters", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and top feature importances"], "model_results": {"accuracy": 0.82, "f1": 0.79, "precision": 0.81, "recall": 0.77, "confusion_matrix": {"true_positive": 7, "false_positive": 2, "true_negative": 8, "false_negative": 3}, "top_feature_importances": {"Previous_Filing_Status_Yes": 0.32, "Income": 0.25, "Employment_Status_Employed": 0.18, "Age": 0.15, "State_California": 0.1}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a social media post will become viral based on post attributes and user engagement metrics.", "raw_table": "post_id,post_length,post_type,user_followers,user_verified,avg_likes,last_post_engagement,viral\n1,120,text,1500,yes,300,0.35,yes\n2,45,image,300,N,50,0.15,no\n3,NA,Video,2000,Yes,400,0.40,Yes\n4,200,text,5000,yes,1200,0.70,yes\n5,78,Text,700,n,80,,no\n6,150,video,3500,YES,850,0.55,yes\n7,32,image,100,n,20,0.05,no\n8,65,image,850,N,90,0.25,No\n9,110,video,4200,Yes,900,0.60,yes\n10,95,text,1300,y,250,0.30,no\n11,87,text,1100,No,200,0.28,no\n12,50,image,900,n,70,0.20,no\n13,140,Video,2700,yes,600,0.48,yes\n14,60,text,100,N,,0.12,no", "model_steps": ["Load the dataset and handle missing values by imputing median for numeric and most frequent for categorical columns", "Standardize 'post_length', 'user_followers', 'avg_likes', and 'last_post_engagement' numeric features", "Normalize inconsistent capitalization in 'post_type' and 'user_verified' columns and convert them to categorical variables", "Encode categorical variables 'post_type' and 'user_verified' using one-hot encoding", "Split the data into training (80%) and test (20%) sets with stratification on the target variable 'viral'", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search over max_depth values [5, 10, 15] and select best based on cross-validation F1 score", "Evaluate the final model on the test set computing accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix for test set predictions", "Identify and report top 3 feature importances from the RandomForest model"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"avg_likes": 0.35, "user_followers": 0.28, "last_post_engagement": 0.25}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Classify whether a field is likely to have a pest infestation based on environmental and crop conditions.", "raw_table": "Field_ID,Soil_Type,Crop_Type,Avg_Temperature,Humidity,Previous_Pest_Infestation,Rainfall_mm,Pest_Infestation\n1,Loam,Wheat,22.5,68,Yes,120,Yes\n2,Sandy,Corn,25.0,55,no,80,No\n3,Clay,soybean,19.8,72,,110,Yes\n4,loam,Corn,23.1,65,Yes,95,Yes\n5,SANDY,Barley,21.3,60,No,NaN,No\n6,Clay,Wheat,20.0,70,No,100,Yes\n7,Loam,Corn,22.0,65,Yes,105,Yes\n8,Sandy,Barley,26.5,,No,90,No\n9,Clay,soybean,NaN,68,Yes,115,Yes\n10,Loam,Wheat,21.7,66,No,108,No\n11,Sandy,Corn,24.3,63,Yes,85,No\n12,Clay,Barley,20.5,70,No,102,Yes", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Standardize categorical values in 'Soil_Type' and 'Previous_Pest_Infestation' columns (fix capitalization and missing entries)", "Impute missing numeric values in 'Avg_Temperature', 'Humidity', and 'Rainfall_mm' using median values", "Encode categorical variables 'Soil_Type', 'Crop_Type', and 'Previous_Pest_Infestation' using one-hot encoding", "Split data into train and test sets with 80% for training and 20% for testing", "Train a RandomForestClassifier with 100 trees on the training set", "Tune max_depth hyperparameter using grid search with 3-fold cross-validation", "Evaluate the model on the test set computing accuracy, precision, recall, and F1 score", "Generate confusion matrix to understand classification errors", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.83, "precision": 0.79, "recall": 0.85, "f1": 0.82, "confusion_matrix": {"true_positive": 5, "false_positive": 2, "true_negative": 7, "false_negative": 1}, "top_feature_importances": {"Previous_Pest_Infestation_Yes": 0.32, "Humidity": 0.25, "Soil_Type_Loam": 0.15}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a given day will experience high air pollution (AQI > 100) based on weather and traffic conditions.", "raw_table": "Date,Temperature_C,Humidity_pct,Wind_Speed_kmh,Weather_Condition,Traffic_Level,AQI_High\n2024-03-01,15.2,55,12,Clear,High,No\n2024-03-02,17.8,60,8,Cloudy,Medium,Yes\n2024-03-03,14.5,58,?,Rain,low,No\n2024-03-04,19.0,62,15,clear,High,Yes\n2024-03-05,,65,10,Fog,medium,No\n2024-03-06,20.1,70,13,Cloudy,Medium,Yes\n2024-03-07,18.3,59,7,Rain,LOW,No\n2024-03-08,16.7,57,11,Clear,High,Yes\n2024-03-09,15.0,55,9,cloud,Fog,No\n2024-03-10,21.4,72,14,Fog,High,Yes\n2024-03-11,22.0,67,12,Clear,Medium,Yes\n2024-03-12,19.5,63,11,rain,Medium,No\n2024-03-13,18.8,60,10,Clear,high,Yes\n", "model_steps": ["Load the CSV dataset and parse columns, noting missing and inconsistent values", "Fill missing numeric values (Temperature_C and Wind_Speed_kmh) using median imputation", "Standardize numeric features: Temperature_C, Humidity_pct, Wind_Speed_kmh", "Normalize 'Weather_Condition' and 'Traffic_Level' categorical variables to consistent lowercase", "One-hot encode the categorical variables Weather_Condition and Traffic_Level", "Split data into training (80%) and testing (20%) sets using stratification on the target AQI_High", "Train a RandomForestClassifier model with 100 trees on the training data", "Perform grid search over max_depth [5, 10, 15] and min_samples_split [2, 5]", "Evaluate model performance on the test set using accuracy, F1 score, precision, and recall", "Generate a confusion matrix to analyze false positives and false negatives"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.81, "recall": 0.85, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"Traffic_Level_High": 0.32, "Weather_Condition_Clear": 0.25, "Wind_Speed_kmh": 0.15, "Temperature_C": 0.12, "Humidity_pct": 0.1, "Weather_Condition_Fog": 0.06}, "best_hyperparameters": {"max_depth": 10, "min_samples_split": 2}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features.", "raw_table": "HouseID,Neighborhood,Bedrooms,Bathrooms,Area_sqft,Year_Built,Garage,Condition,SalePrice\n1,OldTown,3,2,1500,1998,Yes,good,250000\n2,Oldtown,4,3,2000,2005,yes,Fair,310000\n3,Northside,2,1,850,1980,No,Good,140000\n4,Southside,3,,1200,2010,No,poor,180000\n5,Eastside,5,4,3000,2018,Yes,Excellent,450000\n6,EastSide,4,3,2500,2015,yes,excellent,420000\n7,Northside,3,2,1300,1995,no,GOOD,230000\n8,OldTown,2,1,900,1985,No,Fair,160000\n9,Southside,4,3,2100,2008,Yes,Poor,300000\n10,Eastside,3,2,1800,2012,No,good,280000\n11,OldTown,,2,1400,2000,Yes,Good,260000\n12,Southside,4,3,2200,2007,Yes,Fair,320000\n13,Northside,3,2,1600,1990,No,Fair,240000\n14,Eastside,4,3,2700,2016,Yes,excellent,400000", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Correct inconsistent capitalization in categorical columns like Neighborhood and Condition", "Impute missing numeric values in Bedrooms and Bathrooms using median values", "Encode categorical variables Neighborhood, Garage, and Condition using one-hot encoding", "Standardize numeric features: Bedrooms, Bathrooms, Area_sqft, Year_Built", "Split data into train (80%) and test (20%) sets", "Train a Random Forest Regressor to predict SalePrice", "Tune max_depth parameter using grid search with 5-fold cross-validation", "Evaluate model performance on the test set using RMSE, MAE, and R2 metrics", "Identify and report top 3 most important features"], "model_results": {"rmse": 18000, "mae": 13500, "r2": 0.85, "top_feature_importances": {"Area_sqft": 0.45, "Neighborhood_OldTown": 0.18, "Condition_Excellent": 0.12}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a government benefit application will be approved based on applicant and application details.", "raw_table": "Applicant_ID,Age,Income,Employment_Status,Education_Level,Previous_Applications,Application_Month,Application_Approved\n001,34,45000,Employed,Bachelors,2,January,Yes\n002,29,NaN,unemployed,Highschool,0,February,No\n003,45,78000,Employed,Masters,1,MARCH,Yes\n004,52,67000,Self-employed,Phd,4,April,Yes\n005,23,25000,employed,highschool,0,May,No\n006,37,54000,Employed,Bachelors,2,June,Yes\n007,,48000,Unemployed,Somecollege,1,July,No\n008,43,69000,Employed,Bachelors,3,August,Yes\n009,31,52000,Employed,NaN,0,September,No\n010,28,30000,Unemployed,HighSchool,1,October,No\n011,50,73000,Self-employed,Masters,5,November,Yes\n012,35,NaN,Employed,Bachelors,2,December,Yes\n013,40,60000,Employed,PhD,3,January,Yes\n014,27,29000,unemployed,Highschool,0,February,No", "model_steps": ["Load the raw CSV data into a DataFrame", "Handle missing values by imputing mean for numeric columns and 'Unknown' for categorical columns", "Normalize inconsistent capitalization in categorical variables (e.g., Employment_Status, Education_Level, Application_Month)", "Encode categorical variables using one-hot encoding", "Split the dataset into training and testing sets using an 80/20 ratio", "Standardize numeric features like Age and Income", "Train a RandomForestClassifier to predict Application_Approved", "Perform grid search over number of trees and max_depth hyperparameters", "Evaluate model performance using accuracy, precision, recall, and F1 score on the test set", "Generate and analyze a confusion matrix", "Identify and report the top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"True_Positive": 38, "True_Negative": 28, "False_Positive": 7, "False_Negative": 5}, "top_feature_importances": {"Income": 0.32, "Employment_Status_Employed": 0.18, "Previous_Applications": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Predict whether a given region will experience a heatwave event next month based on climate and geographic features.", "raw_table": "Region,Avg_Temperature_C,Precipitation_mm,Vegetation_Type,Elevation_m,Soil_Moisture,Heatwave_Next_Month\nNorthwest,23.5,120.4,Forest,450,0.32,Yes\nSoutheast,28.1,85.2,grassland,150,0.28,No\nMidwest,19.7,,Cropland,300,0.45,No\nNortheast,21.8,110.1,Forest,200,0.33,YES\nSouthwest,30.2,55.0,DESERT,500,0.10,yes\nCentral,25.0,95.5,Cropland,250,missing,No\nCoastal,22.3,100.0,Forest,50,0.40,No\nHighlands,18.9,130.2,Grassland,1200,0.50,No\nDelta,27.5,140.0,wetland,5,0.70,Yes\nPlains,26.0,90.2,Cropland,350,0.38,No\n", "model_steps": ["Load dataset and inspect for missing and inconsistent values", "Standardize capitalization in categorical columns and target variable", "Impute missing numeric values with median of respective columns", "One-hot encode 'Vegetation_Type' categorical variable", "Split data into training and testing sets (80/20)", "Standardize numeric features using training set parameters", "Train a RandomForestClassifier to predict Heatwave_Next_Month", "Perform grid search over number of trees and max_depth hyperparameters", "Evaluate model using accuracy, precision, recall, and F1 score on test set", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.83, "precision": 0.79, "recall": 0.85, "f1": 0.82, "confusion_matrix": {"True_Positive": 5, "True_Negative": 6, "False_Positive": 2, "False_Negative": 1}, "top_feature_importances": {"Avg_Temperature_C": 0.35, "Soil_Moisture": 0.25, "Precipitation_mm": 0.2}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict customer churn probability for a telecom provider based on usage patterns and service features.", "raw_table": "CustomerID,Monthly_Charge,Contract_Type,Tenure_Months,Support_Calls,Internet_Service,Payment_Method,Churn\n1,79.85,Month-to-month,5,3,Fiber optic,ELECTRONIC CHECK,Yes\n2,65.50,One year,14,0,Fiber Optic,Mailed check,no\n3,NaN,Month-to-month,1,1,dsl,Electronic check,Yes\n4,49.95,Two year,24,0,None,Bank transfer,no\n5,88.20,Month-to-month,2,4,Fiber optic,Credit card,Yes\n6,56.75,Month-to-month,,2,Dsl,Electronic Check,No\n7,70.10,One Year,8,1,Fiber optic,credit Card,No\n8,NaN,Month-to-month,3,2,Fiber Optic,electronic check,Yes\n9,45.00,Two year,36,0,None,Mailed Check,no\n10,60.00,Month-to-month,6,3,Dsl,Electronic Check,Yes\n11,72.25,One year,12,0,Fiber optic,Credit Card,no\n12,80.50,Month-to-month,4,,Fiber Optic,Electronic Check,Yes\n13,54.30,Month-to-month,7,1,None,Bank Transfer,no\n14,68.00,One Year,10,1,dsl,Electronic Check,No\n15,85.00,Month-to-month,2,3,Fiber Optic,Electronic check,Yes", "model_steps": ["Load the dataset and identify target variable 'Churn'", "Clean the data by fixing inconsistent capitalization in categorical columns and imputing missing numeric values with median", "Convert categorical columns to lowercase and standardize categories (e.g., 'fiber optic', 'dsl', 'none')", "Encode categorical variables using one-hot encoding", "Split data into train (80%) and test (20%) sets with stratification on the target variable", "Standardize numeric features like Monthly_Charge, Tenure_Months, and Support_Calls", "Train a RandomForestClassifier with default parameters on the training set", "Perform grid search to tune max_depth and n_estimators hyperparameters", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze false positives and false negatives", "Extract and report feature importances from the final Random Forest model"], "model_results": {"accuracy": 0.87, "f1": 0.82, "precision": 0.79, "recall": 0.86, "confusion_matrix": {"true_positive": 36, "true_negative": 45, "false_positive": 10, "false_negative": 6}, "top_feature_importances": {"Contract_Type_Month-to-month": 0.32, "Support_Calls": 0.21, "Monthly_Charge": 0.18, "Internet_Service_Fiber optic": 0.12, "Tenure_Months": 0.09}, "best_hyperparameters": {"max_depth": 8, "n_estimators": 150}}}
{"purpose": "Predict the genre of a movie based on its production and cast features.", "raw_table": "MovieID,Director,LeadActor,DurationMinutes,BudgetMillions,Language,ReleaseMonth,Genre\n1,spielberg,Tom Hanks,142,90,English,December,Drama\n2,Nolan,Christian Bale,152,160,english,July,Action\n3,Scorsese,Leonardo DiCaprio,180,70,English,November,Drama\n4,del Toro,Guillermo,130,45,Spanish,October,Fantasy\n5,burton,Johnny Depp,115,80,English,October,Fantasy\n6,lee,Brie Larson,118,20,English,March,Drama\n7,lee,Samuel L. Jackson,120,30,english,March,Action\n8,Coen,George Clooney,105,,English,April,Comedy\n9,scott,Russell Crowe,155,110,English,May,Action\n10,Cameron,Sam Worthington,162,200,english,December,Action\n11,Spielberg,Matt Damon,135,85,English,December,Drama\n12,nolan,Anne Hathaway,169,165,English,July,Action\n13,Del Toro,Benedict Cumberbatch,134,50,Spanish,October,Fantasy\n14,burton,Helena Bonham Carter,112,75,English,October,Fantasy", "model_steps": ["Load the CSV data into a DataFrame", "Clean the 'Director' and 'Language' columns to ensure consistent capitalization", "Fill missing values in 'BudgetMillions' with the median budget", "Convert categorical columns ('Director', 'LeadActor', 'Language', 'ReleaseMonth') using one-hot encoding", "Split the data into train and test sets with an 80/20 ratio", "Standardize numeric features 'DurationMinutes' and 'BudgetMillions' using StandardScaler", "Train a RandomForestClassifier to predict 'Genre'", "Perform grid search on 'n_estimators' and 'max_depth' hyperparameters", "Evaluate the model on the test set with accuracy, precision, recall, and F1 score", "Generate a confusion matrix to analyze misclassifications"], "model_results": {"accuracy": 0.85, "precision": {"Action": 0.88, "Drama": 0.82, "Fantasy": 0.83, "Comedy": 1.0}, "recall": {"Action": 0.91, "Drama": 0.75, "Fantasy": 0.8, "Comedy": 1.0}, "f1": {"Action": 0.89, "Drama": 0.78, "Fantasy": 0.81, "Comedy": 1.0}, "confusion_matrix": {"Action": {"Action": 10, "Drama": 1, "Fantasy": 0, "Comedy": 0}, "Drama": {"Action": 1, "Drama": 6, "Fantasy": 1, "Comedy": 0}, "Fantasy": {"Action": 0, "Drama": 1, "Fantasy": 4, "Comedy": 0}, "Comedy": {"Action": 0, "Drama": 0, "Fantasy": 0, "Comedy": 2}}, "top_feature_importances": {"BudgetMillions": 0.23, "Director_Spielberg": 0.15, "DurationMinutes": 0.14, "Language_English": 0.12, "ReleaseMonth_October": 0.1, "Director_Nolan": 0.08, "LeadActor_Tom Hanks": 0.06, "LeadActor_Johnny Depp": 0.05, "Language_Spanish": 0.04, "ReleaseMonth_December": 0.03}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 8}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features.", "raw_table": "HouseID,Bedrooms,Bathrooms,Size_sqft,Neighborhood,Year_Built,Has_Garage,Condition,SalePrice\n1,3,2,1500,Downtown,1995,Yes,Good,350000\n2,4,3,2500,Suburb,2010,No,Fair,420000\n3,2,1,900,Suburb,2005,yes,Excellent,275000\n4,3,,1800,uptown,2018,Yes,Good,390000\n5,5,4,3200,DOWNTOWN,2000,No,good,550000\n6,3,2,1600,Suburb,NaN,Yes,Fair,360000\n7,4,3,2400,uptown,2015,Yes,Excellent,480000\n8,3,2,1800,Downtown,1990,No,Good,340000\n9,2,1,850,Suburb,2003,No,FAIR,260000\n10,4,3,2600,Suburb,2012,Yes,Excellent,460000\n11,3,2,1700,Downtown,NaN,Yes,Good,355000\n12,4,3,2500,Suburb,2009,No,Good,435000\n13,3,2,1600,Uptown,2016,YES,Fair,375000\n14,2,1,950,Suburb,2007,No,Good,280000", "model_steps": ["Load the raw CSV data into a DataFrame", "Identify and handle missing values in Bathrooms and Year_Built columns by imputing with median values", "Normalize capitalization and spelling inconsistencies in categorical columns: Neighborhood, Has_Garage, and Condition", "Encode categorical variables Neighborhood, Has_Garage, and Condition using one-hot encoding", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features Bedrooms, Bathrooms, Size_sqft, and Year_Built", "Train a Gradient Boosting Regressor on the training set", "Perform hyperparameter tuning on learning_rate and n_estimators using cross-validation", "Evaluate the model on the test set calculating RMSE, MAE, and R2 score", "Extract and report feature importances from the trained model"], "model_results": {"rmse": 21000, "mae": 16000, "r2": 0.87, "top_feature_importances": {"Size_sqft": 0.42, "Neighborhood_Suburb": 0.18, "Condition_Excellent": 0.12, "Bathrooms": 0.1, "Has_Garage_Yes": 0.08, "Year_Built": 0.05, "Bedrooms": 0.05}, "best_hyperparameters": {"learning_rate": 0.1, "n_estimators": 150}}}
{"purpose": "Predict whether a customer will make a purchase during a promotional campaign based on their demographics and browsing behavior.", "raw_table": "CustomerID,Age,Gender,Region,AvgSessionTime,PagesViewed,LastPurchaseDays,DeviceType,Purchase\nC001,34,Male,North,12.5,8,15,Mobile,Yes\nC002,29,Female,South,7.2,5,NA,Desktop,No\nC003,45,Female,East,9.8,7,20,Tablet,Yes\nC004,23,male,West,15.0,10,5,Mobile,Yes\nC005,38,Female,North,8.7,6,25,Laptop,No\nC006,31,Female,South,10.2,7,,Desktop,No\nC007,27,Male,East,13.1,9,10,Mobile,Yes\nC008,40,Male,West,6.5,4,30,Laptop,no\nC009,35,Female,North,11.0,7,12,tablet,Yes\nC010,22,Female,South,9.0,5,18,Mobile,No\nC011,50,Male,East,5.5,3,40,Desktop,No\nC012,28,Female,West,14.2,8,7,Mobile,Yes\nC013,,Male,North,8.0,5,22,Laptop,No\nC014,33,Female,East,11.5,6,15,Mobile,Yes\nC015,41,Male,South,7.8,4,35,Desktop,No", "model_steps": ["Load the CSV data and parse into a DataFrame", "Identify and handle missing values in 'Age' and 'LastPurchaseDays' by imputing with median values", "Standardize inconsistent capitalization in categorical columns like 'Gender', 'DeviceType', and 'Purchase'", "Encode categorical features 'Gender', 'Region', and 'DeviceType' using one-hot encoding", "Convert target variable 'Purchase' to binary labels: Yes=1, No=0", "Split the dataset into training (80%) and test sets (20%) with stratification on the target variable", "Scale numeric features 'Age', 'AvgSessionTime', 'PagesViewed', 'LastPurchaseDays' using StandardScaler", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search over max_depth parameter with values [5, 10, 15]", "Evaluate the final model on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix to analyze prediction errors", "Identify top 3 most important features based on the trained model"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.9, "f1": 0.87, "confusion_matrix": {"true_positive": 26, "true_negative": 20, "false_positive": 4, "false_negative": 3}, "top_feature_importances": {"LastPurchaseDays": 0.32, "AvgSessionTime": 0.25, "PagesViewed": 0.18}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict whether a taxi trip will exceed 20 minutes in duration.", "raw_table": "trip_id,passenger_count,trip_distance,pickup_time,day_of_week,weather_condition,driver_rating,trip_duration_minutes,long_trip\n1,2,3.5,08:15,monday,Clear,4.5,18,No\n2,1,7.2,19:45,Tuesday,Rain,3.8,25,Yes\n3,3,2.1,12:00,Wednesday,CLEAR,4.9,15,No\n4,1,10.0,23:30,thursday,Fog,NaN,28,Yes\n5,4,1.7,14:05,Friday,Rain,4.0,11,No\n6,2,5.3,07:50,saturday,Clear,3.7,22,Yes\n7,3,4.0,18:10,Sunday,clear,NaN,19,No\n8,2,6.8,20:05,Monday,Rain,4.2,26,Yes\n9,1,3.0,09:30,Wednesday,Clear,4.8,16,No\n10,2,,17:45,Thursday,Fog,3.5,27,Yes\n11,1,8.1,22:15,Friday,Rain,4.1,30,Yes\n12,3,2.5,11:00,Saturday,CLEAR,4.7,14,No\n13,,3.8,06:55,Sunday,Clear,3.9,21,Yes\n14,2,7.0,19:00,monday,Rain,4.0,24,Yes", "model_steps": ["Identify 'long_trip' as the target variable for classification.", "Handle missing values in 'passenger_count', 'trip_distance', and 'driver_rating' by imputing the median for numeric columns.", "Normalize inconsistent capitalization in 'weather_condition' and 'day_of_week' columns to standardize categorical values.", "Convert 'pickup_time' to a numeric feature representing minutes since midnight.", "One-hot encode the categorical features: 'day_of_week' and 'weather_condition'.", "Split the dataset into training and testing sets using an 80/20 ratio with stratification on the target variable.", "Standardize numeric features: 'passenger_count', 'trip_distance', 'driver_rating', and 'pickup_time'.", "Train a RandomForestClassifier with 100 trees on the training data.", "Tune 'max_depth' parameter using 5-fold cross-validation over values [5, 10, 15].", "Evaluate the final model on the test set, computing accuracy, precision, recall, and F1 score.", "Generate a confusion matrix and extract the top 3 feature importances from the trained model."], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": [[8, 2], [3, 11]], "top_feature_importances": {"trip_distance": 0.34, "pickup_time": 0.22, "day_of_week_Saturday": 0.14}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict if a loan applicant will default within the next 12 months based on their financial and demographic information.", "raw_table": "Applicant_ID,Age,Employment_Status,Annual_Income,Credit_Score,Loan_Amount,Loan_Term_Months,Loan_Purpose,Defaulted\n1,45,Employed,55000,720,15000,36,Home Improvement,No\n2,29,Self-employed,67000,NA,25000,60,Debt Consolidation,Yes\n3,37,Unemployed,40000,650,10000,24,Business,No\n4,52,Employed,78000,780,20000,48,HOME IMPROVEMENT,No\n5,,Employed,62000,710,18000,36,Debt consolidation,Yes\n6,33,Self-employed,71000,690,22000,60,Car,No\n7,41,employed,59000,705,13000,36,Car,No\n8,28,Unemployed,35000,670,9000,12,Business,Yes\n9,48,Employed,60000,730,16000,36,Home Improvement,No\n10,55,Self-employed,NaN,720,21000,60,Debt Consolidation,Yes\n11,30,Employed,52000,680,12000,24,Car,No\n12,39,Self-employed,68000,700,19000,48,Home improvement,No\n13,44,Unemployed,45000,640,11000,24,Business,Yes\n14,50,Employed,63000,715,17000,36,Debt Consolidation,No", "model_steps": ["Load data and handle missing values by imputing median for numeric and mode for categorical columns", "Standardize numeric features: Age, Annual_Income, Credit_Score, Loan_Amount, Loan_Term_Months", "Normalize inconsistent capitalization in Employment_Status and Loan_Purpose columns", "One-hot encode categorical variables Employment_Status and Loan_Purpose", "Split data into 80% training and 20% testing sets", "Train a RandomForestClassifier with default parameters on the training set", "Perform grid search over max_depth values [5, 10, 15] and n_estimators [50, 100]", "Evaluate the best model on the test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix for test predictions", "Identify and report the top 3 feature importances"], "model_results": {"accuracy": 0.79, "precision": 0.76, "recall": 0.72, "f1": 0.74, "confusion_matrix": {"true_positive": 8, "true_negative": 10, "false_positive": 3, "false_negative": 4}, "top_feature_importances": {"Credit_Score": 0.32, "Loan_Amount": 0.25, "Employment_Status_Self-employed": 0.14}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict 30-day hospital readmission risk for patients with heart failure.", "raw_table": "PatientID,Age,Gender,BMI,BloodPressure,Cholesterol,Smoker,Diabetes,PreviousAdmissions,DischargeDisposition,Readmitted\n1,72,Male,29.5,140/90,High,Yes,No,2,Home,Yes\n2,65,Female,31.2,130/85,Normal,No,Yes,1,REHAB,No\n3,58,Female,27.0,145/95,high,No,No,0,home,No\n4,80,Male,,150/100,High,Yes,Yes,3,Home,Yes\n5,74,Male,34.8,135/88,High,YES,No,2,Home,Yes\n6,63,Female,26.5,125/80,Normal,No,No,0,HOME,No\n7,70,Male,28.0,,High,No,Yes,1,Home,Yes\n8,55,Female,30.1,120/78,Normal,No,No,0,Home,No\n9,68,Male,33.5,138/92,High,Yes,YES,2,rehab,Yes\n10,60,Female,29.7,140/90,Normal,No,No,0,Home,No\n11,75,male,32.0,145/95,high,yes,No,2,Home,Yes\n12,67,Female,28.8,133/87,Normal,NO,No,1,Home,No\n13,62,Male,27.5,130/85,Normal,No,No,0,Home,No\n14,79,Female,35.0,155/100,High,Yes,Yes,4,Rehab,Yes", "model_steps": ["Load dataset and identify target variable 'Readmitted' and features", "Clean and standardize categorical values in 'Cholesterol', 'Smoker', 'DischargeDisposition', and 'Gender'", "Impute missing values in 'BMI' and 'BloodPressure' using median and mode respectively", "Extract numeric systolic and diastolic values from 'BloodPressure' column and convert to separate numeric features", "Encode categorical variables 'Gender', 'Smoker', 'Diabetes', and 'DischargeDisposition' with one-hot encoding", "Split data into train and test sets with 80% training and 20% testing", "Standardize numeric features such as Age, BMI, SystolicBP, DiastolicBP, PreviousAdmissions", "Train a RandomForestClassifier to predict 'Readmitted'", "Perform grid search over hyperparameters 'n_estimators' and 'max_depth' using 5-fold cross-validation", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix and list top 3 feature importances"], "model_results": {"accuracy": 0.79, "f1": 0.76, "precision": 0.74, "recall": 0.78, "confusion_matrix": {"TruePositive": 7, "FalsePositive": 3, "TrueNegative": 12, "FalseNegative": 2}, "top_feature_importances": {"PreviousAdmissions": 0.28, "SystolicBP": 0.2, "Diabetes_Yes": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Predict whether a loan applicant will default on their loan based on financial and demographic features.", "raw_table": "Applicant_ID,Age,Income,Employment_Status,Credit_Score,Loan_Amount,Loan_Purpose,Defaulted\n1,34,55000,Full-time,720,15000,Car,No\n2,45,NaN,Self-employed,680,20000,Home,Yes\n3,29,48000,full-Time,NaN,12000,car,No\n4,53,72000,Part-time,700,25000,Home,No\n5,40,61000,Full-Time,690,18000,Education,Yes\n6,37,54000,Unemployed,650,14000,Car,No\n7,50,83000,Full-time,710,30000,HOME,Yes\n8,43,59000,Self-Employed,675,16000,Education,No\n9,31,58000,Full-time,NaN,13000,Car,No\n10,28,,Part-time,660,9000,Education,No\n11,46,60500,Self-employed,700,22000,Home,Yes\n12,38,62000,Full-time,695,17000,Car,No\n13,44,67000,full-time,710,19000,education,Yes\n14,35,59000,Part-Time,685,15000,Car,No", "model_steps": ["Load the dataset and identify the target column 'Defaulted'.", "Fix inconsistent capitalization in categorical columns (Employment_Status, Loan_Purpose) and target.", "Handle missing values: impute numeric columns (Age, Income, Credit_Score) using median values.", "Convert categorical variables 'Employment_Status' and 'Loan_Purpose' into one-hot encoded features.", "Encode target variable 'Defaulted' as binary (Yes=1, No=0).", "Split the data into training and test sets with an 80/20 ratio.", "Standardize numeric features: Age, Income, Credit_Score, and Loan_Amount.", "Train a Logistic Regression classifier on the training set.", "Evaluate the model on the test set computing accuracy, precision, recall, and F1 score.", "Generate the confusion matrix and display top feature coefficients influencing default prediction."], "model_results": {"accuracy": 0.79, "precision": 0.75, "recall": 0.72, "f1": 0.74, "confusion_matrix": {"true_positive": 8, "true_negative": 13, "false_positive": 3, "false_negative": 4}, "top_feature_coefficients": {"Credit_Score": -1.25, "Income": -0.85, "Employment_Status_Self-employed": 0.65, "Loan_Amount": 0.78, "Loan_Purpose_Home": 0.5}, "hyperparameters": {"model_type": "Logistic Regression", "regularization": "L2", "C": 1.0, "max_iter": 100}}}
{"purpose": "Predict whether a loan applicant will default on their loan based on financial and demographic features.", "raw_table": "Applicant_ID,Income,Employment_Status,Credit_Score,Loan_Amount,Loan_Purpose,Previous_Default,Defaulted\n1,55000,Full-time,720,15000,Car,No,No\n2,42000,part-time,680,12000,Home,No,No\n3,NaN,UnEmployed,600,8000,Education,Yes,Yes\n4,72000,Full-time,820,20000,Car,No,No\n5,30000,full-Time,640,5000,Home,No,No\n6,48000,Part-Time,700,10000,Car,,No\n7,53000,Self-employed,690,NaN,Education,No,No\n8,61000,Full-time,710,14000,Home,No,No\n9,45000,unemployed,650,9000,Car,Yes,Yes\n10,39000,Full-time,670,11000,Education,No,No\n11,58000,Part-time,NaN,13000,Car,No,No\n12,54000,Full-time,700,15000,Home,No,No\n13,50000,Self-Employed,680,12000,Car,No,No\n14,47000,Full-Time,640,7000,Education,Yes,Yes", "model_steps": ["Load the CSV data into a DataFrame and inspect for inconsistencies and missing values", "Standardize the capitalization of categorical columns Employment_Status and Loan_Purpose", "Impute missing numeric values in Income, Credit_Score, and Loan_Amount using median values", "Impute missing categorical values in Previous_Default with 'No'", "Encode categorical variables Employment_Status, Loan_Purpose, and Previous_Default using one-hot encoding", "Split data into training (80%) and testing (20%) sets with stratification on the target Defaulted", "Standardize numeric features Income, Credit_Score, and Loan_Amount using StandardScaler", "Train a RandomForestClassifier with 100 trees on the training set", "Tune max_depth hyperparameter via grid search with 5-fold cross-validation", "Evaluate the final model on the test set computing accuracy, precision, recall, F1 score, and confusion matrix", "Extract feature importances from the trained model", "Generate and save predictions on the test set"], "model_results": {"accuracy": 0.85, "precision": 0.8, "recall": 0.75, "f1": 0.77, "confusion_matrix": {"true_negative": 10, "false_positive": 2, "false_negative": 3, "true_positive": 9}, "top_feature_importances": {"Credit_Score": 0.32, "Previous_Default_Yes": 0.25, "Income": 0.18, "Loan_Amount": 0.15, "Employment_Status_Full-time": 0.1}, "best_hyperparameters": {"max_depth": 6}}}
{"purpose": "Predict whether a citizen is likely to comply with a new tax regulation based on demographic and financial data.", "raw_table": "Citizen_ID,Age,Income,Employment_Status,Region,Previous_Compliance,Has_Delinquency,Target_Compliance\n1,34,58000,Employed,North,Yes,No,Yes\n2,45,NaN,SELF-EMPLOYED,South,No,Yes,No\n3,29,45000,employed,East,Yes,No,Yes\n4,52,72000,Unemployed,West,No,No,No\n5,41,67000,Employed,North,,Yes,No\n6,37,52000,Employed,South,Yes,No,Yes\n7,,48000,Self-Employed,East,No,Yes,No\n8,33,56000,Employed,west,Yes,No,Yes\n9,49,83000,Unemployed,North,No,No,No\n10,27,47000,Employed,South,Yes,No,Yes\n11,31,51000,Employed,East,Yes,No,Yes\n12,44,61000,Self-employed,West,No,Yes,No\n13,39,59000,Employed,North,Yes,No,Yes\n14,36,NaN,Unemployed,South,No,Yes,No\n15,42,64000,Employed,East,Yes,,Yes", "model_steps": ["Load the dataset and identify missing and inconsistent values", "Impute missing numeric values in Age and Income columns using median", "Standardize capitalization in Employment_Status and Region columns to ensure consistency", "Fill missing categorical values in Previous_Compliance and Has_Delinquency with the most frequent category", "One-hot encode categorical variables: Employment_Status and Region", "Convert target variable Target_Compliance to binary labels (Yes=1, No=0)", "Split data into 80% training and 20% test sets maintaining class distribution", "Standardize numeric features Age and Income using training set statistics", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search over max_depth parameter with values [5, 10, 15]", "Evaluate model performance on the test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix and identify top 3 important features"], "model_results": {"accuracy": 0.8, "f1": 0.78, "precision": 0.75, "recall": 0.82, "confusion_matrix": [[12, 3], [2, 18]], "top_feature_importances": {"Previous_Compliance_Yes": 0.28, "Income": 0.22, "Has_Delinquency_No": 0.17}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a student will pass the final exam based on demographic and study-related features.", "raw_table": "StudentID,Age,Gender,StudyHoursPerWeek,AttendanceRate,PreviousGrade,Course,PassFinal\n1,19,Male,10,0.85,B,Math,Yes\n2,21,Female,8,0.90,A,Science,YES\n3,20,female,12,0.75,C,math,No\n4,22,Male,5,,B,History,No\n5,20,Female,7,0.80,B,Science,Yes\n6,19,Male,9,0.88,C,History,No\n7,21,Female,6,0.72,B,Science,Yes\n8,,Male,11,0.95,A,Math,Yes\n9,20,Female,six,0.90,A,History,No\n10,22,Female,4,0.65,C,Science,No\n11,21,Male,10,0.80,b,Math,Yes\n12,19,Female,7,0.85,A,science,Yes\n13,20,Male,8,0.77,C,History,No\n14,22,Female,9,0.83,B,Math,Yes", "model_steps": ["Load the CSV data and inspect for missing and inconsistent values", "Clean 'Gender' and 'Course' columns by standardizing capitalization", "Handle missing numeric values in 'Age', 'StudyHoursPerWeek', and 'AttendanceRate' by median imputation", "Convert 'StudyHoursPerWeek' from mixed types to numeric, fixing entries like 'six' to 6", "Encode categorical variables ('Gender', 'Course', 'PreviousGrade') using one-hot encoding", "Convert target variable 'PassFinal' to binary labels (Yes=1, No=0)", "Split the cleaned data into train and test sets using an 80/20 ratio with stratification on the target", "Standardize numeric features: 'Age', 'StudyHoursPerWeek', 'AttendanceRate'", "Train a RandomForestClassifier with 100 trees on the training data", "Evaluate the model on the test set calculating accuracy, precision, recall, and F1 score", "Analyze feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "top_feature_importances": {"StudyHoursPerWeek": 0.32, "AttendanceRate": 0.28, "PreviousGrade_B": 0.15, "Course_Math": 0.1, "Gender_Female": 0.08}}}
{"purpose": "Predict whether a taxi trip will take longer than 20 minutes based on trip and passenger features.", "raw_table": "trip_id,passenger_count,trip_distance,pickup_location,dropoff_location,payment_type,trip_duration_longer_than_20min\n1,2,3.5,Downtown,airport,Cash,Yes\n2,1,1.2,suburbs,Downtown,credit_card,No\n3,3,5.0,Downtown,Suburbs,CASH,Yes\n4,1,,Airport,Downtown,Credit_card,No\n5,2,7.8,Suburbs,Downtown,Credit Card,yes\n6,4,2.1,Downtown,airport,cash,No\n7,2,3.0,Airport,suburbs,Credit_card,No\n8,,4.5,Suburbs,Downtown,Cash,Yes\n9,3,6.0,downtown,Airport,CASH,Yes\n10,1,1.8,Downtown,Suburbs,credit_card,No\n11,2,5.5,Suburbs,Airport,Cash,Yes\n12,1,2.0,airport,Downtown,Credit Card,No\n13,3,8.2,Downtown,suburbs,CASH,Yes\n14,2,3.3,Suburbs,airport,Credit_Card,No", "model_steps": ["Load the CSV data into a dataframe and inspect for missing or inconsistent values", "Normalize casing in categorical columns such as payment_type and locations", "Impute missing numeric values in passenger_count and trip_distance with median values", "Encode categorical features (pickup_location, dropoff_location, payment_type) using one-hot encoding", "Convert target variable 'trip_duration_longer_than_20min' from Yes/No strings to binary labels (1/0)", "Split the dataset into training (80%) and test (20%) sets randomly", "Standardize numeric features: passenger_count and trip_distance", "Train a RandomForestClassifier to predict the binary target", "Perform grid search over number of trees (n_estimators) with values [50, 100, 150]", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix to analyze prediction errors", "Identify the top 3 most important features according to the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": [[8, 2], [1, 11]], "top_feature_importances": {"trip_distance": 0.42, "pickup_location_Downtown": 0.25, "passenger_count": 0.15}, "best_hyperparameters": {"n_estimators": 100}}}
{"purpose": "Predict whether a government grant application will be approved based on applicant and project features.", "raw_table": "Applicant_ID,Age,Education_Level,Project_Type,Requested_Amount,Previous_Grants,Submission_Month,Approved\n1,34,Bachelor,Infrastructure,50000,1,January,Yes\n2,45,Master,healthcare,75000,2,March,No\n3,29,PhD,Education,30000,,May,Yes\n4,NaN,High School,Infrastructure,40000,0,February,No\n5,52,Bachelor,HEALTHCARE,90000,3,July,Yes\n6,41,Master,Education,45000,1,August,No\n7,37,Master,Infrastructure,60000,2,April,Yes\n8,30,Bachelor,Education,NaN,1,June,Yes\n9,28,PhD,Education,35000,0,July,No\n10,55,high school,Healthcare,80000,2,March,No\n11,47,Master,Infrastructure,70000,3,October,Yes\n12,33,Bachelor,Education,40000,1,December,Yes\n13,39,PhD,Education,50000,1,August,No\n14,42,Bachelor,Infrastructure,65000,2,September,Yes", "model_steps": ["Load the raw dataset and inspect for missing and inconsistent values", "Normalize categorical variables: standardize casing in Education_Level and Project_Type", "Impute missing numeric values (Age and Requested_Amount) with median values", "Fill missing Previous_Grants with zero as no prior grants", "One-hot encode categorical variables Education_Level, Project_Type, and Submission_Month", "Convert target variable Approved to binary (Yes=1, No=0)", "Split data into training and test sets (80/20)", "Standardize numeric features: Age, Requested_Amount, Previous_Grants", "Train a RandomForestClassifier on the training data", "Perform grid search cross-validation for max_depth and n_estimators hyperparameters", "Evaluate the final model on test set using accuracy, precision, recall, and F1 score", "Generate and analyze a confusion matrix", "Report top 5 important features from the trained model"], "model_results": {"accuracy": 0.79, "precision": 0.81, "recall": 0.75, "f1": 0.78, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 2, "false_negative": 3}, "top_feature_importances": {"Requested_Amount": 0.28, "Previous_Grants": 0.22, "Project_Type_Infrastructure": 0.15, "Education_Level_Master": 0.12, "Age": 0.1}, "best_hyperparameters": {"max_depth": 7, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict whether a field has high or low soil moisture levels.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Temperature_C,Previous_Crop,Yield_tonnes_per_hectare,Soil_Moisture_Level\n1,Loam,120,22,Corn,3.5,High\n2,Sandy,85,25,wheat,2.8,low\n3,Clay,,20,Barley,3.1,High\n4,loam,110,23,Corn,3.7,High\n5,sandy,95,26,soybean,2.5,Low\n6,Clay,88,19,Corn,3.0,High\n7,Loam,100,21,Barley,3.3,High\n8,Loam,NaN,24,Wheat,2.9,Low\n9,SANDY,90,27,Corn,2.7,Low\n10,Clay,92,22,soybean,3.2,High\n11,Loam,115,23,Corn,3.6,High\n12,Clay,80,20,Barley,3.0,Low\n13,Sandy,85,25,Wheat,2.4,Low\n14,Loam,105,22,Corn,3.4,High", "model_steps": ["Handle missing values in Rainfall_mm by imputing with median rainfall", "Standardize Temperature_C and Rainfall_mm features", "Normalize Yield_tonnes_per_hectare feature", "Convert Soil_Type and Previous_Crop categorical columns to lowercase and one-hot encode them", "Encode target Soil_Moisture_Level as binary (High=1, Low=0)", "Split data into training (80%) and testing (20%) sets", "Train a RandomForestClassifier with 100 trees on the training set", "Perform hyperparameter tuning on max_depth and min_samples_split using 5-fold cross-validation", "Evaluate model on test set calculating accuracy, precision, recall, and F1-score", "Generate confusion matrix on test predictions", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "confusion_matrix": [[5, 1], [2, 6]], "top_feature_importances": {"Rainfall_mm": 0.35, "Soil_Type_loam": 0.22, "Yield_tonnes_per_hectare": 0.18}, "best_hyperparameters": {"max_depth": 5, "min_samples_split": 4}}}
{"purpose": "Predict whether a maize crop will have high or low yield based on soil and weather conditions.", "raw_table": "soil_ph,soil_type,rainfall_mm,avg_temp_c,previous_crop,fertilizer_used,yield_category\n6.5,Loam,120,22.4,Corn,Nitrogen,High\n7.1,Clay,85,20.1,Wheat,Nitrogen,Low\n5.8,Sand,100,19.8,CORN,Nitrogen,High\n6.3,Loam, ,21.0,Soybean,None,Low\n7.0,Clay,110,23.5,corn,Nitrogen,High\n6.7,Silt,95,22.0,Wheat,Phosphorus,Low\n6.1,Silt,105,21.5,Corn,Nitrogen,High\n6.8,Loam,115,21.9,Rice,none,Low\n6.4,Sand,90,20.5,Soybean,Nitrogen,Low\n,Clay,100,22.3,Corn,Phosphorus,High\n6.9,Loam,108,21.7,Corn,Nitrogen,High\n6.2,Silt,97,20.8,wheat,Nitrogen,Low\n6.0,Sand,102,21.2,Corn,Nitrogen,High", "model_steps": ["Load the data from CSV and identify missing values and inconsistent capitalization", "Fill missing numerical values in soil_ph and rainfall_mm columns with median values", "Standardize capitalization in categorical columns: soil_type, previous_crop, fertilizer_used", "One-hot encode categorical variables: soil_type, previous_crop, fertilizer_used", "Split data into training and test sets with 80% training data and 20% testing data", "Scale numeric features soil_ph, rainfall_mm, and avg_temp_c using StandardScaler", "Train a RandomForestClassifier to predict yield_category", "Perform grid search for number of estimators (100, 200) and max_depth (3, 5)", "Select the best model based on cross-validation F1 score", "Evaluate final model on test set and compute accuracy, precision, recall, and F1 score"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.85, "recall": 0.78, "best_hyperparameters": {"n_estimators": 200, "max_depth": 5}, "top_feature_importances": {"rainfall_mm": 0.27, "fertilizer_used_Nitrogen": 0.22, "soil_ph": 0.18, "avg_temp_c": 0.15, "soil_type_Loam": 0.08}}}
{"purpose": "Predict whether a taxi trip duration will exceed 15 minutes based on trip attributes.", "raw_table": "trip_id,passenger_count,pickup_location,dropoff_location,trip_distance,fare_amount,payment_type,trip_duration_over_15min\n1,2,Downtown,Airport,5.2,18.5,CARD,True\n2,1,Uptown,Downtown,3.1,12.0,cash,False\n3,3,Suburb,Downtown,8.5,25.0,Card,True\n4,,Downtown,Uptown,2.7,10.0,CARD,False\n5,1,Downtown,Airport,4.0,15.0,cash,True\n6,2,Suburb,Uptown,7.3,22.0,Cash,True\n7,1,uptown,Suburb,6.0,20.0,CARD,True\n8,4,Downtown,Suburb,3.5,13.5,CARD,False\n9,2,Uptown,Downtown,2.9,11.0,card,False\n10,3,Suburb,Airport,10.0,30.0,CARD,True\n11,1,Downtown,Suburb,abc,14.0,CARD,False\n12,2,Suburb,Downtown,5.5,,cash,True\n13,1,Airport,Downtown,4.8,16.0,CARD,True\n14,3,Suburb,Uptown,6.2,19.5,cash,True", "model_steps": ["Load CSV data into a DataFrame", "Clean 'trip_distance' column by replacing non-numeric values ('abc') with NaN and then imputing the mean", "Impute missing values in 'passenger_count' and 'fare_amount' with median values", "Standardize capitalization of 'payment_type' and 'pickup_location' categorical variables", "One-hot encode 'pickup_location', 'dropoff_location', and 'payment_type'", "Split data into train and test sets (80/20 split)", "Standardize numeric features: 'passenger_count', 'trip_distance', and 'fare_amount'", "Train a RandomForestClassifier to predict 'trip_duration_over_15min'", "Perform grid search over number of estimators (50, 100) and max_depth (5, 10)", "Evaluate model on test set computing accuracy, F1 score, precision, and recall", "Generate confusion matrix for test predictions"], "model_results": {"accuracy": 0.86, "f1": 0.87, "precision": 0.88, "recall": 0.86, "confusion_matrix": [[7, 1], [2, 9]], "top_feature_importances": {"trip_distance": 0.35, "fare_amount": 0.2, "pickup_location_Downtown": 0.15, "payment_type_CARD": 0.1, "passenger_count": 0.08}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features.", "raw_table": "HouseID,Bedrooms,Bathrooms,Size_sqft,Neighborhood,Year_Built,Has_Garage,Condition,SalePrice\n1,3,2,1500,Suburb,1995,Yes,Good,350000\n2,4,3,2000,Urban,2005,yes,Fair,450000\n3,2,,850,Suburb,1980,No,Poor,200000\n4,3,2,1300,suburb,1990,No,good,320000\n5,5,4,3000,Urban,2015,Yes,Excellent,600000\n6,4,2,1800,Urban,,Yes,Fair,420000\n7,3,2,1400,Rural,1975,No,Poor,250000\n8,2,1,900,Rural,1985,No,good,210000\n9,4,3,2200,Urban,2010,YES,Excellent,580000\n10,3,2,1600,Suburb,2000,Yes,Fair,370000\n11,3,2,1500,Suburb,1995,Yes,,355000\n12,2,1,800,Rural,1980,No,Poor,190000\n13,4,3,2100,Urban,2008,Yes,Fair,500000\n14,3,2,1450,Suburb,1992,No,Good,340000", "model_steps": ["Load the CSV data into a DataFrame", "Handle missing values: impute missing Bathrooms with median and drop rows with missing Year_Built or Condition", "Normalize inconsistent capitalization in 'Neighborhood', 'Has_Garage', and 'Condition' columns", "Convert categorical variables (Neighborhood, Has_Garage, Condition) using one-hot encoding", "Split the dataset into training (80%) and testing (20%) sets", "Standardize numeric features: Bedrooms, Bathrooms, Size_sqft, Year_Built", "Train a RandomForestRegressor model on the training data", "Perform hyperparameter tuning for number of trees (n_estimators) and max_depth using grid search with cross-validation", "Evaluate the trained model on the test set using RMSE, MAE, and R-squared", "Extract and report the top 5 feature importances from the trained model"], "model_results": {"rmse": 28000, "mae": 21000, "r2": 0.85, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}, "top_feature_importances": {"Size_sqft": 0.35, "Neighborhood_Urban": 0.2, "Condition_Good": 0.15, "Year_Built": 0.12, "Has_Garage_Yes": 0.08}}}
{"purpose": "Predict whether a government grant application will be approved based on applicant and project characteristics.", "raw_table": "Application_ID,Applicant_Age,Applicant_Education,Project_Type,Requested_Funding,Previous_Grants,Submission_Month,Approved\n1,34,Bachelor,Health,50000,2,March,Yes\n2,45,Master,Infrastructure,75000,1,april,No\n3,29,PhD,Education,60000,,May,Yes\n4,52,bachelor,Health,80000,3,June,No\n5,41,Master,Transport,45000,0,July,Yes\n6,37,Doctorate,Education,70000,1,August,No\n7,30,,Infrastructure,55000,2,September,Yes\n8,48,Bachelor,Transport,65000,1,October,No\n9,36,Master,Health,47000,1,November,Yes\n10,43,PhD,Education,62000,2,December,Yes\n11,50,Master,Infrastructure,72000,3,January,No\n12,28,BACHELOR,Transport,53000,0,February,Yes\n13,33,Master,health,68000,1,March,No\n14,,PhD,Education,59000,2,April,Yes", "model_steps": ["Load the CSV data into a dataframe", "Clean and standardize categorical variables (e.g., unify capitalization in Applicant_Education and Project_Type)", "Impute missing numeric and categorical values (Applicant_Age and Applicant_Education) using median and mode respectively", "Convert Submission_Month to one-hot encoded features", "Encode target variable 'Approved' into binary (Yes=1, No=0)", "Split data into training and test sets with an 80/20 ratio", "One-hot encode categorical variables: Applicant_Education and Project_Type", "Standardize numeric features: Applicant_Age, Requested_Funding, Previous_Grants", "Train a RandomForestClassifier with 100 trees", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Compute and analyze confusion matrix", "Identify and report top 3 most important features"], "model_results": {"accuracy": 0.79, "precision": 0.81, "recall": 0.75, "f1": 0.78, "confusion_matrix": [[8, 2], [3, 7]], "top_features": {"Requested_Funding": 0.34, "Project_Type_Health": 0.21, "Previous_Grants": 0.15}}}
{"purpose": "Predict the likelihood of extreme heat days in a region based on climate and geographic features.", "raw_table": "Region,Avg_Temp_C,Precipitation_mm,Land_Type,Year,Extreme_Heat (target)\nNorth,29.5,88.2,Urban,2018,Yes\nSouth,31.0,NaN,Rural,2017,no\nEast,27.3,120.5,urban,2019,Yes\nWest,24.1,95.0,Rural,2018,No\nNorth,30.7,85.3,Urban,2020,Yes\nSouth,NaN,102.4,Rural,2019,No\nEast,28.4,110.0,Urban,2017,yes\nWest,23.8,89.5,Rural,2020,No\nNorth,29.0,90.0,urban,2019,Yes\nSouth,30.1,98.7,Rural,2018,No\nEast,27.9,115.2,Urban,2020,Yes\nWest,24.3,85.7,Rural,2017,No", "model_steps": ["Load data and inspect for missing or inconsistent values", "Standardize 'Extreme_Heat' target values to binary labels (Yes=1, No=0)", "Impute missing numeric values in 'Avg_Temp_C' and 'Precipitation_mm' using median values", "Convert 'Land_Type' categorical variable to lowercase and perform one-hot encoding", "Split data into training (80%) and test (20%) sets, stratified by target", "Standardize numeric features: 'Avg_Temp_C', 'Precipitation_mm', and 'Year'", "Train a RandomForestClassifier with 100 trees and default parameters", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score", "Analyze feature importances to identify key predictors", "Generate predicted probabilities for extreme heat occurrence"], "model_results": {"accuracy": 0.83, "precision": 0.81, "recall": 0.85, "f1": 0.83, "feature_importances": {"Avg_Temp_C": 0.42, "Precipitation_mm": 0.25, "Year": 0.15, "Land_Type_rural": 0.1, "Land_Type_urban": 0.08}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Build a regression model to estimate house prices based on property features and location.", "raw_table": "PropertyID,Location,Size_sqft,Bedrooms,Bathrooms,Year_Built,Has_Garage,Price\n1,Downtown,850,2,1,1995,Yes,350000\n2,Suburban,1200,3,2,2005,YES,420000\n3,Suburban,NaN,3,2,2010,No,400000\n4,Rural,1500,4,3,1980,No,310000\n5,Downtown,700,1,1,2015,yes,375000\n6,Rural,1600,4,3,1975,No,305000\n7,Suburban,1300,3,2,2012,No,415000\n8,Downtown,900,2,1,2018,Yes,380000\n9,suburban,1250,3,2,2008,Yes,430000\n10,Downtown,800,2,1,NaN,No,345000\n11,Rural,1400,3,2,1990,No,320000\n12,Suburban,1100,3,1,2000,yes,395000\n13,Downtown,750,2,1,2016,No,360000\n14,Suburban,1150,3,2,2003,Yes,410000", "model_steps": ["Load raw CSV data into a DataFrame", "Identify and handle missing values: impute Size_sqft with median, Year_Built with mode", "Normalize inconsistent categorical entries in Has_Garage and Location columns (e.g., 'YES', 'yes', 'No', 'No' to consistent lowercase)", "One-hot encode categorical variables: Location and Has_Garage", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features: Size_sqft, Bedrooms, Bathrooms, Year_Built", "Train a RandomForestRegressor model to predict Price", "Perform grid search over number of estimators (50, 100) and max_depth (5, 10)", "Evaluate model using RMSE, MAE, and R-squared on test set", "Analyze feature importances from the trained model"], "model_results": {"rmse": 21000, "mae": 16000, "r2": 0.82, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}, "top_feature_importances": {"Size_sqft": 0.4, "Location_Suburban": 0.15, "Location_Downtown": 0.12, "Has_Garage_yes": 0.1, "Bedrooms": 0.08}}}
{"purpose": "Build a classification model to predict the likelihood of extreme heat days in a region based on climate and geographic features.", "raw_table": "Region,Elevation_m,Avg_Annual_Temp_C,Land_Cover_Type,Distance_to_Coast_km,Extreme_Heat_Day\nNorthWest,450,16.2,forest,120,Yes\nSouthEast,30,28.5,urban,15,Yes\nCentral,200,22.0,agriculture,missing,No\nnorthwest,500,15.8,Forest,130,No\nSouthEast,25,29.1,Urban,20,Yes\nCentral,210,21.5,Agriculture,85,No\nNorthWest,,16.0,forest,110,No\nSouthEast,40,27.8,urban,18,Yes\nCentral,205,22,agriculture,90,no\nNorthWest,480,16.5,Forest,115,No\nSouthEast,35,28.9,Urban,17,Yes\ncentral,195,21.8,agriculture,88,No\nSouthEast,38,28.2,Urban,19,Yes\nNorthWest,470,16.3,forest,112,No", "model_steps": ["Load the dataset from the CSV string into a DataFrame", "Standardize capitalization in categorical columns (Region, Land_Cover_Type, Extreme_Heat_Day)", "Fill missing numeric values in Elevation_m and Distance_to_Coast_km using median imputation", "Encode target variable 'Extreme_Heat_Day' as binary (Yes=1, No=0)", "One-hot encode categorical features Region and Land_Cover_Type", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features Elevation_m, Avg_Annual_Temp_C, and Distance_to_Coast_km", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search to tune max_depth parameter over [5, 10, None]", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze confusion matrix", "Interpret top 3 feature importances"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.81, "recall": 0.85, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"Avg_Annual_Temp_C": 0.42, "Land_Cover_Type_urban": 0.25, "Distance_to_Coast_km": 0.18}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a citizen will apply for government social assistance based on demographic and economic data.", "raw_table": "Age,Income,Employment_Status,Education_Level,Number_of_Dependants,Region,Previous_Assistance,Applied_Assistance\n34,45000,employed,Bachelor,2,North,Yes,No\n,38000,Unemployed,High School,3,south,No,Yes\n29,52000,Employed,Master,1,East,yes,No\n45,60000,Employed,PhD,0,west,No,No\n38,NaN,unemployed,High school,4,North,No,Yes\n50,70000,EMPLOYED,Bachelor,NaN,East,Yes,No\n27,32000,Unemployed,Associate,2,South,No,Yes\n41,62000,Employed,Bachelor,3,West,Yes,No\n33,48000,employed,Master,1,North,no,No\n,55000,Unemployed,High School,,South,Yes,Yes\n36,50000,Employed,Bachelor,2,East,No,No\n42,63000,Employed,Master,2,West,Yes,No\n30,47000,unemployed,High school,1,North,No,Yes", "model_steps": ["Load the raw CSV data into a DataFrame", "Handle missing values: impute numeric columns with median and categorical with the mode", "Standardize inconsistent capitalization in categorical columns such as Employment_Status and Region", "Convert Previous_Assistance and Applied_Assistance to binary encoding (Yes=1, No=0)", "One-hot encode categorical variables: Employment_Status, Education_Level, Region", "Split the dataset into training (80%) and testing (20%) sets with stratification on the target", "Standardize numeric features: Age, Income, Number_of_Dependants", "Train a RandomForestClassifier to predict Applied_Assistance", "Perform grid search cross-validation optimizing max_depth and n_estimators hyperparameters", "Evaluate the model on the test set reporting accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Extract and rank top 5 feature importances from the trained model"], "model_results": {"accuracy": 0.82, "precision": 0.78, "recall": 0.85, "f1": 0.81, "confusion_matrix": {"true_positive": 21, "true_negative": 18, "false_positive": 6, "false_negative": 4}, "top_feature_importances": {"Previous_Assistance": 0.32, "Income": 0.21, "Employment_Status_Employed": 0.15, "Number_of_Dependants": 0.13, "Education_Level_High School": 0.07}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict whether a taxi trip will have a tip amount greater than $5.", "raw_table": "trip_id,passenger_count,trip_distance,rate_code,payment_type,day_of_week,tip_amount,tip_over_5\n1,2,3.1,Standard,Credit,Monday,3.5,No\n2,1,0.7,standard,Cash,Tuesday,6.0,Yes\n3,3,7.2,2,Credit,Wednesday,,No\n4,1,1.5,Standard,credit,Thursday,5.0,No\n5,2,5.0,1,Cash,friday,7.5,Yes\n6,1,0.9,2,Cash,Saturday,0,No\n7,4,10.4,Standard,Credit,Sunday,8.0,Yes\n8,2,2.8,1,Cash,Monday,2.0,No\n9,1,6.0,standard,Credit,Tuesday,10.0,Yes\n10,3,3.3,2,Cash,Wednesday,NaN,No\n11,1,4.1,Standard,Credit,THURSDAY,4.0,No\n12,2,8.5,1,Cash,Friday,12.0,Yes\n13,1,,standard,Credit,Saturday,1.0,No\n14,3,9.0,2,cash,Sunday,15.5,Yes", "model_steps": ["Load raw CSV data into a DataFrame", "Clean and normalize categorical columns: unify 'rate_code' and 'payment_type' capitalization", "Impute missing 'trip_distance' and 'tip_amount' values with median values", "Convert target variable 'tip_over_5' to binary label (Yes=1, No=0)", "Split dataset into training and testing sets with 75% train, 25% test", "One-hot encode categorical variables: rate_code, payment_type, day_of_week", "Standardize numeric features: passenger_count, trip_distance", "Train a GradientBoostingClassifier with default parameters on the training data", "Evaluate model on test set by computing accuracy, precision, recall, and F1 score", "Extract top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.79, "f1": 0.81, "top_feature_importances": {"trip_distance": 0.42, "tip_amount": 0.25, "payment_type_Credit": 0.12}}}
{"purpose": "Predict whether a loan applicant will default on a loan within 12 months.", "raw_table": "ApplicantID,Age,Income,EmploymentStatus,CreditScore,LoanAmount,LoanPurpose,Defaulted\n1,45,85000,Full-time,720,15000,Home Improvement,No\n2,29,54000,contract,680,12000,Debt Consolidation,Yes\n3,NaN,62000,Full-Time,700,10000,Auto,No\n4,52,NaN,Part-time,650,8000,Home improvement,Yes\n5,37,72000,Self-Employed,NaN,20000,Debt consolidation,No\n6,31,48000,full-time,690,15000,Home Improvement,No\n7,44,93000,Full-time,740,25000,Auto,No\n8,28,55000,Unemployed,640,5000,Debt Consolidation,Yes\n9,39,67000,Part-Time,660,17000,Home Improvement,No\n10,50,79000,Self-employed,710,22000,auto,No\n11,35,58000,Full-Time,675,11000,Debt Consolidation,Yes\n12,41,60000,Full-time,690,9000,Home Improvement,No\n13,33,53000,CONTRACT,685,13000,Auto,Yes\n14,48,88000,Part-time,720,16000,Home Improvement,No\n15,29,NaN,Full-Time,695,14000,Debt Consolidation,Yes", "model_steps": ["Load data and identify target variable as 'Defaulted'", "Impute missing numeric values (Age, Income, CreditScore) using median imputation", "Standardize numeric features: Age, Income, CreditScore, LoanAmount", "Normalize inconsistent capitalization and unify EmploymentStatus and LoanPurpose categorical entries", "One-hot encode categorical variables EmploymentStatus and LoanPurpose", "Split data into training and test sets with 80% training and 20% testing", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search to optimize max_depth parameter between 3 and 10", "Evaluate model performance on test set computing accuracy, precision, recall, and F1 score", "Generate confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.8, "precision": 0.78, "recall": 0.75, "f1": 0.765, "confusion_matrix": {"true_positive": 6, "false_positive": 2, "true_negative": 7, "false_negative": 2}, "top_feature_importances": {"CreditScore": 0.32, "LoanAmount": 0.25, "EmploymentStatus_Full-time": 0.15, "Income": 0.12, "LoanPurpose_Debt Consolidation": 0.08}, "best_hyperparameters": {"max_depth": 7, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict whether a manufactured part will pass quality inspection based on sensor readings and machine settings.", "raw_table": "part_id,machine_type,operator_experience,temperature_celsius,pressure_psi,humidity_percent,shift,target_quality_passed\n1,A,5,75.2,101.3,45,Day,Yes\n2,B,3,80.1,99.5,50,night,No\n3,a,7,78.0,NA,48,Night,Yes\n4,C,2,85.5,102.0,47,Day,No\n5,B,4,81.3,100.2,missing,Day,Yes\n6,C,6,77.4,98.7,44,NIGHT,No\n7,B,3,79.0,101.0,46,Day,Yes\n8,A,,74.5,100.5,49,Night,No\n9,C,5,83.2,102.5,50,Day,Yes\n10,A,4,76.8,101.1,missing,Night,No\n11,B,3,80.0,99.8,47,Day,Yes\n12,C,7,82.1,NA,46,Day,No\n13,B,2,79.5,100.7,48,Night,Yes\n14,A,6,75.0,101.3,45,Day,No", "model_steps": ["Handle missing numeric values in pressure_psi and humidity_percent by imputing with median values", "Normalize inconsistent capitalization in machine_type and shift columns", "Fill missing operator_experience values with the median experience", "One-hot encode categorical variables: machine_type and shift", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features: temperature_celsius, pressure_psi, humidity_percent, operator_experience", "Train a RandomForestClassifier with 100 trees", "Perform grid search over max_depth parameter with values [5, 10, 15]", "Evaluate model performance using accuracy, precision, recall, and F1 score on the test set", "Generate confusion matrix for test set predictions"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.81, "recall": 0.85, "top_feature_importances": {"temperature_celsius": 0.25, "pressure_psi": 0.2, "operator_experience": 0.18, "machine_type_B": 0.12, "humidity_percent": 0.1, "shift_Night": 0.08, "machine_type_C": 0.07}, "confusion_matrix": {"True_Positive": 7, "True_Negative": 4, "False_Positive": 1, "False_Negative": 2}, "selected_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a loan applicant will default within one year based on financial and demographic features.", "raw_table": "Applicant_ID,Age,Income,Employment_Status,Credit_Score,Loan_Amount,Loan_Purpose,Defaulted\n1,34,55000,Employed,720,15000,Home Improvement,No\n2,45,NaN,Self-employed,680,20000,Debt Consolidation,Yes\n3,28,47000,employed,690,12000,Car,No\n4,52,80000,Unemployed,NaN,25000,Education,Yes\n5,40,62000,Employed,710,18000,Home improvement,No\n6,37,58000,Self-Employed,650,NaN,Car,No\n7,29,53000,Employed,675,14000,Holiday,Yes\n8,41,59000,Employed,700,16000,Debt Consolidation,No\n9,36,60000,,690,15000,Car,No\n10,50,72000,Employed,730,22000,Education,Yes\n11,33,50000,Self-employed,660,13000,Holiday,No\n12,39,61000,Employed,695,17000,Debt Consolidation,No\n13,47,70000,Unemployed,640,21000,Home Improvement,Yes\n14,31,48000,employed,685,12500,Car,No", "model_steps": ["Load the CSV data into a DataFrame", "Identify and fill missing numeric values with median values", "Standardize capitalization and fill missing values in categorical columns", "One-hot encode categorical variables: Employment_Status and Loan_Purpose", "Split data into training and testing sets with an 80/20 ratio", "Standardize numeric features: Age, Income, Credit_Score, Loan_Amount", "Train a RandomForestClassifier to predict Defaulted", "Perform grid search over n_estimators and max_depth hyperparameters", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.79, "f1": 0.76, "precision": 0.74, "recall": 0.78, "confusion_matrix": {"true_negatives": 7, "false_positives": 2, "false_negatives": 3, "true_positives": 6}, "top_feature_importances": {"Credit_Score": 0.32, "Loan_Amount": 0.25, "Income": 0.15, "Employment_Status_Self-employed": 0.1, "Loan_Purpose_Home Improvement": 0.08}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 8}}}
{"purpose": "Predict whether a given field will have high or low wheat yield based on soil and weather conditions.", "raw_table": "FieldID,SoilType,Rainfall_mm,Temperature_C,PreviousCrop,FertilizerUsage_kg,PlantingDepth_cm,YieldCategory\nF001,Loam,120,22,Corn,50,5,High\nF002,Clay,85,19,Wheat,45,6,Low\nF003,SAND,110,21,soybean,missing,5,High\nF004,Loam,100,20,Corn,40,7,Low\nF005,clay,95,18,Wheat,50,6,Low\nF006,Loam,130,,Corn,55,5,High\nF007,Loam,125,23,Soybean,60,4,High\nF008,Clay,90,20,Wheat,missing,6,Low\nF009,Sand,115,22,Corn,52,5,High\nF010,Loam,105,21,soybean,48,5,High\nF011,Clay,100,19,Wheat,47,6,Low\nF012,Loam,missing,22,Corn,53,5,High\nF013,Loam,110,20,Corn,50,5,High\n", "model_steps": ["Load the CSV data into a dataframe and inspect for missing and inconsistent values", "Normalize categorical values in 'SoilType' and 'PreviousCrop' columns to consistent capitalization", "Impute missing numerical values in 'Rainfall_mm', 'Temperature_C', and 'FertilizerUsage_kg' using median imputation", "One-hot encode categorical features: 'SoilType' and 'PreviousCrop'", "Split the dataset into train and test subsets with an 80/20 ratio, stratifying by 'YieldCategory'", "Standardize numeric features: 'Rainfall_mm', 'Temperature_C', 'FertilizerUsage_kg', and 'PlantingDepth_cm'", "Train a RandomForestClassifier to predict 'YieldCategory'", "Perform grid search for hyperparameters 'n_estimators' and 'max_depth' using cross-validation", "Evaluate the model on the test set computing accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix to understand prediction errors", "Extract and report top feature importances from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.87, "f1": 0.85, "confusion_matrix": {"High": {"High": 7, "Low": 1}, "Low": {"High": 2, "Low": 3}}, "top_feature_importances": {"Rainfall_mm": 0.32, "FertilizerUsage_kg": 0.25, "Temperature_C": 0.18, "SoilType_Loam": 0.12, "PreviousCrop_Corn": 0.07}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a telecom customer will churn in the next month based on usage and service data.", "raw_table": "CustomerID,MonthlyCharges,ContractType,TenureMonths,InternetService,PaymentMethod,Churn\nC001,70.35,Month-to-month,5,Fiber optic,Electronic check,Yes\nC002,45.00,One year,12,DSL,Mailed check,No\nC003,,Month-to-month,3,Fiber optic,electronic Check,Yes\nC004,89.99,Two year,24,Fiber optic,Credit card (automatic),No\nC005,29.75,Month-to-month,1,DSL,Mailed Check,Yes\nC006,64.50,One Year,18,DSL,Credit card (automatic),No\nC007,55.20,Month-to-month,7,No,Electronic Check,Yes\nC008,72.10,Two Year,36,Fiber Optic,Mailed check,No\nC009,NaN,Month-to-month,2,DSL,Electronic check,Yes\nC010,49.99,One year,10,No,Credit Card (Automatic),No\nC011,80.15,Month-to-month,4,Fiber optic,Electronic check,Yes\nC012,38.50,One Year,14,DSL,Mailed check,No\nC013,60.25,Month-to-month,6,Fiber optic,Credit card (automatic),Yes\nC014,50.00,Month-to-month,NaN,DSL,Electronic check,Yes", "model_steps": ["Load data and inspect for missing values and inconsistent capitalization", "Impute missing MonthlyCharges with median value and TenureMonths with mode", "Standardize casing in categorical columns to uniform lowercase", "One-hot encode categorical variables: ContractType, InternetService, PaymentMethod", "Split data into training (80%) and test (20%) sets stratified by Churn", "Scale numeric features MonthlyCharges and TenureMonths using StandardScaler", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search to tune max_depth and min_samples_split", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.78, "f1": 0.8, "confusion_matrix": {"true_positive": 7, "true_negative": 13, "false_positive": 3, "false_negative": 2}, "top_feature_importances": {"ContractType_month-to-month": 0.32, "InternetService_fiber optic": 0.25, "MonthlyCharges": 0.2, "PaymentMethod_electronic check": 0.15, "TenureMonths": 0.08}, "best_hyperparameters": {"max_depth": 7, "min_samples_split": 4}}}
{"purpose": "Predict the likelihood that a citizen will repay a government loan on time.", "raw_table": "Citizen_ID,Age,Employment_Status,Annual_Income,Loan_Amount,Loan_Purpose,Previous_Default,Repayment_Status\n001,34,Employed,55000,15000,Housing,no,Yes\n002,28,Self-Employed,na,12000,Education,Yes,No\n003,45,Unemployed,32000,8000,Healthcare,No,Yes\n004,52,Employed,62000,20000,Housing,No,Yes\n005,38,Employed,58000,15000,Education,yes,No\n006,NaN,Self-Employed,45000,11000,Transportation,No,Yes\n007,30,employed,48000,13000,Education,No,Yes\n008,41,Unemployed,39000,9000,Healthcare,No,No\n009,29,Employed,51000,14000,Transportation,No,Yes\n010,36,Self-Employed,47000,10000,Housing,No,Yes\n011,44,Employed,60000,16000,Education,No,No\n012,33,Employed,58000,15000,Healthcare,No,Yes\n013,27,Self-Employed,42000,7000,Housing,No,Yes\n014,40,Unemployed,39000,8000,Education,Yes,No\n015,50,Employed,64000,21000,Transportation,No,Yes", "model_steps": ["Load the CSV data into a DataFrame, identifying 'Repayment_Status' as the target variable", "Clean data by standardizing 'Employment_Status' values to consistent capitalization and filling missing 'Age' with median age", "Handle missing numeric values in 'Annual_Income' by median imputation", "Convert 'Previous_Default' and 'Repayment_Status' from categorical yes/no to binary 1/0", "One-hot encode categorical variables: 'Employment_Status' and 'Loan_Purpose'", "Split the dataset into training (80%) and testing (20%) sets, stratifying by 'Repayment_Status'", "Standardize numeric features: 'Age', 'Annual_Income', and 'Loan_Amount' using StandardScaler", "Train a RandomForestClassifier with 100 trees on the training data", "Perform hyperparameter tuning over max_depth with values [5, 10, None] using 5-fold cross-validation", "Evaluate the best model on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix and identify top 3 feature importances from the model"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.9, "f1": 0.87, "confusion_matrix": {"true_positive": 54, "true_negative": 37, "false_positive": 6, "false_negative": 3}, "top_feature_importances": {"Previous_Default": 0.32, "Loan_Amount": 0.25, "Employment_Status_Self-Employed": 0.15}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict hourly energy consumption class (High/Medium/Low) based on weather and operational conditions to optimize power grid management.", "raw_table": "Hour,Temperature_C,Humidity_Percent,DayType,Equipment_Status,Energy_Consumption_Class\n0,15.2,85,weekday,ON,Medium\n1,14.8,87,Weekday,ON,medium\n2,14.5,NaN,weekday,off,Low\n3,14.0,90,weekday,OFF,low\n4,13.8,92,WeekEnd,ON,Low\n5,13.5,88,weekend,ON,Low\n6,16.0,80,WeekEnd,OFF,Medium\n7,20.5,75,weekend,on,High\n8,22.1,70,Weekday,ON,HIGH\n9,24.3,65,weekday,On,High\n10,25.0,,weekday,ON,High\n11,26.7,60,weekday,off,High\n12,27.5,58,weekday,OFF,High\n13,28.0,55,Weekday,ON,High", "model_steps": ["Load the data and inspect for missing values and inconsistent capitalization", "Standardize categorical text to lowercase to ensure consistency", "Impute missing numeric values (e.g., humidity) using median imputation", "Label encode the target variable 'Energy_Consumption_Class' into ordinal classes (Low=0, Medium=1, High=2)", "One-hot encode categorical features: 'DayType' and 'Equipment_Status'", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features 'Temperature_C' and 'Humidity_Percent' using z-score normalization", "Train a RandomForestClassifier to predict energy consumption class", "Perform grid search over number of estimators (50, 100) and max_depth (3, 5)", "Evaluate model performance on the test set using accuracy, F1 score, precision, and recall", "Generate a confusion matrix to analyze classification errors"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.82, "recall": 0.8, "confusion_matrix": {"Low": {"Low": 3, "Medium": 0, "High": 0}, "Medium": {"Low": 0, "Medium": 2, "High": 1}, "High": {"Low": 0, "Medium": 1, "High": 4}}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}, "top_feature_importances": {"Temperature_C": 0.42, "Humidity_Percent": 0.25, "Equipment_Status_on": 0.18, "DayType_weekday": 0.15}}}
{"purpose": "Predict the likelihood of wheat crop disease occurrence based on environmental and soil conditions.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Temperature_C,Humidity,Previous_Crop,Disease_Present\nF001,Loam,120,22,High,Wheat,Yes\nF002,Clay,95,19,Medium,Corn,No\nF003,sandy,130,25,high,Rice,Yes\nF004,Loam,missing,21,Medium,Corn,No\nF005,Clay,110,20,Low,Wheat,No\nF006,Loam,105,23,Medium,Barley,Yes\nF007,Sandy,115,22,Medium,Corn,No\nF008,Clay,100,18,High,Rice,Yes\nF009,Loam,125,24,High,Barley,Yes\nF010,loam,90,19,low,Wheat,No\nF011,Sandy,130,26,Medium,Corn,Yes\nF012,Clay,95,20,Medium,Rice,No\nF013,Sandy,120,22,Medium,Wheat,Yes\nF014,Loam,110,21,Medium,Corn,No", "model_steps": ["Load the data and identify the target variable 'Disease_Present'.", "Handle missing values by imputing missing 'Rainfall_mm' with median rainfall.", "Normalize inconsistent capitalization in 'Soil_Type' and 'Humidity' columns.", "Encode categorical variables 'Soil_Type', 'Humidity', and 'Previous_Crop' using one-hot encoding.", "Convert target variable 'Disease_Present' to binary labels.", "Split the dataset into training (80%) and testing (20%) subsets.", "Standardize numeric features 'Rainfall_mm' and 'Temperature_C' using z-score scaling.", "Train a RandomForestClassifier with 100 trees on the training set.", "Perform grid search over 'max_depth' parameter with values [5, 10, None].", "Evaluate model performance on the test set using accuracy, F1 score, precision, and recall.", "Generate and analyze the confusion matrix.", "Extract and report top feature importances from the trained model."], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.82, "recall": 0.86, "confusion_matrix": {"true_positive": 6, "false_positive": 2, "true_negative": 8, "false_negative": 1}, "top_features": {"Humidity_High": 0.27, "Rainfall_mm": 0.23, "Soil_Type_Loam": 0.15, "Temperature_C": 0.14, "Previous_Crop_Wheat": 0.11}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a manufactured part will pass quality inspection based on process parameters and material batch information.", "raw_table": "Part_ID,Machine_Speed,Material_Batch,Operator_Shift,Temperature,Pressure,Defect\n1001,1200,batch_A,morning,350,5.2,Pass\n1002,1150,Batch_A,evening,355,5.0,Pass\n1003,1300,batch_B,Night,340,5.4,Fail\n1004,1250,batch_B,night,345,missing,Pass\n1005,abc,Batch_C,morning,360,5.1,Fail\n1006,1180,batch_C,morning,355,5.3,Pass\n1007,1220,BATCH_A,Evening,349,5.2,Pass\n1008,1190,batch_B,morning,,5.0,Fail\n1009,1210,batch_C,evening,352,5.3,Pass\n1010,1170,batch_A,morning,351,5.1,Pass\n1011,1290,batch_B,night,347,5.4,Fail\n1012,1240,batch_C,Morning,353,5.2,Pass\n1013,1260,Batch_B,night,348,5.3,Fail\n1014,1230,batch_A,evening,354,5.1,Pass", "model_steps": ["Identify target variable 'Defect' and confirm it is binary classification (Pass/Fail)", "Clean 'Machine_Speed' column by replacing non-numeric entries with median value", "Standardize inconsistent capitalization in categorical columns: 'Material_Batch' and 'Operator_Shift'", "Impute missing values in 'Temperature' and 'Pressure' columns with median values", "One-hot encode categorical features 'Material_Batch' and 'Operator_Shift'", "Split dataset into training (80%) and test (20%) sets with stratification on 'Defect'", "Standardize numeric features 'Machine_Speed', 'Temperature', and 'Pressure'", "Train a RandomForestClassifier with default hyperparameters on the training set", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.79, "f1": 0.81, "top_features": {"Temperature": 0.32, "Machine_Speed": 0.28, "Pressure": 0.2}}}
{"purpose": "Predict whether a movie will be a box office hit based on its production and content features.", "raw_table": "MovieID,Genre,DirectorExperience,ProductionBudgetMillions,LeadActorPopularity,ReleaseMonth,RuntimeMinutes,BoxOfficeHit\n1,Action,5,120,8,July,130,Yes\n2,Comedy,10,30,6,December,95,No\n3,Drama,3,15,5,october,110,No\n4,Action,7,200,9,July,140,Yes\n5,Comedy,missing,40,7,August,100,No\n6,Drama,4,25,4,May,105,No\n7,Action,8,150,9,December,135,Yes\n8,Comedy,6,35,6,September,95,No\n9,Drama,2,10,3,March,115,No\n10,Action,9,180,9,July,140,Yes\n11,Comedy,5,50,7,january,98,No\n12,Drama,3,20,5,July,112,No\n13,Action,7,160,8,July,138,Yes\n14,Comedy,4,45,6,August,97,No", "model_steps": ["Load raw CSV data into a DataFrame", "Clean 'DirectorExperience' by imputing missing values with median", "Standardize capitalization in 'ReleaseMonth' to title case", "Encode target variable 'BoxOfficeHit' as binary (Yes=1, No=0)", "One-hot encode categorical variables 'Genre' and 'ReleaseMonth'", "Scale numeric features 'DirectorExperience', 'ProductionBudgetMillions', 'LeadActorPopularity', and 'RuntimeMinutes' using standard scaler", "Split data into train (80%) and test (20%) sets with stratification on target", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search over 'max_depth' with values [5, 10, None]", "Select best model based on cross-validated F1 score", "Evaluate model on test set computing accuracy, precision, recall, and F1 score", "Extract and rank top 3 feature importances from the best model"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "best_hyperparameters": {"max_depth": 10}, "top_feature_importances": {"ProductionBudgetMillions": 0.35, "LeadActorPopularity": 0.25, "Genre_Action": 0.2}, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 2}}}
{"purpose": "Predict whether a government building inspection will result in a major violation.", "raw_table": "inspection_id,building_type,year_built,num_floors,inspector_name,has_previous_violations,avg_occupancy,target_violation\n1001,Residential,1980,2,J. Smith,Yes,85,Yes\n1002,Commercial,1995,,M. Johnson,No,120,No\n1003,Residential,2005,3,Emily Davis,yes,90,No\n1004,Government,1970,4,William Brown,No,75,Yes\n1005,Residential,abc,1,Chris Green,,100,No\n1006,Commercial,2010,5,L. CLARK,No,130,No\n1007,Industrial,1988,2,Patricia Adams,Yes,65,Yes\n1008,government,2000,3,Maria Garcia,No,80,No\n1009,Commercial,1999,3,J. Smith,Yes,115,Yes\n1010,Industrial,1975,2,Michael Lee,no,70,Yes\n1011,Residential,1985,3,Emily Davis,No,95,No\n1012,Government,abc,4,William Brown,Yes,85,Yes\n1013,Commercial,2015,6,M. Johnson,No,140,No\n1014,Industrial,1992,,Patricia Adams,yes,60,Yes", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing or inconsistent values", "Correct data types and clean messy values: standardize 'has_previous_violations' to boolean, fix year_built invalid entries by imputing median year", "Fill missing numeric values such as 'num_floors' with median values grouped by building_type", "Standardize capitalization in categorical columns like 'building_type' and 'inspector_name'", "Encode categorical variables using one-hot encoding for 'building_type' and 'inspector_name'", "Convert target variable 'target_violation' to binary labels (Yes=1, No=0)", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features: 'year_built', 'num_floors', and 'avg_occupancy'", "Train a RandomForestClassifier with 100 trees on the training set", "Evaluate the model on the test set calculating accuracy, precision, recall, and F1 score", "Generate a confusion matrix to analyze false positives and false negatives", "Identify top 3 most important features from the trained model"], "model_results": {"accuracy": 0.79, "precision": 0.75, "recall": 0.82, "f1": 0.78, "confusion_matrix": {"true_positive": 9, "true_negative": 13, "false_positive": 4, "false_negative": 2}, "top_feature_importances": {"has_previous_violations_Yes": 0.32, "avg_occupancy": 0.25, "year_built": 0.18}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a social media post will go viral based on its characteristics and user engagement features.", "raw_table": "post_id,user_type,post_length,num_hashtags,num_mentions,time_posted,avg_user_followers,content_type,is_viral\n1,Influencer,120,5,2,Evening,15000,Image,Yes\n2,regular,45,0,0,MORNING,350,Text,no\n3,Regular,200,3,1,Afternoon,800,Video,Yes\n4,Influencer,NaN,7,5,Night,20000,Video,yes\n5,Guest,75,2,0,morning,1000,Text,No\n6,guest,180,4,,Evening,5000,Image,Yes\n7,Regular,50,1,1,Afternoon,600,Text,No\n8,INFLUENCER,300,10,8,Night,25000,Video,Yes\n9,regular,40,0,0,Morning,400,Text,no\n10,Guest,100,3,2,Evening,1200,Image,No\n11,Regular,NaN,1,1,Afternoon,700,Text,No\n12,Influencer,220,6,4,Night,18000,Video,Yes\n13,Guest,60,,1,Morning,900,Text,No\n14,regular,110,2,1,Evening,1100,Image,No", "model_steps": ["Load the CSV dataset and inspect for missing and inconsistent values", "Standardize the 'user_type' and 'content_type' categorical columns by normalizing case", "Impute missing numeric values ('post_length' and 'num_mentions') using median imputation", "Convert target variable 'is_viral' into a binary label (Yes=1, No=0)", "One-hot encode categorical features: 'user_type', 'time_posted', and 'content_type'", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features: 'post_length', 'num_hashtags', 'num_mentions', and 'avg_user_followers'", "Train a RandomForestClassifier with 100 trees on the training data", "Evaluate model performance using accuracy, F1 score, precision, and recall on the test set", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.81, "recall": 0.88, "top_features": {"num_hashtags": 0.28, "avg_user_followers": 0.24, "user_type_Influencer": 0.18}}}
{"purpose": "Predict customer churn probability for a telecommunications company.", "raw_table": "customer_id,age,gender,monthly_charges,contract_type,tenure_months,tech_support,streaming_tv,churn\n001,45,Male,70.5,Month-to-month,12,Yes,No,Yes\n002,33,Female,80.2,One year,24,No,Yes,No\n003,26,Female,NaN,Month-to-month,3,No,No,Yes\n004,52,Male,99.9,Two Year,48,Yes,Yes,No\n005,38,male,89.1,month-to-month,15,No,No,Yes\n006,29,Female,75.0,One Year,8,No,Yes,No\n007,41,Female,85.3,Month-to-month,NaN,No,No,Yes\n008,47,Male,95.8,Two year,36,Yes,Yes,No\n009,31,Female,mixed,Month-to-month,5,No,No,Yes\n010,36,Male,65.0,One Year,22,Yes,No,No\n011,53,Female,78.4,Month-to-month,18,No,Yes,Yes\n012,44,Male,88.8,Month-to-month,NaN,No,No,Yes\n013,39,Female,82.1,One year,27,No,Yes,No\n014,28,Female,74.5,Month-to-month,6,No,No,Yes\n015,50,male,90.0,Two Year,40,Yes,Yes,No", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Impute missing numeric values in 'monthly_charges' and 'tenure_months' using median imputation", "Standardize inconsistent capitalization in categorical columns like 'gender' and 'contract_type'", "Convert 'gender', 'contract_type', 'tech_support', and 'streaming_tv' into one-hot encoded features", "Split the data into train and test sets with an 80/20 ratio, stratifying on the target 'churn'", "Train a RandomForestClassifier with default hyperparameters on the training data", "Use grid search cross-validation to tune 'max_depth' and 'n_estimators' hyperparameters", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Generate the confusion matrix and extract feature importances from the best estimator"], "model_results": {"accuracy": 0.87, "f1": 0.82, "precision": 0.79, "recall": 0.86, "confusion_matrix": [[12, 3], [2, 18]], "top_feature_importances": {"contract_type_Month-to-month": 0.35, "tenure_months": 0.25, "monthly_charges": 0.18, "tech_support_Yes": 0.12, "streaming_tv_Yes": 0.1}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a government employee will retire within the next year based on demographic and employment data.", "raw_table": "EmployeeID,Age,Department,YearsAtAgency,JobGrade,PerformanceRating,LastPromotion,RetiringNextYear\n1,58,Health,30,GS-13,Exceeds,2019,Yes\n2,45,education,15,gs-11,Meets,2021,No\n3,62,Defense,35,GS-15,Exceeds,2017,YES\n4,39,Health,10,GS-9,Below,2019,No\n5,50,Transportation,20,gs-12,,2020,No\n6,55,Defense,25,GS-14,Exceeds,2018,Yes\n7,66,Education,40,GS-15,Exceeds,2015,Yes\n8,48,health,18,GS-12,Meets,2022,No\n9,53,Defense,22,GS-13,Exceeds,2018,No\n10,60,Transportation,33,GS-14,Exceeds,2016,Yes\n11,44,Education,12,GS-11,Meets,2021,No\n12,38,Health,7,GS-8,Below,2020,No\n13,59,Defense,29,GS-14,Exceeds,2017,Yes\n14,52,Transportation,24,GS-13,Meets,2019,No", "model_steps": ["Load the dataset and identify the target variable 'RetiringNextYear'.", "Clean inconsistent capitalization in categorical columns such as 'Department' and 'RetiringNextYear'.", "Impute missing values in 'PerformanceRating' with the mode value.", "Convert categorical variables ('Department', 'JobGrade', 'PerformanceRating') to one-hot encoded features.", "Encode the target variable 'RetiringNextYear' as binary (Yes=1, No=0).", "Split data into training and test sets with an 80/20 ratio.", "Standardize numeric features: 'Age', 'YearsAtAgency', and 'LastPromotion' year difference calculated from 2023.", "Train a RandomForestClassifier with default parameters on the training set.", "Perform hyperparameter tuning over 'max_depth' and 'n_estimators' using 5-fold cross-validation.", "Evaluate the final model on the test set calculating accuracy, F1 score, precision, and recall.", "Generate a confusion matrix and analyze top 3 feature importances."], "model_results": {"accuracy": 0.85, "f1": 0.82, "precision": 0.8, "recall": 0.85, "confusion_matrix": {"true_positive": 6, "true_negative": 7, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Age": 0.32, "YearsAtAgency": 0.28, "PerformanceRating_Exceeds": 0.15}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features.", "raw_table": "HouseID,Neighborhood,Bedrooms,Bathrooms,Area_sqft,YearBuilt,Garage,Condition,SalePrice\n1,OldTown,3,2,1500,1995,Yes,Good,250000\n2,oldtown,4,3,2000,2001,No,Fair,310000\n3,Maplewood,2,1.5,1200,1980,Yes,Excellent,200000\n4,MapleWood,3,2,1300,1975,Yes,good,215000\n5,Riverside,4,3,2500,2010,Yes,Excellent,400000\n6,Riverside,4,2,,2012,No,Excellent,380000\n7,OldTown,3,,1600,1998,No,Fair,260000\n8,Riverside,5,4,3000,2015,Yes,Excellent,500000\n9,Maplewood,3,2,1400,1985,yes,Fair,230000\n10,OldTown,4,2,1800,2000,No,Good,290000\n11,Maplewood,3,2,1350,1983,No,Good,225000\n12,Riverside,3,2,2100,2008,Yes,excellent,390000\n13,OldTown,2,1,1100,1990,No,Fair,190000\n14,Maplewood,3,2,1450,,Yes,Good,235000\n15,Riverside,4,3,2400,2011,Yes,Good,410000", "model_steps": ["Load CSV data into a DataFrame", "Standardize capitalization in 'Neighborhood' and 'Condition' columns to ensure consistency", "Impute missing values in 'Bathrooms' and 'Area_sqft' using median of respective neighborhoods", "Convert 'Garage' column to binary numeric (Yes=1, No=0)", "One-hot encode categorical variables: 'Neighborhood' and 'Condition'", "Split data into train and test sets (80% train, 20% test)", "Standardize numeric features: Bedrooms, Bathrooms, Area_sqft, YearBuilt", "Train a RandomForestRegressor with 100 trees on the training data", "Evaluate model performance on test data using RMSE, MAE, and R2", "Identify top 3 feature importances from the trained model"], "model_results": {"rmse": 21000, "mae": 16000, "r2": 0.87, "top_feature_importances": {"Area_sqft": 0.38, "Neighborhood_Riverside": 0.22, "YearBuilt": 0.15}}}
{"purpose": "Predict whether a social media post will go viral based on content and user engagement features.", "raw_table": "post_id,user_id,post_length,post_type,num_hashtags,num_mentions,is_verified,avg_user_followers,weekday,time_of_day,viral\n1,U101,120,text,3,1,True,1500,Monday,morning,Yes\n2,U102,250,IMAGE,5,,False,800,Tuesday,evening,No\n3,U103,85,video,0,2,true,2300,Wednesday,afternoon,Yes\n4,U104,300,text,2,0,False,NaN,Thursday,morning,No\n5,U105,45,Text,1,1,false,1200,Friday,night,No\n6,U106,180,image,,2,TRUE,1900,Saturday,afternoon,Yes\n7,U107,200,video,3,NaN,False,2200,Sunday,evening,No\n8,U108,NaN,text,4,1,True,1700,Monday,morning,Yes\n9,U109,95,IMAGE,2,0,false,1300,Tuesday,night,No\n10,U110,130,video,1,3,TRUE,2500,Wednesday,afternoon,Yes\n11,U111,220,text,NaN,1,False,1100,Thursday,morning,No\n12,U112,160,image,2,2,true,2100,Friday,evening,Yes\n13,U113,105,text,3,NaN,False,900,Saturday,night,No\n14,U114,140,video,1,1,TRUE,2000,Sunday,morning,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Fix inconsistent capitalization in 'post_type' and 'is_verified' columns", "Impute missing numeric values in 'post_length' and 'avg_user_followers' using median", "Impute missing categorical values ('num_hashtags', 'num_mentions') with mode or zero where appropriate", "Convert 'viral' target variable to binary (Yes=1, No=0)", "One-hot encode categorical variables: 'post_type', 'weekday', 'time_of_day'", "Convert 'is_verified' to binary (True=1, False=0)", "Split the dataset into training (80%) and test (20%) sets", "Standardize numeric features: 'post_length', 'num_hashtags', 'num_mentions', 'avg_user_followers'", "Train a RandomForestClassifier with 100 trees on the training set", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 important features"], "model_results": {"accuracy": 0.79, "precision": 0.75, "recall": 0.82, "f1": 0.78, "confusion_matrix": {"true_positive": 23, "true_negative": 37, "false_positive": 8, "false_negative": 5}, "top_features": {"num_hashtags": 0.27, "avg_user_followers": 0.22, "post_length": 0.18}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Build a classification model to predict hospital readmission risk within 30 days for diabetic patients.", "raw_table": "PatientID,Age,Gender,BMI,AdmissionType,NumPreviousAdmissions,HbA1c,Smoker,Readmitted\n1,54,Male,31.2,Emergency,2,8.1,Yes,Yes\n2,67,Female,27.5,Elective,1,6.4,No,No\n3,45,male,NaN,Urgent,0,7.2,Yes,No\n4,82,Female,29.8,emergency,3,9.0,No,Yes\n5,39,Female,26.4,Elective,,6.8,No,No\n6,72,Male,35.1,Urgent,1,8.5,Yes,Yes\n7,58,Female,33.0,Emergency,2,7.9,Yes,Yes\n8,49,,28.7,Elective,0,7.0,No,No\n9,64,Male,NaN,Emergency,4,8.2,Yes,Yes\n10,55,Female,30.5,Urgent,1,7.3,No,No\n11,60,Male,32.8,Urgent,2,8.0,YES,Yes\n12,70,Female,29.4,Elective,3,6.7,No,No", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing or inconsistent values", "Standardize inconsistent capitalization in categorical columns such as Gender, AdmissionType, and Smoker", "Impute missing BMI values using median BMI grouped by Gender and AdmissionType", "Fill missing NumPreviousAdmissions with zero assuming no prior admissions", "Encode categorical variables: one-hot encode AdmissionType, Gender, and Smoker", "Split the dataset into training (80%) and test (20%) sets stratified by the target variable Readmitted", "Standardize numeric features including Age, BMI, NumPreviousAdmissions, and HbA1c", "Train a RandomForestClassifier on the training data with 100 trees and max_depth=5", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Identify the top 3 most important features based on feature importance scores"], "model_results": {"accuracy": 0.83, "precision": 0.8, "recall": 0.78, "f1": 0.79, "confusion_matrix": {"true_positive": 18, "false_positive": 5, "true_negative": 32, "false_negative": 5}, "top_feature_importances": {"HbA1c": 0.32, "NumPreviousAdmissions": 0.27, "AdmissionType_Emergency": 0.18}, "hyperparameters": {"n_estimators": 100, "max_depth": 5, "random_state": 42}}}
{"purpose": "Build a classification model to predict whether a manufactured part will be defective based on sensor readings and production parameters.", "raw_table": "Part_ID,Temperature,Pressure,Operator_Shift,Material_Type,Machine_ID,Defect\nP001,75.2,101.3,Morning,Steel,M01,No\nP002,80.1,99.8,evening,steel,M02,Yes\nP003,78.5,100.5,Night,Aluminum,M01,No\nP004,NaN,102.0,Morning,Aluminum,M03,No\nP005,77.0,NaN,Morning,Steel,M02,Yes\nP006,79.3,100.7,NIGHT,aluminum,M01,No\nP007,81.0,101.1,Evening,Steel,M02,Yes\nP008,76.8,100.0,Morning,Steel,M03,No\nP009,75.5,99.9,Morning,Aluminum,M01,No\nP010,78.9,101.0,night,Steel,M02,Yes\nP011,80.0,100.4,Morning,steel,M03,No\nP012,77.5,101.2,Evening,Aluminum,M01,Yes\nP013,79.8,100.8,Night,Steel,M02,No\nP014,76.2,NaN,Morning,Aluminum,M03,No", "model_steps": ["Load the CSV data into a DataFrame.", "Standardize the capitalization of categorical columns: Operator_Shift and Material_Type.", "Impute missing values in the Temperature and Pressure columns using median values.", "Encode categorical variables (Operator_Shift, Material_Type, Machine_ID) with one-hot encoding.", "Split the dataset into training (80%) and testing (20%) sets with stratification on the Defect column.", "Standardize numeric features Temperature and Pressure.", "Train a RandomForestClassifier on the training data.", "Perform grid search to tune max_depth parameter with values [3, 5, 7].", "Evaluate the best model on the test set using accuracy, F1 score, precision, and recall.", "Generate the confusion matrix and identify the top 3 most important features."], "model_results": {"accuracy": 0.85, "f1": 0.82, "precision": 0.8, "recall": 0.85, "confusion_matrix": {"true_positive": 11, "false_positive": 3, "true_negative": 8, "false_negative": 2}, "top_feature_importances": {"Pressure": 0.32, "Temperature": 0.28, "Material_Type_Steel": 0.15}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features.", "raw_table": "PropertyID,Bedrooms,Bathrooms,SizeSqFt,Neighborhood,YearBuilt,ParkingType,SalePrice\n1,3,2,1500,North,1998,garage,350000\n2,4,3,2000,South,2005,Street,450000\n3,2,,850,East,1980,Garage,220000\n4,3,2,1200,west,1995,None,300000\n5,5,4,3000,North,,Garage,650000\n6,4,3,2500,South,2010,street,480000\n7,3,2,1400,East,1999,Garage,330000\n8,3,2,1300,West,2000,None,310000\n9,2,1,900,North,1975,Garage,210000\n10,4,3,2400,South,2012,Street,490000\n11,3,2,1600,East,2003,garage,360000\n12,4,3,2300,West,2008,None,470000\n13,3,2,1700,North,1990,GARAGE,340000\n14,2,1,1000,East,1985,None,230000", "model_steps": ["Load the dataset and inspect for missing values and inconsistencies", "Fix inconsistent capitalization in categorical columns such as ParkingType and Neighborhood", "Impute missing numeric values in Bathrooms and YearBuilt columns using median imputation", "One-hot encode categorical variables: Neighborhood and ParkingType", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features: Bedrooms, Bathrooms, SizeSqFt, YearBuilt", "Train a Gradient Boosting Regressor on the training set", "Tune hyperparameters (n_estimators, learning_rate) via 5-fold cross-validation", "Evaluate model performance on the test set using RMSE, MAE, and R2 metrics", "Identify and report the top 3 feature importances from the trained model"], "model_results": {"rmse": 28000, "mae": 21000, "r2": 0.85, "top_feature_importances": {"SizeSqFt": 0.45, "Neighborhood_South": 0.15, "YearBuilt": 0.12}, "hyperparameters": {"n_estimators": 150, "learning_rate": 0.1}}}
{"purpose": "Predict whether a government loan application will be approved based on applicant and loan details.", "raw_table": "Application_ID,Applicant_Age,Applicant_Income,Loan_Amount,Loan_Purpose,Credit_Score,Employment_Status,Loan_Approved\nA1001,45,55000,20000,Home Improvement,720,Employed,Yes\nA1002,38,62000,15000,Debt Consolidation,NaN,Employed,No\nA1003,29,48000,18000,Car Purchase,650,Self-employed,YES\nA1004,52,72000,25000,Home Improvement,710,Unemployed,No\nA1005,41,MISSING,22000,Education,690,Employed,Yes\nA1006,33,60000,18000,Car Purchase,680,Employed,No\nA1007,47,58000,NaN,Medical,700,Employed,Yes\nA1008,39,53000,16000,Home Improvement,730,Self-employed,No\nA1009,50,61000,21000,Debt Consolidation,695,employed,Yes\nA1010,44,59000,19000,Education,NaN,Employed,No\nA1011,36,54000,17000,Medical,660,Employed,Yes\nA1012,NaN,57000,20000,Car Purchase,675,Employed,No\nA1013,48,58000,23000,Home improvement,710,Employed,Yes\nA1014,42,62000,21000,Debt Consolidation,685,Unemployed,No\n", "model_steps": ["Load raw CSV data into a DataFrame", "Replace inconsistent capitalization in categorical columns (e.g., 'employed', 'Employed', 'Employed' normalized to 'Employed')", "Impute missing numeric values (Applicant_Age, Applicant_Income, Loan_Amount, Credit_Score) using median imputation", "Encode target variable 'Loan_Approved' as binary (Yes=1, No=0)", "One-hot encode categorical variables Loan_Purpose and Employment_Status", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features (Applicant_Age, Applicant_Income, Loan_Amount, Credit_Score)", "Train a RandomForestClassifier with default parameters on the training set", "Perform grid search for max_depth in [5, 10, 15] to optimize model performance", "Evaluate model on test set computing accuracy, F1 score, precision, and recall", "Generate a confusion matrix on test predictions", "Identify top 3 feature importances"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.79, "recall": 0.84, "confusion_matrix": [[12, 3], [2, 13]], "top_feature_importances": {"Credit_Score": 0.32, "Loan_Amount": 0.24, "Applicant_Income": 0.15}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a movie will be a box office hit based on its production budget, genre, director popularity, and release month.", "raw_table": "MovieID,BudgetM,Genre,DirectorPopularity,ReleaseMonth,StarRating,BoxOfficeHit\n1,150,Action,High,May,8.5,Yes\n2,20,Comedy,medium,Nov,6.0,No\n3,80,Action,Low,Dec,7.2,Yes\n4,50,Drama,Medium,aug,7.0,No\n5,120,Action,High,jul,8.8,Yes\n6,10,Comedy,low,Feb,5.5,No\n7,70,Drama,Medium,Mar,7.1,No\n8,90,action,High,Jun,8.0,Yes\n9,30,Comedy,Medium,Oct,6.3,No\n10,110,Drama,High,Apr,7.9,Yes\n11,45,Comedy,medium,Jan,6.5,No\n12,200,Action,HIGH,Jul,9.0,Yes\n13,,Drama,Low,Sep,7.0,No\n14,60,Comedy,Medium,Nov,6.8,No", "model_steps": ["Fix inconsistent capitalization in 'Genre' and 'DirectorPopularity' columns", "Impute missing 'BudgetM' value with the median budget", "Convert 'BoxOfficeHit' target variable to binary (Yes=1, No=0)", "One-hot encode the categorical features: Genre, DirectorPopularity, ReleaseMonth", "Standardize numeric features: BudgetM, StarRating", "Split data into train and test sets (80% train, 20% test)", "Train a RandomForestClassifier with default parameters", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.83, "f1": 0.79, "precision": 0.81, "recall": 0.77, "top_feature_importances": {"BudgetM": 0.35, "DirectorPopularity_High": 0.22, "StarRating": 0.15}}}
{"purpose": "Predict the likelihood of hospital readmission within 30 days for patients discharged after heart failure treatment.", "raw_table": "Patient_ID,Age,Gender,BMI,Diabetes_Status,Previous_Admissions,Discharge_Disposition,Readmitted_30days\n1,65,Male,29.5,Yes,2,Home,Yes\n2,70,Female,NaN,No,1,Rehabilitation,No\n3,80,Female,31.2,yes,3,Home,YES\n4,55,Male,27.8,No,,Nursing Home,No\n5,60,female,24.3,No,0,Home,No\n6,72,Male,35.1,Yes,2,HOME,Yes\n7,58,Female,26.7,No,1,Home,No\n8,68,Male,30.5,Unknown,2,rehabilitation,Yes\n9,75,Female,28.9,No,4,Home,No\n10,63,Male,32.0,Yes,3,Nursing home,Yes\n11,59,Male,27.0,No,1,Home,No\n12,77,Female,34.2,Yes,2,Rehabilitation,Yes\n13,61,Male,29.9,No,1,Home,No\n14,66,Female,30.0,Yes,2,HOME,Yes", "model_steps": ["Handle missing values in BMI by imputing median BMI", "Standardize the numeric columns: Age, BMI, Previous_Admissions", "Normalize categorical columns: Gender and Diabetes_Status (normalize case and fix inconsistent values)", "One-hot encode categorical variables: Gender, Diabetes_Status, Discharge_Disposition", "Split data into training (80%) and test sets (20%) ensuring stratification on the target variable Readmitted_30days", "Train a RandomForestClassifier with default hyperparameters on the training set", "Perform grid search over n_estimators [50, 100, 200] and max_depth [5, 10, None]", "Select best model using 5-fold cross-validation F1 score", "Evaluate final model on test set and compute accuracy, precision, recall, and F1 score", "Generate confusion matrix for test set predictions", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.79, "f1": 0.76, "precision": 0.74, "recall": 0.78, "confusion_matrix": [[5, 2], [2, 5]], "top_feature_importances": {"Previous_Admissions": 0.35, "Diabetes_Status_Yes": 0.25, "Age": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Predict whether a retail customer will make a purchase during a promotional campaign.", "raw_table": "CustomerID,Age,Gender,Annual_Income,Last_Purchase_Category,Days_Since_Last_Purchase,Store_Region,Made_Purchase\nC001,34,Male,55000,Electronics,10,North,Yes\nC002,28,Female,42000,Clothing,25,South,No\nC003,45,Male,62000,Electronics,NA,East,Yes\nC004,23,Female,35000,clothing,5,West,No\nC005,37,female,48000,Home,60,North,Yes\nC006,52,Male,73000,Furniture,30,East,No\nC007,47,Male,67000,Home,15,south,Yes\nC008,30,Female,50000,,20,West,No\nC009,41,Male,58000,Electronics,12,North,Yes\nC010,36,Female,46000,Clothing,NA,East,No\nC011,29,Male,39000,Home,45,west,No\nC012,50,Female,72000,Furniture,7,North,Yes\nC013,33,Male,53000,Clothing,18,South,No\nC014,26,Female,40000,Home,11,East,No\nC015,44,male,61000,Electronics,17,North,Yes", "model_steps": ["Load raw CSV data into a DataFrame", "Clean and standardize categorical values (fix capitalization and fill missing Last_Purchase_Category with 'Unknown')", "Impute missing numeric values in Days_Since_Last_Purchase with median", "Encode categorical variables using one-hot encoding (Gender, Last_Purchase_Category, Store_Region)", "Convert target column Made_Purchase to binary (Yes=1, No=0)", "Split data into train and test sets (85/15 split)", "Standardize numeric features (Age, Annual_Income, Days_Since_Last_Purchase)", "Train a RandomForestClassifier with 100 trees", "Perform grid search for max_depth over [5, 10, 15]", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and feature importances"], "model_results": {"accuracy": 0.87, "precision": 0.84, "recall": 0.9, "f1": 0.87, "confusion_matrix": [[6, 1], [1, 7]], "top_feature_importances": {"Days_Since_Last_Purchase": 0.28, "Annual_Income": 0.22, "Last_Purchase_Category_Electronics": 0.15, "Store_Region_North": 0.12, "Gender_Male": 0.08}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict customer churn probability for a telecom company to reduce subscriber loss.", "raw_table": "customer_id,tenure,monthly_charges,contract_type,internet_service,tech_support,churn\n001,12,70.35,Month-to-month,Fiber optic,Yes,No\n002,45,45.20,Two year,DSL,NO,No\n003,5,89.10,month-to-month,Fiber Optic,No,Yes\n004,22,50.00,One Year,DSL,Mixed,No\n005,60,29.85,Two Year,DSL,Yes,No\n006,,75.25,Month-to-month,Fiber optic,No,Yes\n007,10,105.50,Month-to-month,Fiber optic,NO,Yes\n008,35,65.00,One year,DSL,Yes,No\n009,3,99.99,Month-to-month,Fiber Optic,,Yes\n010,20,55.10,Two year,DSL,No,No\n011,40,49.95,One Year,DSL,Yes,No\n012,15,85.75,Month-to-month,Fiber optic,No,Yes\n013,28,60.00,One year,DSL,Yes,No\n014,8,95.00,Month-to-month,Fiber Optic,No,Yes\n015,50,40.50,Two year,DSL,Yes,No", "model_steps": ["Load CSV data and inspect for missing or inconsistent values", "Impute missing tenure values with median tenure", "Standardize capitalization for categorical fields like contract_type, internet_service, and tech_support", "Convert churn target variable from Yes/No to binary 1/0", "One-hot encode categorical variables: contract_type, internet_service, and tech_support", "Split data into training (80%) and test (20%) sets with stratification on churn", "Standardize numeric features: tenure and monthly_charges using StandardScaler", "Train a RandomForestClassifier with default parameters on training data", "Perform grid search over max_depth in [5,10,15] and n_estimators in [50,100]", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Generate feature importance ranking from the trained random forest", "Plot confusion matrix for test set predictions"], "model_results": {"accuracy": 0.87, "f1": 0.78, "precision": 0.74, "recall": 0.83, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}, "feature_importances": {"contract_type_Month-to-month": 0.32, "monthly_charges": 0.25, "tech_support_No": 0.18, "internet_service_Fiber optic": 0.15, "tenure": 0.1}, "confusion_matrix": {"true_positive": 15, "true_negative": 50, "false_positive": 7, "false_negative": 3}}}
{"purpose": "Build a regression model to estimate house prices based on property and neighborhood features.", "raw_table": "PropertyID,Neighborhood,Size_sqft,Bedrooms,Bathrooms,Year_Built,GarageType,Price\n001,Downtown,850,2,1,1999,Attached,350000\n002,Suburbs,1200,3,2,2010,None,420000\n003,downtown,950,2,1,,Detached,365000\n004,Suburbs,1300,3,2,2015,attached,450000\n005,Midtown,1100,2,1,2005,Detached,400000\n006,midtown,1050,2,,2012,None,390000\n007,Downtown,800,1,1,2000,Attached,340000\n008,Suburbs,1400,4,3,2018,Attached,470000\n009,Midtown,1150,3,2,2011,,430000\n010,Suburbs,1250,3,2,2014,Detached,445000\n011,Downtown,875,2,1,2001,attached,n/a\n012,Suburbs,1300,3,2,2016,None,460000\n013,Midtown,1000,2,1,2008,Detached,395000", "model_steps": ["Load the CSV data into a dataframe", "Standardize capitalization and fix inconsistent values in 'Neighborhood' and 'GarageType' columns", "Handle missing values in 'Bathrooms', 'Year_Built', and 'Price' by imputing median for numeric and mode for categorical or dropping rows where 'Price' is missing", "Convert categorical variables 'Neighborhood' and 'GarageType' using one-hot encoding", "Split data into training set (80%) and test set (20%)", "Standardize numeric features: 'Size_sqft', 'Bedrooms', 'Bathrooms', and 'Year_Built'", "Train a RandomForestRegressor model on the training data", "Tune the number of trees and max_depth using grid search with cross-validation", "Evaluate the final model on the test set using RMSE, MAE, and R2 metrics", "Analyze feature importances from the trained model"], "model_results": {"rmse": 21000, "mae": 16000, "r2": 0.82, "best_hyperparameters": {"n_estimators": 100, "max_depth": 8}, "top_feature_importances": {"Size_sqft": 0.35, "Neighborhood_Suburbs": 0.2, "Bedrooms": 0.15, "GarageType_Attached": 0.1, "Year_Built": 0.08}}}
{"purpose": "Predict whether a customer will purchase a recommended product during a browsing session.", "raw_table": "session_id,customer_age,customer_gender,browse_duration_seconds,page_views,recommendation_clicked,previous_purchases,target_purchase\n1,34,Male,300,5,Yes,2,1\n2,45,Female,150,3,NO,0,0\n3,27,Female,NaN,7,yes,1,1\n4,52,Male,200,4,No,5,0\n5,38,Female,400,8,YES,3,1\n6,23,Male,90,2,No,,0\n7,31,Female,250,NaN,yes,0,1\n8,40,Male,310,6,no,4,0\n9,29,Female,180,3,Yes,1,1\n10,35,Male,NaN,5,No,2,0\n11,48,Female,270,7,yes,3,1\n12,30,Male,210,4,No,1,0\n13,41,Female,220,5,Yes,2,1\n14,26,Male,160,3,YES,0,0", "model_steps": ["Load raw CSV data into a DataFrame", "Handle missing values by imputing median for numeric columns and mode for categorical columns", "Standardize numeric features: customer_age, browse_duration_seconds, page_views, previous_purchases", "Convert customer_gender and recommendation_clicked to lowercase and one-hot encode them", "Split data into train and test sets with 80/20 ratio", "Train a Logistic Regression classifier on the training set", "Perform 5-fold cross-validation to tune regularization strength", "Evaluate model on the test set computing accuracy, precision, recall, and F1 score", "Generate confusion matrix and analyze feature coefficients for interpretability"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "false_positive": 2, "true_negative": 7, "false_negative": 1}, "top_feature_importances": {"recommendation_clicked_yes": 1.2, "browse_duration_seconds": 0.8, "previous_purchases": 0.5, "customer_gender_female": 0.3}, "best_hyperparameter_C": 1.0}}
{"purpose": "Predict whether a social media post will go viral based on its metadata and content features.", "raw_table": "post_id,user_followers,post_length,content_type,posted_hour,has_image,engagement_level\n1,1500,120,text,14,yes,viral\n2,2300,300,Video,20,Yes,not_viral\n3,800,45,Text,9,no,viral\n4,NaN,200,image,18,NO,not_viral\n5,5000,150,video,23,yes,viral\n6,1200,85,Text,7,No,not_viral\n7,3000,NaN,Image,12,yes,viral\n8,2200,100,video,15,YES,not_viral\n9,4000,250,text,19,nO,viral\n10,900,60,text,5,no,not_viral\n11,1700,130,Video,20,Yes,viral\n12,2100,170,image,21,yes,not_viral\n13,NaN,90,text,8,no,viral\n14,2750,110,Video,16,yes,not_viral", "model_steps": ["Handle missing numeric values in 'user_followers' and 'post_length' by imputing with median values", "Standardize the 'user_followers' and 'post_length' numeric features", "Convert 'content_type' and 'has_image' categorical columns to lowercase to ensure consistency", "One-hot encode categorical variables: 'content_type' and 'has_image'", "Map target variable 'engagement_level' to binary labels: 'viral' = 1, 'not_viral' = 0", "Split data into training and test sets with an 80/20 ratio", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search to tune max_depth parameter over [5, 10, 15]", "Evaluate model performance on test set by calculating accuracy, F1 score, precision, and recall", "Generate the confusion matrix for test set predictions"], "model_results": {"accuracy": 0.79, "f1": 0.81, "precision": 0.78, "recall": 0.85, "best_hyperparameters": {"max_depth": 10}, "confusion_matrix": {"true_positive": 11, "true_negative": 10, "false_positive": 3, "false_negative": 2}, "top_feature_importances": {"user_followers": 0.35, "post_length": 0.22, "content_type_video": 0.15, "has_image_yes": 0.12, "posted_hour": 0.16}}}
{"purpose": "Predict whether a movie will be a box office hit based on production and genre features.", "raw_table": "MovieID,Genre,DirectorExperienceYears,BudgetMillions,LeadActorPopularity,FilmingLocation,ReleaseMonth,BoxOfficeHit\n1,Action,10,150,85,Los Angeles,July,Yes\n2,comedy,5,30,40,New York,December,No\n3,Drama,,70,65,los angeles,March,No\n4,Thriller,8,90,75,,October,Yes\n5,Comedy,3,25,30,New York,April,No\n6,action,12,200,90,Los Angeles,July,Yes\n7,Drama,15,120,80,London,November,Yes\n8,Thriller,4,85,60,New York,August,No\n9,Comedy,6,28,50,New york,May,No\n10,Action,11,180,88,Los Angeles,July,Yes\n11,Drama,7,100,70,London,February,No\n12,Thriller,9,95,78,New York,October,Yes\n13,Comedy,2,,45,New York,June,No\n14,Action,13,175,92,los angeles,July,Yes", "model_steps": ["Identify and correct inconsistent capitalization in 'Genre' and 'FilmingLocation' columns", "Impute missing values in 'DirectorExperienceYears' and 'BudgetMillions' using median values", "Encode target variable 'BoxOfficeHit' as binary (Yes=1, No=0)", "One-hot encode categorical variables: 'Genre', 'FilmingLocation', and 'ReleaseMonth'", "Standardize numeric features: 'DirectorExperienceYears', 'BudgetMillions', 'LeadActorPopularity'", "Split data into train and test sets with 80% training and 20% testing", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search over max_depth parameter with values [5, 10, 15]", "Evaluate the model using accuracy, F1 score, precision, and recall on the test set", "Generate confusion matrix to analyze classification errors"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.85, "recall": 0.78, "confusion_matrix": [[9, 2], [3, 11]], "top_feature_importances": {"BudgetMillions": 0.32, "LeadActorPopularity": 0.25, "Genre_Action": 0.15, "DirectorExperienceYears": 0.1, "ReleaseMonth_July": 0.08}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a manufactured part will fail quality inspection based on production parameters.", "raw_table": "Part_ID,Machine_ID,Operator_Experience,Temperature,Pressure,Material_Type,Shift,Defect_Flag\n1,M01,5,350,101.2,Steel,Morning,No\n2,m02,3,360,100.8,steel,Night,Yes\n3,M03,7,355,102.5,Aluminum,Afternoon,No\n4,M01,4,NaN,101.0,STEEL,Morning,No\n5,M02,2,348,99.9,Aluminum,Night,Yes\n6,M03,6,352,100.5,steel,afternoon,No\n7,M01,5,351,100.7,Aluminum,Morning,Yes\n8,M02,3,349,101.3,Steel,Night,\n9,M03,8,353,102.0,Aluminum,Afternoon,No\n10,M01,NaN,347,100.1,Steel,Morning,Yes\n11,M02,4,350,101.4,steel,Night,No\n12,M03,5,354,102.2,Aluminum,Afternoon,No\n13,M01,6,NaN,100.9,Steel,Morning,No\n14,M02,3,349,99.8,steel,Night,Yes", "model_steps": ["Load the data and identify columns with missing values and inconsistent capitalization", "Impute missing numeric values in Temperature and Operator_Experience using median values", "Standardize capitalization in Material_Type and Shift columns to ensure consistency", "Encode categorical variables Material_Type and Shift using one-hot encoding", "Convert Defect_Flag to binary label (Yes=1, No=0) and drop rows with missing target values", "Split the dataset into training (80%) and testing (20%) sets", "Train a RandomForestClassifier on the training data", "Perform grid search to tune max_depth parameter with values [3, 5, 7]", "Evaluate the best model on the testing set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix for test predictions"], "model_results": {"accuracy": 0.79, "f1": 0.76, "precision": 0.74, "recall": 0.78, "confusion_matrix": {"true_positive": 7, "true_negative": 10, "false_positive": 3, "false_negative": 2}, "best_hyperparameters": {"max_depth": 5}, "top_feature_importances": {"Temperature": 0.35, "Operator_Experience": 0.25, "Material_Type_Steel": 0.15, "Pressure": 0.1, "Shift_Night": 0.08, "Material_Type_Aluminum": 0.07}}}
{"purpose": "Predict whether a student will pass the final exam based on their study habits and demographic data.", "raw_table": "StudentID,Hours_Studied,Attendance_Rate,Major,Previous_Grade,Participation,Passed_Final\n1,15,0.95,Engineering,85,High,Yes\n2,8,0.75,engineering,78,medium,yes\n3,12,0.85,Mathematics,NaN,High,Yes\n4,5,0.60,History,65,Low,No\n5,NaN,0.90,History,70,Medium,No\n6,20,0.98,Mathematics,92,High,Yes\n7,7,missing,Engineering,58,Low,no\n8,10,0.80,History,72,Medium,No\n9,18,0.97,mathematics,88,High,Yes\n10,3,0.50,Engineering,55,low,No\n11,NaN,0.85,History,NaN,Medium,No\n12,14,0.93,Engineering,80,High,Yes\n13,6,0.70,Mathematics,60,low,No\n14,9,0.78,History,68,Medium,No\n15,11,0.88,Engineering,75,Medium,Yes", "model_steps": ["Load the CSV data into a dataframe", "Clean the 'Attendance_Rate' column by imputing missing values with the median", "Standardize capitalization in categorical columns: 'Major' and 'Participation'", "Impute missing values in 'Hours_Studied' and 'Previous_Grade' with median values", "Convert target variable 'Passed_Final' to binary (Yes=1, No=0)", "One-hot encode categorical variables: 'Major' and 'Participation'", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features: 'Hours_Studied', 'Attendance_Rate', 'Previous_Grade'", "Train a RandomForestClassifier with 100 trees", "Evaluate model performance using accuracy, precision, recall, and F1 score on the test set", "Generate a confusion matrix to analyze prediction errors"], "model_results": {"accuracy": 0.87, "precision": 0.89, "recall": 0.85, "f1": 0.87, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 1, "false_negative": 2}, "top_feature_importances": {"Hours_Studied": 0.35, "Attendance_Rate": 0.25, "Previous_Grade": 0.2, "Participation_High": 0.1, "Major_Engineering": 0.05, "Participation_Medium": 0.05}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a student will pass the final exam based on demographic and study habit features.", "raw_table": "StudentID,Age,Gender,StudyHoursPerWeek,ParentalEducation,PreviousGrades,AttendanceRate,PassedFinal\n1,17,Male,12,highschool,85,92,Yes\n2,18,Female,8,College,78,85,Yes\n3,17,Male,5,HighSchool,60,75,No\n4,19,Female,15,College,90,98,Yes\n5,18,Male,,Highschool,70,80,No\n6,17,Female,10,College,82,88,Yes\n7,18,Male,7,highschool,65,70,No\n8,19,Female,14,,88,95,Yes\n9,17,Male,9,Highschool,75,82,Yes\n10,18,Female,4,college,55,65,No\n11,19,Male,11,College,80,90,Yes\n12,17,Female,6,Highschool,mixed,78,No\n13,18,Male,13,College,88,93,Yes\n14,19,Female,NaN,Highschool,72,85,No", "model_steps": ["Load the data from CSV and inspect for missing and inconsistent values", "Clean the 'PreviousGrades' column by replacing 'mixed' with the median grade", "Impute missing values in 'StudyHoursPerWeek' and 'ParentalEducation' with median and mode respectively", "Standardize numeric features: Age, StudyHoursPerWeek, PreviousGrades, AttendanceRate", "One-hot encode categorical features: Gender, ParentalEducation", "Encode target variable 'PassedFinal' as binary (Yes=1, No=0)", "Split data into training and testing sets with an 80/20 ratio", "Train a Logistic Regression classifier on the training set", "Evaluate the model using accuracy, precision, recall, and F1 score on the test set", "Generate confusion matrix and identify top three important features based on model coefficients"], "model_results": {"accuracy": 0.85, "precision": 0.88, "recall": 0.82, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 1, "false_negative": 2}, "top_features": {"StudyHoursPerWeek": 1.25, "AttendanceRate": 1.1, "PreviousGrades": 0.95}, "hyperparameters": {"model_type": "LogisticRegression", "regularization": "L2", "C": 1.0}}}
{"purpose": "Predict whether a customer will make a repeat purchase within 30 days based on their first order details.", "raw_table": "CustomerID,OrderValue,ProductCategory,FirstPurchaseDay,CustomerSegment,RepeatPurchase\n1001,59.99,Electronics,Monday,Premium,Yes\n1002,15.50,clothing,Wednesday,standard,No\n1003,NaN,Home & Garden,Friday,Standard,No\n1004,120.00,Electronics,Tuesday,Premium,Yes\n1005,35.00,Clothing,Thursday,,No\n1006,45.75,home & garden,Monday,Standard,No\n1007,80.00,Toys,Saturday,Premium,Yes\n1008,22.00,Toys,Sunday,standard,No\n1009,60.00,Electronics,Friday,Premium,Yes\n1010,NaN,Clothing,Monday,Standard,No\n1011,12.50,Toys,Tuesday,Standard,No\n1012,90.00,Electronics,Wednesday,,Yes\n1013,55.00,Home & garden,Thursday,Premium,Yes", "model_steps": ["Load the raw CSV data into a dataframe", "Identify and impute missing numeric values in OrderValue with median", "Standardize capitalization of categorical columns ProductCategory and CustomerSegment", "Fill missing values in CustomerSegment with the mode 'Standard'", "Encode categorical variables: ProductCategory, FirstPurchaseDay, CustomerSegment using one-hot encoding", "Convert target variable RepeatPurchase to binary (Yes=1, No=0)", "Split data into train and test sets with 80/20 ratio", "Train a Logistic Regression model on the training data", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix for test predictions"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_features": {"OrderValue": 0.45, "CustomerSegment_Premium": 0.31, "ProductCategory_Electronics": 0.22}, "hyperparameters": {"model_type": "LogisticRegression", "regularization": "L2", "C": 1.0, "solver": "liblinear"}}}
{"purpose": "Predict whether a customer will make a purchase during their visit to the ecommerce site.", "raw_table": "session_id,device_type,browser,time_on_site_seconds,pages_viewed,referral_source,purchase\ns1,Mobile,Chrome,120,5,google,Yes\ns2,desktop,Firefox,45,2,Direct,No\ns3,Tablet,chrome,NaN,3,Facebook,No\ns4,Mobile,Safari,300,10,google,Yes\ns5,Desktop,Edge,80,4,Direct,No\ns6,mobile,Chrome,200,7,,Yes\ns7,Desktop,firefox,150,6,Google,No\ns8,Tablet,Safari,60,,Facebook,No\ns9,Desktop,Chrome,90,5,Direct,Yes\ns10,mobile,Edge,NaN,3,Twitter,No\ns11,Tablet,chrome,110,4,google,No\ns12,Desktop,Safari,130,6,Facebook,Yes", "model_steps": ["Load CSV data and inspect for missing or inconsistent values", "Standardize capitalization in 'device_type' and 'browser' columns to ensure consistency", "Impute missing numeric values in 'time_on_site_seconds' and 'pages_viewed' with median values", "Fill missing categorical values in 'referral_source' with the mode 'Direct'", "Convert target variable 'purchase' from Yes/No to binary 1/0", "One-hot encode categorical features: 'device_type', 'browser', and 'referral_source'", "Split data into train (80%) and test (20%) sets randomly", "Standardize numeric features 'time_on_site_seconds' and 'pages_viewed' using z-score normalization", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search over max_depth parameter with values [5, 10, 15]", "Evaluate final model on test set and compute accuracy, precision, recall, and F1 score", "Generate feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.83, "precision": 0.8, "recall": 0.75, "f1": 0.77, "best_hyperparameters": {"max_depth": 10}, "feature_importances": {"time_on_site_seconds": 0.35, "pages_viewed": 0.3, "device_type_Mobile": 0.1, "device_type_Desktop": 0.08, "browser_Chrome": 0.07, "referral_source_google": 0.05, "browser_Safari": 0.05}}}
{"purpose": "Predict whether a student will pass the final exam based on study habits and demographic information.", "raw_table": "student_id,age,gender,study_hours_per_week,attendance_rate,previous_grade,major,final_exam_passed\n1,19,M,12,0.85,85,Engineering,Yes\n2,20,F,NA,0.9,78,Biology,yes\n3,21,M,8,missing,82,engineering,No\n4,22,F,15,0.95,88,Mathematics,Yes\n5,20,F,7,0.7,65,Biology,no\n6,19,M,10,0.8,72,Engineering,YES\n7,23,F,5,0.6,60,,No\n8,21,m,13,0.9,90,Mathematics,Yes\n9,20,F,9,0.85,75,Biology,No\n10,19,M,NA,0.75,70,Engineering,yes\n11,22,F,11,0.88,80,Mathematics,Yes\n12,21,F,14,missing,85,Engineering,Yes\n13,20,M,6,0.65,68,biology,no\n14,22,F,10,0.9,82,Mathematics,Yes", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Clean 'study_hours_per_week' and 'attendance_rate' by imputing missing values with median", "Normalize capitalization and standardize categorical fields: 'gender', 'major', and 'final_exam_passed'", "Encode categorical variables using one-hot encoding", "Split data into training and testing sets with an 80/20 ratio", "Standardize numeric features like 'age', 'study_hours_per_week', 'attendance_rate', and 'previous_grade'", "Train a RandomForestClassifier to predict 'final_exam_passed'", "Perform hyperparameter tuning on number of estimators and max_depth using grid search with 5-fold cross-validation", "Evaluate the final model using accuracy, precision, recall, and F1 score on the test set", "Generate a confusion matrix to analyze classification errors"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.84, "f1": 0.86, "confusion_matrix": {"true_positive": 6, "false_positive": 1, "true_negative": 5, "false_negative": 2}, "top_feature_importances": {"study_hours_per_week": 0.35, "attendance_rate": 0.3, "previous_grade": 0.2, "major_Biology": 0.05, "major_Mathematics": 0.04, "gender_M": 0.03, "gender_F": 0.03}, "hyperparameters": {"n_estimators": 100, "max_depth": 6}}}
{"purpose": "Predict whether a loan application will be approved based on applicant financial and demographic data.", "raw_table": "Applicant_ID,Income,Employment_Status,Credit_Score,Loan_Amount,Loan_Purpose,Approved\n1,55000,Employed,720,15000,Home Renovation,Yes\n2,43000,Self-employed,650,12000,Debt Consolidation,No\n3,62000,employed,700,18000,Auto,Yes\n4,49000,Unemployed,600,10000,Home Renovation,No\n5,58000,Employed,,14000,Debt Consolidation,Yes\n6,52000,Self-employed,680,NaN,Auto,No\n7,59000,Employed,710,16000,Home Renovation,Yes\n8,47000,Employed,NaN,11000,auto,No\n9,53000,Self-employed,690,13000,Home Renovation,Yes\n10,61000,Employed,730,17000,Debt Consolidation,Yes\n11,44000,Unemployed,620,9000,Auto,No\n12,50000,employed,705,13500,Home Renovation,Yes", "model_steps": ["Load dataset and inspect for missing or inconsistent values", "Standardize capitalization in 'Employment_Status' and 'Loan_Purpose' categorical columns", "Impute missing numeric values in 'Credit_Score' and 'Loan_Amount' using median", "Encode categorical variables 'Employment_Status' and 'Loan_Purpose' using one-hot encoding", "Split data into train (80%) and test (20%) sets with stratification on target 'Approved'", "Standardize numeric features 'Income', 'Credit_Score', and 'Loan_Amount'", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search over 'max_depth' values [5, 10, 15] using 5-fold cross-validation", "Evaluate the final model on the test set and compute accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 important features"], "model_results": {"accuracy": 0.83, "precision": 0.85, "recall": 0.8, "f1": 0.82, "confusion_matrix": {"True_Positive": 8, "True_Negative": 7, "False_Positive": 2, "False_Negative": 2}, "top_feature_importances": {"Credit_Score": 0.35, "Loan_Amount": 0.28, "Income": 0.2}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a given day will experience extreme heat based on weather and environmental factors.", "raw_table": "date,temperature_c,humidity_percent,wind_speed_kmh,weather_condition,location_type,extreme_heat\n2024-06-01,35.2,45,15,Sunny,urban,Yes\n2024-06-02,28.5,60,10,Rainy,rural,No\n2024-06-03,NaN,55,7,Foggy,Urban,No\n2024-06-04,40.1,30,20,Sunny,urban,Yes\n2024-06-05,36.0,40,NaN,Sunny,suburban,Yes\n2024-06-06,27.8,70,5,Rainy,Rural,No\n2024-06-07,34.5,50,12,Sunny,Suburban,Yes\n2024-06-08,33.0,52,14,Cloudy,urban,No\n2024-06-09,31.4,58,9,cloudy,Rural,No\n2024-06-10,29.9,65,8,Rainy,urban,No\n2024-06-11,38.5,35,18,sunny,urban,Yes\n2024-06-12,NaN,48,13,Sunny,Rural,Yes\n2024-06-13,30.0,60,10,Foggy,suburban,No\n2024-06-14,39.2,33,20,Sunny,urban,Yes", "model_steps": ["Parse the date column and drop it as it is not used for prediction", "Impute missing numeric values (temperature_c and wind_speed_kmh) using median values", "Normalize numeric features: temperature_c, humidity_percent, wind_speed_kmh", "Standardize capitalization and one-hot encode the weather_condition and location_type categorical columns", "Split data into train and test sets with 80% training and 20% testing", "Train a RandomForestClassifier to predict the binary target extreme_heat", "Perform hyperparameter tuning on number of trees and max_depth using 5-fold cross-validation", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix to identify false positives and false negatives", "Extract and report top 3 feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.9, "f1": 0.86, "confusion_matrix": {"true_positive": 9, "true_negative": 7, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"temperature_c": 0.45, "humidity_percent": 0.25, "weather_condition_Sunny": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a movie will be a box office hit based on pre-release features.", "raw_table": "MovieID,Genre,Director,BudgetMillions,RuntimeMinutes,ReleaseSeason,HasTopStar,MarketingScore,BoxOfficeHit\n1,Action,Smith,150,130,Summer,Yes,8.5,Yes\n2,Comedy,Johnson,40,95,Winter,No,6.0,No\n3,Drama,Williams,NaN,110,fall,Yes,7.0,No\n4,Thriller,BROWN,80,105,Summer,yes,7.8,Yes\n5,Comedy,Lee,35,100,Summer,No,5.5,No\n6,Action,Smith,200,140,Spring,Yes,9.0,Yes\n7,Drama,Johnson,50,115,Winter,No,6.5,No\n8,Action,smith,180,125,Summer,Yes,8.0,Yes\n9,Thriller,Brown,75,108,summer,No,7.2,No\n10,Comedy,Lee,45,98,Fall,No,5.8,No\n11,Action,Davidson,NaN,130,Summer,Yes,8.7,Yes\n12,Drama,Williams,55,110,Winter,No,,No\n13,Thriller,Brown,90,105,Summer,No,7.5,Yes\n14,Comedy,Johnson,38,100,Fall,No,6.1,No", "model_steps": ["Load the dataset from CSV.", "Identify and impute missing numeric values in BudgetMillions and MarketingScore with median.", "Standardize numeric features BudgetMillions, RuntimeMinutes, and MarketingScore.", "Normalize categorical fields Genre, Director, ReleaseSeason, and HasTopStar by lowercasing and stripping whitespace.", "One-hot encode categorical variables Genre, Director, ReleaseSeason, and HasTopStar.", "Split data into train and test sets using 80% train and 20% test with stratification on BoxOfficeHit.", "Train a RandomForestClassifier to predict BoxOfficeHit.", "Perform grid search cross-validation over number of estimators (50, 100) and max_depth (5, 10).", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score.", "Generate confusion matrix and identify top 3 most important features."], "model_results": {"accuracy": 0.85, "precision": 0.88, "recall": 0.82, "f1": 0.85, "confusion_matrix": [[8, 2], [3, 11]], "top_feature_importances": {"HasTopStar_Yes": 0.27, "BudgetMillions": 0.23, "MarketingScore": 0.18}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Predict whether a customer will make a purchase during a visit based on session and user data.", "raw_table": "session_id,user_age,user_gender,device_type,page_views,time_on_site_seconds,referral_source,purchase\nS001,34,M,Mobile,5,300,Google,Yes\nS002,27,female,Desktop,2,120,facebook,No\nS003,45,MOBILE,Tablet,8,450,Organic,Yes\nS004,38,,Desktop,3,?,Direct,No\nS005,23,F,Mobile,7,380,Google,Yes\nS006,31,M,Mobile,1,60,Facebook,No\nS007,29,Female,Tablet,4,250,Direct,No\nS008,40,M,Desktop,6,400,organic,Yes\nS009,,M,Mobile,2,130,Google,No\nS010,36,F,Tablet,9,500,Google,Yes\nS011,28,f,desktop,3,200,Facebook,No\nS012,50,M,Mobile,10,600,Direct,Yes\nS013,33,Female,Mobile,5,310,Google,No\nS014,48,M,desktop,7,420,Organic,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values in user_age and time_on_site_seconds by imputing median values", "Standardize inconsistent capitalization in categorical columns user_gender, device_type, and referral_source", "Encode categorical features: one-hot encode user_gender, device_type, and referral_source", "Convert target variable 'purchase' to binary numeric format (Yes=1, No=0)", "Split the dataset into training and test sets with an 80/20 ratio", "Standardize numeric features user_age, page_views, and time_on_site_seconds using Min-Max scaling", "Train a RandomForestClassifier with 100 trees on the training set", "Evaluate model performance on the test set by calculating accuracy, precision, recall, and F1 score", "Extract and report the top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.8, "f1": 0.81, "top_feature_importances": {"page_views": 0.32, "time_on_site_seconds": 0.27, "referral_source_Google": 0.15}}}
{"purpose": "Predict whether a loan applicant will default within 12 months based on their financial and demographic data.", "raw_table": "ApplicantID,Age,Income,EmploymentStatus,CreditScore,LoanAmount,LoanPurpose,Defaulted\n1,34,55000,Full-time,720,15000,Car,No\n2,52,62000,PART-TIME,680,20000,Home Improvement,Yes\n3,45,,Self-employed,590,12000,Debt Consolidation,No\n4,29,48000,full-Time,710,13000,Car,No\n5,50,52000,Unemployed,NaN,17000,Vacation,Yes\n6,38,61000,Full-time,700,15000,Debt Consolidation,No\n7,27,45000,full-time,650,11000,car,No\n8,41,58000,Part-Time,670,14000,Home improvement,Yes\n9,46,60000,Self-Employed,600,13000,Debt consolidation,No\n10,33,59000,Full-Time,NaN,16000,Vacation,Yes\n11,55,63000,Full-time,720,21000,Car,No\n12,30,47000,PART-time,690,12500,home Improvement,Yes\n13,40,60000,Self-employed,680,15000,Debt Consolidation,No\n14,28,49000,Unemployed,640,10000,Car,Yes\n15,53,61000,Full-time,700,18000,Vacation,No", "model_steps": ["Load the CSV data into a DataFrame", "Clean and standardize EmploymentStatus and LoanPurpose categorical values by lowercasing and stripping whitespace", "Impute missing numerical values in Income and CreditScore with median values", "Encode categorical variables EmploymentStatus and LoanPurpose using one-hot encoding", "Split the dataset into training and test sets with an 80/20 ratio stratified on the target Defaulted", "Standardize numeric features Age, Income, CreditScore, and LoanAmount using z-score normalization", "Train a RandomForestClassifier to predict Defaulted", "Perform hyperparameter tuning on max_depth and n_estimators using cross-validation", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Identify and report the top 5 most important features"], "model_results": {"accuracy": 0.8, "precision": 0.78, "recall": 0.75, "f1": 0.765, "confusion_matrix": {"true_negative": 8, "false_positive": 2, "false_negative": 3, "true_positive": 7}, "top_feature_importances": {"CreditScore": 0.32, "Income": 0.22, "EmploymentStatus_Full-time": 0.15, "LoanAmount": 0.12, "LoanPurpose_Debt Consolidation": 0.08}, "best_hyperparameters": {"max_depth": 6, "n_estimators": 100}}}
{"purpose": "Predict whether a manufactured part will fail quality inspection based on process parameters and operator information.", "raw_table": "Part_ID,Operator,Machine_Temperature,Pressure,Shift,Defect\nP001,alice,350,5.1,Morning,No\nP002,BOB,370,,Evening,Yes\nP003,Charlie,355,5.0,Night,no\nP004,alice,360,4.8,Morning,Yes\nP005,bob,NaN,5.2,evening,No\nP006,Charlie,365,5.4,Night,Yes\nP007,Alice,358,5.1,Morning,No\nP008,BOB,362,5.3,Evening,Yes\nP009,charlie,355,5.0,Night,NO\nP010,Alice,359,5.2,Morning,Yes\nP011,bob,360,5.1,evening,No\nP012,Charlie,370,5.5,Night,Yes\nP013,Alice,361,5.0,Morning,No\nP014,BOB,365,5.4,Evening,Yes", "model_steps": ["Clean 'Operator' and 'Shift' columns by standardizing capitalization", "Impute missing values in 'Pressure' column using median pressure from training data", "Convert 'Defect' target column to binary labels (Yes=1, No=0), handling inconsistent capitalization", "One-hot encode categorical variables: 'Operator' and 'Shift'", "Standardize numeric features: 'Machine_Temperature' and 'Pressure'", "Split data into training and test sets with 80/20 ratio", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search over max_depth parameter with values [3, 5, 7]", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate the confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Machine_Temperature": 0.35, "Pressure": 0.25, "Operator_Alice": 0.15, "Shift_Evening": 0.1, "Operator_Bob": 0.08, "Shift_Morning": 0.07}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a student will pass or fail a final exam based on demographic and academic features.", "raw_table": "StudentID,Gender,Age,StudyHoursPerWeek,PreviousGrade,SchoolType,AttendanceRate,FinalExamResult\n1,M,17,12,85,Public,0.95,Pass\n2,F,16,8,78,Private,0.89,pass\n3,F,17,6,,Public,0.80,Fail\n4,M,18,14,92,public,0.98,Pass\n5,,16,10,75,Private,0.85,Fail\n6,F,17,9,88,Public,missing,Pass\n7,M,15,5,60,Public,0.70,Fail\n8,F,16,7,82,private,0.88,Pass\n9,M,17,11,85,Private,0.90,Pass\n10,F,16,4,55,Public,0.65,fail\n11,M,16,13,90,Private,0.93,Pass\n12,F,17,missing,80,Public,0.87,Fail\n13,M,18,15,95,Private,0.96,Pass\n14,F,16,7,70,Public,0.83,Fail\n15,M,17,9,85,Private,0.91,Pass", "model_steps": ["Load the CSV data into a DataFrame", "Clean and standardize categorical columns: convert 'SchoolType' and 'FinalExamResult' to consistent lowercase values", "Impute missing numeric values in 'StudyHoursPerWeek' and 'AttendanceRate' using median values", "Impute missing categorical values in 'Gender' with the mode", "Convert 'FinalExamResult' to binary target variable: Pass=1, Fail=0", "One-hot encode categorical variables: 'Gender' and 'SchoolType'", "Split data into training (80%) and testing (20%) sets with stratification on the target", "Standardize numeric features: 'Age', 'StudyHoursPerWeek', 'PreviousGrade', and 'AttendanceRate'", "Train a Logistic Regression classifier on the training set", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix for the test predictions"], "model_results": {"accuracy": 0.87, "precision": 0.89, "recall": 0.85, "f1": 0.87, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 1, "false_negative": 2}, "top_features_importance": {"PreviousGrade": 0.45, "StudyHoursPerWeek": 0.3, "AttendanceRate": 0.15, "Gender_F": 0.05, "SchoolType_Private": 0.05}, "hyperparameters": {"model": "Logistic Regression", "regularization": "L2", "C": 1.0, "solver": "lbfgs", "max_iter": 100}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features and location.", "raw_table": "HouseID,SquareFeet,Bedrooms,Bathrooms,Neighborhood,YearBuilt,Condition,HasGarage,SalePrice\n1,1500,3,2,Downtown,1995,Good,Yes,350000\n2,2500,4,3,Suburb,2010,Excellent,yes,520000\n3,1800,,2,suburb,2005,Fair,No,410000\n4,1200,2,1,Downtown,1980,good,No,280000\n5,2000,3,2,Suburb,2015,EXCELLENT,Y,480000\n6,1400,3,1.5,Midtown,1990,Good,No,330000\n7,NaN,3,2,Midtown,1985,Fair,No,300000\n8,1600,3,2,Downtown,NA,Good,Yes,360000\n9,2200,4,3,Midtown,2000,Excellent,Yes,500000\n10,1700,3,2,Suburb,2012,Fair,No,420000\n11,1900,3,2,Downtown,1998,Good,Yes,390000\n12,2100,4,3,Suburb,2018,,yes,530000\n13,1300,2,1,Midtown,1992,Fair,No,310000\n14,1750,3,2,Downtown,2003,Good,Y,400000", "model_steps": ["Identify 'SalePrice' as the target variable for regression.", "Handle missing values: impute missing 'Bedrooms' with median and 'YearBuilt' with mode.", "Standardize inconsistent entries in 'Condition' and 'HasGarage' columns (e.g., normalize capitalization and map 'Y' to 'Yes').", "Drop rows with missing 'SquareFeet' values.", "Encode categorical variables: One-hot encode 'Neighborhood' and 'Condition', and binary encode 'HasGarage'.", "Standardize numeric features: 'SquareFeet', 'Bedrooms', 'Bathrooms', 'YearBuilt'.", "Split data into training and test sets with 80/20 ratio.", "Train a Gradient Boosting Regressor on the training set.", "Perform grid search to optimize learning rate and number of estimators.", "Evaluate the model on the test set using RMSE, MAE, and R2 metrics.", "Analyze top 5 feature importances from the trained model."], "model_results": {"rmse": 22000, "mae": 17000, "r2": 0.87, "top_feature_importances": {"SquareFeet": 0.35, "Neighborhood_Suburb": 0.22, "YearBuilt": 0.15, "Bathrooms": 0.12, "HasGarage_Yes": 0.08}, "best_hyperparameters": {"learning_rate": 0.1, "n_estimators": 150}}}
{"purpose": "Predict whether a customer will make a purchase in the next session based on their browsing behavior and demographics.", "raw_table": "customer_id,age,gender,session_duration_minutes,pages_viewed,device_type,referral_source,purchase_next_session\n1,34,Male,12.5,5,Mobile,Google,Yes\n2,28,Female,7.0,3,desktop,Direct,No\n3,45,Male,NaN,8,Tablet,facebook,Yes\n4,22,Female,15.2,12,mobile,Google,No\n5,37,Other,9.1,7,Desktop,Direct,Yes\n6,29,Female,11,NaN,Mobile,google,No\n7,,Male,8.3,4,Tablet,Facebook,No\n8,41,Female,14.7,9,Desktop,Direct,Yes\n9,35,Male,6.4,3,desktop,,No\n10,26,Female,10.0,5,Mobile,Facebook,Yes\n11,30,Male,NaN,6,Mobile,Google,No\n12,44,Female,13.5,7,tablet,Direct,Yes\n13,38,Female,8.8,5,Desktop,Facebook,No\n14,33,Male,7.5,4,MOBILE,Google,No", "model_steps": ["Load the CSV dataset into a DataFrame", "Handle missing values by imputing median for numeric columns and mode for categorical columns", "Normalize capitalization inconsistencies in categorical columns such as gender, device_type, and referral_source", "Convert target variable 'purchase_next_session' into binary labels (Yes=1, No=0)", "One-hot encode categorical variables: gender, device_type, referral_source", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features: age, session_duration_minutes, pages_viewed", "Train a Logistic Regression classifier on the training data", "Perform 5-fold cross-validation to tune the regularization strength (C parameter)", "Evaluate the trained model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and analyze feature coefficients for interpretability"], "model_results": {"accuracy": 0.79, "precision": 0.81, "recall": 0.75, "f1": 0.78, "confusion_matrix": {"true_positive": 25, "true_negative": 30, "false_positive": 6, "false_negative": 8}, "top_feature_importances": {"session_duration_minutes": 0.45, "pages_viewed": 0.3, "referral_source_Google": 0.15, "device_type_Mobile": 0.1}, "hyperparameters": {"C": 1.0, "penalty": "l2", "solver": "lbfgs"}}}
{"purpose": "Predict whether a customer will make a purchase in their next visit based on browsing behavior and demographics.", "raw_table": "customer_id,age,gender,browser_time_minutes,device_type,visited_pages,last_purchase_days,target_purchase\n1,25,Male,15.2,Mobile,5,30,Yes\n2,NaN,Female,7.8,desktop,2,45,No\n3,35,Female,20.1,Tablet,8,10,Yes\n4,28,male,12.5,Mobile,3,,No\n5,40,Female,5.5,Desktop,1,90,No\n6,22,FEMALE,18.0,Mobile,7,15,Yes\n7,30,Male,15.0,desktop,6,20,yes\n8,45,Male,9.3,Tablet,4,60,No\n9,,Female,11.7,MOBILE,3,40,No\n10,27,Male,NaN,Mobile,5,25,Yes\n11,33,Female,14.2,Desktop,5,35,No\n12,29,Male,16.8,Tablet,7,12,Yes\n13,23,Female,13.5,Mobile,4,50,No\n14,38,Male,9.0,Desktop,,60,No", "model_steps": ["Load dataset and identify target variable 'target_purchase' (binary classification)", "Clean messy values: standardize 'gender' to lowercase, fill missing 'age' and 'browser_time_minutes' with median", "Encode 'gender' and 'device_type' using one-hot encoding", "Impute missing 'visited_pages' with median", "Convert 'target_purchase' to binary labels (Yes=1, No=0)", "Split data into training (80%) and test (20%) sets with stratification on target", "Scale numeric features (age, browser_time_minutes, visited_pages, last_purchase_days) using standard scaler", "Train a RandomForestClassifier with 100 trees", "Perform grid search for max_depth parameter over [5,10,15]", "Evaluate model on test set with accuracy, F1 score, precision, and recall", "Generate feature importance from the trained model"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.82, "recall": 0.87, "best_max_depth": 10, "feature_importances": {"browser_time_minutes": 0.32, "visited_pages": 0.25, "last_purchase_days": 0.18, "age": 0.12, "gender_female": 0.06, "gender_male": 0.05, "device_type_mobile": 0.02, "device_type_desktop": 0.0, "device_type_tablet": 0.0}}}
{"purpose": "Predict hourly energy consumption category (Low, Medium, High) for residential buildings based on weather and building features.", "raw_table": "Hour,Temperature_C,Humidity_Percent,WindSpeed_kmh,Building_Type,Heating_Type,Energy_Consumption_Category\n1,5.2,85,15,Apartment,Electric,Low\n2,7.8,80,12,Detached,Gas,Medium\n3,,75,10,Apartment,Electric,Low\n4,9.1,NaN,8,Detached,Electric,Medium\n5,11.3,65,20,Apartment,Gas,high\n6,14.0,60,25,Detached,Gas,Medium\n7,16.5,55,30,Apartment,electric,High\n8,18.2,50,35,Townhouse,Gas,High\n9,20.1,45,40,Townhouse,Gas,Medium\n10,22.0,40,45,Apartment,Electric,High\n11,23.5,38,50,Detached,Electric,High\n12,24.0,35,55,Detached,Gas,High\n13,23.8,33,50,Townhouse,Gas,Medium\n14,22.5,37,45,Apartment,Electric,Medium", "model_steps": ["Load the dataset and inspect for missing and messy values", "Impute missing numeric values (Temperature_C and Humidity_Percent) with median values", "Normalize capitalization and spelling in categorical columns (e.g., 'electric' to 'Electric', 'high' to 'High')", "One-hot encode categorical variables: Building_Type and Heating_Type", "Convert target variable Energy_Consumption_Category into ordinal labels (Low=0, Medium=1, High=2)", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features: Temperature_C, Humidity_Percent, WindSpeed_kmh", "Train a RandomForestClassifier with 100 trees", "Perform grid search to tune max_depth parameter over [3, 5, 7]", "Evaluate the model on the test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix and feature importances"], "model_results": {"accuracy": 0.79, "f1": 0.78, "precision": 0.8, "recall": 0.76, "confusion_matrix": {"Low": {"Low": 3, "Medium": 1, "High": 0}, "Medium": {"Low": 1, "Medium": 4, "High": 1}, "High": {"Low": 0, "Medium": 1, "High": 4}}, "top_feature_importances": {"Temperature_C": 0.34, "Humidity_Percent": 0.25, "Heating_Type_Electric": 0.15, "Building_Type_Townhouse": 0.1, "WindSpeed_kmh": 0.08}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict the likelihood of hospital readmission within 30 days for diabetic patients.", "raw_table": "PatientID,Age,Gender,AdmissionType,NumPrevAdmissions,HbA1c,HasHypertension,DischargeDisposition,Readmitted\n1,58,Male,Emergency,2,7.5,Yes,Home,Yes\n2,72,Female,Elective,0,8.2,No,Rehab,No\n3,45,Female,emergency,1,NaN,Yes,home,Yes\n4,60,Male,Urgent,3,9.1,Yes,NursingFacility,No\n5,53,Male,Elective,,7.8,No,Home,Yes\n6,67,Female,Emergency,1,8.0,yes,Home,No\n7,49,Female,Elective,0,7.1,No,Home,No\n8,NaN,Male,Emergency,2,8.5,Yes,Home,Yes\n9,62,female,Urgent,1,7.9,No,Rehab,No\n10,55,Male,Elective,0,7.4,Yes,Home,No\n11,50,Female,Emergency,2,8.7,Yes,home,Yes\n12,70,Male,Elective,1,NaN,No,Home,No\n13,65,Female,Urgent,2,8.3,Yes,NursingFacility,Yes\n14,59,Male,Emergency,3,7.0,No,Home,No", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Standardize categorical values in 'AdmissionType', 'DischargeDisposition', and 'Gender' columns to uniform capitalization", "Impute missing numeric values in 'Age' and 'HbA1c' columns using median values of respective columns", "Convert categorical variables ('Gender', 'AdmissionType', 'HasHypertension', 'DischargeDisposition') into one-hot encoded features", "Split the dataset into training (80%) and testing (20%) subsets stratified by the target variable 'Readmitted'", "Standardize numeric features ('Age', 'NumPrevAdmissions', 'HbA1c') using z-score normalization", "Train a RandomForestClassifier to predict hospital readmission", "Perform hyperparameter tuning using grid search over max_depth values [3, 5, 7]", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze prediction errors", "Identify top 3 most important features based on the trained RandomForest model"], "model_results": {"accuracy": 0.79, "precision": 0.75, "recall": 0.68, "f1": 0.71, "confusion_matrix": [[8, 2], [3, 5]], "top_feature_importances": {"HbA1c": 0.32, "NumPrevAdmissions": 0.27, "DischargeDisposition_NursingFacility": 0.15}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Classify whether a field has a high risk of soil nutrient deficiency based on environmental and soil parameters.", "raw_table": "Field_ID,Soil_pH,Organic_Matter,Prev_Crop,Avg_Temp,Crop_Type,Rainfall_mm,Nutrient_Deficiency\nF01,5.8,3.4,Corn,22.5,Corn,120,Yes\nF02,6.2,2.8,Wheat,19.1,Wheat,85,No\nF03,5.5,,Soybean,20.3,Soybean,95,yes\nF04,7.1,4.1,Corn,21.0,Corn,110,No\nF05,6.0,3.0,Wheat,18.7,Wheat,missing,No\nF06,5.7,3.8,Soybean,20.8,Soybean,102,YES\nF07,6.5,2.7,Corn,22.0,Corn,115,No\nF08,5.9,3.2,Soybean,19.5,Soybean,98,Yes\nF09,6.3,3.6,WHEAT,21.3,Wheat,90,no\nF10,5.6,3.9,Corn,22.2,Corn,118,Yes\nF11,6.1,3.1,Soybean,20.0,Soybean,100,No\nF12,5.4,3.7,Wheat,19.8,Wheat,93,No\nF13,6.4,2.9,Corn,21.7,Corn,112,No\nF14,5.8,3.5,Soybean,20.4,Soybean,89,YES", "model_steps": ["Load the CSV data into a dataframe", "Clean 'Nutrient_Deficiency' target variable to standardize capitalization and encode as binary (Yes=1, No=0)", "Impute missing values in 'Organic_Matter' and 'Rainfall_mm' using median values", "Convert categorical variables ('Prev_Crop', 'Crop_Type') to lowercase and apply one-hot encoding", "Split data into training (80%) and testing (20%) sets randomly", "Standardize numeric features: 'Soil_pH', 'Organic_Matter', 'Avg_Temp', 'Rainfall_mm'", "Train a RandomForestClassifier on the training data", "Perform grid search over 'max_depth' parameter with values [3, 5, 7]", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Extract and report top 3 feature importances", "Generate a confusion matrix for test results"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.81, "recall": 0.87, "top_feature_importances": {"Soil_pH": 0.31, "Organic_Matter": 0.27, "Rainfall_mm": 0.18}, "confusion_matrix": {"True_Positive": 6, "True_Negative": 5, "False_Positive": 1, "False_Negative": 1}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Build a classification model to predict whether a taxi trip will exceed 30 minutes based on trip and passenger features.", "raw_table": "trip_id,passenger_count,trip_distance,store_and_fwd_flag,payment_type,day_of_week,trip_duration_exceeds_30min\n1,2,3.5,N,N,Credit,No\n2,1,7.2,Y,Cash,saturday,Yes\n3,NaN,5.1,n,n,CASH,Yes\n4,3,2.3,N,Credit,Monday,No\n5,2,15.8,N,Credit,Sunday,Yes\n6,one,1.2,Y,credit,friday,No\n7,4,8.0,n,Card,Tuesday,Yes\n8,2,3.3,N,Cash,Tuesday,No\n9,1,10.4,N,CASH,THURSDAY,Yes\n10,3,NaN,N,Credit,Wednesday,No\n11,2,7.8,Y,Cash,sunday,Yes\n12,2,3.0,N,credit,saturday,No\n13,1,12.5,N,Cash,Monday,Yes\n14,3,5.9,Y,Credit,Friday,Yes\n15,2,4.4,n,Credit,Monday,No", "model_steps": ["Load the CSV data and identify messy values such as 'one' in passenger_count and inconsistent capitalization in categorical fields.", "Clean the data by converting 'one' to 1, filling missing numeric values with median, and standardizing categorical values to lowercase.", "Convert 'store_and_fwd_flag' and 'payment_type' categorical columns by one-hot encoding.", "Encode 'day_of_week' as categorical variables using one-hot encoding.", "Convert target variable 'trip_duration_exceeds_30min' to binary labels (Yes=1, No=0).", "Split the dataset into training (80%) and test (20%) sets with stratification on the target variable.", "Standardize numeric features 'passenger_count' and 'trip_distance' using z-score scaling.", "Train a RandomForestClassifier on the training data with 100 trees and max_depth=5.", "Perform 5-fold cross-validation on training data to tune max_depth among [3,5,7].", "Evaluate model on test set by computing accuracy, F1 score, precision, and recall.", "Generate confusion matrix and identify top 3 feature importances from the model."], "model_results": {"accuracy": 0.87, "f1": 0.85, "precision": 0.83, "recall": 0.87, "confusion_matrix": [[8, 2], [2, 7]], "top_feature_importances": {"trip_distance": 0.42, "payment_type_cash": 0.19, "day_of_week_saturday": 0.13}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a government employee will retire within the next year based on demographic and job-related factors.", "raw_table": "EmployeeID,Age,Department,YearsAtAgency,PerformanceRating,HasGraduateDegree,RetiringNextYear\n001,52,Health,28,Exceeding,Yes,Yes\n002,47,Education,15,Meets,No,No\n003,62,Finance,33,Exceeding,yes,Yes\n004,55,health,25,Falls Below,No,Yes\n005,48,Transportation,18,Meets,,No\n006,58,Finance,30,Exceeding,Yes,Yes\n007,45,Education,20,MEETS,No,No\n008,61,Transportation,35,Falls Below,No,Yes\n009,50,Health,22,Exceeding,Yes,No\n010,54,Finance,27,Falls Below,No,No\n011,44,Education,12,Meets,No,No\n012,59,Transportation,31,Exceeding,Yes,Yes\n013,53,Health,29,,Yes,No\n014,60,Finance,34,Meets,No,Yes", "model_steps": ["Load the dataset and identify the target variable as RetiringNextYear", "Clean the Department column by standardizing capitalization", "Fill missing PerformanceRating values with the mode 'Meets'", "Convert categorical variables (Department, PerformanceRating, HasGraduateDegree) to one-hot encoded features", "Encode the target variable 'RetiringNextYear' as binary (Yes=1, No=0)", "Split data into train and test sets (80/20 split)", "Standardize numeric features: Age and YearsAtAgency", "Train a Logistic Regression classifier on the training set", "Evaluate model performance using accuracy, precision, recall, and F1 score on the test set", "Analyze model coefficients to identify top predictors of retirement"], "model_results": {"accuracy": 0.85, "precision": 0.8, "recall": 0.83, "f1": 0.815, "top_feature_importances": {"Age": 1.15, "YearsAtAgency": 0.95, "PerformanceRating_Exceeding": 0.7, "Department_Finance": 0.4, "HasGraduateDegree_Yes": 0.35}}}
{"purpose": "Build a regression model to predict hourly electricity consumption based on weather conditions and time features.", "raw_table": "Hour,DayOfWeek,Temperature_C,Humidity_pct,WindSpeed_mps,WeatherCondition,Electricity_Consumption_kWh\n0,Monday,15.2,45,3.2,Clear,120.5\n1,Monday,14.7,47,3.0,clear,115.3\n2,Monday,14.0,50,2.8,Cloudy,110.0\n3,Monday,NaN,52,2.7,Cloudy,108.9\n4,Monday,13.5,54,2.6,Rainy,105.2\n5,Monday,13.0,,2.5,Rainy,102.0\n6,Monday,12.8,56,2.4,Foggy,100.5\n7,Monday,13.5,58,2.5,Foggy,110.1\n8,Monday,16.0,55,3.1,Clear,140.3\n9,Monday,18.5,50,3.5,Clear,160.7\n10,Monday,20.0,48,4.0,Sunny,175.4\n11,Monday,21.5,45,4.2,sunny,180.2\n12,Monday,22.0,44,4.1,Clear,185.0", "model_steps": ["Load the CSV data into a dataframe and inspect for missing and inconsistent values", "Fill missing numeric values in Temperature_C and Humidity_pct columns using median imputation", "Standardize numeric features: Temperature_C, Humidity_pct, and WindSpeed_mps", "Normalize categorical WeatherCondition by lowercasing all entries and then perform one-hot encoding", "Create cyclical features for Hour to capture daily periodicity using sine and cosine transforms", "Split the dataset into training (80%) and testing (20%) sets ensuring temporal order is preserved", "Train a Gradient Boosting Regressor on the training data to predict Electricity_Consumption_kWh", "Tune hyperparameters max_depth and learning_rate using 5-fold cross-validation on training data", "Evaluate the trained model on the test set using RMSE, MAE, and R2 metrics", "Extract and analyze feature importances from the trained model"], "model_results": {"rmse": 7.8, "mae": 5.4, "r2": 0.89, "top_feature_importances": {"Hour_sin": 0.32, "Hour_cos": 0.28, "Temperature_C": 0.2, "WeatherCondition_Clear": 0.1, "Humidity_pct": 0.07, "WindSpeed_mps": 0.03}, "hyperparameters": {"max_depth": 4, "learning_rate": 0.1}}}
{"purpose": "Predict whether a citizen will default on a government loan program.", "raw_table": "Applicant_ID,Age,Employment_Status,Annual_Income,Loan_Amount,Loan_Purpose,Previous_Default,Defaulted\nA001,34,Full-Time,55000,15000,Home Improvement,No,No\nA002,28,part-time,32000,8000,Education,,Yes\nA003,45,Unemployed,NA,12000,Medical,Yes,Yes\nA004,52,Full-Time,72000,20000,Home Improvement,No,No\nA005,39,Full-Time,60000,15000,Vehicle Repair,No,No\nA006,23,Part-Time,28000,7000,Education,No,No\nA007,37,Full-time,62000,18000,Home Improvement,No,No\nA008,48,Unemployed,41000,10000,Medical,Yes,Yes\nA009,31,Part-time,35000,9000,education,No,No\nA010,29,Full-Time,50000,12000,Vehicle Repair,No,No\nA011,54,full-time,85000,22000,Home Improvement,No,No\nA012,40,Unemployed,45000,11000,Medical,Yes,Yes\nA013,26,Part-Time,30000,6000,Education,No,No", "model_steps": ["Load data and identify target variable as 'Defaulted'", "Clean 'Employment_Status' column to normalize capitalization and fix missing values", "Impute missing numeric values in 'Annual_Income' using median strategy", "One-hot encode categorical variables: Employment_Status and Loan_Purpose", "Convert target 'Defaulted' to binary (Yes=1, No=0)", "Split data into training (80%) and testing (20%) sets with stratification on the target", "Standardize numeric features: Age, Annual_Income, Loan_Amount", "Train a Logistic Regression classifier with balanced class weights", "Perform hyperparameter tuning for regularization strength using cross-validation", "Evaluate model on test set computing accuracy, F1 score, precision, and recall", "Generate confusion matrix and analyze false positives and false negatives"], "model_results": {"accuracy": 0.85, "f1": 0.81, "precision": 0.78, "recall": 0.85, "confusion_matrix": {"true_negative": 8, "false_positive": 2, "false_negative": 1, "true_positive": 4}, "top_feature_importances": {"Previous_Default_Yes": 1.75, "Annual_Income": -0.85, "Loan_Amount": 0.65, "Employment_Status_Full-Time": -0.5}, "best_hyperparameter_C": 1.0}}
{"purpose": "Build a classification model to predict whether a manufactured part will be defective based on sensor readings and production parameters.", "raw_table": "Part_ID,Machine_Setting,Temperature,Pressure,Operator_Shift,Material_Batch,Defective\n1,High,75.2,30.5,Day,A1,No\n2,Medium,78.1,29.8,Night,B3,Yes\n3,low,74.0,NaN,Day,A2,No\n4,High,77.5,31.0,Evening,B1,No\n5,Medium,?,30.2,Night,B2,Yes\n6,High,76.8,30.7,Day,A1,No\n7,Medium,75.9,31.1,day,A3,No\n8,Low,73.5,29.9,Evening,B3,Yes\n9,Medium,74.8,30.4,Night,B2,No\n10,High,77.0,30.8,Evening,A1,Yes\n11,LOW,75.5,30.0,Day,A3,No\n12,Medium,76.2,NaN,Day,B1,Yes\n13,High,?,31.3,Night,A2,No\n14,Medium,75.0,30.5,Evening,B3,No", "model_steps": ["Load the CSV data into a dataframe", "Clean the 'Machine_Setting' column by standardizing capitalization to title case", "Impute missing numeric values ('Temperature' and 'Pressure') using median values grouped by 'Machine_Setting'", "Encode categorical variables: 'Machine_Setting', 'Operator_Shift', and 'Material_Batch' using one-hot encoding", "Split the dataset into training and testing sets with an 80/20 ratio, stratifying by the target variable 'Defective'", "Standardize numeric features: 'Temperature' and 'Pressure' using StandardScaler", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search over max_depth values [5, 10, 15] with 5-fold cross-validation", "Select the best model based on F1 score", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.8, "recall": 0.87, "confusion_matrix": {"true_negative": 8, "false_positive": 2, "false_negative": 1, "true_positive": 3}, "top_feature_importances": {"Pressure": 0.32, "Machine_Setting_High": 0.22, "Temperature": 0.18}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a given day will experience extreme heat (temperature above 35\u00b0C) based on environmental and weather conditions.", "raw_table": "Date,Region,Avg_Temperature_C,Humidity_pct,Soil_Type,Wind_Speed_kmh,Extreme_Heat\n2024-06-01,North,34.5,45,Sandy,15,No\n2024-06-02,South,38.2,55,Loamy,7,Yes\n2024-06-03,East,33.1,60,Clay,10,No\n2024-06-04,West,36.0,,Sandy,12,YES\n2024-06-05,North,39.8,50,Loamy,14,Yes\n2024-06-06,East,31.4,65,clay,9,No\n2024-06-07,South,37.3,58,Loamy,NaN,Yes\n2024-06-08,West,35.5,48,Sandy,11,Yes\n2024-06-09,North,32.0,55,Loamy,13,No\n2024-06-10,East,36.7,57,Clay,8,Yes\n2024-06-11,South,34.9,NaN,Sandy,10,No\n2024-06-12,West,30.5,52,Loamy,9,No\n2024-06-13,North,38.0,49,Clay,15,YES\n2024-06-14,East,35.0,61,Sandy,10,No", "model_steps": ["Load the dataset and parse date column as a datetime object", "Identify and handle missing values in Humidity_pct and Wind_Speed_kmh by imputing with median values", "Standardize capitalization in categorical columns: Region, Soil_Type, and Extreme_Heat; unify 'YES' and 'Yes' to 'Yes'", "One-hot encode categorical features Region and Soil_Type", "Convert target variable Extreme_Heat to binary label (Yes=1, No=0)", "Split data into training (80%) and testing (20%) sets with stratification on the target", "Standardize numeric features Avg_Temperature_C, Humidity_pct, and Wind_Speed_kmh using training set statistics", "Train a RandomForestClassifier with 100 trees to predict Extreme_Heat", "Perform grid search over max_depth values [3, 5, 7] with 5-fold cross-validation on training set", "Select best model based on highest F1 score", "Evaluate final model on test set computing accuracy, F1 score, precision, and recall", "Extract and report top 3 feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.8, "recall": 0.87, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}, "top_feature_importances": {"Avg_Temperature_C": 0.4, "Humidity_pct": 0.25, "Soil_Type_Loamy": 0.15}}}
{"purpose": "Predict whether a retail customer will make a purchase during a promotional campaign.", "raw_table": "CustomerID,Age,Gender,Annual_Income,Last_Visit_Days,Preferred_Channel,Campaign_Response\n1,34,Male,56000,10,Online,Yes\n2,28,Female,47000,5,In-Store,No\n3,45,Male,NaN,30,online,Yes\n4,23,Female,39000,3,In-Store,No\n5,37,Male,72000,15,Online,Yes\n6,31,female,65000,,Online,No\n7,50,Male,58000,20,In-store,Yes\n8,29,Female,48000,7,Online,No\n9,,Male,52000,12,Online,Yes\n10,41,Female,70000,18,In-Store,No\n11,36,Male,69000,NaN,In-store,Yes\n12,27,Female,43000,2,Online,No\n13,39,Male,61000,25,in-store,Yes\n14,30,Female,45000,8,Online,No", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing values and inconsistencies", "Standardize categorical values in 'Preferred_Channel' and 'Gender' columns (e.g., unify capitalization)", "Impute missing numeric values in 'Age', 'Annual_Income', and 'Last_Visit_Days' with median values", "Encode 'Gender' and 'Preferred_Channel' columns using one-hot encoding", "Convert target variable 'Campaign_Response' to binary (Yes=1, No=0)", "Split data into training (80%) and test (20%) sets maintaining class distribution", "Standardize numeric features: 'Age', 'Annual_Income', and 'Last_Visit_Days' using z-score normalization", "Train a RandomForestClassifier with 100 trees on the training data", "Evaluate the model on the test set calculating accuracy, precision, recall, and F1 score", "Generate confusion matrix for detailed error analysis", "Identify top 3 important features from the RandomForest model"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 12, "true_negative": 10, "false_positive": 3, "false_negative": 2}, "top_feature_importances": {"Last_Visit_Days": 0.32, "Annual_Income": 0.28, "Preferred_Channel_Online": 0.15}}}
{"purpose": "Predict whether a customer will make a repeat purchase within 30 days after their first order.", "raw_table": "CustomerID,Age,Gender,FirstPurchaseAmount,ProductCategory,DaysSinceLastLogin,RepeatPurchase\nC001,34,Male,45.60,Electronics,5,Yes\nC002,29,Female,NaN,Home & Garden,12,No\nC003,42,female,120.00,Electronics,2,Yes\nC004,23,Male,15.00,Clothing,30,No\nC005,36,Female,60.50,clothing,7,Yes\nC006,28,Male,80.00,Electronics,NA,No\nC007,41,Female,35.00,Toys,3,Yes\nC008,37,Male,55.00,Home & Garden,10,No\nC009,50,Female,NaN,Toys,0,Yes\nC010,31,MALE,90.00,Electronics,1,Yes\nC011,45,Female,40.00,Clothing,14,No\nC012,38,Male,75.00,home & garden,6,Yes\nC013,26,Female,22.00,Toys,20,No\nC014,33,Male,NaN,Electronics,4,Yes\n", "model_steps": ["Fill missing values in FirstPurchaseAmount with the median purchase amount", "Normalize capitalization in Gender and ProductCategory columns", "Convert DaysSinceLastLogin missing values to the median value", "Encode Gender and ProductCategory using one-hot encoding", "Convert target variable RepeatPurchase to binary (Yes=1, No=0)", "Split data into 80% training and 20% test sets", "Standardize numeric features: Age, FirstPurchaseAmount, DaysSinceLastLogin", "Train a Logistic Regression classifier on the training set", "Tune regularization parameter C using grid search with cross-validation", "Evaluate model on test set computing accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze classification errors"], "model_results": {"accuracy": 0.79, "precision": 0.82, "recall": 0.76, "f1": 0.79, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 3}, "top_feature_importances": {"FirstPurchaseAmount": 0.38, "DaysSinceLastLogin": 0.25, "ProductCategory_Electronics": 0.15, "Gender_Female": 0.1, "Age": 0.07, "ProductCategory_Clothing": 0.05}, "best_hyperparameters": {"C": 0.1}}}
{"purpose": "Build a classification model to predict extreme heat days based on weather and location features.", "raw_table": "Day,Temperature_C,Humidity_Percent,Wind_Speed_kmh,Weather_Condition,Region,Extreme_Heat\n1,35,45,15,Sunny,North,Yes\n2,28,50,12,cloudy,South,No\n3,40,35,,Sunny,East,YES\n4,33,55,20,Rainy,West,No\n5,38,48,14,Sunny,South,Yes\n6,27,60,10,cloudy,East,No\n7,42,40,18,Sunny,North,Yes\n8,30,52,9,Rainy,west,No\n9,36,44,15,Sunny,South,Yes\n10,31,50,11,Cloudy,East,No\n11,37,46,16,sunny,North,Yes\n12,29,55,13,Rainy,South,No\n13,39,43,17,Sunny,East,Yes\n14,25,65,8,Rainy,West,No", "model_steps": ["Load the CSV data into a DataFrame", "Standardize capitalization and fix inconsistent categorical values in 'Weather_Condition' and 'Region'", "Handle missing values by imputing the missing Wind_Speed_kmh with the median value", "Encode categorical variables 'Weather_Condition' and 'Region' using one-hot encoding", "Convert target variable 'Extreme_Heat' to binary labels (Yes=1, No=0)", "Split dataset into training (80%) and testing (20%) sets", "Standardize numeric features: Temperature_C, Humidity_Percent, Wind_Speed_kmh", "Train a RandomForestClassifier with default parameters on the training set", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Extract and report feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.89, "recall": 0.83, "f1": 0.86, "top_feature_importances": {"Temperature_C": 0.42, "Humidity_Percent": 0.25, "Wind_Speed_kmh": 0.15, "Weather_Condition_Sunny": 0.1, "Region_North": 0.08}}}
{"purpose": "Build a regression model to predict hourly electricity consumption based on weather and time features.", "raw_table": "Hour,Temperature_C,Day_Type,Wind_Speed_mps,Holiday,Electricity_Consumption_kWh\n0,5.2,weekday,3.5,No,120.4\n1,4.8,Weekday,4.1,no,115.6\n2,NaN,weekday,3.8,No,113.2\n3,3.9,weekend,2.9,yes,97.4\n4,3.7,weekend,3.0,Yes,95.8\n5,4.0,weekday,3.2,NO,105.1\n6,6.5,Weekday,5.5,No,130.7\n7,10.2,Weekday,6.0,No,160.3\n8,14.0,weekday,5.8,No,210.5\n9,16.1,Weekday,4.9,No,230.8\n10,17.5,Weekday,5.0,No,245.2\n11,18.3,weekday,4.7,No,255.7\n12,19.0,Weekend,3.8,No,260.1\n13,20.5,weekend,3.5,No,270.4\n14,21.3,weekday,3.2,no,275.3", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Normalize inconsistencies in categorical columns (e.g., standardize capitalization in Day_Type and Holiday)", "Impute missing numeric values in Temperature_C using median imputation", "One-hot encode categorical variables Day_Type and Holiday", "Split data into training (80%) and testing (20%) sets", "Standardize numeric features: Hour, Temperature_C, and Wind_Speed_mps", "Train a RandomForestRegressor on the training set", "Tune hyperparameters max_depth and n_estimators using grid search with 5-fold cross-validation", "Evaluate model performance on the test set using RMSE, MAE, and R-squared", "Analyze feature importances from the trained model", "Generate predicted vs actual consumption plot (not included in JSON)"], "model_results": {"rmse": 8.7, "mae": 6.3, "r2": 0.92, "best_hyperparameters": {"max_depth": 6, "n_estimators": 100}, "feature_importances": {"Temperature_C": 0.42, "Hour": 0.35, "Day_Type_weekday": 0.12, "Holiday_No": 0.06, "Wind_Speed_mps": 0.05}}}
{"purpose": "Predict whether a manufactured part will be defective based on sensor readings and production parameters.", "raw_table": "Part_ID,Machine_ID,Temperature,Pressure,Operator_Shift,Material_Type,Defect_Status\n1,M01,75.2,101.5,Day,Steel,No\n2,M02,80.1,99.8,Night,steel,Yes\n3,M01,NaN,100.2,Day,Aluminum,No\n4,M03,77.0,NaN,Night,Aluminum,No\n5,M01,78.5,102.3,Night,Copper,Yes\n6,m02,76.3,101.0,Day,Copper,No\n7,M03,79.0,100.5,Day,Steel,No\n8,M01,75.8,100.9,Night,aluminum,Yes\n9,M02,80.5,101.2,Day,Copper,No\n10,M03,77.2,99.9,Day,Steel,No\n11,M02,NaN,100.7,Night,Steel,Yes\n12,M01,78.1,101.8,Day,Copper,No\n13,M03,76.7,101.1,Night,Steel,No\n14,M01,79.3,100.3,Day,Steel,Yes", "model_steps": ["Load the CSV data into a DataFrame.", "Identify and handle missing values in Temperature and Pressure by imputing with the median.", "Standardize capitalization inconsistencies in categorical variables Machine_ID and Material_Type.", "One-hot encode categorical variables: Machine_ID, Operator_Shift, Material_Type.", "Split data into 80% training and 20% testing sets stratified by Defect_Status.", "Standardize numeric features Temperature and Pressure using training set statistics.", "Train a RandomForestClassifier to predict Defect_Status.", "Tune hyperparameters max_depth and n_estimators using 5-fold cross-validation on the training set.", "Evaluate the final model on the test set reporting accuracy, F1 score, precision, and recall.", "Generate a confusion matrix and extract top 3 feature importances."], "model_results": {"accuracy": 0.86, "f1": 0.83, "precision": 0.81, "recall": 0.85, "confusion_matrix": {"true_negative": 12, "false_positive": 2, "false_negative": 3, "true_positive": 7}, "top_feature_importances": {"Temperature": 0.27, "Pressure": 0.22, "Material_Type_Steel": 0.16}, "best_hyperparameters": {"max_depth": 7, "n_estimators": 100}}}
{"purpose": "Predict whether a social media post will go viral based on post attributes and user information.", "raw_table": "post_id,user_type,post_length,has_image,post_time,likes,shares,comments,post_category,is_viral\n1,Influencer,120,Yes,Morning,350,45,30,Lifestyle,Yes\n2,casual,55,NO,Evening,23,3,5,Tech,No\n3,Brand,200,Yes,Night,400,50,40,Fashion,Yes\n4,Influencer,NaN,Yes,Afternoon,310,60,35,Lifestyle,Yes\n5,Casual,80,No,Morning,15,1,2,Tech,No\n6,Brand,150,yes,Night,390,48,38,Fashion,Yes\n7,INFLUENCER,100,No,Evening,280,40,25,Lifestyle,No\n8,casual,70,No,Morning,18,2,3,Tech,No\n9,Brand,130,Yes,Afternoon,410,55,45,Fashion,Yes\n10,Casual,90,No,Night,22,4,6,Tech,No\n11,Influencer,110,Yes,Morning,330,50,32,Lifestyle,Yes\n12,Brand,NaN,Yes,Night,400,52,41,Fashion,Yes\n13,Casual,60,No,Afternoon,20,3,4,Tech,No\n14,Influencer,105,Yes,Evening,NaN,44,28,Lifestyle,Yes", "model_steps": ["Load the CSV data and identify the target variable as 'is_viral'.", "Clean inconsistent capitalization in 'user_type' and 'has_image' columns.", "Impute missing numeric values in 'post_length' and 'likes' using median values.", "Encode categorical variables 'user_type', 'post_time', and 'post_category' using one-hot encoding.", "Convert 'has_image' column from Yes/No to binary 1/0.", "Split data into training and test sets with an 80/20 ratio.", "Standardize numeric features including 'post_length', 'likes', 'shares', and 'comments'.", "Train a RandomForestClassifier to predict 'is_viral'.", "Tune 'max_depth' parameter using grid search with cross-validation.", "Evaluate model performance on test data using accuracy, precision, recall, and F1 score.", "Generate and analyze the confusion matrix.", "Report top 3 feature importances from the trained model."], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"shares": 0.28, "likes": 0.25, "post_category_Fashion": 0.15}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a given field will have a high or low yield based on soil and weather conditions.", "raw_table": "FieldID,SoilType,Rainfall(mm),Temperature(C),FertilizerUsed,PreviousCrop,YieldCategory\n1,Loam,200,22.5,Yes,Wheat,High\n2,Clay,150,19.0,yes,Corn,Low\n3,SAND,300,25.2,No,Rice,High\n4,Loam,NA,21.1,Yes,Corn,High\n5,Clay,180,20.0,No,wheat,Low\n6,Loam,210,22.8,YES,Rice,High\n7,Silt,190,23.5,No,Corn,Low\n8,sand,160,24.0,Yes,Wheat,Low\n9,Clay,170,19.5,No,Rice,Low\n10,Silt,220,23.0,yes,Wheat,High\n11,Loam,205,21.9,Yes,Corn,High\n12,Clay,185,20.5,No,Wheat,Low\n13,Silt,195,22.7,No,Rice,Low\n14,Sand,210,24.1,Yes,Wheat,High", "model_steps": ["Load the raw CSV data into a dataframe", "Clean the 'SoilType' and 'FertilizerUsed' columns by standardizing capitalization and fixing missing values", "Impute missing Rainfall values using the median rainfall of the corresponding SoilType", "Convert categorical variables ('SoilType', 'FertilizerUsed', 'PreviousCrop') to one-hot encoded features", "Encode target variable 'YieldCategory' as binary (High=1, Low=0)", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features ('Rainfall(mm)', 'Temperature(C)') to zero mean and unit variance", "Train a RandomForestClassifier with 100 trees on the training set", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Extract and report the top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "top_feature_importances": {"FertilizerUsed_Yes": 0.32, "Rainfall(mm)": 0.27, "SoilType_Loam": 0.18}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Build a regression model to predict hourly electricity consumption of residential buildings based on weather and building characteristics.", "raw_table": "Building_ID,Building_Type,Outdoor_Temperature_C,Hour_of_Day,Day_of_Week,Humidity_Percent,Wind_Speed_m_s,Electricity_Consumption_kWh\nB001,Apartment,22,14,Monday,45,3.2,5.6\nB002,Detached House,18,2,Tuesday,55,,2.1\nB003,Apartment,NaN,9,Wednesday,70,1.5,4.2\nB004,Detached house,25,18,Thursday,40,2.8,6.8\nb005,Townhouse,20,21,Friday,60,3.0,5.0\nB006,Townhouse,19,7,Saturday,50,2.1,3.9\nB007,Apartment,23,15,Sunday,48,3.4,6.1\nB008,Detached House,17,6,Monday,,1.7,2.8\nB009,Townhouse,21,20,Tuesday,65,NaN,5.4\nB010,Apartment,22,13,Wednesday,47,2.9,5.9\nB011,Detached house,18,3,Thursday,52,3.3,3.0\nB012,Apartment,19,10,Friday,53,2.2,4.8\nB013,townhouse,24,17,Saturday,49,3.1,6.3\nB014,Detached House,NaN,8,Sunday,58,2.0,4.1", "model_steps": ["Load CSV data into a DataFrame", "Clean 'Building_Type' column by normalizing capitalization", "Impute missing numeric values with median for Temperature, Humidity, and Wind Speed", "Convert 'Day_of_Week' and 'Building_Type' into one-hot encoded features", "Split data into training (80%) and test (20%) sets randomly", "Standardize numeric features: Temperature, Hour_of_Day, Humidity, Wind Speed", "Train a Gradient Boosting Regressor to predict Electricity_Consumption_kWh", "Tune number of estimators and learning rate using 5-fold cross-validation", "Evaluate model on test set using RMSE, MAE, and R2 metrics", "Identify and report top 3 most important features"], "model_results": {"rmse": 0.85, "mae": 0.65, "r2": 0.78, "top_feature_importances": {"Hour_of_Day": 0.36, "Outdoor_Temperature_C": 0.29, "Building_Type_Apartment": 0.15}, "hyperparameters": {"n_estimators": 150, "learning_rate": 0.1}}}
{"purpose": "Predict the likelihood of hospital readmission within 30 days for diabetic patients.", "raw_table": "PatientID,Age,Gender,HbA1c_Level,Previous_Admissions,Medication_Adherence,Smoker,Readmitted\n001,58,Male,7.5,2,Yes,No,Yes\n002,65,Female,8.1,NA,No,Yes,No\n003,45,Female,6.8,0,Yes,No,No\n004,50,Male,High,1,yes,No,Yes\n005,72,Female,7.9,3,No,yes,Yes\n006,60,male,7.2,2,Mixed,No,No\n007,55,Female,8.5,2,Yes,No,Yes\n008,49,Male,6.4,0,Yes,No,No\n009,66,Female,7.7,4,No,NO,Yes\n010,70,Male,7.1,1,No,Yes,No\n011,53,Female,,2,Yes,No,No\n012,47,male,6.9,0,Yes,No,No\n013,62,Female,8.0,3,No,yes,Yes", "model_steps": ["Load dataset and inspect for missing and inconsistent values", "Impute missing HbA1c_Level with median value", "Standardize capitalization in Gender, Medication_Adherence, and Smoker columns", "Convert 'High' in HbA1c_Level to numeric (e.g. 9.0) and convert Medication_Adherence 'Mixed' to 'No'", "One-hot encode Gender, Medication_Adherence, and Smoker categorical variables", "Split data into training (80%) and test (20%) sets stratified on Readmitted target", "Standardize numeric features Age, HbA1c_Level, and Previous_Admissions", "Train a RandomForestClassifier to predict Readmitted", "Perform grid search over max_depth and n_estimators hyperparameters", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 important features"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.8, "f1": 0.81, "confusion_matrix": {"true_positive": 8, "true_negative": 10, "false_positive": 2, "false_negative": 2}, "top_feature_importances": {"HbA1c_Level": 0.35, "Previous_Admissions": 0.3, "Medication_Adherence_Yes": 0.15}, "best_hyperparameters": {"max_depth": 6, "n_estimators": 100}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features.", "raw_table": "id,bedrooms,bathrooms,sqft_living,condition,zipcode,year_built,has_garage,price\n1,3,2,1180,Good,98103,1995,Yes,450000\n2,4,3,2570,Excellent,98052,2010,yes,850000\n3,2,1,770,FAIR,98115,1980,No,320000\n4,3,,1340,Good,98103,1975,No,400000\n5,5,3,3000,excellent,98052,2015,Yes,920000\n6,4,2,1900,Good,98103,NaN,No,600000\n7,3,2,1500,Poor,98115,1960,No,350000\n8,4,2.5,2000,Good,98103,2005,yes,680000\n9,NaN,1,900,Good,98115,1985,No,360000\n10,3,2,1300,Fair,98052,1990,No,450000\n11,4,3,2200,Good,98103,2000,Yes,720000\n12,3,2,1400,GOOD,98103,1998,Yes,480000\n13,2,1,800,Poor,98115,1970,No,300000\n14,5,4,3500,Excellent,98052,2018,Yes,1000000", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values in bedrooms, bathrooms, and year_built columns (impute median for numeric)", "Standardize inconsistent capitalization in 'condition' and 'has_garage' columns", "Convert 'condition' and 'has_garage' categorical columns into one-hot encoded variables", "Drop the 'id' and 'zipcode' columns as they are not predictive for price", "Split the dataset into train and test sets with an 80/20 ratio", "Standardize numeric features: bedrooms, bathrooms, sqft_living, year_built", "Train a Gradient Boosting Regressor model on the training data", "Evaluate the model performance on the test set using RMSE, MAE, and R\u00b2 metrics", "Extract and report the top 3 feature importances from the trained model"], "model_results": {"rmse": 45000, "mae": 35000, "r2": 0.82, "top_features": {"sqft_living": 0.45, "condition_Excellent": 0.15, "has_garage_Yes": 0.1}}}
{"purpose": "Build a classification model to predict whether a taxi trip will have a tip above 20% of the fare.", "raw_table": "trip_id,passenger_count,trip_distance,payment_type,store_and_fwd_flag,tip_percentage\n1,1,2.5,Card,N,0.22\n2,2,5.1,CASH,Y,0.15\n3,,3.0,card,N,NA\n4,1,0.8,Card,N,0.25\n5,3,7.2,Cash,N,0.18\n6,1,1.4,CARD,N,0.30\n7,2,4.5,cash,Y,0.05\n8,1,3.3,Card,N,0.21\n9,1,2.0,CASH,Y,0.19\n10,4,6.0,CARD,N,0.23\n11,2,5.7,Cash,N,0.12\n12,1,2.2,card,N,0.20\n13,2,3.8,CASH,Y,0.24\n14,1,1.1,Card,N,missing\n15,,2.9,CARD,N,0.28", "model_steps": ["Load data and handle missing values in passenger_count by imputing median", "Convert tip_percentage to binary target: 1 if tip_percentage > 0.20 else 0", "Standardize capitalization of payment_type and store_and_fwd_flag columns", "One-hot encode payment_type and store_and_fwd_flag categorical variables", "Split data into train and test sets with 80/20 ratio, stratified by target", "Standardize numeric features: passenger_count and trip_distance", "Train a Logistic Regression classifier on training data", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score", "Extract and report top feature coefficients from the logistic regression model"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.8, "f1": 0.815, "top_features": {"tip_above_20%": {"trip_distance": 1.45, "payment_type_Card": 0.95, "store_and_fwd_flag_N": 0.65, "passenger_count": 0.4, "payment_type_Cash": -0.75, "store_and_fwd_flag_Y": -0.6}}}}
{"purpose": "Build a classification model to predict whether a field's soil moisture level is 'Optimal' or 'Suboptimal' for crop growth.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Avg_Temperature_C,Previous_Crop,Soil_Moisture_Status\nF001,Loam,120,22.5,Wheat,Optimal\nF002,clay,85,19.8,Corn,suboptimal\nF003,Sand,NaN,25.1,Soybean,Optimal\nF004,Loam,110,21.0,Wheat,Optimal\nF005,CLAY,78,18.5,Corn,SubOptimal\nF006,Silt,95,20.3,Barley,Optimal\nF007,Sand,60,27.2,Corn,Optimal\nF008,Loam,100,,Soybean,suboptimal\nF009,LoaM,115,23.1,Barley,Optimal\nF010,Silt,88,20.0,Wheat,optimal\nF011,Clay,NaN,19.0,Corn,Suboptimal\nF012,Sand,70,26.7,Wheat,Optimal\nF013,Silt,92,21.5,Soybean,Optimal\nF014,Loam,105,22.0,Corn,Suboptimal", "model_steps": ["Load the CSV data into a dataframe", "Normalize categorical values in 'Soil_Type' and 'Soil_Moisture_Status' columns to lowercase", "Impute missing values in 'Rainfall_mm' and 'Avg_Temperature_C' with column median", "Encode categorical variables 'Soil_Type' and 'Previous_Crop' using one-hot encoding", "Map target variable 'Soil_Moisture_Status' to binary classes: 'optimal' = 1, 'suboptimal' = 0", "Split the dataset into training (80%) and testing (20%) sets with stratification on the target", "Standardize numeric features: 'Rainfall_mm' and 'Avg_Temperature_C' using StandardScaler", "Train a RandomForestClassifier with 100 trees on the training set", "Perform a grid search over max_depth in [5, 10, None] using 3-fold cross-validation", "Select the best model based on F1 score", "Evaluate final model on test data computing accuracy, precision, recall, and F1 score", "Generate confusion matrix for test predictions"], "model_results": {"accuracy": 0.83, "f1": 0.85, "precision": 0.88, "recall": 0.82, "confusion_matrix": [[5, 1], [2, 6]], "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}, "top_feature_importances": {"Rainfall_mm": 0.32, "Soil_Type_loam": 0.22, "Avg_Temperature_C": 0.18, "Previous_Crop_Corn": 0.12, "Soil_Type_clay": 0.08, "Previous_Crop_Soybean": 0.05, "Soil_Type_silt": 0.03}}}
{"purpose": "Predict whether a social media post will go viral based on its characteristics.", "raw_table": "post_id,num_likes,num_comments,post_length,content_type,user_followers,user_verified,day_posted,target_viral\n1,120,15,250,text,5000,Yes,Monday,Yes\n2,35,0,80,image,300,No,Friday,No\n3,NaN,5,300,video,12000,yes,Wednesday,Yes\n4,10,1,,Text,800,No,Tuesday,No\n5,200,40,500,video,25000,Yes,Sunday,Yes\n6,17,3,150,image,450,NO,Thursday,No\n7,55,7,NaN,text,1500,No,Monday,No\n8,300,50,450,Video,32000,Yes,Saturday,Yes\n9,5,0,60,IMAGE,200,No,Friday,No\n10,100,20,350,text,,Yes,Wednesday,Yes\n11,75,10,200,image,1800,No,Sunday,No\n12,0,0,40,text,100,No,Tuesday,No\n13,250,35,400,video,22000,Yes,Monday,Yes\n14,NaN,NaN,100,text,400,No,Thursday,No", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing or inconsistent values", "Standardize categorical columns: normalize 'content_type' and 'user_verified' to consistent lowercase values", "Fill missing numeric values with median for columns 'num_likes', 'num_comments', and 'post_length'", "Encode categorical variables: one-hot encode 'content_type', 'day_posted', and map 'user_verified' to binary", "Split data into training and test sets with an 80/20 ratio, stratifying by the target variable 'target_viral'", "Scale numeric features using Min-Max scaling", "Train a RandomForestClassifier with default parameters to predict 'target_viral'", "Perform hyperparameter tuning via grid search on 'n_estimators' and 'max_depth'", "Evaluate the final model on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.86, "precision": 0.84, "recall": 0.88, "f1": 0.86, "confusion_matrix": [[14, 3], [2, 21]], "top_feature_importances": {"user_followers": 0.35, "num_comments": 0.22, "content_type_video": 0.18}, "best_hyperparameters": {"n_estimators": 200, "max_depth": 10}}}
{"purpose": "Predict hourly electricity consumption (kWh) based on weather and building characteristics.", "raw_table": "Hour,Temperature_C,Humidity_Percent,Day_Type,Building_Type,Previous_Hour_Consumption_kWh,Is_Holiday,Electricity_Consumption_kWh\n0,22.5,55,weekday,Office,150.2,No,158.7\n1,21.8,58,Weekday,office,148.9,No,155.2\n2,20.9,60,Weekend,Office,140.3,Yes,145.1\n3,20.1,64,weekend,Residential,135.7,yes,132.5\n4,19.5,65,WEEKEND,Residential,130.0,Yes,128.9\n5,19.0,,Weekday,Residential,125.6,No,120.4\n6,21.2,59,Weekday,Commercial,130.4,No,135.7\n7,24.0,54,Weekday,commercial,135.9,No,142.3\n8,27.5,50,Weekday,Office,140.0,No,150.6\n9,29.0,48,Weekday,Office,145.7,No,155.9\n10,30.2,45,Weekday,Office,150.3,No,160.4\n11,31.0,44,Weekday,Office,155.0,No,165.1\n12,31.5,43,Weekday,Office,160.2,No,170.2\n13,32.0,42,Holiday,Office,165.0,Yes,172.5", "model_steps": ["Load the dataset and inspect for missing or inconsistent data", "Clean 'Day_Type' and 'Is_Holiday' columns to standardize capitalization and fix inconsistent values", "Impute missing values in 'Humidity_Percent' with median humidity", "Convert categorical features ('Day_Type', 'Building_Type', 'Is_Holiday') using one-hot encoding", "Split data into training (80%) and test (20%) sets, stratified by 'Day_Type'", "Standardize numeric features: 'Temperature_C' and 'Previous_Hour_Consumption_kWh'", "Train a Gradient Boosting Regressor to predict 'Electricity_Consumption_kWh'", "Perform hyperparameter tuning on learning rate and max depth using 5-fold cross-validation", "Evaluate model performance on the test set using RMSE, MAE, and R-squared", "Analyze feature importances from the trained model", "Generate predicted vs actual consumption plot (not included in JSON)"], "model_results": {"rmse": 4.5, "mae": 3.2, "r2": 0.92, "best_hyperparameters": {"learning_rate": 0.1, "max_depth": 4}, "top_feature_importances": {"Previous_Hour_Consumption_kWh": 0.55, "Temperature_C": 0.25, "Is_Holiday_Yes": 0.1, "Building_Type_Office": 0.05, "Day_Type_Weekday": 0.05}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features and location.", "raw_table": "HouseID,Location,Bedrooms,Bathrooms,Size_sqft,Year_Built,Garage,Has_Pool,SalePrice\n1,Downtown,3,2,1500,1998,yes,No,350000\n2,Suburb,4,3,2000,2005,Yes,yes,450000\n3,Downtown,2,1,900,2010,no,No,280000\n4,Riverside,3,2,1800,,Yes,No,370000\n5,Suburb,4,3,2200,2012,YES,yes,480000\n6,Riverside,3,2,1750,2000,no,No,360000\n7,suburb,3,2,1600,2003,No,Yes,390000\n8,Downtown,2,1,1100,2008,Yes,no,310000\n9,Riverside,4,,2100,2007,Yes,No,440000\n10,Suburb,3,2,1700,2006,yes,Yes,400000\n11,Downtown,3,2,1450,1999,No,No,340000\n12,Riverside,3,2,1800,2001,yes,no,365000\n13,Suburb,4,3,2250,2015,Yes,Yes,500000\n14,Downtown,2,1,1000,2011,No,No,300000", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Standardize casing in categorical columns (e.g., Location, Garage, Has_Pool) and correct typos", "Impute missing numeric values (Year_Built and Bathrooms) using median values", "Convert categorical variables Location, Garage, and Has_Pool into one-hot encoded features", "Split data into training and test sets with an 80/20 ratio", "Scale numeric features Bedrooms, Bathrooms, Size_sqft, and Year_Built using standard scaler", "Train a Gradient Boosting Regressor on the training set", "Tune hyperparameters max_depth and learning_rate using grid search with 5-fold cross-validation", "Evaluate model performance on the test set using RMSE, MAE, and R-squared metrics", "Analyze feature importances from the trained model"], "model_results": {"rmse": 22000, "mae": 17000, "r2": 0.87, "best_hyperparameters": {"max_depth": 4, "learning_rate": 0.1}, "top_feature_importances": {"Size_sqft": 0.35, "Location_Downtown": 0.2, "Bedrooms": 0.15, "Garage_yes": 0.1, "Year_Built": 0.08, "Has_Pool_yes": 0.07, "Bathrooms": 0.05}}}
{"purpose": "Build a classification model to predict whether a government grant application will be approved.", "raw_table": "Applicant_ID,Age,Education_Level,Previous_Grants,Department,Application_Score,Approved\nGOV1001,45,Bachelor,2,Health,88,Yes\nGOV1002,38,master,0,Education,74,No\nGOV1003,29,,1,Transportation,65,No\nGOV1004,52,PhD,3,Health,90,Yes\nGOV1005,47,Bachelor,NaN,Education,82,Yes\nGOV1006,33,Master,1,Health,70,no\nGOV1007,41,Bachelor,2,Transportation,75,Yes\nGOV1008,50,PhD,4,education,95,Yes\nGOV1009,28,Master,0,Health,60,No\nGOV1010,36,Bachelor,1,Transportation,68,No\nGOV1011,43,PhD,3,Health,85,Yes\nGOV1012,39,bachelor,2,Education,78,No\nGOV1013,31,Master,1,Transportation,72,No", "model_steps": ["Load the dataset and handle missing values by imputing the missing Education_Level with the mode.", "Standardize capitalization inconsistencies in categorical columns like Education_Level and Department.", "Convert the target variable 'Approved' to binary numeric values (Yes=1, No=0).", "One-hot encode categorical variables: Education_Level and Department.", "Split data into train and test sets with an 80/20 ratio, stratifying by the target variable.", "Standardize numeric features such as Age, Previous_Grants, and Application_Score.", "Train a RandomForestClassifier with default hyperparameters on the training data.", "Perform grid search over max_depth (values: 3, 5, 7) and n_estimators (values: 50, 100) using 5-fold cross-validation.", "Evaluate the best model on the test set, computing accuracy, precision, recall, and F1 score.", "Generate a confusion matrix and identify the top 3 most important features."], "model_results": {"accuracy": 0.85, "precision": 0.88, "recall": 0.81, "f1": 0.84, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 1, "false_negative": 2}, "top_features": {"Application_Score": 0.42, "Previous_Grants": 0.25, "Education_Level_PhD": 0.15}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict whether a trip will be delayed by more than 10 minutes.", "raw_table": "trip_id,vehicle_type,driver_experience_years,route_length_km,departure_hour,weather_condition,traffic_level,is_delayed\n1,Truck,5,12.5,8,mild,high,Yes\n2,van,2,8.0,14,Heavy,medium,No\n3,Truck,10,15.0,6,Moderate,High,YES\n4,Van,3,,20,Mild,low,No\n5,truck,7,10.0,9,moderate,High,Yes\n6,Van,4,11.2,18,Heavy,Medium,no\n7,Truck,NaN,13.1,7,Mild,High,Yes\n8,Van,1,9.5,12,moderate,Low,No\n9,truck,6,14.3,19,HEAVY,Medium,Yes\n10,van,2,7.8,15,Mild,Low,No\n11,Truck,9,16.0,5,Mild,High,Yes\n12,Van,3,10.5,13,Moderate,Medium,No\n13,Truck,8,11.0,11,Heavy,Medium,Yes\n14,Van,5,9.0,17,Mild,Low,No", "model_steps": ["Load the CSV data into a DataFrame", "Clean and normalize categorical variables: standardize vehicle_type and weather_condition capitalization", "Impute missing numeric values in driver_experience_years and route_length_km with median values", "Convert is_delayed target variable to binary (Yes=1, No=0)", "One-hot encode categorical features vehicle_type, weather_condition, and traffic_level", "Split data into training (80%) and test sets (20%) with stratification on is_delayed", "Standardize numeric features driver_experience_years, route_length_km, and departure_hour", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search to tune max_depth (values 3, 5, 7)", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Compute confusion matrix to analyze misclassifications"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_negative": 5, "false_positive": 1, "false_negative": 2, "true_positive": 6}, "top_feature_importances": {"traffic_level_high": 0.25, "driver_experience_years": 0.2, "vehicle_type_Truck": 0.18, "weather_condition_Heavy": 0.15, "route_length_km": 0.12}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Build a classification model to predict whether a manufactured part is defective based on sensor readings and process parameters.", "raw_table": "PartID,MachineID,OperatorShift,TemperatureC,PressureBar,SpeedRPM,Defect\n1,M01,Day,75.3,5.2,1200,No\n2,M02,NIGHT,80.1,5.5,1150,Yes\n3,m01,Day,NaN,5.1,1180,No\n4,M03,Evening,78,5.3,NaN,No\n5,M02,Night,77.5,wrong,1170,Yes\n6,M01,Day,76.9,5.0,1195,No\n7,M03,evening,79.2,5.4,1210,Yes\n8,M02,DAY,74.8,5.2,1165,No\n9,M01,Day,75.0,5.3,1185,No\n10,M03,Evening,78.3,5.5,1190,Yes\n11,M02,Night,77.1,5.4,1175,No\n12,M01,Day,76.2,5.1,NaN,No\n13,M03,Evening,78.7,5.6,1205,Yes", "model_steps": ["Load data from CSV and inspect for missing and inconsistent values", "Clean and standardize categorical columns (e.g., unify 'Night', 'NIGHT', 'night' to 'Night')", "Impute missing numeric values using median imputation", "Convert 'PressureBar' entries with invalid values ('wrong') to NaN and impute", "One-hot encode 'MachineID' and 'OperatorShift' categorical variables", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features: TemperatureC, PressureBar, SpeedRPM", "Train a RandomForestClassifier to predict 'Defect'", "Tune the max_depth hyperparameter using grid search with 5-fold cross-validation", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix to analyze false positives and false negatives"], "model_results": {"accuracy": 0.85, "precision": 0.8, "recall": 0.75, "f1": 0.77, "confusion_matrix": {"true_negative": 7, "false_positive": 2, "false_negative": 3, "true_positive": 5}, "top_feature_importances": {"PressureBar": 0.3, "TemperatureC": 0.25, "SpeedRPM": 0.2, "MachineID_M02": 0.1, "OperatorShift_Night": 0.08, "MachineID_M03": 0.07}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Build a classification model to predict whether a citizen application for government assistance will be approved.", "raw_table": "Applicant_ID,Age,Income,Employment_Status,Marital_Status,Number_of_Dependents,State,Application_Status\n1,34,55000,Employed,Married,2,CA,Approved\n2,28,42000,unemployed,Single,0,NY,Denied\n3,45,78000,Employed,Married,3,TX,Approved\n4,NaN,36000,Employed,Single,1,ca,Denied\n5,22,28000,Part-time,Single,0,FL,Denied\n6,38,52000,Employed,Married,2,TX,Approved\n7,41,58000,Self-Employed,Divorced,1,NY,Approved\n8,29,NaN,Unemployed,Single,0,FL,Denied\n9,50,67000,Employed,Married,4,CA,Approved\n10,33,48000,Employed,Single,1,TX,Denied\n11,27,30000,freelancer,Single,0,NY,Denied\n12,44,61000,Employed,Married,2,FL,Approved\n13,35,57000,Employed,Married,3,TX,Approved\n14,30,45000,Employed,,1,CA,Denied", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values: impute Age and Income with median values, fill missing Marital_Status with 'Unknown'", "Normalize inconsistent capitalization in Employment_Status and State columns", "Consolidate similar Employment_Status categories (e.g., map 'freelancer' to 'Self-Employed')", "One-hot encode categorical variables: Employment_Status, Marital_Status, and State", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features: Age, Income, Number_of_Dependents", "Train a RandomForestClassifier on the training set", "Perform grid search cross-validation over max_depth values [5, 10, 15]", "Evaluate model performance on the test set using accuracy, F1 score, precision, and recall", "Generate and analyze the confusion matrix", "Extract and report top 3 feature importances"], "model_results": {"accuracy": 0.857, "f1": 0.84, "precision": 0.81, "recall": 0.88, "confusion_matrix": {"true_positive": 12, "true_negative": 6, "false_positive": 3, "false_negative": 2}, "top_feature_importances": {"Income": 0.32, "Employment_Status_Employed": 0.21, "Number_of_Dependents": 0.14}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a government building inspection will pass or fail based on inspection and building characteristics.", "raw_table": "Building_ID,Year_Built,Building_Type,Number_of_Floors,Inspector_Experience,Last_Inspection_Score,Has_Fire_Alarm,Inspection_Result\n001,1998,Residential,3,5,87,yes,Pass\n002,2005,Commercial,10,ten,92,Yes,Pass\n003,1980,Industrial,5,7,65,no,Fail\n004,1995,Residential,,3,70,No,Fail\n005,2010,Commercial,12,8,95,yes,Pass\n006,2000,Commercial,8,6,missing,Yes,Pass\n007,1975,Industrial,4,4,60,No,Fail\n008,2015,Residential,2,9,98,yes,Pass\n009,1988,Commercial,7,five,80,No,Fail\n010,2003,Industrial,6,5,75,Yes,Pass\n011,1990,Residential,3,6,82,YES,Pass\n012,2007,Commercial,9,7,90,yes,Pass\n013,1992,Residential,3,NaN,78,no,Fail\n014,2012,Industrial,5,8,88,Yes,Pass", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Impute missing numeric values (e.g., Number_of_Floors, Last_Inspection_Score, Inspector_Experience) using median or mode as appropriate", "Normalize capitalization and standardize categorical values for Has_Fire_Alarm and Building_Type", "Convert categorical variables (Building_Type, Has_Fire_Alarm) into one-hot encoded features", "Convert Inspector_Experience to numeric, handling strings like 'ten', 'five', and 'NaN' appropriately", "Split data into training and testing sets with an 80/20 ratio", "Standardize numeric features: Year_Built, Number_of_Floors, Inspector_Experience, Last_Inspection_Score", "Train a RandomForestClassifier model to predict Inspection_Result", "Perform grid search over number of trees (n_estimators) and max_depth parameters", "Evaluate model performance using accuracy, F1 score, precision, and recall on the test set", "Generate and analyze the confusion matrix to understand misclassifications", "Identify top 3 most important features contributing to the model's predictions"], "model_results": {"accuracy": 0.86, "f1": 0.85, "precision": 0.88, "recall": 0.83, "confusion_matrix": {"Pass": {"Pass": 9, "Fail": 1}, "Fail": {"Pass": 2, "Fail": 2}}, "top_feature_importances": {"Last_Inspection_Score": 0.35, "Has_Fire_Alarm_yes": 0.25, "Inspector_Experience": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 6}}}
{"purpose": "Build a classification model to predict whether a delivery will be delayed based on shipment and route features.", "raw_table": "ShipmentID,Distance_km,Vehicle_Type,Departure_Time,Weather,Package_Weight_kg,Delay\nS001,120,Truck,morning,sunny,15,No\nS002,300,van,Afternoon,rainy,7,Yes\nS003,50,Truck,morning,Sunny,20,No\nS004,500,Van,night,cloudy,25,Yes\nS005,150,truck,Morning,rainy,12,No\nS006,90,Van,afternoon,sunny,,No\nS007,400,Truck,Night,Cloudy,30,Yes\nS008,200,van,morning,sunny,18,No\nS009,350,Truck,afternoon,rainy,22,Yes\nS010,80,Van,Morning,Cloudy,10,No\nS011,60,truck,morning,rainy,8,No\nS012,420,Van,Night,sunny,27,Yes\nS013,110,Truck,Afternoon,cloudy,16,No\nS014,250,van,Morning,RAINY,14,Yes", "model_steps": ["Load the CSV data into a dataframe", "Fix inconsistent capitalization in categorical columns: Vehicle_Type, Departure_Time, Weather", "Impute missing value in Package_Weight_kg with median weight", "Convert categorical variables to one-hot encoded features", "Split data into train and test sets with 80% train and 20% test", "Standardize numeric features Distance_km and Package_Weight_kg", "Train a RandomForestClassifier to predict Delay", "Perform grid search to tune max_depth parameter between 3 and 10", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix for test predictions"], "model_results": {"accuracy": 0.85, "precision": 0.88, "recall": 0.8, "f1": 0.84, "confusion_matrix": {"true_positive": 8, "true_negative": 7, "false_positive": 2, "false_negative": 2}, "top_feature_importances": {"Distance_km": 0.35, "Departure_Time_afternoon": 0.18, "Weather_rainy": 0.15, "Package_Weight_kg": 0.12, "Vehicle_Type_Truck": 0.1}, "best_hyperparameters": {"max_depth": 7}}}
{"purpose": "Predict whether a customer will make a repeat purchase within 30 days based on their first order behavior and demographics.", "raw_table": "CustomerID,Age,Gender,FirstOrderCategory,OrderAmount,DaysUntilRepeatPurchase,RepeatPurchase\n001,25,Male,Electronics,199.99,15,Yes\n002,34,Female,Home,89.5,NA,No\n003,47,Female,APPAREL,59.99,40,No\n004,29,male,Books,15.0,5,Yes\n005,,Female,Electronics,300,20,Yes\n006,38,Male,Toys,45.0,NA,No\n007,22,Female,Home,75.5,10,Yes\n008,41,Male,APPAREL,NA,35,No\n009,36,Female,GARDEN,120.0,NA,No\n010,27,Male,Books,25.0,8,Yes\n011,30,female,Toys,50,NA,No\n012,28,Male,Electronics,220.5,12,Yes\n013,45,Female,Home,95,NA,No\n014,39,Male,Books,18.5,NA,No", "model_steps": ["Load data and identify missing values and inconsistent capitalization in categorical columns", "Impute missing Age values using median age", "Standardize capitalization in categorical columns (e.g., FirstOrderCategory and Gender)", "Encode categorical variables using one-hot encoding", "Impute missing OrderAmount with median order amount", "Split data into training and test sets (80/20 split, stratified on RepeatPurchase)", "Train a RandomForestClassifier with default hyperparameters", "Perform grid search over number of trees (n_estimators: 50, 100, 200)", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Extract and analyze feature importances from the best model"], "model_results": {"accuracy": 0.86, "precision": 0.82, "recall": 0.79, "f1": 0.805, "best_hyperparameters": {"n_estimators": 100}, "top_feature_importances": {"OrderAmount": 0.32, "DaysUntilRepeatPurchase": 0.28, "FirstOrderCategory_Electronics": 0.12, "Age": 0.1, "Gender_Female": 0.08}}}
{"purpose": "Predict whether a citizen will repay a government loan on time based on demographic and loan characteristics.", "raw_table": "Citizen_ID,Age,Employment_Status,Annual_Income,Loan_Amount,Loan_Purpose,Previous_Default,Repayment_Status\n1,34,Employed,55000,15000,Home_Improvement,No,Yes\n2,45,Unemployed,32000,5000,Education,No,No\n3,29,Employed,62000,20000,Health,Yes,No\n4,52,Self-employed,,12000,Business,No,Yes\n5,41,employed,48000,9000,Home_Improvement,No,yes\n6,37,Unemployed,40000,7000,Education,No,No\n7,,Self-employed,72000,25000,Business,Yes,No\n8,30,Employed,58000,15000,Health,No,Yes\n9,47,Employed,50000,13000,home_improvement,No,Yes\n10,33,Unemployed,35000,6000,Education,No,No\n11,39,Employed,60000,18000,Business,No,Yes\n12,44,Self-employed,67000,22000,Health,Yes,No\n13,28,employed,55000,11000,Home_Improvement,,Yes\n14,31,Employed,59000,14000,Education,No,Yes", "model_steps": ["Load data and identify target variable 'Repayment_Status' as binary classification (Yes/No).", "Standardize inconsistent capitalization in categorical columns such as 'Employment_Status' and 'Loan_Purpose'.", "Handle missing numeric data: impute missing 'Age' and 'Annual_Income' values using median values.", "Encode categorical variables 'Employment_Status', 'Loan_Purpose', and 'Previous_Default' using one-hot encoding.", "Split data into train and test sets with 80% for training and 20% for testing.", "Standardize numeric features 'Age', 'Annual_Income', and 'Loan_Amount' using z-score normalization.", "Train a RandomForestClassifier on the training set with 100 trees and default parameters.", "Perform grid search on max_depth [5, 10, 15] using 5-fold cross-validation to optimize model performance.", "Evaluate model on test data calculating accuracy, precision, recall, and F1 score.", "Compute and analyze confusion matrix to understand false positives and false negatives.", "Extract and rank top 5 feature importances from the fitted model."], "model_results": {"accuracy": 0.79, "f1": 0.81, "precision": 0.76, "recall": 0.87, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Previous_Default_Yes": 0.28, "Loan_Amount": 0.22, "Annual_Income": 0.18, "Employment_Status_Employed": 0.12, "Loan_Purpose_Home_Improvement": 0.1}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a customer will make a repeat purchase within 30 days based on their first purchase details and demographics.", "raw_table": "CustomerID,Age,Gender,FirstPurchaseAmount,ProductCategory,SignupSource,RepeatPurchase\n1001,34,Male,59.99,Electronics,Online,Yes\n1002,28,Female,120.5,Fashion,Offline,No\n1003,45,female,NaN,Home,online,Yes\n1004,39,M,75.0,Electronics,Online,No\n1005,22,Female,40.75,Fashion,Referral,yes\n1006,31,male,89.99,Home,Offline,No\n1007,27,F,130.00,Electronics,Offline,No\n1008,,Male,55.25,Fashion,Online,Yes\n1009,35,Female,NaN,home,Referral,No\n1010,41,Male,100.0,Electronics,Online,Yes\n1011,29,Female,67.5,Fashion,offline,No\n1012,33,Female,80.0,Home,Referral,Yes\n1013,26,Male,45.0,Fashion,Online,No", "model_steps": ["Load the dataset and inspect for missing values and inconsistent formatting", "Impute missing numeric values (Age, FirstPurchaseAmount) with median values", "Normalize inconsistent capitalization in categorical columns (Gender, ProductCategory, SignupSource, RepeatPurchase)", "Encode 'RepeatPurchase' target variable as binary (Yes=1, No=0)", "One-hot encode categorical features: Gender, ProductCategory, SignupSource", "Split the data into training (80%) and test (20%) sets with stratification on the target", "Standardize numeric features: Age and FirstPurchaseAmount", "Train a Logistic Regression classifier on the training data", "Perform a grid search over regularization parameter C to optimize model performance", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Identify and rank top 3 most important features based on model coefficients"], "model_results": {"accuracy": 0.83, "precision": 0.79, "recall": 0.85, "f1": 0.82, "confusion_matrix": {"true_positive": 18, "true_negative": 21, "false_positive": 5, "false_negative": 3}, "top_feature_importances": {"FirstPurchaseAmount": 0.45, "SignupSource_Online": 0.3, "Age": 0.25}, "best_hyperparameters": {"C": 0.1}}}
{"purpose": "Predict the likelihood of a customer defaulting on a credit card payment next month.", "raw_table": "CustomerID,Age,Income,EmploymentStatus,CreditScore,MonthlySpend,HasDefaulted\n1,34,55000,Employed,720,1200,No\n2,45,NaN,Self-employed,680,NaN,Yes\n3,29,48000,employed,690,850,No\n4,53,72000,Unemployed,NaN,1500,Yes\n5,40,61000,Employed,710,1100,No\n6,NaN,58000,Self-employed,650,900,Yes\n7,31,50000,Employed,680,950,No\n8,47,67000,Employed,730,1300,No\n9,38,60000,Unemployed,670,1000,Yes\n10,26,45000,Employed,NaN,800,No\n11,50,72000,self-Employed,700,1400,Yes\n12,44,NaN,Employed,690,1050,No\n13,37,58000,unemployed,660,980,Yes\n14,30,52000,Self-Employed,675,870,No\n15,55,75000,Employed,740,1600,Yes", "model_steps": ["Load the raw data and inspect for missing and inconsistent values", "Standardize capitalization in categorical column 'EmploymentStatus' and correct inconsistencies", "Impute missing numeric values (Age, Income, CreditScore, MonthlySpend) using median imputation", "Convert target variable 'HasDefaulted' to binary (Yes=1, No=0)", "One-hot encode the 'EmploymentStatus' categorical variable", "Split the dataset into training (80%) and testing (20%) sets", "Standardize numeric features: Age, Income, CreditScore, MonthlySpend", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search over max_depth=[3,5,7] and min_samples_split=[2,5]", "Evaluate the best model on the test set by computing accuracy, precision, recall, and F1 score", "Generate a confusion matrix and extract feature importances"], "model_results": {"accuracy": 0.8, "precision": 0.78, "recall": 0.75, "f1": 0.76, "confusion_matrix": [[5, 2], [1, 7]], "top_feature_importances": {"CreditScore": 0.32, "MonthlySpend": 0.25, "Income": 0.15, "EmploymentStatus_Self-employed": 0.1, "Age": 0.08, "EmploymentStatus_Unemployed": 0.07, "EmploymentStatus_Employed": 0.03}, "best_hyperparameters": {"max_depth": 5, "min_samples_split": 2, "n_estimators": 100}}}
{"purpose": "Predict whether a student will pass the final exam based on their demographic and academic features.", "raw_table": "StudentID,Age,Gender,Hours_Studied_per_Week,Attendance_Percentage,Previous_Grade_Level,Parental_Education,Passed_Final\n1,17,Male,15,92,10,College,Yes\n2,16,Female,8,85,9,High School,No\n3,17,Female,12,NaN,10,College,Yes\n4,16,male,7,78,9,high school,No\n5,18,Female,20,95,11,College,Yes\n6,17,Female,10,88,10,College,Yes\n7,16,Male,5,70,9,No formal education,No\n8,17,Female,nine,90,10,College,Yes\n9,16,Male,6,82,9,High School,No\n10,18,Female,18,96,11,College,Yes\n11,17,Male,14,89,10,College,Yes\n12,,Female,11,85,10,College,Yes\n13,16,Female,7,75,9,High school,No\n14,17,Male,13,88,10,College,Yes", "model_steps": ["Load the CSV data into a dataframe.", "Clean data: fix inconsistent capitalization in 'Gender' and 'Parental_Education' columns.", "Impute missing values in 'Age' with the median age and 'Hours_Studied_per_Week' by converting textual 'nine' to numeric 9.", "Encode target variable 'Passed_Final' as binary (Yes=1, No=0).", "One-hot encode categorical variables 'Gender' and 'Parental_Education'.", "Standardize numeric features: 'Age', 'Hours_Studied_per_Week', and 'Attendance_Percentage'.", "Split data into training and testing sets with 80/20 ratio.", "Train a RandomForestClassifier on the training set.", "Perform grid search cross-validation to tune max_depth among [3, 5, 7].", "Evaluate the model on the test set using accuracy, F1 score, precision, and recall.", "Generate and analyze the confusion matrix.", "Identify and report top 3 feature importances from the trained model."], "model_results": {"accuracy": 0.86, "f1": 0.88, "precision": 0.9, "recall": 0.86, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"Hours_Studied_per_Week": 0.38, "Attendance_Percentage": 0.3, "Parental_Education_College": 0.15}, "best_hyperparameter": {"max_depth": 5}}}
{"purpose": "Build a classification model to predict if a taxi ride will experience heavy traffic delay.", "raw_table": "ride_id,ride_distance_km,pickup_zone,driver_rating,ride_hour,weather_condition,traffic_delay\n1,5.2,Downtown,4.8,8,Clear,No\n2,3.5,Uptown,3.9,18,Rain,Yes\n3,7.8,Suburb,4.5,7,Fog,No\n4,2.1,downtown,4.0,22,Clear,Yes\n5,,Midtown,4.3,16,Clear,No\n6,4.7,Uptown,NaN,9,Rain,Yes\n7,6.0,Suburb,4.7,12,Clear,No\n8,8.2,MIDTOWN,3.5,19,Fog,Yes\n9,3.3,Downtown,4.2,14,Clear,No\n10,5.5,Uptown,4.1,20,Rain,Yes\n11,4.0,Suburb,4.0,6,Clear,No\n12,7.1,Midtown,4.6,17,Fog,Yes\n13,5.8,Downtown,3.8,21,Clear,Yes\n14,3.9,uptown,4.9,8,Rain,No\n15,6.4,Suburb,4.4,13,Clear,No", "model_steps": ["Correct inconsistent capitalization in 'pickup_zone' and 'weather_condition' columns", "Impute missing values in 'ride_distance_km' and 'driver_rating' with column medians", "Convert 'traffic_delay' target variable to binary labels (Yes=1, No=0)", "One-hot encode 'pickup_zone' and 'weather_condition' categorical features", "Scale numeric features 'ride_distance_km', 'driver_rating', and 'ride_hour' using StandardScaler", "Split data into train (80%) and test (20%) sets using stratified sampling on the target", "Train a GradientBoostingClassifier on the training set", "Perform grid search over learning_rate and n_estimators with 5-fold cross-validation", "Evaluate the model on the test set calculating accuracy, F1 score, precision, and recall", "Generate and analyze the confusion matrix", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.87, "f1": 0.85, "precision": 0.83, "recall": 0.87, "confusion_matrix": {"true_positive": 6, "true_negative": 7, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"ride_hour": 0.32, "weather_condition_Rain": 0.25, "pickup_zone_Downtown": 0.18}, "best_hyperparameters": {"learning_rate": 0.1, "n_estimators": 100}}}
{"purpose": "Predict whether a customer will make a purchase during a website session.", "raw_table": "session_id,session_length_minutes,device_type,traffic_source,pages_visited,referral_code,previous_purchases,target_purchase\n1,12.5,Mobile,organic,5,,3,Yes\n2,7.3,Desktop,Paid,3,ABCD123,0,No\n3,NaN,mobile,Organic,7,,2,Yes\n4,15.2,Tablet,Paid,8,XYZ789,1,Yes\n5,3.1,Desktop,Organic,2,,0,No\n6,9.8,Mobile,Paid,5,abcd123,1,Yes\n7,11.0,Desktop,Direct,6,,5,Yes\n8,6.5,Tablet,organic,4,XYZ789,0,No\n9,8.7,Mobile,Paid,3,,1,No\n10,NaN,desktop,Direct,7,,2,Yes\n11,4.3,Tablet,Paid,3,,0,No\n12,13.1,Mobile,Direct,6,ABCD123,4,Yes\n13,10.0,Desktop,Paid,5,,3,Yes\n14,5.5,Mobile,Organic,3,,1,No", "model_steps": ["Load the dataset and handle missing values in session_length_minutes by imputing the median.", "Normalize inconsistent capitalization in categorical columns: device_type and traffic_source.", "Convert target_purchase to binary labels (Yes=1, No=0).", "One-hot encode categorical variables: device_type, traffic_source, and referral_code.", "Split data into training and test sets with an 80/20 ratio.", "Standardize numeric features: session_length_minutes, pages_visited, previous_purchases.", "Train a RandomForestClassifier with 100 trees on the training data.", "Perform grid search to tune max_depth parameter over [5, 10, None].", "Evaluate the best model on the test set computing accuracy, precision, recall, and F1 score.", "Generate and analyze the confusion matrix.", "Identify top 3 important features influencing purchase prediction."], "model_results": {"accuracy": 0.86, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 12, "true_negative": 8, "false_positive": 3, "false_negative": 2}, "top_feature_importances": {"session_length_minutes": 0.31, "previous_purchases": 0.25, "traffic_source_Paid": 0.18}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict if a manufactured part will fail quality inspection based on sensor readings and production conditions.", "raw_table": "Part_ID,Machine_ID,Operator_Shift,Temperature,Pressure,Material_Batch,Defect_Flag\nP001,M01,Morning,75.2,30.1,BatchA,0\nP002,m01,Evening,78.5,29.8,batCha,1\nP003,M02,NIGHT,74.0,31.0,BatchB,0\nP004,M03,Morning,?,30.5,BatchA,0\nP005,M02,Morning,76.1,missing,BatchB,1\nP006,M01,Evening,77.3,30.7,BatchC,0\nP007,M03,Night,75.0,30.0,BatchA,0\nP008,M02,Evening,76.8,29.9,BatchB,1\nP009,m01,Morning,75.5,30.3,Batcha,0\nP010,M03,Night,77.0,31.2,BatchC,1\nP011,M02,Morning,74.5,30.4,BatchB,0\nP012,M01,Evening,missing,30.6,BatchC,1\nP013,M03,Night,75.8,?,BatchA,0\nP014,M02,Morning,76.0,30.2,BatchB,0\nP015,M01,Evening,77.1,30.8,batchc,1", "model_steps": ["Load CSV data and identify columns with missing or inconsistent values", "Normalize categorical columns by fixing inconsistent capitalization (e.g., Machine_ID, Operator_Shift, Material_Batch)", "Impute missing numeric values (Temperature and Pressure) with median values", "One-hot encode categorical variables: Machine_ID, Operator_Shift, Material_Batch", "Split data into training (80%) and test (20%) sets, stratified by Defect_Flag", "Standardize numeric features: Temperature and Pressure", "Train a RandomForestClassifier to predict Defect_Flag", "Perform grid search on number of trees (n_estimators) and max_depth", "Evaluate model using accuracy, precision, recall, and F1 score on test set", "Compute confusion matrix", "Identify and report top 3 feature importances"], "model_results": {"accuracy": 0.87, "precision": 0.83, "recall": 0.79, "f1": 0.81, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 2, "false_negative": 2}, "top_feature_importances": {"Pressure": 0.28, "Material_Batch_BatchC": 0.22, "Temperature": 0.2}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a household will exceed its monthly energy consumption threshold based on usage patterns and household characteristics.", "raw_table": "Household_ID,Region,Avg_Daily_Usage_kWh,Num_Occupants,Heating_Type,Has_Solar,Monthly_Exceeded\nH001,North,35.2,4,Gas,yes,Yes\nH002,south,28.7,3,Electric,No,No\nH003,East,41.5,5,gas,Yes,Yes\nH004,West,,2,Electric,NO,No\nH005,North,22.1,3,Gas,,No\nH006,South,30.3,4,Electric,Yes,No\nH007,East,38.7,NaN,Gas,yes,Yes\nH008,West,27.9,3,Electric,No,No\nH009,North,33.0,4,gas,No,Yes\nH010,south,40.2,5,Electric,Yes,Yes\nH011,East,36.8,3,Gas,Yes,Yes\nH012,West,25.4,2,electric,No,No\nH013,North,29.6,4,Gas,Yes,No", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Impute missing numeric values (e.g., Avg_Daily_Usage_kWh, Num_Occupants) using median values", "Normalize capitalization in categorical columns such as Region, Heating_Type, Has_Solar", "Encode categorical variables with one-hot encoding (Region, Heating_Type, Has_Solar)", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features (Avg_Daily_Usage_kWh, Num_Occupants)", "Train a RandomForestClassifier to predict Monthly_Exceeded", "Perform grid search over n_estimators (50, 100) and max_depth (5, 10)", "Evaluate model performance using accuracy, precision, recall, and F1 score on the test set", "Generate the confusion matrix to visualize prediction errors"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Avg_Daily_Usage_kWh": 0.42, "Num_Occupants": 0.18, "Heating_Type_Gas": 0.15, "Has_Solar_Yes": 0.12, "Region_North": 0.07}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features and location.", "raw_table": "HouseID,Size_sqft,Bedrooms,Bathrooms,Neighborhood,Year_Built,Garage,Condition,SalePrice\n1,1500,3,2,GreenVille,1999,Yes,Good,320000\n2,2500,4,3,Downtown,2010,No,Excellent,550000\n3,1800,3,2,Suburbia,2005,,Fair,350000\n4,1300,2,1,greenVille,1990,yes,Poor,280000\n5,2200,4,3,Downtown,2015,Yes,Excellent,600000\n6,1600,3,2,Suburbia,2000,No,Good,340000\n7,1700,3,2,Downtown,NA,Yes,Fair,400000\n8,1400,2,1,Suburbia,1985,no,Poor,260000\n9,2100,4,3,GreenVille,2018,YES,Excellent,620000\n10,1900,3,2,Downtown,2008,Yes,Good,480000\n11,1550,3,2,Suburbia,1995,No,Good,330000\n12,2300,4,3,Downtown,2012,Yes,Excellent,580000\n13,1750,3,2,Suburbia,2003,Yes,Fair,370000\n14,1650,3,2,GreenVille,1997,No,Good,310000", "model_steps": ["Load the raw CSV dataset into a DataFrame", "Standardize capitalization and correct inconsistent values in the Neighborhood and Garage columns", "Impute missing Year_Built values with the median year", "Convert categorical variables (Neighborhood, Garage, Condition) into one-hot encoded features", "Split the data into train (80%) and test (20%) sets", "Standardize numeric features such as Size_sqft, Year_Built, Bedrooms, and Bathrooms", "Train a RandomForestRegressor on the training set to predict SalePrice", "Tune the max_depth hyperparameter with 5-fold cross-validation", "Evaluate the model on the test set calculating RMSE, MAE, and R\u00b2", "Extract and report the top 5 feature importances from the trained model"], "model_results": {"rmse": 28000, "mae": 21000, "r2": 0.87, "top_feature_importances": {"Size_sqft": 0.42, "Neighborhood_Downtown": 0.18, "Year_Built": 0.15, "Condition_Excellent": 0.1, "Bedrooms": 0.07}, "best_hyperparameters": {"max_depth": 12}}}
{"purpose": "Predict whether a movie will be a box office hit based on its features before release.", "raw_table": "MovieID,Genre,Director,Runtime,BudgetMillions,LeadActorPopularity,ReleaseMonth,IsHit\n1,Action,Smith,120,150,85,July,Yes\n2,Comedy,Johnson,90,40,70,December,No\n3,Drama,Williams,110,30,90,March,No\n4,Action,smith,130,200,95,August,Yes\n5,Comedy,Lee,95,25,60,June,No\n6,Drama,Johnson,105,15,NaN,November,No\n7,Action,Lee,115,180,88,July,Yes\n8,Romance,Williams,100,10,75,February,No\n9,Comedy,Johnson,NaN,50,65,December,No\n10,Action,Lee,125,220,80,July,Yes\n11,Drama,Smith,108,35,92,April,No\n12,Romance,lee,98,12,70,may,No\n13,Action,Smith,121,175,89,July,Yes\n14,Comedy,Lee,91,28,68,December,No", "model_steps": ["Identify 'IsHit' as the binary target variable and convert Yes/No to 1/0", "Clean 'Director' column by standardizing capitalization (e.g., 'smith' and 'Smith' to 'Smith')", "Impute missing numeric values in 'Runtime' and 'LeadActorPopularity' columns using median values", "One-hot encode categorical variables 'Genre', 'Director', and 'ReleaseMonth'", "Split the dataset into training (80%) and testing (20%) sets", "Standardize numeric features: 'Runtime', 'BudgetMillions', and 'LeadActorPopularity'", "Train a RandomForestClassifier with 100 trees on the training data", "Tune 'max_depth' hyperparameter using grid search with cross-validation", "Evaluate the model on the test set calculating accuracy, precision, recall, and F1 score", "Generate a confusion matrix and extract feature importances", "Save the trained model and preprocessing pipeline for future inference"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.81, "recall": 0.88, "confusion_matrix": {"true_positive": 6, "false_positive": 2, "true_negative": 8, "false_negative": 1}, "top_feature_importances": {"BudgetMillions": 0.32, "LeadActorPopularity": 0.25, "Genre_Action": 0.15, "ReleaseMonth_July": 0.12, "Director_Smith": 0.1}, "best_hyperparameters": {"max_depth": 7, "n_estimators": 100}}}
{"purpose": "Predict hourly energy consumption category (Low, Medium, High) based on environmental and operational features.", "raw_table": "Hour,Temperature_C,Humidity_Percent,Day_Type,Equipment_Status,Energy_Consumption_Category\n0,22.5,55,weekday,ON,Medium\n1,21.7,58,weekday,ON,Low\n2,20.1,60,weekday,OFF,Low\n3,19.5,63,weekday,OFF,Low\n4,19.2,65,weekend,OFF,Low\n5,19,67,WeekEnd,ON,Medium\n6,20.3,64,weekend,on,Medium\n7,23.1,59,weekday,ON,High\n8,27.0,50,Weekday,ON,High\n9,29.5,47,weekday,ON,High\n10,31.2,45,weekday,ON,High\n11,32.0,44,weekday,ON,High\n12,31.5,46,weekday,ON,High\n13,30.0,48,weekday,ON,High\n14,28.5,49,weekday,OFF,Medium", "model_steps": ["Load the CSV data into a DataFrame", "Normalize inconsistent capitalization in 'Day_Type' and 'Equipment_Status' columns", "Impute missing or ambiguous values if any (none explicitly present but check)", "Convert categorical columns 'Day_Type' and 'Equipment_Status' using one-hot encoding", "Split data into train and test sets with an 80/20 ratio stratified on the target 'Energy_Consumption_Category'", "Standardize numeric features: Hour, Temperature_C, Humidity_Percent", "Train a RandomForestClassifier to predict 'Energy_Consumption_Category'", "Perform grid search to tune 'n_estimators' and 'max_depth'", "Evaluate model on test set using accuracy, F1-score, precision, and recall", "Generate and analyze confusion matrix to identify class-wise performance", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.87, "f1": 0.85, "precision": 0.86, "recall": 0.84, "confusion_matrix": {"Low": {"Low": 3, "Medium": 0, "High": 0}, "Medium": {"Low": 1, "Medium": 4, "High": 1}, "High": {"Low": 0, "Medium": 1, "High": 5}}, "top_feature_importances": {"Temperature_C": 0.42, "Hour": 0.3, "Equipment_Status_ON": 0.18}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a customer will make a repeat purchase within 30 days based on their recent order details.", "raw_table": "CustomerID,OrderAmount,ProductCategory,OrderDayOfWeek,CustomerSegment,RepeatPurchase\n1001,120.50,Electronics,Monday,Gold,Yes\n1002,45.00,Clothing,Tuesday,Silver,No\n1003,78.99,Home & Kitchen,friday,Gold,Yes\n1004,,Electronics,Wednesday,BRONZE,No\n1005,32.10,clothing,Saturday,Silver,No\n1006,200.00,Electronics,Sunday,Gold,Yes\n1007,15.50,Books,Monday,,No\n1008,89.99,Home & Kitchen,Thursday,Silver,Yes\n1009,55.75,Books,Tuesday,Bronze,No\n1010,NaN,Clothing,friday,Silver,No\n1011,150.00,Electronics,Monday,Gold,Yes\n1012,75.25,Home & Kitchen,Wednesday,Silver,No\n1013,20.00,Books,thursday,Bronze,No\n1014,95.00,Electronics,Saturday,Gold,Yes", "model_steps": ["Load raw data and inspect for missing or inconsistent values", "Standardize categorical values by correcting capitalization in ProductCategory, OrderDayOfWeek, and CustomerSegment", "Impute missing OrderAmount values with the median OrderAmount", "Fill missing CustomerSegment values with the mode segment", "Encode categorical variables (ProductCategory, OrderDayOfWeek, CustomerSegment) using one-hot encoding", "Convert target variable RepeatPurchase into binary format (Yes=1, No=0)", "Split data into training (80%) and test (20%) sets randomly", "Standardize numeric feature OrderAmount using z-score normalization", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search on max_depth parameter with values [5, 10, 15]", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 important features from the model"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.8, "f1": 0.815, "confusion_matrix": {"true_positive": 5, "true_negative": 6, "false_positive": 1, "false_negative": 2}, "top_feature_importances": {"CustomerSegment_Gold": 0.28, "OrderAmount": 0.24, "ProductCategory_Electronics": 0.18}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a loan application will be approved based on applicant financial and demographic data.", "raw_table": "Applicant_ID,Age,Income,Employment_Status,Credit_Score,Loan_Amount,Loan_Purpose,Approved\n1,34,65000,Full-time,720,15000,Car,Yes\n2,28,54000,Part-Time,680,12000,Education,no\n3,45,82000,Self-employed,710,18000,Home,YES\n4,52,,Unemployed,690,20000,car,No\n5,37,72000,Full-Time,NaN,16000,Education,Yes\n6,29,58000,Full-time,660,13000,car,No\n7,41,60000,Part-time,700,14000,Home,Yes\n8,50,78000,Full-time,730,21000,Home,YES\n9,31,NaN,Full-time,675,15000,education,No\n10,27,62000,Self-Employed,690,12000,Car,Yes\n11,48,84000,Full-time,710,19000,Education,No\n12,35,70000,Part-Time,NaN,15000,Home,YES\n13,39,68000,full-time,700,16000,Car,Yes\n14,44,75000,Full-Time,720,17000,Education,No\n15,33,67000,Unemployed,700,14000,Home,YES", "model_steps": ["Load the raw CSV data into a DataFrame", "Standardize capitalization in categorical columns (e.g., Employment_Status, Loan_Purpose, Approved) and fix inconsistent entries", "Impute missing numeric values (Income and Credit_Score) using median imputation", "Encode the target variable 'Approved' as binary (Yes=1, No=0)", "One-hot encode categorical features: Employment_Status and Loan_Purpose", "Split the dataset into training (80%) and testing (20%) sets with stratification on the target", "Standardize numeric features: Age, Income, Credit_Score, Loan_Amount", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search over max_depth parameter [5, 10, 15] using 5-fold cross-validation", "Evaluate the trained model on the test set and compute accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Extract and report top 5 feature importances from the trained model"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.88, "f1": 0.86, "confusion_matrix": [[12, 3], [2, 18]], "top_feature_importances": {"Credit_Score": 0.27, "Income": 0.22, "Loan_Amount": 0.15, "Employment_Status_Full-time": 0.12, "Loan_Purpose_Home": 0.1}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a student will pass or fail a final exam based on demographic and study-related features.", "raw_table": "Student_ID,Age,Gender,Study_Hours_per_Week,Previous_Grade,Attendance_Rate,Parental_Education_Level,Final_Exam_Result\n1,17,Male,15,85,0.95,Bachelor,Pass\n2,16,Female,8,78,0.88,Master,Pass\n3,18,Male,5,62,0.75,high school,Fail\n4,17,female,12,80,NaN,Bachelor,Pass\n5,16,M,7,70,0.65,Bachelor,Fail\n6,17,Female,NaN,55,0.70,Master,Fail\n7,18,Male,20,90,0.98,Bachelor,Pass\n8,16,Female,10,68,0.80,Master,Fail\n9,17,Male,13,82,0.90,Bachelor,Pass\n10,16,female,9,75,0.85,High School,Fail\n11,17,Male,11,79,0.93,Master,Pass\n12,18,Female,4,60,0.60,bachelor,Fail\n13,16,Male,14,88,0.97,Master,Pass\n14,17,Female,NaN,65,0.72,Bachelor,Fail", "model_steps": ["Load the raw CSV data into a DataFrame", "Identify and handle missing values in 'Study_Hours_per_Week' and 'Attendance_Rate' by imputing with the median", "Standardize inconsistent capitalization in 'Gender' and 'Parental_Education_Level' columns", "Encode 'Gender' and 'Parental_Education_Level' using one-hot encoding", "Split the data into training (80%) and test (20%) sets, stratified by the 'Final_Exam_Result'", "Standardize numeric features: 'Age', 'Study_Hours_per_Week', 'Previous_Grade', and 'Attendance_Rate'", "Train a RandomForestClassifier to predict 'Final_Exam_Result'", "Perform grid search cross-validation over number of trees (n_estimators) and max_depth hyperparameters", "Evaluate the final model on the test set by computing accuracy, F1 score, precision, and recall", "Generate the confusion matrix for the test predictions", "Identify top 3 feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.81, "recall": 0.85, "confusion_matrix": [[8, 2], [3, 11]], "top_feature_importances": {"Study_Hours_per_Week": 0.32, "Previous_Grade": 0.27, "Attendance_Rate": 0.18}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 6}}}
{"purpose": "Build a classification model to predict whether a farm field will have high pest infestation based on environmental and crop features.", "raw_table": "Field_ID,Crop_Type,Soil_pH,Rainfall_mm,Avg_Temperature_C,Pesticide_Used,Previous_Infestation,High_Pest_Infestation\n1,Corn,6.5,120,22,Yes,No,Yes\n2,wheat,7.0,85,19,No,Yes,No\n3,Soybean,5.8,200,25,yes,No,Yes\n4,Corn,,150,21,No,No,No\n5,Rice,6.2,250,28,Yes,Yes,Yes\n6,Corn,6.8,90,20,No,N/A,No\n7,Soybean,6.0,180,23,No,Yes,Yes\n8,Wheat,7.1,70,18,Yes,No,No\n9,rice,6.3,230,27,yes,No,Yes\n10,Corn,6.4,110,22,N,Yes,No\n11,Soybean,5.9,190,24,Yes,No,Yes\n12,Wheat,7.2,80,20,No,No,No\n13,Rice,6.1,240,28,Yes,Yes,Yes\n14,Corn,6.7,100,21,Yes,No,No", "model_steps": ["Load the dataset and handle missing values in Soil_pH by imputing with the median.", "Standardize capitalization and convert categorical columns Crop_Type and Pesticide_Used to consistent formats.", "Encode categorical variables Crop_Type, Pesticide_Used, and Previous_Infestation using one-hot encoding.", "Convert target variable High_Pest_Infestation to binary labels (Yes=1, No=0).", "Split the data into train and test sets with an 80/20 ratio.", "Standardize numeric features Soil_pH, Rainfall_mm, and Avg_Temperature_C using z-score scaling.", "Train a RandomForestClassifier with 100 trees on the training data.", "Perform grid search to tune max_depth parameter over values [5, 10, 15].", "Evaluate the best model on the test set using accuracy, precision, recall, and F1 score.", "Generate and inspect the confusion matrix for test predictions.", "Identify and rank top 3 feature importances from the trained model."], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.89, "f1": 0.86, "confusion_matrix": [[7, 2], [1, 8]], "top_feature_importances": {"Previous_Infestation_Yes": 0.32, "Pesticide_Used_Yes": 0.25, "Rainfall_mm": 0.18}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Build a classification model to predict whether a given day will have extreme heat conditions based on weather and environmental features.", "raw_table": "date,temp_c,humidity_pct,wind_speed_kmh,region,weather_condition,is_extreme_heat\n2024-06-01,35,45,12,North,Sunny,Yes\n2024-06-02,32,55,15,South,cloudy,No\n2024-06-03,NA,60,8,East,Sunny,No\n2024-06-04,40,50,20,North,Rainy,Yes\n2024-06-05,38,NaN,18,South,Sunny,Yes\n2024-06-06,30,65,5,West,Cloudy,No\n2024-06-07,33,58,12,East,Rainy,No\n2024-06-08,36,53,14,west,sunny,Yes\n2024-06-09,29,62,10,North,Sunny,No\n2024-06-10,42,48,25,South,rainy,Yes\n2024-06-11,31,59,7,East,cloudy,No\n2024-06-12,37,52,16,West,Sunny,Yes\n2024-06-13,34,57,13,North,Sunny,No\n2024-06-14,39,50,22,South,Cloudy,Yes", "model_steps": ["Load CSV data and parse dates", "Handle missing values by imputing median for numeric columns and mode for categorical columns", "Normalize capitalization in categorical columns 'region' and 'weather_condition' to ensure consistency", "Convert categorical variables 'region' and 'weather_condition' using one-hot encoding", "Split data into train and test sets with 80% for training and 20% for testing", "Standardize numeric features 'temp_c', 'humidity_pct', and 'wind_speed_kmh' using training set statistics", "Train a RandomForestClassifier to predict 'is_extreme_heat'", "Perform grid search over 'n_estimators' (50, 100) and 'max_depth' (5, 10)", "Evaluate model performance on test set computing accuracy, F1 score, precision, and recall", "Extract and report top 3 feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.81, "recall": 0.85, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}, "top_feature_importances": {"temp_c": 0.42, "humidity_pct": 0.25, "weather_condition_Sunny": 0.15}, "confusion_matrix": {"true_positive": 5, "true_negative": 6, "false_positive": 2, "false_negative": 1}}}
{"purpose": "Predict whether a customer will make a repeat purchase within 30 days after their first order.", "raw_table": "CustomerID,Age,Gender,FirstOrderAmount,FirstOrderCategory,DaysToRepeatPurchase,RepeatPurchase\n1,25,Male,59.99,Electronics,15,Yes\n2,42,Female,120.00,Home,NA,No\n3,33,Female,89.5,beauty,20,Yes\n4,28,Male,39.99,Electronics,5,Yes\n5,47,Female,250,Furniture,NA,No\n6,NA,Male,75.0,Home,10,Yes\n7,36,Female,99.99,Beauty,NA,No\n8,29,Male,45.0,electronics,,No\n9,53,Female,200.0,Furniture,30,Yes\n10,40,Male,60,Home,8,Yes\n11,31,female,85.0,Beauty,NA,No\n12,26,Male,55.5,Electronics,3,Yes\n13,38,Female,150.0,Home,NA,No", "model_steps": ["Load CSV data and identify target variable RepeatPurchase", "Handle missing values in Age and DaysToRepeatPurchase by imputing median and mode respectively", "Normalize inconsistent capitalization in categorical columns Gender and FirstOrderCategory", "One-hot encode Gender and FirstOrderCategory", "Split data into training (80%) and test (20%) sets randomly", "Standardize numeric columns Age and FirstOrderAmount", "Train a RandomForestClassifier to predict RepeatPurchase", "Tune hyperparameters max_depth and n_estimators using 5-fold cross-validation", "Evaluate model on test set computing accuracy, precision, recall, and F1 score", "Generate confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.83, "precision": 0.8, "recall": 0.85, "f1": 0.82, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"FirstOrderAmount": 0.34, "DaysToRepeatPurchase": 0.27, "FirstOrderCategory_Electronics": 0.15, "Age": 0.1, "Gender_Female": 0.08}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a bank customer will default on their credit card payment next month.", "raw_table": "CustomerID,Age,Gender,Income,Marital_Status,Credit_Score,Payment_History,Default_Next_Month\n001,45,Male,58000,Married,700,Good,No\n002,38,Female,NaN,Single,650,Fair,No\n003,29,Female,42000,Single,720,good,No\n004,52,Male,80000,Married,680,Poor,Yes\n005,47,Male,62000,Divorced,NaN,Fair,No\n006,35,Female,54000,Single,710,Good,No\n007,58,Male,73000,Married,690,Poor,Yes\n008,41,FEMALE,67000,Married,670,Good,No\n009,33,Male,46000,Single,655,unknown,No\n010,26,Female,40000,Single,690,Fair,No\n011,50,Male,75000,Married,710,Poor,Yes\n012,36,Female,60000,Single,NaN,Good,No\n013,44,Male,58000,Divorced,660,Fair,No\n014,39,Female,NaN,Married,705,Good,No", "model_steps": ["Load CSV data into a DataFrame", "Handle missing numeric values by imputing median values for Income and Credit_Score", "Standardize capitalization and clean Payment_History entries (e.g., 'good' to 'Good', 'unknown' to NaN)", "Encode categorical variables: Gender, Marital_Status, and Payment_History using one-hot encoding", "Split data into train and test sets (80/20) stratified by Default_Next_Month", "Standardize numeric features: Age, Income, Credit_Score", "Train a RandomForestClassifier with default parameters on the training set", "Perform grid search to tune max_depth and n_estimators", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Generate the confusion matrix and plot feature importances"], "model_results": {"accuracy": 0.86, "precision": 0.79, "recall": 0.71, "f1": 0.75, "confusion_matrix": {"true_positive": 5, "true_negative": 7, "false_positive": 2, "false_negative": 2}, "top_feature_importances": {"Credit_Score": 0.35, "Payment_History_Good": 0.22, "Income": 0.15, "Age": 0.12, "Payment_History_Poor": 0.08, "Marital_Status_Single": 0.05, "Gender_Female": 0.03}, "best_hyperparameters": {"max_depth": 4, "n_estimators": 100}}}
{"purpose": "Predict whether a loan application will be approved based on applicant financial and demographic data.", "raw_table": "Applicant_ID,Age,Income,Employment_Status,Credit_Score,Loan_Amount,Loan_Purpose,Approved\n1,34,58000,Full-Time,720,15000,Home,Yes\n2,29,NaN,part-time,680,12000,Car,No\n3,45,67000,Full-time,750,20000,Education,Yes\n4,23,45000,Unemployed,660,8000,Car,No\n5,54,82000,Full-Time,NaN,25000,Home,yes\n6,31,70000,Full-Time,710,18000,home,Yes\n7,38,62000,Self-employed,690,15000,Car,No\n8,27,58000,Part-Time,675,10000,Education,No\n9,41,75000,Full-Time,720,22000,Home,Yes\n10,36,NaN,full-time,730,17000,Car,Yes\n11,30,60000,Full-Time,700,14000,Education,No\n12,50,90000,Full-Time,770,30000,Home,Yes\n13,28,52000,Part-time,NaN,12000,Car,No\n14,33,61000,Full-Time,705,16000,Education,Yes", "model_steps": ["Load data and inspect for missing values and inconsistent capitalization", "Standardize 'Employment_Status' and 'Loan_Purpose' values to consistent casing", "Impute missing numeric values ('Income' and 'Credit_Score') using median values", "Encode categorical variables using one-hot encoding", "Split data into training (80%) and test (20%) sets stratified by target variable 'Approved'", "Standardize numeric features ('Age', 'Income', 'Credit_Score', 'Loan_Amount')", "Train a RandomForestClassifier with default hyperparameters on the training set", "Evaluate model performance using accuracy, F1 score, precision, and recall on the test set", "Extract and report feature importances from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.8, "recall": 0.87, "feature_importances": {"Credit_Score": 0.35, "Income": 0.25, "Loan_Amount": 0.15, "Employment_Status_Full-Time": 0.1, "Age": 0.08, "Loan_Purpose_Home": 0.07}}}
{"purpose": "Predict whether a manufactured part will be defective based on sensor readings and production conditions.", "raw_table": "Part_ID,Temperature,Pressure,Operator_Shift,Material_Batch,Machine_Status,Defective\n001,350,5.1,Morning,BatchA,Running,No\n002,360,5.3,evening,BatchB,Running,Yes\n003,355,NaN,Night,BatchA,Running,No\n004,370,5.7,MORNING,BatchC,Stopped,Yes\n005,365,5.5,Night,BatchB,Running,No\n006,NaN,5.4,Evening,BatchA,Running,No\n007,345,5.0,Morning,BatchC,Running,No\n008,360,5.3,Night,BatchB,Stopped,Yes\n009,355,5.2,Morning,BatchA,Running,No\n010,362,5.6,Evening,BatchC,Running,Yes\n011,358,5.4,Morning,BatchB,Running,No\n012,NaN,5.1,Night,BatchB,Running,Yes\n013,365,5.5,Morning,BatchA,Running,No\n014,370,5.7,Evening,BatchC,Stopped,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Handle missing values by imputing Temperature with median and Pressure with mean", "Normalize inconsistent capitalization in Operator_Shift to standard lowercase values", "Encode categorical variables: Operator_Shift, Material_Batch, Machine_Status using one-hot encoding", "Split data into 80% training and 20% test sets with stratification on Defective target", "Standardize numeric features Temperature and Pressure using training set statistics", "Train a RandomForestClassifier to predict the Defective column", "Perform grid search over number of estimators and max_depth hyperparameters", "Evaluate model on test set calculating accuracy, F1 score, precision, and recall", "Generate confusion matrix to analyze misclassifications"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.81, "recall": 0.88, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Temperature": 0.32, "Machine_Status_Stopped": 0.27, "Pressure": 0.15, "Operator_Shift_evening": 0.12, "Material_Batch_BatchB": 0.09, "Operator_Shift_morning": 0.05}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict house sale prices based on property features and location.", "raw_table": "Property_ID,Neighborhood,Bedrooms,Bathrooms,Size_sqft,Year_Built,Garage,Condition,Sale_Price\n1,Downtown,3,2,1500,2005,Yes,Good,350000\n2,Suburb,4,3,2500,2010,no,Fair,420000\n3,Midtown,2,1,900,1980,Yes,Excellent,280000\n4,Suburb,3,,1800,1995,yes,Poor,310000\n5,DOWNTOWN,4,3,2000,2018,No,good,450000\n6,Midtown,3,2,1300,2000,No,Good,330000\n7,Suburb,5,4,3000,2015,YES,Excellent,520000\n8,Downtown,2,1,850,1975,No,Fair,275000\n9,Midtown,3,2,1400,2003,Yes,,360000\n10,Suburb,4,3,2200,2012,Yes,Good,480000\n11,Midtown,3,2,1600,1998,no,Poor,310000\n12,Downtown,2,1,1000,1985,No,Fair,290000\n13,Suburb,3,2,1700,2007,yes,Good,375000\n14,Midtown,4,3,2100,2011,Yes,Good,440000", "model_steps": ["Load the CSV data into a DataFrame", "Standardize capitalization in categorical columns like Neighborhood and Condition", "Fill missing numeric values in Bathrooms with the median", "Fill missing categorical Condition values with the mode", "Convert Garage column to binary indicator (1 for Yes, 0 for No)", "One-hot encode Neighborhood and Condition categorical variables", "Split data into train (80%) and test sets (20%) with random seed 42", "Standardize numeric features: Bedrooms, Bathrooms, Size_sqft, Year_Built", "Train a GradientBoostingRegressor on the training set", "Tune max_depth and learning_rate via grid search with 5-fold cross-validation", "Evaluate final model on the test set computing RMSE, MAE, and R^2", "Extract and report top 5 feature importances"], "model_results": {"rmse": 21000, "mae": 15000, "r2": 0.87, "top_feature_importances": {"Size_sqft": 0.35, "Neighborhood_Suburb": 0.18, "Bedrooms": 0.15, "Condition_Good": 0.12, "Garage": 0.1}, "best_hyperparameters": {"max_depth": 4, "learning_rate": 0.1}}}
{"purpose": "Predict hourly electricity consumption category (Low, Medium, High) based on weather and temporal features.", "raw_table": "Hour,Temperature_C,Humidity_Percent,Wind_Speed_kmh,Day_Type,Electricity_Consumption_Category\n0,15.2,85,12,weekday,Low\n1,14.8,88,8,Weekday,Low\n2,15,missing,10,weekday,Low\n3,16.5,80,9,weekday,Low\n4,17.1,78,13,Weekend,Medium\n5,18.4,75,15,weekend,Medium\n6,20.0,70,20,Weekend,Medium\n7,22.5,65,25,Weekday,High\n8,24.1,60,30,Weekday,High\n9,25.0,58,35,weekend,High\n10,27.2,55,40,Weekday,High\n11,29.0,50,45,Weekend,High\n12,30.5,48,50,Weekend,High\n13,31.0,47,55,Weekday,High", "model_steps": ["Load the data from CSV string into a dataframe", "Clean and standardize the Day_Type categorical column (handle inconsistent capitalization)", "Impute missing values in Humidity_Percent column with median humidity", "Convert Electricity_Consumption_Category target variable into ordinal labels (Low=0, Medium=1, High=2)", "Split data into training (80%) and test (20%) sets stratified by target", "One-hot encode the Day_Type categorical feature", "Standardize numeric features: Hour, Temperature_C, Humidity_Percent, Wind_Speed_kmh", "Train a RandomForestClassifier with 100 trees", "Perform a grid search tuning max_depth parameter over [3, 5, 7]", "Evaluate model using accuracy, F1 score, precision, and recall on test set", "Generate and visualize the confusion matrix", "Extract and report feature importances from the trained model"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.82, "recall": 0.79, "confusion_matrix": {"Low": {"Low": 4, "Medium": 0, "High": 0}, "Medium": {"Low": 1, "Medium": 3, "High": 1}, "High": {"Low": 0, "Medium": 1, "High": 5}}, "top_feature_importances": {"Temperature_C": 0.35, "Hour": 0.25, "Humidity_Percent": 0.15, "Wind_Speed_kmh": 0.1, "Day_Type_weekday": 0.08, "Day_Type_weekend": 0.07}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Build a regression model to estimate house prices based on property features and location.", "raw_table": "PropertyID,Location,Size_sqft,Bedrooms,Bathrooms,Year_Built,Garage,Has_Garden,Price\n101,Downtown,850,2,1,1995,Yes,TRUE,320000\n102,Suburb,1200,3,2,2005,No,false,400000\n103,downtown,NaN,2,1,1988,Yes,True,310000\n104,Suburb,1500,4,3,2010,Yes,FALSE,550000\n105,Suburb,1250,3,2,missing,No,True,430000\n106,Rural,1100,3,2,2000,No,True,280000\n107,Rural,950,2,1,1990,No,false,260000\n108,Downtown,800,1,1,1985,Yes,True,295000\n109,Suburb,1300,3,2,2007,Yes,TRUE,450000\n110,Downtown,850,2,1,1998,Yes,TRUE,330000\n111,Rural,NaN,3,2,2003,No,False,275000\n112,Suburb,1400,3,2,2006,Yes,True,470000\n113,Downtown,900,2,1,1992,Yes,TRUE,300000\n114,Rural,1000,2,1,1995,No,TRUE,265000\n115,Suburb,1350,3,2,2008,Yes,TRUE,460000", "model_steps": ["Load the raw CSV data into a DataFrame", "Identify and handle missing values in 'Size_sqft' and 'Year_Built' by imputing median values", "Normalize inconsistent capitalization in 'Location' and 'Has_Garden' columns", "Convert 'Garage' and 'Has_Garden' from Yes/No/True/False strings to binary numeric values", "One-hot encode the 'Location' categorical variable", "Split the dataset into training (80%) and testing (20%) sets", "Standardize numeric features: Size_sqft, Year_Built, Bedrooms, Bathrooms", "Train a Gradient Boosting Regressor to predict 'Price'", "Tune hyperparameters 'n_estimators' and 'learning_rate' using 5-fold cross-validation", "Evaluate the model on the test set with RMSE, MAE, and R2 metrics", "Identify and report feature importances from the trained model"], "model_results": {"rmse": 24000, "mae": 18000, "r2": 0.87, "hyperparameters": {"n_estimators": 150, "learning_rate": 0.1}, "feature_importances": {"Size_sqft": 0.42, "Bedrooms": 0.18, "Bathrooms": 0.12, "Location_Downtown": 0.1, "Location_Suburb": 0.08, "Year_Built": 0.05, "Garage": 0.03, "Has_Garden": 0.02}}}
{"purpose": "Predict the risk of hospital readmission within 30 days for diabetic patients based on clinical and demographic features.", "raw_table": "PatientID,Age,Gender,HbA1c_level,Blood_Pressure,Previous_Readmissions,Smoking_Status,Diabetes_Type,Readmitted_30days\n1,58,Male,7.8,130,1,Former,Type 2,Yes\n2,45,Female,6.5,120,0,never,type 1,No\n3,67,Male,8.2,140,2,Current,Type 2,Yes\n4,52,FEMALE,7.0,135,,Former,Type 2,No\n5,70,Male,NaN,150,3,Current,Type 2,Yes\n6,38,Female,6.8,115,0,Never,Type 1,No\n7,49,male,7.5,128,1,Current,Type 2,Yes\n8,55,Female,7.1,125,1,Former,Type 2,No\n9,62,Female,8.0,145,2,never,Type 2,Yes\n10,47,Male,6.9,118,0,,Type 1,No\n11,73,Male,7.9,138,4,Current,Type 2,Yes\n12,60,Female,7.2,NaN,1,Former,Type 2,No\n13,50,Male,7.4,132,1,never,Type 2,Yes\n14,44,Female,6.7,122,0,Never,Type 1,No", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Standardize capitalization in categorical columns such as Gender, Smoking_Status, and Diabetes_Type", "Impute missing numeric values (HbA1c_level and Blood_Pressure) using median imputation", "Convert categorical variables into one-hot encoded features", "Split data into training (80%) and test (20%) sets stratified by the target variable Readmitted_30days", "Standardize numeric features (Age, HbA1c_level, Blood_Pressure, Previous_Readmissions) using StandardScaler", "Train a RandomForestClassifier with 100 trees on the training set", "Perform a grid search over max_depth = [5, 10, 15] and min_samples_split = [2, 5]", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix and identify feature importances"], "model_results": {"accuracy": 0.79, "precision": 0.81, "recall": 0.76, "f1": 0.78, "confusion_matrix": {"true_positive": 9, "true_negative": 11, "false_positive": 3, "false_negative": 4}, "top_feature_importances": {"Previous_Readmissions": 0.32, "HbA1c_level": 0.25, "Age": 0.15, "Smoking_Status_Current": 0.1, "Blood_Pressure": 0.08}, "best_hyperparameters": {"max_depth": 10, "min_samples_split": 2}}}
{"purpose": "Build a classification model to predict whether a bus trip will be delayed by more than 10 minutes.", "raw_table": "trip_id,route_number,departure_time,day_of_week,weather_condition,traffic_level,previous_trip_delay,driver_experience,delayed\n1,45,08:15,Monday,Clear,High,No,5 years,Yes\n2,12,14:30,Tuesday,Rain,Medium,yes,3 Years,no\n3,45,07:00,Wednesday,clear,Low,No,2 years,No\n4,23,18:45,Fri,Cloudy,High,Yes,6 years,Yes\n5,12,09:10,Thursday,Fog,Medium,N/A,1 year,No\n6,23,16:00,Monday,Rain,High,No,4 years,Yes\n7,12,12:20,Tuesday,CLEAR,Low,No,unknown,No\n8,45,10:05,Friday,Cloudy,Medium,Yes,3 years,Yes\n9,23,13:50,Thursday,Fog,High,Yes,5 years,Yes\n10,45,17:30,Monday,Rain,Medium,No,4 years,No\n11,12,08:00,Wednesday,Clear,Low,No,3 years,No\n12,23,15:40,Tuesday,Cloudy,Medium,Yes,2 years,Yes\n13,45,11:15,Thursday,Fog,High,Yes,4 years,Yes\n14,12,14:00,Friday,Rain,Medium,No,3 years,No", "model_steps": ["Load raw CSV data into a DataFrame", "Clean and standardize 'weather_condition' values to lowercase (e.g., 'Clear', 'clear', 'CLEAR' -> 'clear')", "Impute missing values in 'previous_trip_delay' and 'driver_experience' columns", "Convert categorical variables ('route_number', 'day_of_week', 'weather_condition', 'traffic_level', 'previous_trip_delay', 'driver_experience') into one-hot encoded features", "Convert 'departure_time' into numeric features: 'hour' and 'minute'", "Split dataset into train (80%) and test (20%) sets with stratification on target 'delayed'", "Standardize numeric features (hour, minute, driver experience years)", "Train a RandomForestClassifier with grid search over max_depth (values: 5, 10, 15)", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Generate and analyze confusion matrix", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.81, "recall": 0.88, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"traffic_level_High": 0.23, "previous_trip_delay_Yes": 0.19, "weather_condition_rain": 0.15}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a loan applicant will default on their loan based on financial and demographic features.", "raw_table": "ApplicantID,Age,Income,EmploymentStatus,CreditScore,LoanAmount,LoanPurpose,Defaulted\n1,34,55000,Full-time,720,15000,Home Improvement,No\n2,45,NaN,Part-Time,680,8000,auto,Yes\n3,29,48000,Full-time,NA,12000,Debt Consolidation,No\n4,52,72000,Self-employed,710,20000,home improvement,No\n5,47,65000,Full-Time,690,18000,Medical,Yes\n6,38,58000,full-time,NA,15000,Debt Consolidation,No\n7,41,,Part-time,650,,Auto,Yes\n8,33,60000,Full-time,700,14000,Debt Consolidation,No\n9,50,67000,Self-Employed,690,21000,Medical,Yes\n10,28,53000,Part-time,680,13000,Home Improvement,No\n11,39,59000,full-time,720,16000,Auto,No\n12,44,62000,Full-time,675,17000,Medical,Yes\n13,31,58000,Part-Time,700,14000,Debt consolidation,No\n14,55,70000,Full-time,710,22000,Home Improvement,Yes", "model_steps": ["Load raw CSV data into a DataFrame", "Clean 'EmploymentStatus' and 'LoanPurpose' categorical columns by normalizing capitalization", "Impute missing numeric values in 'Income', 'CreditScore', and 'LoanAmount' using median values", "Encode categorical variables 'EmploymentStatus' and 'LoanPurpose' using one-hot encoding", "Split data into 70% training and 30% testing sets, stratified by the target variable 'Defaulted'", "Standardize numeric features: 'Age', 'Income', 'CreditScore', and 'LoanAmount'", "Train a RandomForestClassifier on the training data", "Perform a grid search over 'max_depth' and 'n_estimators' hyperparameters using cross-validation", "Select the best model and generate predictions on the test set", "Evaluate the model using accuracy, F1 score, precision, and recall", "Compute and display the confusion matrix", "Extract and report the top 3 feature importances"], "model_results": {"accuracy": 0.79, "f1": 0.76, "precision": 0.74, "recall": 0.79, "confusion_matrix": {"true_positive": 19, "true_negative": 36, "false_positive": 7, "false_negative": 5}, "top_feature_importances": {"CreditScore": 0.32, "LoanAmount": 0.25, "Income": 0.18}, "best_hyperparameters": {"max_depth": 8, "n_estimators": 100}}}
{"purpose": "Train a regression model to estimate daily average surface temperature based on environmental and atmospheric conditions.", "raw_table": "Date,Region,Humidity,WindSpeed,CloudCover,Precipitation,SoilMoisture,AvgTemp\n2024-01-01,North,82,5.5,75,0.0,0.23,15.2\n2024-01-02,North,78,4.8,65,,0.20,14.8\n2024-01-03,South,55,7.2,20,0.0,0.15,25.1\n2024-01-04,South,60,NaN,30,0.1,0.18,24.7\n2024-01-05,East,70,3.9,50,0.0,0.22,18.5\n2024-01-06,East,65,4.0,55,0.0,0.21,17.9\n2024-01-07,West,85,6.1,80,0.05,0.30,12.3\n2024-01-08,West,80,5.5,85,0.0,missing,13.0\n2024-01-09,North,77,5.2,70,0.0,0.25,15.0\n2024-01-10,South,58,7.0,25,0.0,0.17,26.0\n2024-01-11,East,NaN,4.5,60,0.0,0.20,19.2\n2024-01-12,West,83,6.3,78,0.1,0.28,13.1\n2024-01-13,North,79,5.1,68,0.0,0.24,15.5\n2024-01-14,South,59,7.4,28,0.0,0.16,25.5", "model_steps": ["Parse the raw CSV data and load into a DataFrame", "Handle missing values: impute Humidity and WindSpeed with median values; replace 'missing' in SoilMoisture with median", "Convert 'Region' categorical variable using one-hot encoding", "Drop 'Date' column as it is not used for prediction", "Split dataset into 80% training and 20% test sets", "Standardize numeric features: Humidity, WindSpeed, CloudCover, Precipitation, SoilMoisture", "Train a RandomForestRegressor to predict AvgTemp", "Perform grid search to optimize number of trees (n_estimators) and max_depth", "Evaluate model on test set calculating RMSE, MAE, and R2 score", "Analyze feature importances from the trained model"], "model_results": {"rmse": 1.15, "mae": 0.89, "r2": 0.87, "top_feature_importances": {"Humidity": 0.3, "SoilMoisture": 0.25, "CloudCover": 0.18, "WindSpeed": 0.15, "Region_South": 0.07, "Region_North": 0.05}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 8}}}
{"purpose": "Predict whether a loan applicant will default on their loan based on financial and demographic features.", "raw_table": "Applicant_ID,Income,Employment_Status,Credit_Score,Loan_Amount,Loan_Term,Marital_Status,Defaulted\n1,55000,Full-time,720,15000,36,Married,No\n2,43000,Part-Time,680,12000,60,single,Yes\n3,NaN,full-Time,NaN,8000,24,Married,No\n4,62000,UnEmployed,710,20000,48,Divorced,Yes\n5,48000,Full-time,690,NaN,36,Married,No\n6,52000,Part-time,670,13000,36,Single,Yes\n7,58000,Full-Time,710,14000,60,Married,No\n8,47000,Full-time,665,11000,36,Single,Yes\n9,50000,Self-employed,700,15000,24,Single,No\n10,45000,Part-Time,695,12500,48,Married,Yes\n11,53000,Full-Time,NaN,13500,36,Married,\n12,60000,Full-time,730,18000,36,Single,No\n13,49000,part-time,680,NaN,24,Divorced,Yes\n14,51000,Full-Time,705,14000,36,Married,No", "model_steps": ["Load CSV data into a DataFrame and inspect for missing or messy values", "Impute missing numeric values (Income, Credit_Score, Loan_Amount) using median imputation", "Normalize capitalization in categorical columns Employment_Status and Marital_Status", "Fill missing target values with 'No' assuming conservative approach", "One-hot encode categorical variables Employment_Status and Marital_Status", "Split data into training (80%) and testing (20%) sets stratified by the target variable Defaulted", "Standardize numeric features: Income, Credit_Score, Loan_Amount, Loan_Term", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search over max_depth parameter with values [5, 10, 15]", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Generate and inspect confusion matrix to understand false positive and false negative rates"], "model_results": {"accuracy": 0.79, "precision": 0.75, "recall": 0.81, "f1": 0.78, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}, "feature_importances": {"Credit_Score": 0.34, "Income": 0.25, "Loan_Amount": 0.18, "Employment_Status_Full-time": 0.08, "Loan_Term": 0.07, "Marital_Status_Married": 0.05, "Employment_Status_Part-time": 0.03}, "confusion_matrix": {"true_positive": 13, "true_negative": 16, "false_positive": 5, "false_negative": 3}}}
{"purpose": "Predict wheat crop yield category based on soil and weather conditions.", "raw_table": "Soil_pH,Rainfall_mm,Temperature_C,Soil_Type,Fertilizer_Used,Crop_Condition,Yield_Category\n6.5,120,22,Sandy,Yes,Good,High\n7.2,85,19,Loam,NO,poor,Low\n5.8,100,21,Clay,Yes,Good,Medium\n6.1,,20,Loam,yes,Good,High\n7.0,95,23,Sandy,No,Good,High\n6.8,110,20,CLAY,Yes,Poor,Low\nNaN,105,22,Loam,No,Good,Medium\n6.3,90,18,Sandy,Yes,Good,High\n5.9,115,21,Clay,No,Poor,Low\n6.7,100,19,Loam,Yes,Good,Medium\n6.0,95,20,Sandy,No,GOOD,High\n7.1,85,22,Loam,yes,Poor,Low\n6.4,105,21,Clay,Yes,Good,Medium", "model_steps": ["Impute missing values in Rainfall_mm and Soil_pH columns using median values", "Normalize capitalization and consistency in categorical columns (Soil_Type, Fertilizer_Used, Crop_Condition)", "One-hot encode Soil_Type, Fertilizer_Used, and Crop_Condition columns", "Split data into training (80%) and testing (20%) sets using stratified sampling on Yield_Category", "Standardize numeric features Soil_pH, Rainfall_mm, and Temperature_C", "Train a RandomForestClassifier to predict Yield_Category", "Perform grid search over number of estimators and max_depth parameters", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.83, "f1": 0.82, "precision": 0.8, "recall": 0.85, "confusion_matrix": {"High": {"High": 5, "Medium": 1, "Low": 0}, "Medium": {"High": 1, "Medium": 4, "Low": 1}, "Low": {"High": 0, "Medium": 1, "Low": 6}}, "top_feature_importances": {"Rainfall_mm": 0.34, "Soil_pH": 0.28, "Crop_Condition_Good": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Predict regional average daily temperature based on environmental and geographic features.", "raw_table": "Region,Elevation_m,Land_Cover,Avg_Daily_Rainfall_mm,Soil_Type,Day_Length_hr,Avg_Temperature_C\nNorth,350,Forest,5.2,Loam,12.5,15.3\nsouth,500,Grassland,3.8,Sandy,12.3,18.7\nEast,120,forest,NA,Clay,12.7,16.1\nWest,250,Urban,2.1,Loam,12.6,20.4\nNorth,400,Shrubland,4.5,loam,12.4,14.8\nSouth,480,Grassland,3.9,Sandy,12.2,19.0\nEast,130,Forest,4.8,Clay,12.8,17.0\nWest,260,Urban,2.3,Loam,NA,21.1\nNorth,NaN,Forest,5.0,Loam,12.5,15.0\nSouth,510,Grassland,,Sandy,12.1,18.5\nEast,125,Forest,4.7,Clay,12.7,16.5\nWest,240,Urban,2.2,Loam,12.6,20.0\nNorth,370,Shrubland,4.6,Loam,12.4,15.1", "model_steps": ["Load CSV data into a DataFrame", "Handle missing values by imputing numeric columns with median and categorical columns with mode", "Normalize capitalization inconsistencies in categorical variables (e.g., 'south' to 'South', 'forest' to 'Forest')", "One-hot encode categorical features: Region, Land_Cover, Soil_Type", "Standardize numeric features: Elevation_m, Avg_Daily_Rainfall_mm, Day_Length_hr", "Split data into training and testing sets with 80/20 ratio", "Train a Gradient Boosting Regressor to predict Avg_Temperature_C", "Perform 5-fold cross-validation to tune learning rate and number of estimators", "Evaluate model on test set using RMSE, MAE, and R2 metrics", "Extract and report feature importances from the trained model"], "model_results": {"rmse": 0.85, "mae": 0.67, "r2": 0.92, "feature_importances": {"Elevation_m": 0.32, "Avg_Daily_Rainfall_mm": 0.25, "Day_Length_hr": 0.18, "Region_South": 0.1, "Land_Cover_Forest": 0.08, "Soil_Type_Loam": 0.07}, "hyperparameters": {"learning_rate": 0.1, "n_estimators": 150, "max_depth": 3}}}
{"purpose": "Predict the likelihood of 30-day hospital readmission for diabetic patients based on clinical and demographic data.", "raw_table": "PatientID,Age,Gender,BMI,HbA1c,DiabetesType,MedCompliance,PreviousAdmissions,Readmitted\n1,54,Male,29.3,7.8,Type 2,High,2,Yes\n2,63,Female,NaN,8.5,Type 1,medium,1,No\n3,45,Female,26.7,6.9,Type 2,Low,0,No\n4,50,Male,31.0,9.1,type 2,HIGH,3,Yes\n5,38,Female,22.1,7.3,Type 1,Medium,0,No\n6,60,male,28.5,8.0,Type 2,Low,1,Yes\n7,55,Female,NaN,7.7,Type 2,High,2,Yes\n8,47,Male,27.5,NaN,Type 1,medium,1,No\n9,52,Female,30.2,8.4,Type 2,High,2,Yes\n10,43,male,25.4,7.1,Type 2,low,0,No\n11,49,Female,29.9,7.9,Type 1,Medium,1,No\n12,58,Male,28.8,8.3,TYPE 2,High,3,Yes\n13,44,female,26.0,7.4,Type 1,Medium,0,No\n14,61,Male,32.1,8.8,Type 2,low,4,Yes", "model_steps": ["Load and inspect the dataset for missing and inconsistent values", "Standardize capitalization in categorical variables (e.g., Gender, DiabetesType, MedCompliance)", "Impute missing BMI values using median BMI grouped by DiabetesType and Gender", "Impute missing HbA1c values using median HbA1c grouped by DiabetesType", "Encode categorical variables: Gender, DiabetesType, and MedCompliance using one-hot encoding", "Convert target variable 'Readmitted' to binary (Yes=1, No=0)", "Split the data into train (80%) and test (20%) sets with stratification on the target", "Standardize numeric features: Age, BMI, HbA1c, PreviousAdmissions", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search to tune max_depth in [5, 10, 15]", "Evaluate the final model on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze a confusion matrix for test predictions"], "model_results": {"accuracy": 0.79, "precision": 0.77, "recall": 0.81, "f1": 0.79, "confusion_matrix": {"true_negative": 8, "false_positive": 3, "false_negative": 2, "true_positive": 11}, "top_feature_importances": {"HbA1c": 0.32, "MedCompliance_High": 0.2, "PreviousAdmissions": 0.18, "BMI": 0.12, "Age": 0.08, "DiabetesType_Type 2": 0.05, "Gender_Male": 0.05}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a movie will be a box office hit based on its attributes.", "raw_table": "movie_id,title,genre,duration_minutes,budget_millions,lead_actor_popularity,director_experience_years,release_month,box_office_hit\n1,Star Quest,Sci-Fi,130,150,8.5,12,July,Yes\n2,Love in Paris,romance,95,20,6.0,5,February,No\n3,THE LAST STAND,Action,110,80,,10,December,Yes\n4,Funny Days,Comedy,85,15,unknown,3,August,No\n5,Haunted Night,Horror,100,25,4.5,7,October,No\n6,Deep Oceans,Documentary,70,5,3.0,2,May,No\n7,Space Odyssey,Sci-fi,140,200,9.0,15,July,Yes\n8,City Lights,Comedy,100,30,5.5,6,March,No\n9,Shadow Realm,Action,115,90,7.8,11,November,Yes\n10,Romantic Escape,Romance,105,25,6.5,4,February,No\n11,THE FINAL ACT,action,125,110,8.0,13,December,Yes\n12,Funny Days 2,Comedy,88,18,5.0,4,August,No", "model_steps": ["Load the dataset from CSV string and parse columns", "Identify and handle missing values and inconsistent data in 'lead_actor_popularity' and 'genre' columns", "Normalize capitalization in categorical columns 'genre' and 'box_office_hit' to ensure consistency", "Split data into training (80%) and testing (20%) sets with stratification on the target variable 'box_office_hit'", "One-hot encode the 'genre' and 'release_month' categorical variables", "Impute missing numeric values in 'lead_actor_popularity' using median imputation", "Standardize numeric features: 'duration_minutes', 'budget_millions', 'lead_actor_popularity', and 'director_experience_years'", "Train a RandomForestClassifier to predict 'box_office_hit' (Yes/No)", "Perform hyperparameter tuning over number of trees (n_estimators) and maximum depth (max_depth) using cross-validation", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix of predictions vs actuals", "Identify and report top 3 feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.83, "precision": 0.85, "recall": 0.8, "f1": 0.82, "confusion_matrix": {"true_positive": 5, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"budget_millions": 0.35, "lead_actor_popularity": 0.25, "genre_Sci-Fi": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Predict whether a social media post will go viral based on post characteristics and user engagement metrics.", "raw_table": "post_id,post_length,post_type,author_follower_count,day_posted,has_image,engagement_score,target_viral\n1,120,text,5000,Monday,yes,300,yes\n2,250,Image,7000,Tuesday,Yes,450,yes\n3,80,video,3500,Wednesday,no,120,no\n4,100,Text,4500,Thursday,No,150,no\n5,300,video,9000,Friday,yes,550,YES\n6,150,image,8000,,yes,400,yes\n7,200,Text,NA,Monday,No,350,yes\n8,95,video,6200,Tuesday,yes,200,no\n9,110,image,5000,Wednesday,Yes,180,no\n10,130,Video,6700,Thursday,no,270,yes\n11,85,text,4000,Friday,yes,140,no\n12,175,image,7200,Monday,yes,480,yes\n13,90,text,3900,Tuesday,no,110,no\n14,210,image,8000,Wednesday,yes,460,yes\n15,160,Video,7500,Friday,NO,320,no", "model_steps": ["Load the CSV data and inspect for missing or inconsistent values", "Impute missing values in 'author_follower_count' with median and 'day_posted' with mode", "Normalize inconsistent capitalization in categorical columns 'post_type', 'has_image', and 'target_viral'", "Convert categorical columns 'post_type', 'day_posted', and 'has_image' into one-hot encoded variables", "Convert target variable 'target_viral' into a binary label (yes=1, no=0)", "Split data into train and test sets with 80% training and 20% testing", "Standardize numeric features 'post_length', 'author_follower_count', and 'engagement_score'", "Train a RandomForestClassifier on the training data", "Tune max_depth hyperparameter using grid search with 3-fold cross-validation", "Evaluate model on test set and compute accuracy, precision, recall, and F1 score", "Generate a confusion matrix and extract feature importances"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.9, "f1": 0.87, "confusion_matrix": {"true_positive": 8, "true_negative": 5, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"engagement_score": 0.42, "author_follower_count": 0.25, "post_type_video": 0.15, "has_image_yes": 0.12, "post_length": 0.06}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a customer will make a repeat purchase within 30 days based on their first order behavior and demographics.", "raw_table": "customer_id,age,gender,first_order_value,first_order_category,days_to_repeat_purchase,repeat_purchase\n1001,34,Male,120.5,Electronics,12,yes\n1002,27,Female,89.99,Clothing,,no\n1003,,female,45,Home,25,Yes\n1004,45,Male,missing,Electronics,8,YES\n1005,29,Female,150.0,clothing,15,no\n1006,41,Male,200,Electronics,7,Yes\n1007,36,,60,Home,30,No\n1008,23,Female,13.5,Clothing,28,YES\n1009,38,Male,85.75,home,3,yes\n1010,30,Female,110,Electronics,missing,No\n1011,28,Female,75,Clothing,18,no\n1012,35,Male,NaN,Home,20,No\n1013,42,Female,130.25,Electronics,5,YES", "model_steps": ["Load the dataset and identify missing and inconsistent values", "Impute missing numeric values in 'age' and 'first_order_value' columns using median imputation", "Fill missing categorical values in 'gender' with mode and standardize capitalization in 'gender', 'first_order_category', and 'repeat_purchase' columns", "Convert target variable 'repeat_purchase' to binary numeric: yes=1, no=0", "One-hot encode categorical features: 'gender' and 'first_order_category'", "Split the dataset into 80% training and 20% testing sets with stratification on the target", "Standardize numeric features 'age' and 'first_order_value'", "Train a RandomForestClassifier with default parameters on the training set", "Perform grid search over 'n_estimators' (50, 100) and 'max_depth' (5, 10) using 5-fold cross-validation", "Evaluate the final model on the test set with accuracy, precision, recall, and F1 score", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.83, "precision": 0.85, "recall": 0.8, "f1": 0.82, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}, "top_feature_importances": {"first_order_value": 0.32, "days_to_repeat_purchase": 0.28, "first_order_category_Electronics": 0.15}}}
{"purpose": "Predict whether a manufactured part will fail quality inspection based on sensor readings and process parameters.", "raw_table": "Part_ID,Machine_ID,Operator,Temperature,Pressure,Speed,Material_Type,Defect\n1,M1,John,200,30,150,TypeA,No\n2,M2,Anne,210,28,155,typeB,Yes\n3,m1,Bob,NaN,27,149,TypeA,No\n4,M3,ANNE,205,31,152,TypeC,No\n5,M2,John,198,29,,TypeB,Yes\n6,M1,Bob,202,30,150,TypeA,No\n7,M3,Anne,215,NaN,153,Typec,Yes\n8,M2,Anne,208,28,151,TypeB,No\n9,M1,John,NaN,30,150,TypeA,No\n10,M3,Bob,207,32,NaN,typeC,Yes\n11,M2,John,204,29,154,TypeB,No\n12,M3,Anne,203,31,153,TypeC,Yes\n13,M1,Bob,199,30,149,TypeA,No\n14,M2,Anne,206,27,152,typeB,Yes", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Standardize the casing for categorical fields 'Machine_ID', 'Operator', and 'Material_Type'", "Impute missing numeric values with median values for each feature", "Fill missing 'Speed' values using median speed grouped by 'Machine_ID'", "Encode categorical variables using one-hot encoding", "Split the data into training (80%) and testing (20%) sets", "Scale numeric features using StandardScaler", "Train a RandomForestClassifier with default parameters on the training set", "Perform grid search over number of estimators (50, 100, 150) and max_depth (5, 10, None)", "Evaluate the final model on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix to analyze types of classification errors", "Identify top five feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.82, "recall": 0.78, "f1": 0.8, "confusion_matrix": {"True_Negative": 8, "False_Positive": 2, "False_Negative": 3, "True_Positive": 7}, "top_feature_importances": {"Pressure": 0.27, "Material_Type_TypeB": 0.22, "Temperature": 0.18, "Speed": 0.15, "Machine_ID_M2": 0.1}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Predict whether a movie will be a box office hit based on its production and genre features.", "raw_table": "MovieID,Genre,Director,ProductionBudget,Runtime,ReleaseMonth,LeadActorPopularity,BoxOfficeHit\n1,Action,Smith,150000000,120,July,85,Yes\n2,Comedy,johnson,40000000,90,december,70,No\n3,Drama,Lee,30000000,110,March,60,No\n4,Action,smith,200000000,130,August,95,Yes\n5,Comedy,Wilson,35000000,95,November,65,No\n6,Horror,Nguyen,15000000,85,october,40,No\n7,Drama,Lee,45000000,115,April,55,No\n8,Action,Smith,180000000,,July,90,Yes\n9,Comedy,Johnson,42000000,100,December,72,No\n10,Horror,Nguyen,16000000,88,October,,No\n11,Action,Smith,190000000,125,August,92,Yes\n12,Drama,lee,38000000,112,May,58,No\n13,Comedy,Wilson,37000000,93,,68,No\n14,Action,Smith,210000000,135,July,88,Yes", "model_steps": ["Load the CSV data into a DataFrame and inspect for inconsistencies.", "Standardize capitalization for categorical variables 'Genre' and 'Director'.", "Handle missing numeric values in 'Runtime' and 'LeadActorPopularity' by imputing median values.", "Fill missing 'ReleaseMonth' with the mode month and standardize month names to title case.", "Encode categorical variables 'Genre', 'Director', and 'ReleaseMonth' using one-hot encoding.", "Convert target variable 'BoxOfficeHit' to binary labels (Yes=1, No=0).", "Split data into train and test sets using an 80/20 ratio.", "Standardize numeric features 'ProductionBudget', 'Runtime', and 'LeadActorPopularity'.", "Train a RandomForestClassifier with 100 trees on the training data.", "Perform grid search over max_depth values [5,10,15] and select the best model based on validation F1 score.", "Evaluate the final model on the test set by computing accuracy, precision, recall, and F1 score.", "Generate and analyze the confusion matrix and identify top 3 feature importances."], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"ProductionBudget": 0.42, "LeadActorPopularity": 0.25, "Genre_Action": 0.15}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a social media post will go viral based on post characteristics and user engagement features.", "raw_table": "post_id,post_length,post_type,user_followers,engagement_rate,day_of_week,is_sponsored,viral\n1,120,text,1500,0.05,Monday,No,No\n2,250,Image,3000,0.15,FRIDAY,Yes,Yes\n3,75,video,800,,Wednesday,No,No\n4,300,Text,12000,0.20,Sunday,Yes,Yes\n5,180,image,5000,0.12,Tuesday,,No\n6,95,video,700,0.03,thursday,No,No\n7,400,Text,20000,0.30,Monday,Yes,Yes\n8,150,Image,4500,0.10,Saturday,No,No\n9,85,Video,900,0.04,Wednesday,No,No\n10,220,text,10000,0.18,Friday,Yes,Yes\n11,160,Image,3500,0.09,Tuesday,No,No\n12,130,video,4000,0.07,Monday,No,No\n13,110,text,600,0.02,Thursday,No,No\n14,270,image,18000,0.25,Sunday,Yes,Yes\n15,140,Video,7500,0.11,friday,No,Yes", "model_steps": ["Load the CSV data and inspect for missing and inconsistent values", "Normalize 'post_type' and 'day_of_week' to lowercase to handle inconsistent capitalization", "Impute missing 'engagement_rate' values using median value", "Convert categorical variables 'post_type', 'day_of_week', and 'is_sponsored' into one-hot encoded features", "Split data into training (80%) and testing (20%) sets randomly, stratifying on the 'viral' target", "Standardize numeric features 'post_length', 'user_followers', and 'engagement_rate' to zero mean and unit variance", "Train a RandomForestClassifier with default parameters on the training set", "Perform hyperparameter tuning using grid search over 'max_depth' (values: 3, 5, 10) and 'n_estimators' (50, 100)", "Select the best model based on F1 score on validation folds", "Evaluate the final model performance on the test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and feature importance plot for the final model"], "model_results": {"accuracy": 0.87, "f1": 0.85, "precision": 0.88, "recall": 0.82, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 1, "false_negative": 2}, "top_feature_importances": {"engagement_rate": 0.32, "user_followers": 0.25, "is_sponsored_Yes": 0.15, "post_length": 0.12, "post_type_video": 0.08}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a customer will make a purchase in the next session based on their browsing behavior and demographics.", "raw_table": "CustomerID,Age,Gender,DeviceType,PagesViewed,TimeOnSiteMinutes,PreviousPurchases,SessionHour,PurchaseNextSession\n1,25,Male,Mobile,5,12.5,2,14,Yes\n2,34,Female,desktop,3,7,0,9,No\n3,45,Female,Mobile,NaN,15.2,1,20,Yes\n4,23,male,Tablet,7,18.1,3,17,Yes\n5,38,Female,Desktop,2,5.5,0,11,No\n6,29,,Mobile,4,NaN,1,13,No\n7,52,Female,Tablet,8,20.3,5,19,Yes\n8,40,Male,Mobile,6,15,2,21,Yes\n9,31,Male,Desktop,NaN,6.7,0,10,No\n10,27,Female,Mobile,3,8.9,0,8,No\n11,36,mALE,tablet,5,13,1,16,Yes\n12,22,FEMALE,Mobile,4,9.5,,15,No\n13,47,Male,Desktop,7,17,4,22,Yes\n14,30,Female,Mobile,5,11.2,2,14,No", "model_steps": ["Handle missing numeric values in PagesViewed, TimeOnSiteMinutes, and PreviousPurchases by imputing median values", "Standardize numeric features: Age, PagesViewed, TimeOnSiteMinutes, PreviousPurchases, SessionHour", "Normalize categorical columns Gender and DeviceType by converting to lowercase and correcting inconsistent capitalization", "One-hot encode categorical variables Gender and DeviceType", "Convert target variable PurchaseNextSession from 'Yes'/'No' to binary 1/0", "Split data into train and test sets with 80/20 ratio, stratified on the target variable", "Train a Logistic Regression classifier on the training data", "Perform hyperparameter tuning on regularization strength with cross-validation", "Evaluate the model on the test set, computing accuracy, precision, recall, and F1 score", "Generate confusion matrix and plot ROC curve"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"TimeOnSiteMinutes": 0.45, "PagesViewed": 0.3, "PreviousPurchases": 0.15, "DeviceType_mobile": 0.07, "Gender_female": 0.03}, "best_hyperparameters": {"C": 0.1, "penalty": "l2", "solver": "liblinear"}}}
{"purpose": "Predict whether a household's daily energy consumption will exceed 30 kWh based on weather and household characteristics.", "raw_table": "Temperature,Humidity,DayOfWeek,HouseholdType,NumOccupants,HeatingType,DailyEnergyUsage,HighUsage\n22.5,55,Monday,Apartment,3,electric,28.4,No\n18.3,68,Tuesday,Detached,4,Gas,35.2,Yes\n25.1,,Wednesday,TOWNHOUSE,2,Electric,27.8,No\n20.0,60,Thursday,Detached,5,,33.5,Yes\n21.5,58,Friday,apartment,3,Gas,29.9,No\n19.8,65,Saturday,Detached,NaN,Electric,37.0,Yes\n23.0,53,Sunday,Townhouse,4,Gas,26.7,No\n17.6,70,Monday,Detached,6,Electric,40.1,Yes\n24.2,55,Tuesday,apartment,2,Gas,25.4,No\n22.8,59,Wednesday,Townhouse,3,Electric,31.2,Yes\n20.5,62,Thursday,Detached,4,Gas,34.0,Yes\n21.0,60,Friday,Apartment,3,Gas,27.0,No\n19.5,64,Saturday,Townhouse,5,Electric,36.3,Yes\n23.3,57,Sunday,Detached,3,Gas,29.1,No", "model_steps": ["Load data from CSV and inspect for missing or inconsistent values", "Impute missing Humidity and NumOccupants with median values", "Standardize capitalization in categorical columns HouseholdType and HeatingType", "One-hot encode categorical variables HouseholdType, HeatingType, and DayOfWeek", "Split data into training and test sets (80/20)", "Standardize numerical features Temperature, Humidity, and NumOccupants", "Train a RandomForestClassifier to predict HighUsage", "Perform grid search over max_depth and n_estimators parameters", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Compute confusion matrix and extract feature importances", "Generate final predictions on the test set"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.8, "recall": 0.87, "confusion_matrix": {"true_positive": 6, "true_negative": 7, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"HeatingType_Electric": 0.28, "NumOccupants": 0.22, "Temperature": 0.18, "HouseholdType_Detached": 0.12, "Humidity": 0.1}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a loan applicant will default on their loan based on financial and demographic features.", "raw_table": "ApplicantID,Age,Income,EmploymentStatus,CreditScore,LoanAmount,LoanPurpose,PreviousDefaults,Defaulted\n1,34,55000,Full-time,720,15000,Home,0,No\n2,29,48000,part-time,680,8000,Car,1,Yes\n3,42,NaN,Self-employed,640,12000,home,0,No\n4,37,62000,Full-Time,700,20000,Education,0,No\n5,51,58000,Unemployed,NaN,9000,car,2,Yes\n6,23,43000,Full-time,690,7000,Home,0,No\n7,45,67000,Full-time,710,16000,Home,0,No\n8,31,50000,Part-Time,680,11000,Car,1,Yes\n9,39,60000,Full-time,NaN,13000,Education,0,No\n10,28,47000,Self-employed,670,9000,Home,NaN,Yes\n11,46,72000,Full-Time,730,21000,Home,0,No\n12,33,49000,full-time,685,10000,Car,0,No\n13,55,80000,Retired,750,5000,Home,0,No", "model_steps": ["Load the dataset and identify missing values in Income, CreditScore, and PreviousDefaults columns", "Impute missing numeric values with median of respective columns", "Standardize numeric features: Age, Income, CreditScore, LoanAmount, PreviousDefaults", "Normalize capitalization inconsistencies in EmploymentStatus and LoanPurpose columns", "One-hot encode the categorical variables EmploymentStatus and LoanPurpose", "Convert the target variable 'Defaulted' into binary format (Yes=1, No=0)", "Split the data into training and testing sets with 80% for training and 20% for testing", "Train a RandomForestClassifier with 100 trees on the training data", "Evaluate the model performance on the test set by computing accuracy, precision, recall, and F1 score", "Generate the confusion matrix to analyze true/false positives and negatives"], "model_results": {"accuracy": 0.85, "precision": 0.8, "recall": 0.75, "f1": 0.77, "confusion_matrix": {"true_positive": 6, "true_negative": 10, "false_positive": 2, "false_negative": 2}, "top_feature_importances": {"CreditScore": 0.3, "PreviousDefaults": 0.25, "Income": 0.15, "LoanAmount": 0.1, "EmploymentStatus_Full-time": 0.08, "Age": 0.07, "LoanPurpose_Home": 0.05}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a retail customer will make a purchase during a promotional campaign.", "raw_table": "CustomerID,Age,Gender,Annual_Income,Membership_Level,Last_Purchase_Amount,Days_Since_Last_Purchase,Made_Purchase\n1,34,Male,54000,Gold,150.5,30,Yes\n2,29,Female,48000,Silver,NA,45,No\n3,40,female,62000,Gold,200.0,15,Yes\n4,23,Male,35000,Bronze,50.0,,No\n5,37,Female,58000,Silver,120.0,60,Yes\n6,,Male,45000,Silver,75.0,90,No\n7,45,Female,72000,Gold,220.0,5,Yes\n8,31,male,50000,Bronze,65.0,80,No\n9,28,Female,47000,silver,80.0,70,No\n10,38,Male,60000,Gold,180.0,20,Yes\n11,50,Female,80000,Gold,250.0,3,Yes\n12,35,Male,51000,Silver,100.0,50,No\n13,27,Female,42000,Bronze,40.0,85,No\n14,41,Male,67000,Gold,210.0,10,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values: impute missing Age with median, Days_Since_Last_Purchase with mean, and Last_Purchase_Amount missing values with median", "Standardize inconsistent capitalization in Gender and Membership_Level columns", "Convert categorical variables Gender and Membership_Level into one-hot encoded features", "Split data into training (80%) and test (20%) sets stratified by target variable Made_Purchase", "Standardize numeric features: Age, Annual_Income, Last_Purchase_Amount, Days_Since_Last_Purchase", "Train a RandomForestClassifier to predict Made_Purchase", "Perform a grid search over n_estimators (50, 100) and max_depth (5, 10) using 5-fold cross-validation", "Evaluate the best model on the test set calculating accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "confusion_matrix": {"True_Positive": 6, "True_Negative": 7, "False_Positive": 1, "False_Negative": 2}, "top_feature_importances": {"Days_Since_Last_Purchase": 0.32, "Membership_Level_Gold": 0.25, "Last_Purchase_Amount": 0.18}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features and location.", "raw_table": "PropertyID,Neighborhood,HouseStyle,LotArea,OverallQual,YearBuilt,GarageCars,BsmtQual,SalePrice\n1,Cul-de-Sac,2Story,8450,7,2003,2,Ex,208500\n2,CUL-DE-SAC,1Story,9600,6,1976,1,Gd,181500\n3,CollgCr,2Story,11250,7,2001,2,TA,223500\n4,CollgCr,2Story,9550,7,1915,1,Fa,140000\n5,OldTown,1.5Fin,14260,8,2000,3,Ex,250000\n6,Oldtown,1Story,14115,5,1993,2,TA,143000\n7,Edwards,1Story,10084,5,2004,2,Missing,307000\n8,Edwards,1.5Fin,10382,5,1973,2,Gd,200000\n9,NAmes,2Story,6120,4,1931,1,Fa,129900\n10,NAmes,1Story,7420,4,1939,1,TA,118000\n11,Mitchel,1Story,11200,6,1970,2,Gd,129500\n12,Sawyer,2Story,11924,7,1967,2,TA,345000\n13,Sawyer,1Story,13737,5,2008,3,Ex,144000\n14,SawyerW,2Story,10642,8,2005,3,Gd,279500", "model_steps": ["Load the CSV data into a DataFrame", "Clean 'Neighborhood' column to standardize capitalization", "Impute missing values in 'BsmtQual' with 'TA' (typical average)", "One-hot encode categorical features: 'Neighborhood', 'HouseStyle', and 'BsmtQual'", "Split data into train and test sets (80% train, 20% test)", "Standardize numeric features: 'LotArea', 'OverallQual', 'YearBuilt', 'GarageCars'", "Train a RandomForestRegressor to predict 'SalePrice'", "Tune hyperparameters using grid search over n_estimators and max_depth", "Evaluate model performance on test set with RMSE, MAE, and R2", "Extract and report top 5 feature importances from the trained model"], "model_results": {"rmse": 18750, "mae": 14200, "r2": 0.82, "top_feature_importances": {"OverallQual": 0.38, "Neighborhood_Cul-de-Sac": 0.12, "YearBuilt": 0.1, "GarageCars": 0.08, "LotArea": 0.07}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 12}}}
{"purpose": "Predict whether a citizen is likely to default on a government-issued loan based on demographic and financial data.", "raw_table": "Applicant_ID,Age,Employment_Status,Annual_Income,Loan_Amount,Credit_Score,Marital_Status,Defaulted\n1,45,Employed,55000,15000,720,Married,No\n2,29,Unemployed,32000,8000,680,Single,Yes\n3,38,Self-employed,45000,12000,NaN,married,No\n4,52,Employed,60000,18000,740,Divorced,No\n5,30,employed,40000,10000,690,Single,Yes\n6,41,Unemployed,NaN,NaN,650,Single,Yes\n7,35,Employed,48000,12500,710,Married,No\n8,28,Self-employed,39000,9000,700,Single,Yes\n9,,Employed,52000,14000,730,Married,No\n10,46,Unemployed,35000,8500,670,divorced,Yes\n11,33,Employed,42000,9500,NaN,Single,No\n12,50,Employed,58000,16000,725,Married,No\n13,39,Self-employed,47000,11500,690,Single,Yes\n14,27,Unemployed,31000,7800,660,Single,Yes", "model_steps": ["Load the raw CSV data into a DataFrame", "Identify and handle missing values: impute numeric columns (Age, Annual_Income, Loan_Amount, Credit_Score) with median values", "Correct inconsistent capitalization in categorical columns (Employment_Status, Marital_Status)", "One-hot encode categorical variables: Employment_Status and Marital_Status", "Separate features and target variable Defaulted (binary classification)", "Split data into training and testing sets with an 80/20 ratio, stratified by Defaulted label", "Standardize numeric features (Age, Annual_Income, Loan_Amount, Credit_Score) using training set statistics", "Train a RandomForestClassifier with default parameters on the training data", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Analyze feature importances from the trained Random Forest model"], "model_results": {"accuracy": 0.79, "precision": 0.76, "recall": 0.82, "f1": 0.79, "top_feature_importances": {"Credit_Score": 0.37, "Loan_Amount": 0.25, "Annual_Income": 0.18, "Employment_Status_Self-employed": 0.1, "Age": 0.06, "Marital_Status_Married": 0.04}, "confusion_matrix": {"true_negatives": 7, "false_positives": 2, "false_negatives": 1, "true_positives": 4}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a social media post will go viral based on post metadata and user engagement features.", "raw_table": "post_id,user_type,post_length,media_type,hashtags_count,followers_count,time_posted,target_viral\n1,Influencer,120,Image,5,15000,Morning,Yes\n2,regular,45,Video,2,300,Evening,No\n3,Regular,200,Text,0,1200,Night,No\n4,Business,150,Image,3,8500,Morning,Yes\n5,INfluencer,80,video,4,20000,Afternoon,Yes\n6,regular,NaN,Text,1,150,Night,No\n7,Business,100,image,3,7600,Evening,No\n8,regular,60,Video,2,400,Morning,No\n9,Influencer,95,Image,8,25000,Evening,Yes\n10,BUSINESS,130,Text,NaN,9000,Afternoon,No\n11,regular,75,Video,3,500,Afternoon,No\n12,Influencer,110,Image,6,18000,Morning,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Standardize capitalization in categorical columns: user_type, media_type, time_posted, and target_viral", "Impute missing numeric values in post_length with median", "Impute missing numeric values in hashtags_count with zero", "Encode target variable 'target_viral' as binary (Yes=1, No=0)", "One-hot encode categorical features: user_type, media_type, time_posted", "Split data into train and test sets with 75% training and 25% testing", "Standardize numeric features: post_length, hashtags_count, followers_count", "Train a RandomForestClassifier with 100 trees", "Evaluate the model using accuracy, precision, recall, and F1 score on the test set", "Generate confusion matrix to analyze classification errors"], "model_results": {"accuracy": 0.83, "precision": 0.79, "recall": 0.85, "f1": 0.82, "confusion_matrix": {"true_positive": 5, "true_negative": 10, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"followers_count": 0.35, "hashtags_count": 0.2, "user_type_Influencer": 0.18, "post_length": 0.12, "media_type_Image": 0.08, "time_posted_Morning": 0.07}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a social media post will go viral based on post characteristics and user engagement metrics.", "raw_table": "post_id,user_followers,post_length,content_type,post_time,engagement_score,platform,viral\n1,1500,120,Image,Morning,350,Instagram,Yes\n2,800,45,Video,Evening,120,facebook,No\n3,NaN,75,text,Afternoon,200,Twitter,No\n4,2200,300,Video,Night,500,Instagram,Yes\n5,1100,85,Image,Morning,180,Facebook,No\n6,900,55,Text,,160,Twitter,No\n7,3000,200,video,Evening,700,Instagram,Yes\n8,400,35,Image,Night,50,Facebook,No\n9,850,100,Text,Morning,210,Twitter,No\n10,2700,150,IMAGE,Afternoon,600,Instagram,Yes\n11,1300,60,Video,Night,250,facebook,No\n12,NaN,90,Text,Morning,NaN,Twitter,No\n13,1600,130,Image,Evening,400,Instagram,Yes", "model_steps": ["Load the CSV data into a dataframe and inspect for missing values and inconsistencies", "Fill missing numeric values (user_followers, engagement_score) with median values", "Standardize capitalization in categorical columns (content_type, platform) to ensure uniformity", "Impute missing categorical values in post_time with the mode value", "Encode categorical variables (content_type, post_time, platform) using one-hot encoding", "Normalize numeric features (user_followers, post_length, engagement_score) using Min-Max scaling", "Split data into training and test sets with an 80/20 ratio, stratifying on the viral target", "Train a RandomForestClassifier to predict the viral label", "Tune the model's max_depth and n_estimators parameters using grid search with 5-fold cross-validation", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix to understand classification errors", "Identify and report the top 3 most important features from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.8, "f1": 0.815, "confusion_matrix": {"true_positives": 7, "false_positives": 2, "true_negatives": 9, "false_negatives": 3}, "top_feature_importances": {"engagement_score": 0.38, "user_followers": 0.25, "content_type_Video": 0.15}, "best_hyperparameters": {"max_depth": 6, "n_estimators": 100}}}
