{"purpose": "Predict whether a student will pass the final exam based on demographics and study habits.", "raw_table": "StudentID,Age,Gender,StudyHoursPerWeek,AttendanceRate,PreviousGrade,ParentalEducation,PassedFinal\n1,17,Male,12,0.85,B,College,Yes\n2,18,Female,8,0.90,a,Highschool,No\n3,17,Male,5,0.75,C,College,No\n4,19,FEMALE,15,0.95,A,Masters,Yes\n5,,Female,10,0.80,B,College,Yes\n6,18,Male,7,,B,highschool,No\n7,17,Female,20,0.98,A,Masters,Yes\n8,18,Male,NaN,0.65,C,College,No\n9,19,Female,13,0.92,B,Highschool,Yes\n10,17,Male,6,0.70,D,Highschool,No\n11,18,Female,9,0.88,B,College,Yes\n12,17,Male,4,0.60,C,Highschool,No\n13,19,Female,11,0.90,B,Masters,Yes\n14,18,Male,NaN,0.85,B,College,No", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Impute missing numeric values in Age and StudyHoursPerWeek with median", "Fill missing AttendanceRate with mean attendance rate", "Standardize capitalization in Gender and ParentalEducation columns to ensure consistency", "Encode target variable 'PassedFinal' as binary (Yes=1, No=0)", "One-hot encode categorical features: Gender and ParentalEducation", "Split data into train and test sets with 80% training and 20% testing", "Standardize numeric features: Age, StudyHoursPerWeek, AttendanceRate, and PreviousGrade encoded ordinally (A=4, B=3, C=2, D=1, a=missing/error treated as NaN and imputed)", "Train a RandomForestClassifier with 100 trees on the training data", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Compute and analyze confusion matrix to understand false positives and false negatives"], "model_results": {"accuracy": 0.79, "precision": 0.83, "recall": 0.77, "f1": 0.8, "confusion_matrix": {"true_positive": 9, "true_negative": 6, "false_positive": 2, "false_negative": 3}, "top_feature_importances": {"StudyHoursPerWeek": 0.35, "AttendanceRate": 0.28, "PreviousGrade": 0.18, "ParentalEducation_College": 0.1, "Gender_Female": 0.09}}}
{"purpose": "Train a classification model to predict whether a customer will make a purchase during a browsing session.", "raw_table": "session_id,session_length_seconds,user_age,user_gender,device_type,page_views,referral_source,purchase_made\n1,300,25,Male,Desktop,15,Google,Yes\n2,45,38,Female,mobile,3,facebook,No\n3,200,NaN,Male,Tablet,12,Direct,Yes\n4,150,29,MALE,Mobile,7,google,No\n5,600,45,Female,Desktop,30,Bing,Yes\n6,80,22,Female,Desktop,5,email,No\n7,NaN,35,Male,mobile,10,Direct,Yes\n8,240,28,Female,Tablet,20,Facebook,Yes\n9,100,33,Male,desktop,8,Email,No\n10,360,40,Female,Mobile,18,Google,Yes\n11,250,30,Male,Tablet,NaN,Direct,No\n12,120,26,Female,mobile,6,bing,No\n13,90,,Male,Desktop,4,Email,No\n14,400,31,Female,Mobile,22,Google,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values: impute missing user_age and session_length_seconds with median values and page_views with median", "Standardize inconsistent capitalization in categorical columns: user_gender, device_type, and referral_source", "Encode categorical variables using one-hot encoding for user_gender, device_type, and referral_source", "Split data into training and test sets using an 80/20 ratio with stratification on the target variable purchase_made", "Standardize numeric features: session_length_seconds, user_age, and page_views", "Train a RandomForestClassifier with default hyperparameters on the training set", "Evaluate model performance on the test set computing accuracy, F1 score, precision, and recall", "Analyze feature importances from the trained RandomForestClassifier", "Generate and analyze the confusion matrix for test predictions"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.8, "recall": 0.87, "feature_importances": {"session_length_seconds": 0.35, "page_views": 0.3, "referral_source_Google": 0.1, "user_gender_Female": 0.08, "device_type_Mobile": 0.07, "user_age": 0.05, "referral_source_Facebook": 0.05}, "confusion_matrix": {"true_positive": 24, "true_negative": 22, "false_positive": 6, "false_negative": 4}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict customer churn probability for a telecommunications provider.", "raw_table": "customer_id,age,contract_type,monthly_charges,total_charges,internet_service,tech_support,churn\n001,34,Month-to-month,70.35,1540.75,Fiber optic,Yes,No\n002,45,One year,89.10,890.10,Dsl,No,Yes\n003,27,Month-to-month,NaN,NaN,Fiber optic,No,Yes\n004,62,Two year,56.75,3410.00,,Yes,No\n005,38,Month-to-month,99.99,999.99,Fiber Optic,no,Yes\n006,51,One year,65.20,1320.30,DSL,Yes,No\n007,29,Month-to-month,80.40,1600.00,Fiber optic,No,Yes\n008,41,Two year,50.00,2000.00,DSL,No,No\n009,33,Month-to-month,78.90,NaN,fiber optic,Yes,Yes\n010,55,One Year,60.15,1804.50,Dsl,Yes,No\n011,48,Two Year,54.30,3260.40,Fiber optic,No,No\n012,26,Month-to-month,85.00,850.00,Dsl,No,Yes\n013,39,Month-to-month,72.40,1420.00,Fiber optic,Yes,No\n014,50,One year,65.50,1310.00,Fiber optic,No,No", "model_steps": ["Load the raw CSV data into a DataFrame", "Clean data by fixing inconsistent capitalization in 'internet_service' and 'contract_type' columns", "Impute missing numeric values in 'monthly_charges' and 'total_charges' with column means", "Encode categorical variables: One-hot encode 'contract_type', 'internet_service', and 'tech_support'", "Convert target variable 'churn' to binary (Yes=1, No=0)", "Split data into training and test sets with 80/20 ratio", "Standardize numeric features 'age', 'monthly_charges', and 'total_charges' using StandardScaler", "Train a RandomForestClassifier on the training set", "Perform grid search to tune 'max_depth' with values [3, 5, 7]", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix", "Identify top 3 feature importances from the trained Random Forest"], "model_results": {"accuracy": 0.79, "precision": 0.75, "recall": 0.81, "f1": 0.78, "confusion_matrix": [[7, 3], [2, 7]], "top_feature_importances": {"contract_type_Month-to-month": 0.32, "internet_service_Fiber optic": 0.25, "monthly_charges": 0.18}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a social media post will go viral based on post characteristics and user engagement metrics.", "raw_table": "post_id,post_length,user_followers,post_topic,post_time,device_type,likes,shares,viral\n1,120,1500,Sports,morning,Mobile,300,45,Yes\n2,85,800,Politics,Afternoon,Desktop,50,5,No\n3,200,2300,Entertainment,MORNING,Mobile,500,80,Yes\n4,45,100,Politics,Evening,tablet,10,2,no\n5,160,NaN,Entertainment,Night,Mobile,400,70,Yes\n6,95,1200,TECH,Afternoon,Desktop,150,25,No\n7,110,1800,Sports,morning,Mobile,350,60,Yes\n8,130,2100,Entertainment,Afternoon,Desktop,420,78,Yes\n9,60,,Politics,Night,Mobile,20,1,No\n10,75,900,Tech,Morning,Laptop,120,15,No\n11,140,1700,Sports,Evening,mobile,380,65,Yes\n12,100,1300,Entertainment,Afternoon,Mobile,300,50,Yes\n13,55,500,Politics,morning,Desktop,30,3,No\n14,125,2000,TECH,Night,Mobile,410,72,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Normalize target variable 'viral' values to binary labels (Yes=1, No=0)", "Impute missing values in 'user_followers' with median", "Standardize 'post_length' and 'user_followers'", "One-hot encode categorical variables: 'post_topic', 'post_time', and 'device_type' (standardize capitalization first)", "Split data into training (80%) and test (20%) sets", "Train a RandomForestClassifier on the training set", "Perform grid search to tune max_depth and n_estimators parameters", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Extract and report top 5 feature importances", "Generate confusion matrix for test predictions"], "model_results": {"accuracy": 0.86, "precision": 0.89, "recall": 0.84, "f1": 0.86, "top_feature_importances": {"shares": 0.32, "likes": 0.28, "user_followers": 0.15, "post_topic_Entertainment": 0.08, "post_time_Morning": 0.07}, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict the occurrence of extreme heat days based on weather and environmental features.", "raw_table": "Date,Region,Avg_Temperature,Humidity,Wind_Speed,Land_Cover_Type,Extreme_Heat\n2024-06-01,North,35.2,45,12.3,Urban,Yes\n2024-06-02,North,33.8,50,10.1,urban,Yes\n2024-06-03,South,28.4,65,8.2,Forest,No\n2024-06-04,East,31.0,55,,Agriculture,No\n2024-06-05,West,36.5,40,15.0,Urban,Yes\n2024-06-06,South,27.9,70,7.5,forest,No\n2024-06-07,East,29.3,60,9.0,Agriculture,No\n2024-06-08,North,34.7,47,11.8,Urban,Yes\n2024-06-09,West,35.0,43,12.5,Urban,Yes\n2024-06-10,South,30.2,58,9.5,Forest,No\n2024-06-11,East,31.5,54,10.0,Agriculture,No\n2024-06-12,West,37.1,38,16.2,Urban,Yes\n2024-06-13,North,NaN,46,13.0,Urban,Yes\n2024-06-14,South,29.8,65,8.7,Forest,No\n2024-06-15,East,30.7,59,9.9,Agriculture,No", "model_steps": ["Load the dataset and parse the Date column as a datetime object", "Identify and impute missing values in Wind_Speed with the median value", "Standardize Avg_Temperature, Humidity, and Wind_Speed numeric features", "Normalize Region and Land_Cover_Type categorical variables via one-hot encoding, correcting inconsistent capitalization", "Split data into training (80%) and test sets (20%) stratified by the target 'Extreme_Heat'", "Train a RandomForestClassifier on the training set with default hyperparameters", "Perform grid search over n_estimators (50, 100, 200) and max_depth (5, 10, None) using 5-fold cross-validation", "Evaluate model performance on the test set using accuracy, F1 score, precision, and recall", "Compute and analyze the confusion matrix", "Extract and rank top 3 feature importances from the final model"], "model_results": {"accuracy": 0.87, "f1": 0.85, "precision": 0.83, "recall": 0.87, "confusion_matrix": {"True_Positive": 7, "True_Negative": 6, "False_Positive": 1, "False_Negative": 1}, "top_feature_importances": {"Avg_Temperature": 0.38, "Humidity": 0.22, "Region_North": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Predict the likelihood of a customer making a purchase during their visit based on browsing and demographic data.", "raw_table": "session_id,age,gender,browser_time_minutes,num_pages_viewed,referral_source,device_type,purchase\nS001,25,Male,15,5,Google,Mobile,Yes\nS002,NaN,Female,8.5,3,facebook,Desktop,No\nS003,34,Female,20,10,Direct,Mobile,YES\nS004,42,male,5,2,Google,Tablet,No\nS005,29,Female,NaN,7,Instagram,Mobile,Yes\nS006,31,Male,12,NaN,Google,desktop,No\nS007,27,Female,18,6,Facebook,Mobile,Yes\nS008,22,Male,9,4,Direct,Mobile,No\nS009,45,Female,25,NaN,instagram,Desktop,Yes\nS010,38,Male,11,5,Google,Tablet,No\nS011,NaN,Female,14,8,Google,Mobile,Yes\nS012,30,Male,7,3,Direct,Mobile,No\nS013,26,Female,16,6,Facebook,Mobile,Yes\nS014,35,Male,10,5,Google,Tablet,No", "model_steps": ["Load dataset and inspect for missing and inconsistent values", "Impute missing numeric values using median imputation", "Normalize inconsistent categorical values (e.g., unify gender casing and device_type)", "Convert target variable 'purchase' to binary (Yes=1, No=0)", "One-hot encode categorical variables: gender, referral_source, device_type", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features: age, browser_time_minutes, num_pages_viewed", "Train a Logistic Regression classifier on the training set", "Perform 5-fold cross-validation to tune regularization strength (C parameter)", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Analyze feature coefficients to identify top predictors"], "model_results": {"accuracy": 0.79, "precision": 0.82, "recall": 0.75, "f1": 0.78, "top_features": {"num_pages_viewed": 0.56, "browser_time_minutes": 0.47, "referral_source_Google": 0.32, "device_type_Mobile": 0.29, "gender_Female": 0.15}, "hyperparameters": {"C": 1.0, "solver": "lbfgs", "max_iter": 100}}}
{"purpose": "Build a classification model to predict whether a taxi ride will have a high tip (tip greater than 20% of fare).", "raw_table": "ride_id,passenger_count,trip_distance,payment_type,store_and_fwd_flag,rate_code,tip_amount,fare_amount,high_tip\n1,1,3.2,Credit,No,Standard,2.5,12.0,0\n2,2,1.1,CASH,No,Standard,1,8.0,0\n3,1,7.8,Credit,No,Standard,5.5,25.0,1\n4,3,,credit,Yes,standard,3.0,15.0,0\n5,1,2.5,CASH,No,Standard,0,10.0,0\n6,2,4.0,credit,No,Standard,5.0,20.0,1\n7,1,0.8,Cash,No,Standard,,6.0,0\n8,4,10.5,Credit,No,Standard,7.0,40.0,1\n9,1,3.0,Credit,No,Standard,1.5,13.0,0\n10,2,6.5,cash,No,Standard,4.0,22.0,1\n11,1,1.7,Credit,No,Standard,1.2,9.0,0\n12,3,5.0,Cash,No,Standard,3.5,18.0,1\n13,1,3.4,Credit,No,Standard,2.0,14.0,0\n14,2,2.2,cash,No,Standard,0.5,11.0,0\n15,,4.1,Credit,No,Standard,4.5,19.0,1", "model_steps": ["Load the CSV data into a DataFrame.", "Identify and handle missing values in 'trip_distance', 'passenger_count', and 'tip_amount' by imputing median for numeric columns.", "Normalize capitalization in categorical columns 'payment_type' and 'store_and_fwd_flag' for consistency.", "Convert 'payment_type' and 'store_and_fwd_flag' into categorical variables and one-hot encode them.", "Split data into train and test sets with an 80/20 ratio.", "Standardize numeric features: 'passenger_count', 'trip_distance', 'fare_amount', and 'tip_amount'.", "Train a RandomForestClassifier to predict 'high_tip'.", "Tune 'n_estimators' parameter by grid search over [50, 100, 150].", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score.", "Generate feature importance rankings from the trained RandomForest model."], "model_results": {"accuracy": 0.87, "precision": 0.83, "recall": 0.79, "f1": 0.81, "top_features": {"tip_amount": 0.35, "fare_amount": 0.22, "trip_distance": 0.15, "payment_type_Credit": 0.1, "passenger_count": 0.08}, "best_hyperparameters": {"n_estimators": 100}}}
{"purpose": "Develop a regression model to predict hourly electricity consumption based on weather and operational features.", "raw_table": "Hour,Temperature_C,Weather_Condition,Day_Type,Previous_Hour_Consumption_kWh,Energy_Consumption_kWh\n0,15.2,Clear,weekday,120.5,125.7\n1,14.8,clear,weekday,118.2,121.3\n2,14.5,Cloudy,weekday,115.0,117.6\n3,14.3,,weekday,113.5,115.0\n4,14.2,cloudy,Weekday,110.8,112.3\n5,14.5,Rain,weekday,109.7,111.5\n6,15.0,Rain,weekend,115.2,120.1\n7,16.0,Clear,weekend,130.0,135.4\n8,18.5,Clear,Weekday,140.4,145.7\n9,20.0,Sunny,Weekday,150.0,155.2\n10,22.5,Sunny,Weekday,160.5,165.8\n11,23.0,SUNNY,weekday,165.0,170.1\n12,23.5,Rain,Weekday,170.0,175.3\n13,24.0,Cloudy,weekday,175.5,180.0", "model_steps": ["Load data and identify missing and inconsistent values", "Impute missing Weather_Condition at hour 3 using mode value", "Normalize casing in Weather_Condition to lowercase for consistency", "One-hot encode Weather_Condition and Day_Type categorical variables", "Split data into training (80%) and testing (20%) sets by hour", "Standardize numeric features: Temperature_C and Previous_Hour_Consumption_kWh", "Train a Gradient Boosting Regressor to predict Energy_Consumption_kWh", "Tune hyperparameters: number of estimators and learning rate using cross-validation", "Evaluate model performance on test set using RMSE, MAE, and R2 metrics", "Generate predicted vs actual consumption plot for test data"], "model_results": {"rmse": 2.45, "mae": 1.85, "r2": 0.92, "best_hyperparameters": {"n_estimators": 100, "learning_rate": 0.1}, "top_feature_importances": {"Previous_Hour_Consumption_kWh": 0.55, "Temperature_C": 0.3, "Weather_Condition_clear": 0.08, "Weather_Condition_rain": 0.04, "Day_Type_weekday": 0.03}}}
{"purpose": "Build a regression model to estimate house sale prices based on property and location features.", "raw_table": "HouseID,Bedrooms,Bathrooms,Area_sqft,Neighborhood,YearBuilt,Condition,HasGarage,SalePrice\n1,3,2,1500,Downtown,1995,Good,Yes,350000\n2,4,3,2000,Suburb,2005,Excellent,yes,450000\n3,2,1,850,Suburb,1980,Fair,No,200000\n4,3,,1200,Midtown,2010,good,Yes,320000\n5,5,4,3000,Downtown,2018,Excellent,YES,650000\n6,3,2,NaN,Suburb,1999,Poor,No,280000\n7,4,3,1800,Midtown,2002,GOOD,Yes,400000\n8,2,1,900,midtown,1975,Fair,No,180000\n9,3,2,1600,Downtown,2000,Excellent,Yes,380000\n10,,2,1400,Suburb,2015,Good,Yes,360000\n11,4,3,2200,Suburb,2012,Excellent,No,470000\n12,3,2,1550,Downtown,NaN,Fair,Yes,330000\n13,3,2,1400,Midtown,2008,Poor,No,300000\n14,4,3,2500,DOWNTOWN,2019,Excellent,Yes,620000", "model_steps": ["Load CSV data and identify target variable as SalePrice", "Handle missing numeric values by imputing median values for Bathrooms, Area_sqft, and YearBuilt", "Clean and standardize categorical variables capitalization (e.g., Neighborhood, Condition, HasGarage)", "Encode categorical variables: One-hot encode Neighborhood, Condition, and binary encode HasGarage (Yes=1, No=0)", "Split data into training (80%) and testing (20%) sets randomly", "Standardize numeric features Bedrooms, Bathrooms, Area_sqft, and YearBuilt using training set statistics", "Train a RandomForestRegressor with 100 trees on the training set", "Perform grid search over max_depth parameter with values [5, 10, None] using 5-fold cross-validation", "Select best model based on lowest RMSE on validation folds", "Evaluate final model on test set and compute RMSE, MAE, and R2", "Identify top 3 feature importances from the trained RandomForest model"], "model_results": {"rmse": 28000, "mae": 22000, "r2": 0.85, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}, "top_feature_importances": {"Area_sqft": 0.35, "Neighborhood_Downtown": 0.2, "YearBuilt": 0.15}}}
{"purpose": "Predict the likelihood of extreme rainfall occurrence based on meteorological and geographic features.", "raw_table": "temperature_c,humidity_percent,wind_speed_kmh,region,soil_type,rainfall_extreme\n22.5,80,15,North,Loam,Yes\n19.0,75,12,South,Clay,No\n25.3,90,20,East,Sand,YES\n18.2,,10,west,Loam,No\n21.0,85,14,North,Clay,Yes\n20,88,NaN,South,sand,No\n23.5,92,18,East,Loam,Yes\n17.8,70,11,West,Clay,No\n24.1,89,16,North,Loam,Yes\n19.5,77,13,South,Clay,No\n22.0,85,15,East,Sand,Yes\n16.7,65,9,west,Clay,No\n20.5,82,14,North,Loam,Yes\n21.7,87,17,East,Loam,Yes", "model_steps": ["Correct inconsistent capitalization in 'region' and 'soil_type' columns", "Impute missing numeric values in 'humidity_percent' and 'wind_speed_kmh' using median imputation", "Convert target variable 'rainfall_extreme' to binary (Yes=1, No=0)", "One-hot encode categorical variables 'region' and 'soil_type'", "Split data into training (80%) and testing sets (20%) with stratification on target", "Standardize numeric features: temperature_c, humidity_percent, wind_speed_kmh", "Train a RandomForestClassifier with 100 trees on the training data", "Tune max_depth hyperparameter using 5-fold cross-validation", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "top_feature_importances": {"humidity_percent": 0.34, "region_North": 0.22, "temperature_c": 0.18}, "best_hyperparameters": {"max_depth": 6}}}
{"purpose": "Build a classification model to predict whether a taxi trip will have a high tip amount.", "raw_table": "trip_id,trip_distance,payment_type,passenger_count,trip_time_in_secs,taxi_type,tip_amount_high\n1,2.5,Card,1,600,Yellow,Yes\n2,5.1,cash,2,900,Green,No\n3,3.2,CARD,1,720,Yellow,Yes\n4,7.8,Cash,3,1100,Green,No\n5,1.0,CARD,,450,Yellow,Yes\n6,4.5,cash,2,890,green,No\n7,2.2,Card,1,530,Yellow,yes\n8,3.0,Cash,1,,Yellow,No\n9,6.7,card,3,1050,Green,No\n10,5.5,Cash,2,980,Yellow,No\n11,4.0,CARD,1,800,Yellow,Yes\n12,3.8,cash,2,840,Green,No\n13,2.0,Card,1,600,Yellow,Yes\n14,8.1,Cash,3,1150,green,No\n15,3.3,CARD,1,750,Yellow,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Clean and standardize the 'payment_type' and 'taxi_type' columns to lowercase", "Impute missing 'passenger_count' with the mode and 'trip_time_in_secs' with the median", "Convert 'tip_amount_high' target variable to binary 1 (Yes) and 0 (No)", "One-hot encode 'payment_type' and 'taxi_type' categorical features", "Split data into training (80%) and testing (20%) sets with stratification on target", "Standardize numeric features: 'trip_distance' and 'trip_time_in_secs'", "Train a RandomForestClassifier with 100 trees and max_depth=5", "Evaluate model performance on test set calculating accuracy, precision, recall, and F1 score", "Compute and display the confusion matrix", "Determine feature importances from the trained model"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.8, "f1": 0.82, "confusion_matrix": {"true_negatives": 20, "false_positives": 3, "false_negatives": 5, "true_positives": 22}, "top_feature_importances": {"trip_distance": 0.35, "trip_time_in_secs": 0.3, "payment_type_card": 0.15, "taxi_type_yellow": 0.1, "payment_type_cash": 0.1}, "hyperparameters": {"n_estimators": 100, "max_depth": 5, "random_state": 42}}}
{"purpose": "Predict whether a movie will be a box office hit based on its production and genre features.", "raw_table": "MovieID,Genre,DirectorExperience,ProductionBudget,ReleaseSeason,LeadActorPopularity,MarketingSpend,BoxOfficeHit\n1,Action,5,150,summer,High,20,Yes\n2,Comedy,2,30,winter,medium,5,No\n3,Drama,,80,Fall,Low,8,No\n4,action,7,200,Summer,high,25,Yes\n5,Horror,1,15,spring,Low,2,No\n6,Comedy,3,50,Winter,Medium,7,No\n7,Drama,4,120,Fall,Medium,15,Yes\n8,Thriller,6,130,summer,High,18,Yes\n9,Horror,2,10,Spring,low,3,No\n10,Thriller,8,160,Summer,High,22,Yes\n11,Comedy,three,40,Winter,Medium,6,No\n12,Action,5,180,Summer,High,20,Yes\n13,Drama,4,100,Fall,Medium,12,No\n14,Horror,2,25,spring,Low,,No", "model_steps": ["Load dataset and inspect for missing and inconsistent values", "Standardize capitalization and fix inconsistent Genre and ReleaseSeason values", "Impute missing DirectorExperience with median value", "Convert LeadActorPopularity to ordered categorical variable (Low < Medium < High)", "One-hot encode Genre and ReleaseSeason categorical variables", "Convert target BoxOfficeHit to binary (Yes=1, No=0)", "Split the data into train (80%) and test (20%) sets", "Standardize numeric features: DirectorExperience, ProductionBudget, MarketingSpend, LeadActorPopularity", "Train a RandomForestClassifier to predict BoxOfficeHit", "Perform grid search over number of trees (n_estimators) and max_depth", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.81, "recall": 0.85, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"ProductionBudget": 0.35, "LeadActorPopularity": 0.25, "MarketingSpend": 0.2}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Build a classification model to predict whether a taxi trip will have a high tip (>=20% of fare).", "raw_table": "trip_id,trip_distance,passenger_count,pickup_neighborhood,day_of_week,tip_percentage,high_tip\n1,3.2,1,Downtown,monday,18.5,No\n2,5.1,2,Midtown,Tuesday,25.0,Yes\n3,2.0,1,midtown,Wednesday,15.0,No\n4,7.8,3,Uptown,Thursday,22.3,Yes\n5,1.5,1,Downtown,Friday,,No\n6,4.3,2,Suburb,Saturday,30.0,Yes\n7,-,1,suburb,Sunday,10.0,No\n8,6.0,4,Uptown,Monday,20.0,Yes\n9,3.7,3,Downtown,Tuesday,17.5,No\n10,5.5,2,Midtown,Wednesday,21.0,Yes\n11,2.8,1,Suburb,thursday,19.0,No\n12,4.1,2,Midtown,Friday,23.5,Yes\n13,3.0,2,Uptown,Saturday,16.0,No\n14,8.2,3,Downtown,Sunday,28.0,Yes", "model_steps": ["Load the dataset and identify missing and inconsistent values", "Impute missing trip_distance with median value and correct inconsistent capitalization in pickup_neighborhood and day_of_week columns", "Convert tip_percentage to numeric and drop rows where trip_distance is missing or invalid", "Create binary target variable 'high_tip' from tip_percentage (Yes if tip_percentage >= 20, else No)", "Split data into train and test sets with 80% train and 20% test", "One-hot encode categorical variables: pickup_neighborhood and day_of_week", "Standardize numeric features: trip_distance and passenger_count", "Train a RandomForestClassifier to predict high_tip", "Perform grid search over number of estimators [50, 100] and max_depth [5, 10]", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix and report top 3 feature importances"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.81, "recall": 0.88, "confusion_matrix": [[7, 2], [1, 10]], "top_feature_importances": {"tip_percentage": 0.0, "trip_distance": 0.38, "passenger_count": 0.12, "pickup_neighborhood_Uptown": 0.15, "pickup_neighborhood_Midtown": 0.1, "pickup_neighborhood_Suburb": 0.05, "pickup_neighborhood_Downtown": 0.05, "day_of_week_Monday": 0.05, "day_of_week_Tuesday": 0.03, "day_of_week_Wednesday": 0.02, "day_of_week_Thursday": 0.03, "day_of_week_Friday": 0.02}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Build a classification model to predict the occurrence of heatwaves based on meteorological and geographic features.", "raw_table": "Region,AvgTemp,CumulativeRainfall,WindSpeed,SoilType,Urbanization,Heatwave\nNorth,32.5,120.4,15.2,Clay,High,Yes\nsouth,30.1,85.3,12.7,Sand,Medium,no\nEast,29.8,,14.1,Loam,low,No\nWEST,33.2,110.0,16.0,Clay,High,Yes\nNorth,31.6,95.5,NaN,Silt,Medium,yes\nEast,28.7,80.0,13.6,sand,Low,No\nSouth,30.0,90.1,14.0,Loam,Medium,No\nWest,32.8,105.6,15.4,clay,High,YES\nnorth,31.2,100.8,14.7,Silt,Medium,No\nSouth,29.9,88.9,13.0,Loam,Medium,no\nEast,30.5,92.3,13.8,Sand,Low,No\nWEST,33.0,115.2,16.3,Clay,High,Yes\nSouth,30.3,87.7,12.9,Loam,Medium,No", "model_steps": ["Load the CSV data into a dataframe", "Clean and standardize categorical values for 'Region', 'SoilType', 'Urbanization', and 'Heatwave' columns (e.g., unify capitalization)", "Impute missing numeric values in 'CumulativeRainfall' and 'WindSpeed' using median values", "Encode categorical variables using one-hot encoding", "Convert target variable 'Heatwave' to binary (Yes=1, No=0)", "Split dataset into training (80%) and testing (20%) sets with stratification on the target", "Standardize numeric features to zero mean and unit variance", "Train a RandomForestClassifier with 100 trees and maximum depth of 5", "Perform 5-fold cross-validation to fine-tune max_depth parameter", "Evaluate the model on the test set calculating accuracy, F1 score, precision, and recall", "Generate the confusion matrix for test predictions"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.79, "recall": 0.84, "confusion_matrix": [[15, 3], [4, 22]], "top_feature_importances": {"AvgTemp": 0.35, "Urbanization_High": 0.23, "CumulativeRainfall": 0.17, "WindSpeed": 0.15, "SoilType_Clay": 0.1}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a field will have high or low crop yield based on soil, weather, and farming practice features.", "raw_table": "FieldID,SoilType,Rainfall_mm,AvgTemp_C,FertilizerType,YieldCategory\n1,Loam,200,22.5,Organic,High\n2,Sandy,150,25.1,Chemical,Low\n3,Clay,,21.0,organic,High\n4,Loam,180,23.3,None,Low\n5,SANDY,170,24.0,Chemical,Low\n6,Clay,210,20.5,Organic,High\n7,Loam,190,22.0,Chemical,High\n8,Silt,160,23.5,Organic,low\n9,Silt,175,22.8,Chemical,High\n10,Clay,155,21.5,,Low\n11,Loam,185,22.7,Organic,High\n12,Sandy,165,24.2,Chemical,Low\n13,Silt,200,23.0,Organic,High\n14,Clay,180,21.0,Chemical,Low", "model_steps": ["Load the CSV data into a pandas DataFrame", "Standardize capitalization of categorical variables SoilType, FertilizerType, and YieldCategory to ensure consistency", "Impute missing Rainfall_mm values using the median rainfall", "Impute missing FertilizerType values with the mode", "Encode SoilType and FertilizerType using one-hot encoding", "Encode target variable YieldCategory as binary: 'High' = 1, 'Low' = 0", "Split data into training (80%) and test (20%) sets with stratification on target", "Standardize numeric features Rainfall_mm and AvgTemp_C using StandardScaler fitted on training data", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search over max_depth values [3,5,7] using 5-fold cross-validation", "Evaluate the best model on the test set computing accuracy, precision, recall, and F1-score", "Generate and analyze the confusion matrix", "Identify and report top 3 feature importances from the Random Forest"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.84, "f1": 0.86, "confusion_matrix": [[9, 2], [2, 11]], "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}, "top_feature_importances": {"Rainfall_mm": 0.32, "SoilType_Loam": 0.22, "FertilizerType_Organic": 0.19}}}
{"purpose": "Build a classification model to predict extreme wildfire risk based on climate and vegetation conditions.", "raw_table": "Temperature_C,Precipitation_mm,Wind_Speed_kmh,Vegetation_Type,Soil_Moisture_Percent,Region,Wildfire_Risk\n29.5,12.3,15.2,Forest,35,North,High\n32.1,NaN,20.5,grassland,22,South,Medium\n28.3,18.0,12.0,Shrubland,NA,East,Low\n31.0,7.8,30.1,Forest,40,North,High\n27.5,10,18.7,Grassland,28,South,Medium\n33.2,11.0,14.9,forest,33,North,High\n26.0,15.5,10.0,Shrubland,25,East,Low\n30.5,9.7,25.3,Grassland,27.5,South,Medium\n29.0,8.5,19.0,Forest,38,North,High\n31.8,,22.0,Forest,36,north,High\n27.8,14.2,11.5,Shrubland,30,East,Low\n32.5,7.0,17.0,Grassland,29.0,South,Medium\n28.7,12.0,13.5,Forest,,North,High\n", "model_steps": ["Load the CSV data into a DataFrame", "Clean missing values by imputing median for numeric columns and most frequent for categorical columns", "Normalize capitalization in the Vegetation_Type and Region columns to ensure consistency", "One-hot encode the Vegetation_Type and Region categorical variables", "Split data into train and test sets with an 80/20 ratio, stratifying by Wildfire_Risk", "Standardize numeric features: Temperature_C, Precipitation_mm, Wind_Speed_kmh, Soil_Moisture_Percent", "Train a RandomForestClassifier to predict Wildfire_Risk", "Perform grid search over number of estimators [50, 100] and max_depth [5, 10]", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Output top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.84, "precision": 0.83, "recall": 0.85, "top_feature_importances": {"Wind_Speed_kmh": 0.31, "Vegetation_Type_Forest": 0.25, "Soil_Moisture_Percent": 0.18}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Predict whether a social media post will go viral based on post characteristics and user engagement metrics.", "raw_table": "post_id,post_type,user_followers,post_length,hashtags_count,posted_time,engagement_level,is_viral\n1,Image,1200,150,3,Morning,High,Yes\n2,video,800,300,5,Afternoon,Medium,No\n3,Text,NaN,100,,night,Low,No\n4,Image,2500,200,2,Evening,High,Yes\n5,Video,1500,250,4,MORNING,Medium,No\n6,Text,700,80,1,Afternoon,,No\n7,Image,NaN,NaN,3,Night,Medium,Yes\n8,Video,1800,400,5,Evening,High,Yes\n9,Text,900,120,2,morning,Low,No\n10,Image,1100,150,NaN,Afternoon,High,Yes\n11,Video,1300,350,4,Evening,Medium,No\n12,Text,600,70,1,Morning,Low,No\n13,Image,2000,220,3,NIGHT,High,Yes\n14,Video,nan,300,5,Afternoon,Medium,No\n15,Text,850,110,,Evening,Low,No", "model_steps": ["Load the CSV data into a DataFrame", "Clean the data by fixing inconsistent capitalization in 'post_type' and 'posted_time', and impute missing numeric values with the median", "Fill missing categorical values in 'hashtags_count' and 'engagement_level' with mode or a placeholder", "Convert the target variable 'is_viral' to binary (Yes=1, No=0)", "Split data into 80% training and 20% test sets", "One-hot encode categorical features 'post_type' and 'posted_time'", "Standardize numeric features: 'user_followers', 'post_length', 'hashtags_count'", "Train a RandomForestClassifier with default parameters on the training set", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Compute and visualize the confusion matrix", "Identify top 3 feature importances from the trained model", "Generate predictions on the test set and summarize predicted vs actual viral posts"], "model_results": {"accuracy": 0.8, "precision": 0.75, "recall": 0.78, "f1": 0.76, "confusion_matrix": {"true_positive": 7, "true_negative": 8, "false_positive": 2, "false_negative": 2}, "top_feature_importances": {"engagement_level_High": 0.28, "user_followers": 0.22, "post_type_Image": 0.15}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a taxi trip will have a tip greater than 20% of the fare amount.", "raw_table": "trip_id,passenger_count,trip_distance,store_and_fwd_flag,payment_type,trip_duration_minutes,tip_percentage,high_tip\n1,2,3.4,N,NOCARD,12,18,No\n2,1,1.2,Y,CASH,5,25,Yes\n3,3,7.1,N,card,20,,Yes\n4,2,4.0,n,Card,15,15,No\n5,NA,2.5,Y,cash,8,22,Yes\n6,1,5.3,N,Cash,17,10,No\n7,4,6.0,Y,card,18,30,Yes\n8,1,3.3,N,NOCARD,13,19,No\n9,2,3.1,N,Card,11,NA,No\n10,3,5.5,Y,CASH,NA,28,Yes\n11,2,4.7,n,cash,16,24,Yes\n12,1,1.0,N,card,4,12,No", "model_steps": ["Load the dataset from CSV and parse the columns appropriately", "Handle missing values in trip_duration_minutes by imputing median values", "Standardize inconsistent capitalization and fix mixed types in store_and_fwd_flag and payment_type columns", "Impute missing passenger_count values with the mode", "Convert categorical variables (store_and_fwd_flag, payment_type) using one-hot encoding", "Split the data into training and test sets using an 80/20 split", "Standardize numeric features: passenger_count, trip_distance, trip_duration_minutes", "Train a RandomForestClassifier to predict high_tip", "Tune the model using grid search on max_depth and n_estimators", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Extract and report the top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.79, "recall": 0.83, "confusion_matrix": {"true_positive": 7, "true_negative": 8, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"tip_percentage": 0.36, "trip_distance": 0.22, "trip_duration_minutes": 0.18}, "best_hyperparameters": {"max_depth": 6, "n_estimators": 100}}}
{"purpose": "Predict whether a student will pass the final exam based on their study habits and demographic information.", "raw_table": "StudentID,Hours_Studied,Attendance_Percentage,Parental_Education,Gender,Previous_Grade,Passed_Final\n1,15,90,College,Male,85,Yes\n2,8,75,high school,Female,70,No\n3,20,95,College,Male,88,Yes\n4,5,60,No formal education,Female,55,No\n5,12,85,College,Female,78,Yes\n6,NaN,80,High School,Male,72,No\n7,10,88,college,Female,80,Yes\n8,3,50,,Male,60,No\n9,18,92,High school,Male,90,Yes\n10,7,65,College,Female,65,No\n11,14,85,College,Female,82,Yes\n12,6,70,High School,Male,68,No\n13,16,95,College,Female,89,Yes\n14,NaN,75,College,Male,,No", "model_steps": ["Load the dataset and identify the target variable 'Passed_Final'.", "Handle missing values by imputing numeric columns with median and categorical columns with mode.", "Normalize capitalization inconsistencies in 'Parental_Education' and 'Gender' columns.", "Convert target variable 'Passed_Final' to binary label (Yes=1, No=0).", "One-hot encode categorical variables 'Parental_Education' and 'Gender'.", "Split data into training and testing sets with an 80/20 ratio.", "Standardize numeric features 'Hours_Studied', 'Attendance_Percentage', and 'Previous_Grade'.", "Train a Logistic Regression classifier on the training set.", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score.", "Analyze feature coefficients to identify most influential predictors."], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "top_feature_importances": {"Attendance_Percentage": 0.35, "Hours_Studied": 0.3, "Previous_Grade": 0.2, "Parental_Education_College": 0.1, "Gender_Female": 0.05}}}
{"purpose": "Predict whether a loan applicant will default on their loan within one year.", "raw_table": "ApplicantID,Age,Income,EmploymentStatus,LoanAmount,LoanPurpose,CreditScore,Defaulted\n1,34,58000,Full-time,15000,Home Improvement,720,No\n2,27,NaN,part-time,8000,Debt Consolidation,690,No\n3,45,120000,SELF-EMPLOYED,20000,Home Improvement,680,Yes\n4,52,85000,Full-Time,25000,Car Purchase,NaN,No\n5,23,40000,Unemployed,5000,Education,620,Yes\n6,37,65000,full-time,18000,Home improvement,710,No\n7,29,73000,Part-Time,NaN,Debt consolidation,700,No\n8,41,98000,Self-employed,22000,Car purchase,690,Yes\n9,33,,Full-time,14000,Education,705,No\n10,50,90000,Full-time,24000,Car Purchase,700,Yes\n11,27,45000,unemployed,7000,Education,615,Yes\n12,39,67000,Part-time,16000,Debt Consolidation,695,No\n13,48,82000,Full-time,21000,Home Improvement,715,No\n14,31,60000,Full-time,13000,Home improvement,705,No", "model_steps": ["Load the dataset and inspect for missing values and inconsistent capitalization in categorical columns", "Fill missing numeric values (Income, LoanAmount, CreditScore) using median imputation", "Standardize capitalization and unify categories in EmploymentStatus and LoanPurpose columns", "One-hot encode the categorical variables EmploymentStatus and LoanPurpose", "Split the data into train and test sets with an 80/20 ratio, stratifying on the target Defaulted", "Standardize numeric features: Age, Income, LoanAmount, CreditScore", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search over max_depth parameter (range 3 to 10) to optimize model", "Evaluate the final model on the test set using accuracy, precision, recall, and F1 score", "Generate and inspect the confusion matrix", "Identify top 3 feature importances from the RandomForest model"], "model_results": {"accuracy": 0.85, "precision": 0.78, "recall": 0.72, "f1": 0.75, "confusion_matrix": {"true_negative": 18, "false_positive": 4, "false_negative": 7, "true_positive": 18}, "top_feature_importances": {"CreditScore": 0.32, "LoanAmount": 0.25, "EmploymentStatus_Self-employed": 0.12}, "best_hyperparameters": {"max_depth": 7, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict whether a taxi trip will exceed 20 minutes in duration.", "raw_table": "trip_id,passenger_count,pickup_hour,day_of_week,weather_condition,pickup_location,trip_duration_minutes,long_trip\n1,2,8,Monday,Clear,Downtown,15,No\n2,1,18,FRIDAY,Rain,Suburb,25,Yes\n3,4,14,Wednesday,Clear,Airport,30,Yes\n4,3,NaN,Tuesday,Fog,Downtown,12,No\n5,1,9,Monday,clear,Suburb,20,No\n6,5,20,Thursday,Snow,Downtown,35,Yes\n7,2,7,Saturday,Fog,Suburb,18,No\n8,3,13,Sunday,Clear,Downtown,mixed,No\n9,1,22,Monday,Rain,Airport,28,Yes\n10,3,16,Tuesday,Clear,Downtown,10,No\n11,2,11,Wednesday,Clear,Suburb,22,Yes\n12,,19,Thursday,Rain,Downtown,27,Yes\n13,1,6,Friday,Clear,Airport,14,No\n14,3,17,Saturday,Clear,Suburb,19,No", "model_steps": ["Load data and identify target variable 'long_trip' as binary classification target", "Handle missing values: impute numeric missing 'passenger_count' and 'pickup_hour' with median", "Fix inconsistent capitalization in 'weather_condition' column by converting all to lowercase", "Parse and correct mixed-type 'trip_duration_minutes' entries; handle 'mixed' as missing and impute with median", "Encode categorical variables 'day_of_week', 'weather_condition', and 'pickup_location' using one-hot encoding", "Split data into train and test sets with 80/20 ratio, stratifying on 'long_trip'", "Standardize numeric features: 'passenger_count' and 'pickup_hour'", "Train a RandomForestClassifier with 100 trees", "Perform grid search tuning max_depth parameter over [5, 10, 15]", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and feature importance ranking"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": [[6, 1], [2, 5]], "top_feature_importances": {"trip_duration_minutes": 0.25, "pickup_hour": 0.2, "weather_condition_clear": 0.15, "day_of_week_monday": 0.1, "pickup_location_downtown": 0.08}, "best_max_depth": 10}}
{"purpose": "Predict whether a movie will be rated as a blockbuster based on its features.", "raw_table": "MovieID,Genre,Director,ProductionBudget,RuntimeMinutes,ReleaseMonth,LeadActorPopularity,Blockbuster\n1,Action,Smith,150000000,130,July,85,Yes\n2,Comedy,Jones,40000000,95,December,70,No\n3,Drama,Brown,25000000,110,may,55,No\n4,Action,Smith,NaN,140,July,80,Yes\n5,Horror,Lee,15000000,90,october,60,No\n6,Comedy,Clark,45000000,100,December,72,No\n7,Action,Smith,180000000,150,July,95,Yes\n8,Drama,Williams,30000000,120,May,50,No\n9,Action,smith,170000000,135,July,88,Yes\n10,Comedy,Jones,35000000,100,,65,No\n11,Horror,Lee,16000000,85,October,58,No\n12,Drama,Brown,28000000,115,May,52,No\n13,Action,Smith,165000000,132,July,90,Yes\n14,Comedy,Clark,42000000,98,December,75,No", "model_steps": ["Load the CSV data into a DataFrame", "Clean the 'ProductionBudget' column by imputing missing values with the median", "Standardize casing in categorical columns such as 'Genre', 'Director', and 'ReleaseMonth' (e.g., converting all to lowercase)", "Impute missing 'ReleaseMonth' with the mode value 'July'", "Convert categorical columns 'Genre', 'Director', and 'ReleaseMonth' into one-hot encoded features", "Convert target variable 'Blockbuster' to binary (Yes=1, No=0)", "Split data into training set (80%) and test set (20%) with stratification on target", "Standardize numeric features 'ProductionBudget', 'RuntimeMinutes', and 'LeadActorPopularity'", "Train a RandomForestClassifier with 100 trees", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Extract and present feature importances from the trained model"], "model_results": {"accuracy": 0.92, "precision": 0.9, "recall": 0.95, "f1": 0.92, "top_feature_importances": {"ProductionBudget": 0.45, "LeadActorPopularity": 0.3, "ReleaseMonth_July": 0.1, "Genre_Action": 0.08, "RuntimeMinutes": 0.07}}}
{"purpose": "Predict whether a movie will be a box office hit based on its production and genre attributes.", "raw_table": "MovieID,Genre,DirectorExperience,ProductionBudgetMillions,LeadActorPopularity,ReleaseMonth,HasSequel,BoxOfficeHit\n1,Action,5,150,8.7,July,Yes,Yes\n2,comedy,2,30,6.5,December,No,No\n3,Drama,NA,20,7.8,March,No,No\n4,Action,7,200,9.0,July,YES,Yes\n5,Horror,3,15,5.3,October,No,No\n6,Comedy,4,50,7.2,August,No,Yes\n7,Drama,6,40,8.1,February,No,Yes\n8,Action,5,missing,9.5,May,Yes,Yes\n9,Horror,1,10,4.7,october,No,No\n10,Comedy,3,45,6.0,June,No,No\n11,Drama,4,35,7.9,April,No,Yes\n12,Action,8,180,9.2,July,Yes,Yes\n13,Comedy,3,40,6.3,November,No,No", "model_steps": ["Clean the 'DirectorExperience' column by imputing missing values using the median experience.", "Correct inconsistent capitalization in 'Genre' and 'ReleaseMonth' columns.", "Fill missing values in 'ProductionBudgetMillions' with the median budget.", "Convert 'HasSequel' and 'BoxOfficeHit' to binary variables (Yes=1, No=0).", "One-hot encode categorical variables: 'Genre' and 'ReleaseMonth'.", "Standardize numeric features: 'DirectorExperience', 'ProductionBudgetMillions', and 'LeadActorPopularity'.", "Split data into training (80%) and test (20%) sets.", "Train a RandomForestClassifier to predict 'BoxOfficeHit'.", "Perform grid search over 'max_depth' with values [3, 5, 7].", "Evaluate model on test set using accuracy, F1 score, precision, and recall.", "Generate and analyze the confusion matrix.", "Extract feature importances from the trained model."], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.79, "recall": 0.83, "confusion_matrix": {"true_positive": 5, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"LeadActorPopularity": 0.32, "ProductionBudgetMillions": 0.28, "Genre_Action": 0.15, "HasSequel": 0.1, "ReleaseMonth_July": 0.08}, "best_hyperparameter": {"max_depth": 5}}}
{"purpose": "Predict whether a student will pass or fail the final exam based on demographic and academic features.", "raw_table": "StudentID,Gender,Age,StudyHours,PreviousGrade,SchoolType,AttendanceRate,PassedFinal\n1,Male,17,15,85,Public,0.95,Pass\n2,Female,16,8,,Private,0.80,Fail\n3,Female,18,12,78,public,0.90,Pass\n4,Male,17,5,65,Private,missing,Fail\n5,Female,16,20,92,Public,0.98,Pass\n6,Male,17,7,70,Private,0.85,Fail\n7,Female,17,,88,Public,0.92,Pass\n8,male,16,10,75,Private,0.87,Fail\n9,Female,18,13,80,Public,0.93,Pass\n10,Female,17,9,missing,Private,0.89,Fail\n11,Male,16,14,77,Public,0.94,Pass\n12,Female,17,11,82,Private,0.91,Pass\n13,male,18,6,68,public,0.84,Fail\n14,Female,16,16,90,Public,0.96,Pass", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values in PreviousGrade, StudyHours, and AttendanceRate columns by imputing median for numeric and mode for categorical where applicable", "Standardize inconsistent capitalization in Gender and SchoolType columns to ensure consistency", "Encode categorical variables Gender and SchoolType using one-hot encoding", "Split data into training and testing sets with 80% for training and 20% for testing, stratifying on the target PassedFinal", "Standardize numeric features: Age, StudyHours, PreviousGrade, AttendanceRate", "Train a RandomForestClassifier to predict PassedFinal", "Perform grid search over max_depth parameter with values [3, 5, 7]", "Evaluate the model on the test set calculating accuracy, F1 score, precision, and recall", "Generate and interpret the confusion matrix", "Identify top 3 most important features influencing the prediction"], "model_results": {"accuracy": 0.85, "f1": 0.87, "precision": 0.84, "recall": 0.9, "confusion_matrix": {"true_positive": 7, "false_positive": 2, "true_negative": 6, "false_negative": 1}, "top_feature_importances": {"StudyHours": 0.35, "PreviousGrade": 0.3, "AttendanceRate": 0.2}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a movie will be a box office hit based on its production and genre features.", "raw_table": "MovieID,Genre,Director,Production_Budget_Million,Lead_Actor_Age,Release_Month,Runtime_Minutes,Box_Office_Hit\n1,Action,Smith,150,35,July,130,Yes\n2,Comedy,Wang,30,28,december,95,No\n3,Drama,Johnson,20,45,March,110,No\n4,Action,smith,200,40,July,140,Yes\n5,Comedy,O'Neill,25,38,December,100,No\n6,Drama,Johnson,NaN,50,April,115,No\n7,Action,Lee,180,37,July,135,Yes\n8,Comedy,Wang,35,29,august,90,No\n9,Drama,Johnson,22,47,March,112,No\n10,Action,Lee,170,36,July,138,Yes\n11,Comedy,O'Neill,28,39,December,105,No\n12,Drama,Smith,19,44,March,,No\n13,Action,Lee,190,38,July,132,Yes\n14,Comedy,O'Neill,27,NaN,December,98,No", "model_steps": ["Load the CSV data into a DataFrame", "Correct inconsistent capitalization in 'Director' and 'Release_Month' columns", "Impute missing numeric values in 'Production_Budget_Million', 'Lead_Actor_Age', and 'Runtime_Minutes' with median values", "Convert 'Box_Office_Hit' into binary target variable (Yes=1, No=0)", "One-hot encode categorical features: 'Genre', 'Director', 'Release_Month'", "Split data into training (80%) and testing (20%) sets", "Standardize numeric features: 'Production_Budget_Million', 'Lead_Actor_Age', 'Runtime_Minutes'", "Train a RandomForestClassifier with 100 estimators on the training set", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Compute feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.83, "precision": 0.89, "recall": 0.78, "f1": 0.83, "top_feature_importances": {"Production_Budget_Million": 0.35, "Genre_Action": 0.22, "Director_Lee": 0.15, "Release_Month_July": 0.12, "Runtime_Minutes": 0.08, "Lead_Actor_Age": 0.05, "Genre_Comedy": 0.03}}}
{"purpose": "Predict whether a manufactured part will fail quality inspection based on sensor readings and production parameters.", "raw_table": "Part_ID,Machine_ID,Operator_Shift,Temperature_C,Pressure_psi,Material_Grade,Defect_Flag\nP001,M01,Morning,75.2,30.5,A,No\nP002,M02,Evening,77.1,29.8,B,Yes\nP003,M01,NIGHT,74.8,31.0,a,No\nP004,M03,Morning,?,30.0,C,No\nP005,M02,Evening,76.5,28.5,B,YES\nP006,M01,Morning,75.0,30.2,A,No\nP007,M03,Night,74.9,31.1,c,No\nP008,M02,Morning,75.5,NaN,B,No\nP009,M01,Evening,76.0,30.7,A,Yes\nP010,M03,Night,75.3,30.9,C,No\nP011,M02,Morning,76.2,29.9,B,No\nP012,M01,EVENING,74.7,30.6,A,No\nP013,M03,Morning,75.1,31.2,C,Yes\nP014,M01,Morning,75.4,30.4,A,No", "model_steps": ["Load the data and identify target variable 'Defect_Flag' for binary classification", "Clean data by imputing missing numeric values with column medians", "Normalize inconsistent capitalization in 'Material_Grade' and 'Operator_Shift' columns", "Encode 'Material_Grade', 'Operator_Shift', and 'Machine_ID' using one-hot encoding", "Convert target 'Defect_Flag' to binary labels (Yes=1, No=0)", "Split data into training (80%) and test sets (20%) with stratification on target", "Standardize numeric features 'Temperature_C' and 'Pressure_psi' using training set statistics", "Train a Random Forest classifier with default hyperparameters", "Perform grid search over number of trees [50, 100] and max_depth [5, 10]", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Analyze feature importances from the best performing Random Forest model"], "model_results": {"accuracy": 0.83, "f1": 0.78, "precision": 0.75, "recall": 0.82, "confusion_matrix": {"true_positive": 9, "true_negative": 15, "false_positive": 3, "false_negative": 2}, "top_feature_importances": {"Material_Grade_A": 0.27, "Temperature_C": 0.22, "Pressure_psi": 0.18, "Operator_Shift_Morning": 0.12, "Machine_ID_M01": 0.08}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features and location.", "raw_table": "id,bedrooms,bathrooms,sqft_living,condition,zipcode,year_built,floors,price\n1,3,2,1180,Good,98103,1995,1,450000\n2,4,3,1980,Excellent,98052,2010,2,780000\n3,2,1.5,860,good,98115,1978,1,350000\n4,3,,1200,Fair,98052,1985,1,410000\n5,four,2,1500,Excellent,98103,2005,2,600000\n6,3,2,2000,good,98103,2000,2,720000\n7,5,3,2500,Excellent,98052,2018,2,980000\n8,3,2,1300,Fair,98115,1990,1,460000\n9,3,2,1700,,98052,1992,2,610000\n10,2,1,900,Good,98103,1980,1,395000\n11,3,2,1350,excellent,98052,1997,1,560000\n12,3,2,1400,Good,98115,2003,1,590000\n13,4,3,2200,Fair,98052,2011,2,820000", "model_steps": ["Load data and inspect for missing and inconsistent values", "Correct inconsistent capitalization in 'condition' column and standardize categories", "Impute missing 'bathrooms' values using median of similar properties", "Convert 'bedrooms' from text to numeric where needed and impute if necessary", "One-hot encode 'condition' and 'zipcode' categorical variables", "Split data into training (80%) and testing (20%) sets", "Standardize numeric features including 'sqft_living', 'year_built', and 'floors'", "Train a Gradient Boosting Regressor on the training set", "Perform grid search to tune 'n_estimators' and 'max_depth'", "Evaluate model performance on test set using RMSE, MAE, and R2 metrics", "Analyze top 5 feature importances from the trained model"], "model_results": {"rmse": 42000, "mae": 32000, "r2": 0.85, "best_hyperparameters": {"n_estimators": 150, "max_depth": 4}, "top_feature_importances": {"sqft_living": 0.42, "zipcode_98052": 0.15, "condition_Excellent": 0.12, "year_built": 0.1, "bedrooms": 0.07}}}
{"purpose": "Predict hourly energy consumption level category (Low, Medium, High) based on weather and operational features.", "raw_table": "Hour,Temperature_C,Humidity_Percent,Wind_Speed_mps,Day_Type,Energy_Source,Consumption_Level\n0,15.2,55,3.1,weekday,solar,Low\n1,14.8,58,2.9,weekday,Solar,low\n2,NaN,60,3.4,Weekday,solar,Medium\n3,13.5,65,3.0,weekday,wind,Medium\n4,13.1,67,2.8,holiday,wind,high\n5,12.8,70,3.1,Holiday,wind,High\n6,13.0,72,3.3,holiday,wind,Medium\n7,14.5,68,3.7,weekday,solar,Medium\n8,17.0,60,4.0,Weekday,solar,High\n9,19.5,55,4.5,weekday,solar,High\n10,21.0,50,4.8,weekday,solar,High\n11,22.3,47,5.0,Weekday,solar,High\n12,23.0,45,5.2,weekday,solar,High\n13,22.5,46,5.1,weekday,wind,High\n14,20.0,52,4.7,weekday,wind,Medium", "model_steps": ["Load dataset and identify target variable: Consumption_Level", "Fix inconsistent capitalization in categorical columns: Day_Type and Energy_Source", "Impute missing numeric value in Temperature_C using median temperature", "Encode categorical variables Day_Type and Energy_Source using one-hot encoding", "Split data into train and test sets with 80/20 ratio, stratified on Consumption_Level", "Standardize numeric features: Temperature_C, Humidity_Percent, Wind_Speed_mps", "Train a RandomForestClassifier to predict Consumption_Level", "Perform grid search over n_estimators [50, 100] and max_depth [5, 10]", "Evaluate model performance on test set using accuracy, F1-score, precision, and recall", "Extract and report top 3 feature importances"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.82, "recall": 0.79, "top_feature_importances": {"Temperature_C": 0.35, "Hour": 0.25, "Energy_Source_solar": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Predict whether a student will pass the final exam based on study habits and demographic information.", "raw_table": "StudentID,Hours_Studied,Attendance,Parental_Education,School_Type,Previous_Grade,Pass_Final\n1,15,High,Bachelor,Public,85,Yes\n2,7,medium,Master,private,78,yes\n3,NaN,Low,High School,Public,65,No\n4,20,High,master,Public,92,Yes\n5,5,Low,High school,Private,58,No\n6,10,Medium,Bachelor,Public,72,Yes\n7,12,Medium,Bachelor,Public,NaN,Yes\n8,3,low,High school,Private,50,No\n9,18,HIGH,Master,public,88,Yes\n10,8,Medium,Bachelor,Private,70,No\n11,14,Medium,Master,Public,80,Yes\n12,NaN,Medium,High School,Private,60,No\n13,11,High,Bachelor,Public,75,Yes\n14,6,Low,High school,Private,62,No\n15,16,High,Master,Public,90,Yes", "model_steps": ["Load the dataset and identify missing values", "Impute missing numeric values (Hours_Studied, Previous_Grade) with the median", "Standardize numeric features Hours_Studied and Previous_Grade", "Normalize Attendance and Parental_Education categorical features by fixing capitalization inconsistencies", "One-hot encode categorical variables: Attendance, Parental_Education, and School_Type", "Convert target variable 'Pass_Final' to binary labels (Yes=1, No=0)", "Split data into training and testing sets with an 80/20 ratio", "Train a Logistic Regression classifier on the training set", "Tune the regularization parameter C using 5-fold cross-validation", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Generate the confusion matrix for test predictions"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.9, "f1": 0.87, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_features": {"Hours_Studied": 0.78, "Attendance_High": 0.45, "Previous_Grade": 0.52, "Parental_Education_Master": 0.33}, "best_hyperparameter_C": 0.1}}
{"purpose": "Predict whether a social media post will go viral based on its attributes and user engagement features.", "raw_table": "post_id,user_type,post_length,image_included,hashtags_count,day_posted,avg_engagement,time_to_viral,is_viral\n1,Influencer,120,Yes,5,Monday,1500,2,Yes\n2,regular,300,no,2,Friday,230,7,No\n3,BUSINESS,45,Yes,0,Wednesday,500,4,No\n4,Influencer,,YES,7,Tuesday,2000,1,Yes\n5,regular,80,No,1,Thursday,120,10,No\n6,business,200,No,3,Monday,700,3,Yes\n7,Regular,150,yes,4,Sunday,450,6,No\n8,Influencer,90,Yes,NaN,Saturday,1800,2,Yes\n9,BUSINESS,250,No,5,Monday,900,5,No\n10,regular,130,No,2,Wednesday,300,8,No\n11,Influencer,110,YES,6,Friday,1600,1,Yes\n12,regular,100,yes,1,Tuesday,400,9,No\n13,business,210,no,4,Thursday,,4,No\n14,Influencer,95,Yes,3,Monday,1700,2,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Clean and normalize categorical columns: standardize capitalization in 'user_type' and 'image_included' columns", "Impute missing values in 'post_length', 'hashtags_count', and 'avg_engagement' using median or mode as appropriate", "Convert 'is_viral' target variable to binary numeric format (Yes=1, No=0)", "One-hot encode 'user_type' and 'day_posted' categorical variables", "Encode 'image_included' as binary (Yes=1, No=0)", "Split data into train and test sets (80% train, 20% test) with stratification on 'is_viral'", "Standardize numeric features: 'post_length', 'hashtags_count', 'avg_engagement', 'time_to_viral'", "Train a RandomForestClassifier with default parameters on the training set", "Evaluate accuracy, precision, recall, and F1 score on the test set", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "top_feature_importances": {"avg_engagement": 0.35, "time_to_viral": 0.25, "user_type_Influencer": 0.15}}}
{"purpose": "Predict whether a student will pass the final exam based on their study habits and demographic information.", "raw_table": "StudentID,Hours_Studied,Attendance,Gender,Previous_Grade,School_Type,Pass_Final\n1,12,85,Male,88,Public,Yes\n2,5,70,Female,76,public,No\n3,9,90,Female,85,Private,Yes\n4,7,,Male,65,Private,No\n5,15,95,Male,92,Public,Yes\n6,3,65,Female,58,Public,No\n7,11,80,Male,80,private,Yes\n8,8,75,Male,70,Public,No\n9,10,88,Female,84,Private,Yes\n10,N/A,60,Female,55,Public,No\n11,14,92,Male,90,Public,Yes\n12,6,78,Female,72,Private,No\n13,13,85,male,89,Public,Yes\n14,4,68,Female,60,Private,No\n15,7,82,Female,77,Public,Yes", "model_steps": ["Load CSV data and identify target variable 'Pass_Final' as binary classification", "Handle missing values by imputing median for 'Hours_Studied' and mean for 'Attendance'", "Normalize inconsistent capitalization in categorical columns 'School_Type' and 'Gender'", "Encode categorical variables 'Gender' and 'School_Type' using one-hot encoding", "Convert target variable 'Pass_Final' to binary format (Yes=1, No=0)", "Split data into training and test sets with 80% train and 20% test", "Standardize numeric features: 'Hours_Studied', 'Attendance', and 'Previous_Grade'", "Train a RandomForestClassifier with 100 trees on the training data", "Perform hyperparameter tuning on max_depth with values [5, 10, 15]", "Evaluate model performance using accuracy, precision, recall, and F1 score on the test set", "Generate and analyze confusion matrix to understand classification errors"], "model_results": {"accuracy": 0.87, "precision": 0.89, "recall": 0.84, "f1": 0.86, "top_feature_importances": {"Hours_Studied": 0.35, "Previous_Grade": 0.28, "Attendance": 0.2, "School_Type_Private": 0.1, "Gender_Male": 0.07}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}, "confusion_matrix": [[7, 1], [2, 5]]}}
{"purpose": "Predict whether a manufactured metal part will fail quality inspection based on production parameters.", "raw_table": "PartID,MachineID,OperatorShift,Temperature,Pressure,MaterialBatch,Defect,PassInspection\n1,M01,Morning,350,5.2,BatchA,No,Yes\n2,m02,Afternoon,355,5.0,BatchB,Yes,No\n3,M01,morning,360,5.1,BatchA,No,Yes\n4,M03,Night,370,4.9,BatchC,No,Yes\n5,M02,Afternoon,NaN,5.3,BatchB,Yes,No\n6,M01,Morning,365,5.2,batchA,No,Yes\n7,M03,Night,355,NaN,BatchC,Yes,No\n8,M02,Afternoon,358,5.4,BatchB,No,Yes\n9,M01,Morning,362,5.1,BatchD,No,Yes\n10,M03,Night,375,5.0,BatchC,Yes,No\n11,M02,Afternoon,360,5.3,BatchB,No,Yes\n12,M01,Morning,355,5.2,BatchA,No,Yes\n13,M01,Morning,361,5.1,BatchA,No,Yes\n14,m03,Night,370,5.1,BatchC,Yes,No", "model_steps": ["Load dataset and inspect for missing and inconsistent values", "Correct inconsistent capitalization in categorical columns (MachineID, OperatorShift, MaterialBatch)", "Impute missing numeric values using median for Temperature and Pressure", "Encode categorical variables using one-hot encoding", "Split data into training and test sets with 80% training and 20% test", "Standardize numeric features (Temperature, Pressure) using training set statistics", "Train a RandomForestClassifier to predict PassInspection (Yes/No)", "Perform grid search on number of trees (n_estimators) and max_depth", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate and analyze confusion matrix"], "model_results": {"accuracy": 0.86, "f1": 0.85, "precision": 0.88, "recall": 0.82, "confusion_matrix": {"true_positive": 6, "false_positive": 1, "true_negative": 5, "false_negative": 2}, "top_feature_importances": {"MaterialBatch_BatchB": 0.22, "Temperature": 0.2, "Pressure": 0.18, "OperatorShift_Morning": 0.15, "MachineID_M01": 0.12}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a manufactured part will fail quality inspection based on sensor measurements and production parameters.", "raw_table": "PartID,MachineID,Temperature,Pressure,Operator,Shift,MaterialBatch,Defect\n1,M01,75.2,120.5,alice,Day,BatchA,Yes\n2,M02,80.1,115.0,Bob,Night,BatchB,No\n3,m01,78.5,NaN,Charlie,day,BatchA,No\n4,M03,82.0,118.3,Dave,Day,BatchC,Yes\n5,M02,77.8,116.7,alice,Night,BatchB,No\n6,M01,NaN,121.2,Bob,Day,BatchA,Yes\n7,M03,79.5,119.0,Charlie,Night,BatchC,No\n8,M02,81.3,117.5,Dave,DAY,BatchB,Yes\n9,m01,76.0,114.8,alice,Day,BatchA,No\n10,M03,83.2,NaN,Bob,Night,BatchC,Yes\n11,M02,80.0,115.5,Charlie,Day,BatchB,No\n12,M01,77.1,120.0,Dave,Night,BatchA,No\n13,M03,82.5,118.0,alice,Day,BatchC,Yes\n14,M02,79.7,116.0,Bob,Night,BatchB,Yes", "model_steps": ["Load the CSV data and inspect for missing values and inconsistent capitalization.", "Standardize capitalization of categorical columns like MachineID, Operator, and Shift.", "Impute missing numeric values (Temperature and Pressure) using median values grouped by MachineID.", "Encode categorical variables (MachineID, Operator, Shift, MaterialBatch) using one-hot encoding.", "Convert target variable 'Defect' to binary labels (Yes=1, No=0).", "Split data into training and test sets with an 80/20 ratio, stratifying by the target variable.", "Standardize numeric features (Temperature, Pressure) to zero mean and unit variance.", "Train a RandomForestClassifier with 100 trees on the training data.", "Perform grid search over max_depth values [3, 5, 7] and select the best model based on F1 score using 5-fold cross-validation.", "Evaluate the selected model on the test set and compute accuracy, precision, recall, and F1 score.", "Analyze feature importances from the trained model to identify key predictors of defects."], "model_results": {"accuracy": 0.85, "f1": 0.82, "precision": 0.79, "recall": 0.85, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}, "feature_importances": {"Pressure": 0.27, "Temperature": 0.25, "MachineID_M01": 0.15, "Operator_alice": 0.12, "Shift_Day": 0.1, "MaterialBatch_BatchA": 0.11}, "confusion_matrix": {"true_positive": 11, "true_negative": 18, "false_positive": 3, "false_negative": 2}}}
{"purpose": "Build a classification model to predict whether a manufactured part will pass quality inspection based on sensor readings and production parameters.", "raw_table": "Part_ID,Plant,Temperature,Pressure,Operator,Material_Type,Shift,Inspection_Result\n1,PlantA,75.2,30.1,John,Type1,Morning,Pass\n2,planta,78.5,29.5,Mary,Type2,Afternoon,Fail\n3,PlantB,NaN,31.0,Steve,Type1,Night,Pass\n4,PlantA,74.0,29.8,,Type3,Morning,Pass\n5,PlantB,76.3,not_available,Mary,Type2,Afternoon,Fail\n6,PlantC,77.1,30.0,Steve,Type1,Morning,Pass\n7,PlantA,75.7,30.2,John,Type3,Night,Pass\n8,plantc,78.0,30.5,Mary,Type2,Afternoon,Fail\n9,PlantB,74.5,29.7,John,Type1,Morning,Pass\n10,PlantC,76.8,31.2,Steve,Type3,Night,Pass\n11,PlantA,75.0,30.0,Mary,Type2,Morning,Fail\n12,Plantb,77.2,30.3,John,Type1,Afternoon,Pass", "model_steps": ["Load the CSV data into a DataFrame", "Fix inconsistent capitalization in the 'Plant' column", "Handle missing and malformed values in 'Temperature' and 'Pressure' by imputing median values", "Fill missing values in 'Operator' with the mode", "Convert 'Pressure' column from string to numeric after cleaning non-numeric entries", "One-hot encode categorical variables: 'Plant', 'Operator', 'Material_Type', 'Shift'", "Split the dataset into training (80%) and testing (20%) sets", "Standardize numeric features: 'Temperature' and 'Pressure'", "Train a RandomForestClassifier with default parameters to predict 'Inspection_Result'", "Perform grid search over 'n_estimators' and 'max_depth' with 5-fold cross-validation", "Evaluate model performance using accuracy, F1-score, precision, and recall on the test set", "Generate and analyze a confusion matrix", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.79, "recall": 0.84, "confusion_matrix": {"true_positive": 5, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Pressure": 0.32, "Material_Type_Type2": 0.25, "Temperature": 0.22}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Predict whether a movie will be a box office hit based on its production and genre features.", "raw_table": "MovieID,Title,Genre,BudgetMillions,DirectorExperienceYears,LeadActorPopularity,ReleaseMonth,IsHit\n1,Star Quest,Sci-Fi,120,15,High,July,Yes\n2,Love Again,Romance,30,8,medium,February,No\n3,Haunted Nights,Horror,15,,Low,October,No\n4,Fast Wheels,Action,80,10,High,June,Yes\n5,Sunny Days,Comedy,25,5,Medium,MARCH,No\n6,Deep Blue,Drama,40,7,LOW,November,No\n7,Galaxy Run,Sci-Fi,110,20,High,August,Yes\n8,Desert Storm,Action,70,12,Medium,July,Yes\n9,Magic Tales,Fantasy,50,4,Medium,December,No\n10,Final Hour,Thriller,60,10,High,May,Yes\n11,The Unknown,Horror,20,6,Low,October,No\n12,City Lights,Drama,35,7,Medium,April,No\n13,Runaway,Action,90,15,High,July,Yes\n14,Forever Us,Romance,28,8,Medium,February,No", "model_steps": ["Handle missing values in DirectorExperienceYears by imputing with the median experience", "Standardize numeric features: BudgetMillions and DirectorExperienceYears", "Normalize LeadActorPopularity categories to consistent capitalization and map to ordinal values (Low=0, Medium=1, High=2)", "One-hot encode Genre and ReleaseMonth categorical variables", "Split data into training and test sets with 80/20 ratio", "Train a RandomForestClassifier to predict IsHit (Yes/No)", "Tune max_depth hyperparameter using 5-fold cross-validation", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix to analyze classification errors"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.88, "recall": 0.79, "confusion_matrix": {"True_Positive": 7, "True_Negative": 5, "False_Positive": 1, "False_Negative": 2}, "top_feature_importances": {"BudgetMillions": 0.32, "LeadActorPopularity": 0.25, "Genre_Sci-Fi": 0.12, "DirectorExperienceYears": 0.1, "ReleaseMonth_July": 0.08}, "best_max_depth": 5}}
{"purpose": "Predict whether a student will pass the final exam based on demographic and academic performance features.", "raw_table": "StudentID,Age,Gender,StudyHours,AttendanceRate,PreviousGrade,SchoolType,PassFinal\nS01,17,M,15,0.95,88,public,Yes\nS02,16,F,NaN,0.85,76,Private,No\nS03,18,M,12,0.80,65,public,Yes\nS04,17,f,5,0.60,50,public,no\nS05,17,F,20,NaN,90,Private,Yes\nS06,16,M,8,0.70,55,Public,No\nS07,17,F,10,0.75,70,private,Yes\nS08,18,M,13,0.90,85,Public,yes\nS09,16,F,7,0.65,45,Public,No\nS10,17,M,NaN,0.88,80,Private,Yes\nS11,18,F,14,0.92,NaN,Public,Yes\nS12,17,M,9,0.60,60,private,No", "model_steps": ["Load the CSV data into a DataFrame", "Standardize inconsistent capitalization in categorical columns (Gender, SchoolType, PassFinal)", "Impute missing numeric values (StudyHours, AttendanceRate, PreviousGrade) using median values of respective columns", "Encode 'Gender' and 'SchoolType' as one-hot vectors", "Encode target variable 'PassFinal' as binary (Yes=1, No=0)", "Split data into training (80%) and test sets (20%) with stratification on the target", "Standardize numeric features (Age, StudyHours, AttendanceRate, PreviousGrade) using training set statistics", "Train a RandomForestClassifier with default parameters on the training set", "Evaluate model performance on the test set with accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze classification errors"], "model_results": {"accuracy": 0.83, "precision": 0.85, "recall": 0.9, "f1": 0.87, "confusion_matrix": {"true_positive": 5, "false_positive": 1, "true_negative": 4, "false_negative": 1}, "top_feature_importances": {"StudyHours": 0.32, "PreviousGrade": 0.28, "AttendanceRate": 0.2, "SchoolType_Private": 0.12, "Gender_F": 0.08}}}
{"purpose": "Predict the likelihood of hospital readmission within 30 days for diabetic patients.", "raw_table": "PatientID,Age,Gender,HbA1c_Level,Previous_Admissions,Medication_Adherence,Smoker,Readmitted\n1,57,Male,8.5,2,High,No,Yes\n2,65,Female,7.2,1,medium,YES,No\n3,45,Female,9.1,,Low,No,Yes\n4,72,Male,6.8,3,High,No,No\n5,59,FEMALE,7.9,1,Medium,No,Yes\n6,53,Male,8.3,2,low,Yes,No\n7,38,Female,7.5,0,High,NO,No\n8,,Male,9.0,4,Medium,No,Yes\n9,60,Female,7.1,1,High,No,No\n10,50,Male,missing,2,Medium,No,Yes\n11,48,Male,8.2,1,LOW,Yes,No\n12,55,Female,7.7,2,Medium,No,Yes\n13,62,Female,8.0,3,High,Yes,No\n14,70,Male,7.3,2,medium,No,Yes", "model_steps": ["Load the dataset and identify data types", "Clean the data by imputing missing Age values with median and replacing 'missing' in HbA1c_Level with mean", "Normalize categorical values in Gender, Medication_Adherence, and Smoker columns (e.g., standardize capitalization)", "Convert categorical variables Gender, Medication_Adherence, and Smoker into one-hot encoded features", "Split the data into train (80%) and test (20%) sets stratified by the target Readmitted", "Standardize numeric features Age, HbA1c_Level, and Previous_Admissions using z-score scaling", "Train a RandomForestClassifier to predict Readmitted", "Perform hyperparameter tuning with grid search on number of trees (n_estimators) and max_depth", "Evaluate the model on the test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.79, "f1": 0.77, "precision": 0.75, "recall": 0.8, "confusion_matrix": [[8, 3], [2, 11]], "top_feature_importances": {"HbA1c_Level": 0.32, "Medication_Adherence_High": 0.21, "Previous_Admissions": 0.18, "Age": 0.12, "Smoker_Yes": 0.08, "Gender_Female": 0.05, "Medication_Adherence_Medium": 0.04}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Build a regression model to estimate house prices based on property and neighborhood features.", "raw_table": "HouseID,Size_sqft,Bedrooms,Bathrooms,Neighborhood,Year_Built,Has_Garage,Condition,Price\n1,1500,3,2,Downtown,2005,Yes,Good,350000\n2,2000,4,3,Suburb,2010,yes,Excellent,450000\n3,850,2,1,downtown,1995,No,Fair,200000\n4,1200,,2,Suburb,2000,No,Poor,220000\n5,1750,3,2,Suburb,2015,Yes,Excellent,480000\n6,900,2,1,Downtown,1985,No,good,210000\n7,2500,4,3,Suburb,2018,Yes,Excellent,510000\n8,1600,3,2,Downtown,2008,Yes,Good,370000\n9,1400,3,1,suburb,2003,No,Fair,300000\n10,1800,3,2,Downtown,2012,Yes,Good,400000\n11,1300,3,2,Suburb,2007,No,Excellent,340000\n12,1100,2,1,Downtown,1999,No,Poor,210000", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Correct inconsistent capitalization in 'Neighborhood' and 'Condition' columns", "Impute missing 'Bedrooms' value with median number of bedrooms", "Convert 'Has_Garage' from Yes/No to binary 1/0", "One-hot encode categorical variables: 'Neighborhood' and 'Condition'", "Split data into train and test sets (80/20 split)", "Standardize numeric features: 'Size_sqft', 'Bedrooms', 'Bathrooms', 'Year_Built'", "Train a RandomForestRegressor model on the training data", "Tune hyperparameters using grid search over number of estimators and max depth", "Evaluate model performance on test data using RMSE, MAE, and R2 metrics", "Analyze feature importances to identify top predictors of house price"], "model_results": {"rmse": 22000, "mae": 17000, "r2": 0.85, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}, "top_feature_importances": {"Size_sqft": 0.42, "Neighborhood_Suburb": 0.21, "Condition_Excellent": 0.15, "Bedrooms": 0.1, "Has_Garage": 0.07, "Year_Built": 0.05}}}
{"purpose": "Predict whether a social media post will go viral based on post and user characteristics.", "raw_table": "post_id,user_id,post_length,post_time,device_type,user_followers,content_type,engagement_level,is_viral\n1,101,120,Morning,Mobile,1500,Image,High,Yes\n2,102,45,Afternoon,Desktop,3500,Video,medium,No\n3,103,300,NIGHT,Mobile,NaN,Text,Low,No\n4,104,85,Morning,Tablet,5000,image,Medium,Yes\n5,105,60,Afternoon,Mobile,2500,Video,High,Yes\n6,106,200,Night,Desktop,1000,Text,Low,No\n7,107,NaN,Morning,mobile,2000,Image,Medium,No\n8,108,150,Afternoon,Tablet,1800,video,High,Yes\n9,109,75,Night,Desktop,2200,Text,Low,No\n10,110,95,Morning,Mobile,3000,Image,Medium,Yes\n11,111,110,Afternoon,Desktop,NaN,Video,Low,No\n12,112,130,Night,Tablet,4000,Text,medium,Yes\n13,113,55,Morning,Mobile,2700,image,High,Yes\n14,114,80,Afternoon,Desktop,2100,Video,Low,No\n", "model_steps": ["Load raw CSV data into a DataFrame", "Handle missing values by imputing median for numeric columns and mode for categorical columns", "Normalize capitalization inconsistencies in categorical columns (e.g., device_type, content_type, engagement_level)", "Convert categorical variables device_type, content_type, engagement_level into one-hot encoded features", "Encode target variable 'is_viral' as binary (Yes=1, No=0)", "Split data into train and test sets with 80% training and 20% testing", "Standardize numeric features such as post_length and user_followers", "Train a RandomForestClassifier with 100 trees and max_depth=5 on the training set", "Evaluate the model using accuracy, precision, recall, and F1 score on the test set", "Generate confusion matrix to analyze true positives, false positives, true negatives, and false negatives"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positives": 7, "false_positives": 2, "true_negatives": 8, "false_negatives": 1}, "top_feature_importances": {"user_followers": 0.3, "engagement_level_High": 0.25, "content_type_Image": 0.15, "post_length": 0.1, "device_type_Mobile": 0.08}, "hyperparameters": {"n_estimators": 100, "max_depth": 5, "random_state": 42}}}
{"purpose": "Predict whether a customer will make a purchase during their session on an ecommerce website.", "raw_table": "session_id,device_type,browser,pages_visited,time_spent_seconds,referrer,items_in_cart,purchase\nS001,Mobile,Chrome,5,300,google,2,Yes\nS002,Desktop,firefox,3,120,Facebook,0,No\nS003,MOBILE,Safari,7,450,,1,Yes\nS004,Desktop,Chrome,4,200,google,0,No\nS005,Tablet,Edge,6,350,bing,3,Yes\nS006,Mobile,Chrome,NaN,420,facebook,1,Yes\nS007,desktop,Chrome,2,80,Google,0,No\nS008,Mobile,chrome,8,500,google,4,Yes\nS009,Tablet,Firefox,3,150,,0,No\nS010,Mobile,Safari,5,300,google,1,Yes\nS011,Desktop,Edge,4,210,bing,0,No\nS012,Mobile,Chrome,6,400,facebook,2,Yes\nS013,Tablet,chrome,7,480,Google,3,Yes\nS014,Mobile,firefox,NaN,350,google,1,No", "model_steps": ["Load the CSV data into a DataFrame.", "Identify and fix inconsistent capitalization in 'device_type' and 'browser' columns by converting all entries to lowercase.", "Handle missing numeric values in 'pages_visited' by imputing with the median value.", "Fill missing categorical referrer values with a new category 'unknown'.", "Convert categorical variables ('device_type', 'browser', 'referrer') using one-hot encoding.", "Convert target variable 'purchase' from Yes/No to binary 1/0.", "Split the dataset into training and test sets with an 80/20 ratio.", "Standardize numeric features 'pages_visited', 'time_spent_seconds', and 'items_in_cart'.", "Train a Logistic Regression classifier to predict purchase.", "Evaluate the model using accuracy, precision, recall, and F1 score on the test set.", "Generate a confusion matrix to analyze classification errors."], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 2, "false_negative": 1}, "top_features": {"time_spent_seconds": 0.45, "items_in_cart": 0.3, "pages_visited": 0.15, "referrer_google": 0.1}, "hyperparameters": {"model_type": "LogisticRegression", "regularization": "L2", "C": 1.0}}}
{"purpose": "Predict the likelihood of extreme heat days occurring in a given region based on atmospheric and geographic features.", "raw_table": "Region,Avg_Temperature_C,Precipitation_mm,Wind_Speed_kmh,Soil_Type,Urbanization_Level,Extreme_Heat_Day\nnorth,29.5,12.3,15.4,clay,high,1\nSouth,31.2,7.8,12.1,sand,Medium,1\nEAST,28.7,20.0,9.7,Loam,low,0\nwest,26.1,15.5,14.2,Clay,medium,0\nnorth,NaN,10.2,17.3,sand,High,1\nsouth,30.0,8.0,missing,loam,medium,1\nEast,27.9,18.1,10.5,clay,Low,0\nWEST,25.8,14.7,13.9,loam,Medium,0\nNorth,29.0,11.0,15.0,Sand,High,1\nsouth,30.5,9.1,11.8,clay,HigH,1\neast,28.3,19.5,9.9,LoAm,low,0\nwest,26.7,16.0,13.5,sand,Medium,0\nnorth,29.8,NaN,16.2,clay,high,1", "model_steps": ["Load dataset from CSV string into a DataFrame", "Standardize capitalization in categorical columns: Region, Soil_Type, Urbanization_Level", "Impute missing numeric values in Avg_Temperature_C, Precipitation_mm, and Wind_Speed_kmh with median values", "Encode categorical variables using one-hot encoding", "Split data into training (80%) and test (20%) sets, stratified by target variable Extreme_Heat_Day", "Standardize numeric features to mean=0 and variance=1 on training set and apply to test set", "Train a RandomForestClassifier with 100 trees on the training data", "Tune max_depth hyperparameter using 5-fold cross-validation", "Evaluate model performance on test set calculating accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze classification errors"], "model_results": {"accuracy": 0.85, "precision": 0.88, "recall": 0.82, "f1": 0.85, "top_feature_importances": {"Avg_Temperature_C": 0.34, "Urbanization_Level_Medium": 0.15, "Region_north": 0.12, "Wind_Speed_kmh": 0.1, "Soil_Type_clay": 0.08}, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 1, "false_negative": 2}, "best_hyperparameters": {"max_depth": 8}}}
{"purpose": "Predict whether a manufactured part will fail quality inspection based on sensor readings and production conditions.", "raw_table": "Part_ID,Temperature,Pressure,Machine_Type,Operator_Shift,Defect\n1,350,101.5,A,Day,No\n2,355,NaN,B,Night,Yes\n3,348,99.9,a,Day,No\n4,360,102.3,C,Day,No\n5,NaN,100.1,B,Night,Yes\n6,349,101.0,B,Day,No\n7,360,105.0,C,Night,Yes\n8,355,100.0,A,day,No\n9,351,102.5,C,NIGHT,Yes\n10,352,100.5,B,Day,No\n11,354,101.2,C,Night,Yes\n12,NaN,99.8,B,Day,No\n13,350,100.9,A,Night,No\n14,357,NaN,C,Day,Yes\n15,353,101.7,a,Day,No", "model_steps": ["Identify the target variable 'Defect' and separate features and labels", "Clean and standardize categorical columns: unify 'Machine_Type' and 'Operator_Shift' capitalization", "Impute missing numeric values in 'Temperature' and 'Pressure' columns using median values", "One-hot encode categorical features 'Machine_Type' and 'Operator_Shift'", "Split the dataset into training (80%) and testing (20%) sets", "Standardize numeric features 'Temperature' and 'Pressure' using training set statistics", "Train a RandomForestClassifier on the training data", "Perform grid search to tune max_depth parameter over values [3, 5, 7]", "Evaluate model performance on the test set using accuracy, F1 score, precision, and recall", "Generate a confusion matrix to analyze the classification errors"], "model_results": {"accuracy": 0.87, "f1": 0.84, "precision": 0.82, "recall": 0.86, "confusion_matrix": {"true_positive": 6, "true_negative": 7, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Pressure": 0.32, "Temperature": 0.28, "Machine_Type_B": 0.15, "Operator_Shift_Night": 0.1, "Machine_Type_C": 0.08}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict customer churn probability for a telecommunications company.", "raw_table": "CustomerID,Gender,Age,MonthlyCharges,ContractType,PaymentMethod,TenureMonths,Churn\n001,Male,34,70.5,Month-to-month,Electronic check,5,Yes\n002,Female,45,95.3,One year,Mailed check,24,No\n003,Female,NaN,85.0,Month-to-month,Electronic Check,8,yes\n004,Male,29,60.1,Two year,Bank transfer,36,No\n005,Female,52,NaN,Month-to-month,credit card,2,Yes\n006,Male,40,75.6,Month-to-month,Electronic check,10,Yes\n007,Male,38,90.0,Two year,Electronic Check,48,No\n008,Female,28,68.2,One year,Mailed check,15,No\n009,Female,33,55.4,Month-to-month,Electronic check,6,Yes\n010,Male,,89.9,Month-to-month,Electronic Check,1,Yes\n011,Female,46,99.5,One year,mailed check,30,No\n012,Male,41,65.0,Month-to-month,Electronic check,7,Yes\n013,Female,36,77.3,Two year,Bank Transfer,50,No\n014,Male,50,82.1,Month-to-month,Credit Card,3,Yes", "model_steps": ["Handle missing values in Age and MonthlyCharges by imputing median values", "Standardize numeric features Age, MonthlyCharges, and TenureMonths", "Normalize target column Churn by converting all variations of 'yes' and 'no' to binary 1 and 0", "One-hot encode categorical variables Gender, ContractType, and PaymentMethod", "Split data into training and test sets with an 80/20 ratio", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search over max_depth values [5, 10, 15] using 5-fold cross-validation", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Compute confusion matrix for the test predictions", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.79, "precision": 0.81, "recall": 0.75, "f1": 0.78, "confusion_matrix": [[8, 3], [4, 9]], "top_feature_importances": {"ContractType_Month-to-month": 0.32, "TenureMonths": 0.25, "MonthlyCharges": 0.18}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict whether a delivery will be on-time based on delivery and driver attributes.", "raw_table": "DeliveryID,DriverAge,VehicleType,Distance_km,PickupTime,TrafficCondition,DeliveryDay,OnTime\nD001,34,van,12.5,10:30,Mild,Monday,Yes\nD002,42,Truck,25.0,15:45,Heavy,Tuesday,No\nD003,29,Van,15.2,11:15,moderate,Wednesday,Yes\nD004,NaN,car,7.8,09:00,Light,Thursday,Yes\nD005,45,Truck,40.1,16:00,HEAVY,Friday,No\nD006,38,Car,18.5,13:30,Light,Saturday,Yes\nD007,31,van,22.0,14:20,Moderate,Sunday,No\nD008,50,truck,30,08:45,Heavy,Monday,No\nD009,27,Car,5.5,10:00,,Tuesday,Yes\nD010,36,Van,20.3,12:00,Mild,Wednesday,Yes\nD011,40,car,NaN,09:30,Light,Thursday,No\nD012,33,Van,16.7,13:10,Mild,Friday,Yes\nD013,28,Car,14.0,11:50,moderate,Saturday,No\nD014,NaN,Truck,35.5,17:00,Heavy,Sunday,No", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing and inconsistent values", "Standardize capitalization in categorical columns: VehicleType and TrafficCondition", "Impute missing numeric values (DriverAge, Distance_km) using median", "Impute missing TrafficCondition with the mode 'Moderate'", "Convert PickupTime to numeric feature representing hour of day", "One-hot encode categorical variables: VehicleType, TrafficCondition, DeliveryDay", "Split the dataset into training and testing sets with 80/20 ratio", "Standardize numeric features: DriverAge, Distance_km, PickupHour", "Train a RandomForestClassifier with 100 trees to predict OnTime", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Extract and list top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.79, "precision": 0.81, "recall": 0.75, "f1": 0.78, "top_feature_importances": {"Distance_km": 0.32, "TrafficCondition_Heavy": 0.21, "PickupHour": 0.15}}}
{"purpose": "Build a classification model to predict whether a taxi trip will exceed 20 minutes in duration.", "raw_table": "trip_id,pickup_zone,dropoff_zone,passenger_count,trip_distance,trip_time_minutes,payment_type,exceeds_20_min\n1,Midtown,Downtown,2,3.5,22,Credit Card,Yes\n2,uptown,Downtown,1,2.1,15,Cash,No\n3,Midtown,midtown,3,1.8,14,Credit card,No\n4,Downtown,Uptown,2,4.0,26,Cash,Yes\n5,Downtown,Downtown,1,0.5,8,Credit Card,No\n6,Uptown,Downtown,,3.2,19,Cash,No\n7,Midtown,Downtown,2,3.7,,Credit Card,Yes\n8,Midtown,Uptown,1,4.5,28,Cash,Yes\n9,MIDTOWN,Downtown,3,3.0,21,Credit card,Yes\n10,Downtown,Uptown,1,2.8,17,Cash,No\n11,Midtown,Downtown,2,3.4,23,Credit Card,Yes\n12,Midtown,Uptown,2,4.1,27,credit card,Yes", "model_steps": ["Load data from CSV and inspect for missing and inconsistent values", "Normalize capitalization in categorical columns to ensure consistency (e.g., pickup_zone, payment_type)", "Impute missing numeric values in passenger_count and trip_time_minutes using median values", "Convert target variable 'exceeds_20_min' to binary label (Yes=1, No=0)", "One-hot encode categorical variables: pickup_zone, dropoff_zone, payment_type", "Split data into train and test sets (80/20)", "Standardize numeric features: passenger_count, trip_distance", "Train a RandomForestClassifier on the training data", "Perform grid search over number of estimators (50, 100) and max_depth (3, 5)", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.83, "f1": 0.85, "precision": 0.88, "recall": 0.82, "confusion_matrix": {"true_positive": 5, "false_positive": 1, "true_negative": 7, "false_negative": 2}, "top_feature_importances": {"trip_time_minutes": 0.4, "trip_distance": 0.25, "pickup_zone_Midtown": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict the likelihood of hospital readmission within 30 days for diabetic patients based on clinical and demographic data.", "raw_table": "PatientID,Age,Gender,HbA1c,Insulin_Therapy,Num_Visits_LastYear,Comorbidity_Level,Readmitted\n1,65,M,7.8,Yes,4,High,Yes\n2,54,F,6.2,NO,3,medium,No\n3,72,M,8.9,yes,5,High,Yes\n4,45,F,5.5,,1,Low,No\n5,60,F,7.1,Yes,2,Medium,No\n6,38,M,6.7,No,0,low,No\n7,50,F,missing,No,3,Medium,Yes\n8,68,M,8.4,YES,6,High,YES\n9,59,F,7.3,No,4,Medium,no\n10,70,M,8.0,Yes,5,High,Yes\n11,55,F,6.9,no,2,Medium,No\n12,62,M,7.6,Yes,3,High,Yes\n13,47,F,6.4,No,1,Low,No\n14,69,M,8.2,yes,7,HIGH,Yes", "model_steps": ["Load the raw dataset and inspect for missing and inconsistent values", "Impute missing HbA1c values with median HbA1c", "Standardize capitalization and convert 'Insulin_Therapy' and 'Comorbidity_Level' to consistent categories", "Encode categorical variables ('Gender', 'Insulin_Therapy', 'Comorbidity_Level') using one-hot encoding", "Convert the target variable 'Readmitted' to binary (Yes=1, No=0)", "Split data into training and testing sets with an 80/20 ratio, stratified on target", "Standardize numeric features 'Age', 'HbA1c', and 'Num_Visits_LastYear'", "Train a RandomForestClassifier to predict hospital readmission", "Tune max_depth and n_estimators hyperparameters using grid search with 5-fold cross-validation", "Evaluate the final model on the test set calculating accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.78, "f1": 0.8, "confusion_matrix": {"true_positive": 7, "false_positive": 2, "true_negative": 9, "false_negative": 2}, "top_feature_importances": {"HbA1c": 0.34, "Comorbidity_Level_High": 0.22, "Num_Visits_LastYear": 0.15}, "best_hyperparameters": {"max_depth": 6, "n_estimators": 100}}}
{"purpose": "Predict whether a manufactured part will pass quality inspection based on production parameters.", "raw_table": "Part_ID,Machine_ID,Operator_Level,Temperature,Pressure,Material_Type,Shift,Defect\n1,M01,expert,75,30,Steel,Day,No\n2,m02,Intermediate,80,35,steel,Night,Yes\n3,M01,EXPERT,78,,Aluminum,Day,No\n4,M03,Beginner,85,40,Aluminum,Night,Yes\n5,M02,Intermediate,NaN,33,Steel,Day,No\n6,M01,Expert,77,31,aluminum,Day,No\n7,M03,Beginner,90,38,Steel,Night,Yes\n8,M02,Intermediate,82,36,Steel,Day,No\n9,M03,Beginner,88,39,Aluminum,night,Yes\n10,M01,expert,76,32,Steel,Day,No\n11,M02,Intermediate,81,35,Aluminum,Day,No\n12,M03,Beginner,NaN,40,Steel,Night,Yes\n13,M01,Expert,79,34,Steel,Day,No\n14,M02,intermediate,83,37,Aluminum,Night,Yes", "model_steps": ["Load the raw data and identify missing values", "Clean and standardize categorical columns (e.g., unify 'Operator_Level' casing, 'Material_Type', 'Shift')", "Impute missing numeric values (Temperature and Pressure) using median imputation", "Encode categorical variables using one-hot encoding", "Split data into training set (80%) and test set (20%) stratified by target", "Standardize numeric features (Temperature and Pressure) using training set statistics", "Train a RandomForestClassifier on the training data", "Perform grid search over 'max_depth' with 3-fold cross-validation", "Select best model and evaluate on test set", "Compute accuracy, precision, recall, and F1 score", "Generate confusion matrix", "Identify and report top 3 feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_negative": 7, "false_positive": 1, "false_negative": 1, "true_positive": 4}, "top_feature_importances": {"Pressure": 0.32, "Temperature": 0.28, "Operator_Level_Expert": 0.15}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a customer will make a purchase during a session based on behavior and demographics.", "raw_table": "session_id,user_age,user_gender,device_type,page_views,time_on_site_seconds,referral_source,purchase_made\nS001,25,MOBILE,Male,Mobile,5,300,google,Yes\nS002,42,Female,Desktop,10,700,Facebook,No\nS003,NaN,Female,MOBILE,7,450,Direct,Yes\nS004,37,male,Desktop,4,230,google,No\nS005,29,Female,MOBILE,8,520,facebook,Yes\nS006,33,Male,Tablet,12,900,Direct,No\nS007,45,Female,desktop,6,400,Google,Yes\nS008,22,male,mobile,3,150,Direct,No\nS009,38,FEMALE,Desktop,9,650,Facebook,Yes\nS010,27,Male,Tablet,NaN,520,google,No\nS011,31,Unknown,Mobile,5,300,Direct,Yes\nS012,40,Female,desktop,11,800,Facebook,No\nS013,35,Male,Mobile,7,480,google,Yes\nS014,28,Female,Tablet,4,260,Facebook,No", "model_steps": ["Load the CSV data into a DataFrame", "Correct inconsistent capitalization in 'user_gender', 'device_type', and 'referral_source' columns", "Impute missing values: fill missing 'user_age' with median age; fill missing 'time_on_site_seconds' with median time", "Convert 'purchase_made' target variable to binary (Yes=1, No=0)", "Split data into train (80%) and test (20%) sets", "One-hot encode categorical variables: 'user_gender', 'device_type', 'referral_source'", "Standardize numeric features: 'user_age', 'page_views', 'time_on_site_seconds'", "Train a RandomForestClassifier with 100 trees and max_depth=5", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Compute confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.78, "f1": 0.76, "precision": 0.74, "recall": 0.79, "confusion_matrix": [[5, 2], [3, 8]], "top_feature_importances": {"time_on_site_seconds": 0.32, "page_views": 0.25, "referral_source_Facebook": 0.15, "user_gender_Female": 0.1, "device_type_Mobile": 0.08}, "hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a loan applicant will default based on their financial and personal data.", "raw_table": "Applicant_ID,Age,Income,Employment_Status,Credit_Score,Loan_Amount,Loan_Purpose,Default\n1,34,55000,Full-time,720,15000,home improvement,No\n2,45,NaN,Self-employed,680,20000,Debt Consolidation,Yes\n3,29,48000,Part-time,650,12000,car,No\n4,54,62000,Full-Time,NaN,18000,Home Improvement,No\n5,31,40000,full-time,700,10000,Car,No\n6,40,58000,Self-Employed,710,15000,Debt consolidation,Yes\n7,,53000,Full-time,690,14000,Car,No\n8,37,60000,Part-Time,675,13000,home improvement,No\n9,50,72000,Full-time,730,25000,Debt Consolidation,Yes\n10,28,45000,part-time,640,11000,car,No\n11,46,61000,Self-employed,695,,Debt Consolidation,Yes\n12,33,47000,Full-time,665,12500,Home Improvement,No\n13,38,56000,Full-time,700,16000,Car,No", "model_steps": ["Impute missing numeric values in Age, Income, Credit_Score, and Loan_Amount columns using median values", "Standardize numeric features: Age, Income, Credit_Score, Loan_Amount", "Normalize capitalization in Employment_Status and Loan_Purpose columns and combine similar categories (e.g., 'full-time' and 'Full-Time')", "One-hot encode Employment_Status and Loan_Purpose categorical variables", "Split data into training (80%) and testing (20%) sets with stratification on Default", "Train a RandomForestClassifier with default hyperparameters on the training set", "Perform grid search over max_depth values [5, 10, 15] using 5-fold cross-validation", "Evaluate the best model on the test set computing accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Identify and report top 5 feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.78, "f1": 0.8, "confusion_matrix": {"true_positive": 7, "true_negative": 14, "false_positive": 3, "false_negative": 2}, "top_feature_importances": {"Credit_Score": 0.32, "Income": 0.21, "Loan_Amount": 0.18, "Employment_Status_Self-employed": 0.12, "Loan_Purpose_Debt Consolidation": 0.1}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict customer churn probability for a telecom operator to reduce subscriber loss.", "raw_table": "customer_id,tenure_months,monthly_charges,contract_type,internet_service,tech_support,churn\nC001,12,70.35,Month-to-month,Fiber optic,Yes,Yes\nC002,5,85.10,Month-to-month,Fiber optic,No,No\nC003,48,45.00,Two year,DSL,No,No\nC004,2,NaN,Month-to-month,DSL,Yes,YES\nC005,36,60.50,One year,Fiber optic,no,No\nC006,24,75.00,One year,DSL,Yes,No\nC007,1,90.99,month-To-Month,Fiber optic,No,Yes\nC008,18,55.20,Two Year,DSL,No,No\nC009,22,80.75,One year,Fiber optic,Yes,No\nC010,30,67.89,One Year,DSL,No,No\nC011,7,NaN,Month-to-month,DSL,No,Yes\nC012,40,59.99,Two year,Fiber optic,Yes,No\nC013,10,72.30,Month-to-month,Fiber optic,No,Yes\nC014,15,65.00,One year,DSL,Yes,No", "model_steps": ["Load raw CSV data and inspect for missing or inconsistent values", "Clean 'contract_type' and 'internet_service' columns by standardizing capitalization", "Impute missing 'monthly_charges' values using median monthly charge grouped by 'contract_type'", "Encode categorical variables ('contract_type', 'internet_service', 'tech_support') using one-hot encoding", "Convert 'churn' target variable to binary: Yes=1, No=0", "Split data into training (80%) and testing (20%) sets with stratification on churn label", "Standardize numeric features: 'tenure_months' and 'monthly_charges'", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search over max_depth values [5, 10, 15] with 3-fold cross-validation", "Select best model based on highest F1 score on validation folds", "Evaluate final model on test set calculating accuracy, precision, recall, and F1 score", "Generate confusion matrix and list top 3 feature importances"], "model_results": {"accuracy": 0.86, "f1": 0.79, "precision": 0.75, "recall": 0.84, "confusion_matrix": [[18, 3], [4, 22]], "top_feature_importances": {"contract_type_Month-to-month": 0.32, "tech_support_No": 0.26, "monthly_charges": 0.18}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a retail customer will make a purchase during their visit based on demographic and visit features.", "raw_table": "CustomerID,Age,Gender,Store_Section,Visit_Duration_Minutes,Previous_Purchases,Day_of_Week,Purchase\n1,34,Female,Electronics,45,3,Monday,Yes\n2,27,Male,clothing,30,1,Tuesday,No\n3,45,Female,Home & Garden,,5,Wednesday,Yes\n4,52,Male,Electronics,50,10,THURSDAY,Yes\n5,,Female,clothing,25,0,Friday,No\n6,39,male,Sports,40,2,Saturday,No\n7,29,Female,Clothing,35,1,Sunday,No\n8,41,Female,Electronics,55,8,Monday,Yes\n9,38,Male,Home & garden,20,0,Tuesday,No\n10,33,Female,Sports,NaN,4,Wednesday,Yes\n11,47,Male,Electronics,60,7,Thursday,Yes\n12,26,Female,Clothing,22,1,Friday,No\n13,50,Male,Home & Garden,48,6,Saturday,Yes\n14,36,Female,Sports,37,3,Sunday,No", "model_steps": ["Load data and inspect for missing and inconsistent values", "Standardize categorical values (e.g., unify capitalization in Store_Section and Day_of_Week)", "Impute missing numeric values in Age and Visit_Duration_Minutes using median imputation", "Encode categorical variables Gender, Store_Section, and Day_of_Week using one-hot encoding", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features Age, Visit_Duration_Minutes, and Previous_Purchases", "Train a RandomForestClassifier with default parameters on the training set", "Evaluate model on test set computing accuracy, precision, recall, and F1 score", "Extract and report top 3 feature importances", "Generate confusion matrix for test set predictions"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.78, "f1": 0.8, "top_feature_importances": {"Visit_Duration_Minutes": 0.32, "Previous_Purchases": 0.28, "Store_Section_Electronics": 0.15}, "confusion_matrix": {"true_positives": 7, "true_negatives": 4, "false_positives": 1, "false_negatives": 2}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a movie will be rated as a 'hit' or 'flop' based on its production and genre features.", "raw_table": "MovieID,Genre,BudgetMillion,DirectorExperienceYears,MainActorPopularityScore,ReleaseMonth,Country,BoxOfficeHit\n1,Action,150,12,85,July,USA,Hit\n2,comedy,30,5,70,December,USA,Flop\n3,Drama,45,7,60,August,UK,Hit\n4,Sci-Fi,200,15,90,March,usa,Hit\n5,Comedy,25,,65,November,UK,Flop\n6,Drama,60,10,55,May,Canada,Hit\n7,Action,180,14,NaN,July,USA,Hit\n8,drama,50,8,58,January,Canada,Flop\n9,Comedy,35,6,74,December,UK,Flop\n10,Sci-Fi,210,20,95,June,USA,Hit\n11,Action,NaN,11,88,July,Canada,Hit\n12,Comedy,28,4,68,December,UK,Flop\n13,Drama,55,9,62,May,usa,Hit\n14,Sci-Fi,190,16,92,April,USA,Hit", "model_steps": ["Load the CSV data into a dataframe", "Standardize capitalization in 'Genre' and 'Country' columns", "Impute missing numeric values in 'BudgetMillion', 'DirectorExperienceYears', and 'MainActorPopularityScore' using median imputation", "Convert 'Genre', 'ReleaseMonth', and 'Country' categorical columns using one-hot encoding", "Convert target variable 'BoxOfficeHit' to binary labels (Hit=1, Flop=0)", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features 'BudgetMillion', 'DirectorExperienceYears', and 'MainActorPopularityScore' using z-score normalization", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search over 'max_depth' parameter with values [5, 10, 15]", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate feature importance ranking from the trained RandomForest model"], "model_results": {"accuracy": 0.86, "precision": 0.89, "recall": 0.83, "f1": 0.86, "top_features": {"BudgetMillion": 0.32, "DirectorExperienceYears": 0.25, "MainActorPopularityScore": 0.2, "Genre_Action": 0.1, "ReleaseMonth_July": 0.08, "Country_USA": 0.05}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Build a classification model to predict whether a taxi trip will exceed 20 minutes duration.", "raw_table": "trip_id,passenger_count,trip_distance,pickup_neighborhood,dropoff_neighborhood,payment_type,trip_duration_minutes\n1,2,3.5,Downtown,Uptown,card,18\n2,1,1.2,Downtown,Downtown,cash,9\n3,3,5.0,Uptown,Downtown,CARD,24\n4,,2.3,Midtown,Downtown,card,15\n5,1,0.8,Uptown,Uptown,cash,7\n6,2,4.2,Midtown,Midtown,Cash,21\n7,1,3.3,DOWNTOWN,Midtown,card,22\n8,2,2.7,Uptown,Midtown,cash,19\n9,4,6.1,Midtown,Uptown,CARD,28\n10,3,3.0,Downtown,Midtown,card,17\n11,1,1.5,Midtown,Midtown,Card,13\n12,2,5.5,uptown,Downtown,cash,25\n13,2,3.8,Downtown,uptown,cash,20\n14,3,2.0,Midtown,Uptown,cash,14\n15,1,0.5,Downtown,Downtown,card,8", "model_steps": ["Load data and inspect for missing or inconsistent values", "Impute missing passenger_count with median", "Normalize capitalization in pickup_neighborhood, dropoff_neighborhood, and payment_type columns", "Create binary target: trip_duration_long = 1 if trip_duration_minutes > 20 else 0", "One-hot encode pickup_neighborhood, dropoff_neighborhood, and payment_type", "Split data into train and test sets (80/20)", "Standardize numeric features: passenger_count and trip_distance", "Train RandomForestClassifier with 100 trees", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix for test predictions"], "model_results": {"accuracy": 0.87, "precision": 0.82, "recall": 0.78, "f1": 0.8, "confusion_matrix": [[7, 1], [2, 5]], "top_feature_importances": {"trip_distance": 0.42, "pickup_neighborhood_Uptown": 0.15, "payment_type_card": 0.12, "passenger_count": 0.1, "dropoff_neighborhood_Downtown": 0.08}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Build a classification model to predict the likelihood of wheat crop disease occurrence based on environmental and field conditions.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Temperature_C,Humidity_Percent,Fertilizer_Type,Disease_Occurred\n1,Loam,120,22.5,80,Organic,Yes\n2,Sandy,85,25.0,65,chemical,No\n3,Clay,150,18.0,90,Organic,Yes\n4,Silt,NA,20.0,75,Organic,No\n5,sandy,95,23.5,60,NONE,No\n6,Loam,110,19.5,85,chemical,Yes\n7,Loam,130,21.0,88,Organic,Yes\n8,Clay,140,17.5,NA,chemical,No\n9,Silt,100,24.0,70,Organic,No\n10,Sandy,90,22.0,65,nOne,No\n11,Loam,115,20.5,80,Organic,Yes\n12,Loam,105,21.5,82,chemical,Yes\n13,Clay,135,18.5,89,Organic,Yes\n14,Silt,95,23.0,72,chemical,No", "model_steps": ["Load data from CSV string and inspect for missing values and inconsistencies", "Standardize categorical values in Soil_Type and Fertilizer_Type columns by normalizing case and correcting typos", "Impute missing numeric values (Rainfall_mm and Humidity_Percent) using median values grouped by Soil_Type", "Encode categorical variables Soil_Type and Fertilizer_Type using one-hot encoding", "Convert target variable Disease_Occurred to binary labels (Yes=1, No=0)", "Split the dataset into training (80%) and testing (20%) sets using stratified sampling to preserve class distribution", "Standardize numeric features (Rainfall_mm, Temperature_C, Humidity_Percent) using z-score scaling based on training data", "Train a RandomForestClassifier with 100 trees on the training set", "Tune max_depth hyperparameter via 5-fold cross-validation on training set with values [5,10,15]", "Evaluate the final model on the test set, computing accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Extract and rank feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "confusion_matrix": {"True_Positive": 7, "True_Negative": 5, "False_Positive": 1, "False_Negative": 2}, "top_feature_importances": {"Humidity_Percent": 0.29, "Rainfall_mm": 0.25, "Soil_Type_Loam": 0.15, "Temperature_C": 0.12, "Fertilizer_Type_Organic": 0.1, "Soil_Type_Sandy": 0.09}, "best_max_depth": 10}}
{"purpose": "Predict whether a loan applicant will default on their loan based on financial and demographic features.", "raw_table": "ApplicantID,Age,Income,LoanAmount,EmploymentStatus,CreditScore,MaritalStatus,Defaulted\n1,45,55000,15000,Full-Time,700,married,No\n2,38,NaN,12000,Part-time,680,single,Yes\n3,29,40000,8000,full-time,NaN,Single,no\n4,50,82000,20000,Self-employed,730,Married,No\n5,42,60000,NaN,Unemployed,690,Divorced,Yes\n6,35,48000,11000,Full-Time,720,single,No\n7,NaN,52000,13000,Part-Time,705,Single,No\n8,31,45000,9000,Full-time,695,married,Yes\n9,54,90000,25000,SELF-EMPLOYED,740,Married,no\n10,47,57000,16000,Full-Time,715,Divorced,No\n11,39,61000,14000,Full-time,690,single,Yes\n12,36,58000,12000,Part-time,685,Single,No\n13,44,65000,NaN,Full-time,700,Married,No\n14,33,43000,8500,Unemployed,660,Single,Yes", "model_steps": ["Load dataset from CSV string.", "Identify target variable as 'Defaulted' (binary classification: Yes/No).", "Handle missing numeric values in 'Age', 'Income', 'LoanAmount', and 'CreditScore' by imputing with median values.", "Standardize inconsistent capitalization in categorical columns: 'EmploymentStatus' and 'MaritalStatus'.", "Encode categorical variables 'EmploymentStatus' and 'MaritalStatus' using one-hot encoding.", "Convert target variable 'Defaulted' to binary labels (Yes=1, No=0).", "Split data into training (80%) and testing (20%) sets with stratification on target variable.", "Train a RandomForestClassifier on the training data.", "Perform grid search to tune 'max_depth' and 'n_estimators' hyperparameters using 5-fold cross-validation.", "Evaluate the final model on the test set computing accuracy, precision, recall, and F1 score.", "Generate and analyze the confusion matrix.", "Extract and report top 3 feature importances from the trained model."], "model_results": {"accuracy": 0.86, "precision": 0.8, "recall": 0.75, "f1": 0.77, "confusion_matrix": {"true_negative": 8, "false_positive": 2, "false_negative": 3, "true_positive": 7}, "top_feature_importances": {"CreditScore": 0.32, "LoanAmount": 0.25, "Income": 0.15}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict whether a day will experience extreme heat conditions based on meteorological and environmental features.", "raw_table": "Date,Temperature_C,Humidity_pct,Wind_Speed_kmh,Weather_Condition,Region,Extreme_Heat\n2024-06-01,35.2,45,15,Sunny,North,Yes\n2024-06-02,32.8,50,10,Rainy,south,No\n2024-06-03,NaN,55,5,Cloudy,East,No\n2024-06-04,38.0,40,20,Sunny,North,Yes\n2024-06-05,29.5,65,12,rainy,West,No\n2024-06-06,36.5,42,18,SUNNY,North,Yes\n2024-06-07,27.0,70,8,Cloudy,East,No\n2024-06-08,33.1,,14,Sunny,West,No\n2024-06-09,34.7,48,9,Cloudy,South,Yes\n2024-06-10,31.3,53,7,Rainy,West,No\n2024-06-11,39.2,38,22,SUNNY,south,Yes\n2024-06-12,28.4,67,11,cloudy,East,No\n2024-06-13,37.8,43,19,Sunny,North,Yes\n2024-06-14,30.0,60,13,Rainy,West,No", "model_steps": ["Parse Date column as datetime and extract day of month as a numeric feature", "Fill missing Temperature_C values with median temperature", "Fill missing Humidity_pct values with mean humidity", "Standardize numeric features: Temperature_C, Humidity_pct, Wind_Speed_kmh, and day of month", "Normalize capitalization in Weather_Condition and Region columns (e.g., lowercase all values)", "One-hot encode categorical variables: Weather_Condition and Region", "Split data into training (80%) and test (20%) sets with stratification on Extreme_Heat", "Train a RandomForestClassifier with 100 trees on the training set", "Tune max_depth hyperparameter using 5-fold cross-validation, choosing max_depth=5", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 most important features"], "model_results": {"accuracy": 0.85, "precision": 0.88, "recall": 0.8, "f1": 0.84, "confusion_matrix": {"true_negatives": 6, "false_positives": 1, "false_negatives": 2, "true_positives": 5}, "top_feature_importances": {"Temperature_C": 0.42, "Humidity_pct": 0.25, "Weather_Condition_Sunny": 0.15}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a retail customer will make a purchase during a promotional campaign.", "raw_table": "CustomerID,Age,Gender,Annual_Income,Membership_Status,Last_Purchase_Amount,Preferred_Channel,Promo_Responded\n1,34,Male,55000,Gold,120.5,Online,Yes\n2,28,Female,48000,silver,85.0,in-store,No\n3,45,Male,NaN,Platinum,200.0,Online,Yes\n4,23,Female,32000,,NaN,In-Store,no\n5,54,Male,75000,Gold,150.25,online,Yes\n6,30,Female,43000,Silver,95.5,In-store,No\n7,41,Female,62000,Gold,130.0,Online,Yes\n8,37,,58000,platinum,110.0,in-store,No\n9,29,Male,47000,Silver,88.0,Online,No\n10,50,Female,69000,Gold,140.0,Online,Yes\n11,33,Male,NaN,Silver,NaN,online,No\n12,26,Female,35000,Silver,75.0,In-Store,No\n13,39,Male,60000,Gold,135.0,Online,Yes\n14,44,female,66000,Platinum,160.0,in-store,Yes", "model_steps": ["Load CSV data into a pandas DataFrame", "Handle missing numeric values in Annual_Income and Last_Purchase_Amount using median imputation", "Standardize capitalization inconsistencies in Membership_Status and Preferred_Channel columns", "Fill missing Membership_Status with the mode value", "Encode Gender, Membership_Status, and Preferred_Channel using one-hot encoding", "Convert target variable Promo_Responded to binary (Yes=1, No=0)", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features Age, Annual_Income, and Last_Purchase_Amount", "Train a RandomForestClassifier with 100 trees", "Perform 5-fold cross-validation to tune max_depth parameter over [5, 10, 15]", "Evaluate model on test set computing accuracy, F1-score, precision, and recall", "Generate feature importances from the trained model"], "model_results": {"accuracy": 0.82, "f1": 0.79, "precision": 0.81, "recall": 0.77, "top_feature_importances": {"Membership_Status_Gold": 0.23, "Last_Purchase_Amount": 0.2, "Preferred_Channel_Online": 0.15, "Annual_Income": 0.13, "Age": 0.1}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a retail customer will make a purchase during their website visit.", "raw_table": "CustomerID,Age,Gender,Browser,TimeOnSiteMinutes,PagesVisited,PreviousPurchases,DeviceType,Purchase\n1001,34,Male,Chrome,12,5,2,Mobile,Yes\n1002,27,Female,firefox,8,3,,Desktop,No\n1003,45,Female,Safari,15,7,5,Tablet,Yes\n1004,33,M,Edge,NaN,4,1,desktop,No\n1005,29,Female,Chrome,10,6,3,mobile,Yes\n1006,NaN,Female,Chrome,5,2,0,Mobile,No\n1007,38,Male,chrome,20,8,7,Desktop,Yes\n1008,41,Male,Firefox,14,,4,Tablet,Yes\n1009,22,Female,Safari,7,3,0,Mobile,No\n1010,36,Male,Edge,13,6,2,Desktop,Yes\n1011,28,Male,Firefox,9,4,1,Mobile,No\n1012,35,Female,Safari,11,5,2,Tablet,Yes\n1013,44,Male,Chrome,18,7,6,Desktop,Yes\n1014,31,Female,Edge,10,4,1,Desktop,No", "model_steps": ["Load the raw CSV data into a DataFrame", "Clean data by standardizing categorical values (e.g., normalize browser and device type names to lowercase)", "Impute missing numeric values with median (Age, TimeOnSiteMinutes, PagesVisited)", "Impute missing categorical values with the mode (PreviousPurchases missing treated as zero)", "Encode target variable 'Purchase' as binary (Yes=1, No=0)", "One-hot encode categorical variables: Gender, Browser, DeviceType", "Split data into training (80%) and test sets (20%) stratified by target variable", "Standardize numeric features: Age, TimeOnSiteMinutes, PagesVisited, PreviousPurchases", "Train a RandomForestClassifier with 100 trees on the training set", "Evaluate model performance on test set with accuracy, precision, recall, and F1 score", "Generate feature importances from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "feature_importances": {"TimeOnSiteMinutes": 0.25, "PagesVisited": 0.2, "PreviousPurchases": 0.18, "Browser_chrome": 0.1, "Age": 0.08, "DeviceType_mobile": 0.06, "Gender_male": 0.05, "Browser_firefox": 0.04, "DeviceType_desktop": 0.04}}}
{"purpose": "Build a classification model to predict if a retail store transaction will result in a high-value purchase.", "raw_table": "TransactionID,StoreType,CustomerAge,PurchaseAmount,PaymentMethod,DayOfWeek,IsHoliday,HighValuePurchase\n1,Supermarket,34,120.5,CreditCard,Monday,No,Yes\n2,convenience,22,15.0,CASH,Friday,No,No\n3,SuperMarket,45,250.0,DebitCard,Saturday,Yes,Yes\n4,Pharmacy,38,,CreditCard,Tuesday,No,No\n5,Supermarket,29,80,CREDITCARD,Sunday,No,No\n6,Convenience,19,35,DebitCard,Monday,No,No\n7,pharmacy,41,200,Cash,Wednesday,Yes,Yes\n8,Supermarket,33,NaN,Cash,Thursday,No,No\n9,Convenience,27,55,CreditCard,Saturday,No,No\n10,Pharmacy,50,300,DebitCard,Friday,Yes,Yes\n11,convenience,31,40,CreditCard,Monday,No,No\n12,Supermarket,47,400,Creditcard,Sunday,No,Yes\n13,Pharmacy,26,18,Cash,Tuesday,No,No\n14,Convenience,35,22,DebitCard,Wednesday,No,No", "model_steps": ["Load CSV data into a DataFrame", "Fix inconsistent capitalization in 'StoreType' and 'PaymentMethod' columns", "Impute missing 'PurchaseAmount' values with median purchase amount", "Convert 'HighValuePurchase' target column to binary encoding (Yes=1, No=0)", "One-hot encode categorical variables: 'StoreType', 'PaymentMethod', 'DayOfWeek', and 'IsHoliday'", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features: 'CustomerAge' and 'PurchaseAmount'", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search over max_depth parameter with values [5, 10, 15]", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and feature importance ranking"], "model_results": {"accuracy": 0.86, "precision": 0.82, "recall": 0.79, "f1": 0.805, "confusion_matrix": {"true_positive": 11, "true_negative": 28, "false_positive": 4, "false_negative": 3}, "top_feature_importances": {"PurchaseAmount": 0.42, "StoreType_Supermarket": 0.18, "PaymentMethod_CreditCard": 0.12, "IsHoliday_Yes": 0.1}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a social media post will go viral based on its characteristics and user engagement features.", "raw_table": "post_id,user_id,post_length,post_type,has_image,followers_count,avg_likes,last_24h_comments,post_hour,viral\n1,1001,120,text,Yes,1500,100,15,13,Yes\n2,1002,45,Image,No,230,50,,9,No\n3,1003,300,Video,yes,5000,300,40,20,Yes\n4,1004,80,Text,Yes,800,75,12,22,No\n5,1005,150,image,No,2000,120,20,18,Yes\n6,1006,60,Text,YES,1200,MISSING,10,7,No\n7,1007,500,Video,No,10000,500,60,23,Yes\n8,1008,200,video,Yes,3500,200,35,11,Yes\n9,1009,30,Text,No,300,25,5,6,No\n10,1010,90,Image,yEs,900,85,14,16,No\n11,1011,400,Video,yes,7000,400,50,21,Yes\n12,1012,70,Text,no,650,60,8,14,No\n13,1013,250,Image,Yes,4000,180,38,19,Yes\n14,1014,55,text,No,900,70,9,10,No", "model_steps": ["Load the CSV data into a DataFrame and identify missing values", "Normalize capitalization and values in categorical columns (e.g., 'post_type' and 'has_image')", "Impute missing numeric values in 'avg_likes' with median value", "Convert the target variable 'viral' into binary labels (Yes=1, No=0)", "One-hot encode categorical features: 'post_type' and 'has_image'", "Split the dataset into training (80%) and testing (20%) sets", "Standardize numeric features: 'post_length', 'followers_count', 'avg_likes', 'last_24h_comments', 'post_hour'", "Train a RandomForestClassifier with 100 trees on the training data", "Evaluate the trained model on the test set calculating accuracy, precision, recall, and F1 score", "Generate a confusion matrix and identify the top 3 important features"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": [[10, 2], [3, 19]], "top_feature_importances": {"avg_likes": 0.35, "followers_count": 0.25, "post_type_Video": 0.15}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a manufactured part will pass quality inspection based on production parameters and machine settings.", "raw_table": "Part_ID,Machine_ID,Operator_Shift,Temperature_C,Pressure_psi,Material_Type,Defect_Flag\n1,M01,Morning,215,50,Steel,No\n2,M02,Afternoon,220,NaN,steel,Yes\n3,m01,Night,210,48,Aluminum,No\n4,M03,Morning,NaN,52,Aluminum,No\n5,M02,Afternoon,225,51,Steel,Yes\n6,M01,Morning,218,49,Steel,No\n7,m03,Night,219,50,aluminum,No\n8,M02,Afternoon,221,52,Steel,Yes\n9,M01,Morning,217,47,Steel,No\n10,M03,Night,216,50,Aluminum,No\n11,M02,Morning,222,53,Steel,Yes\n12,M01,Afternoon,214,50,Steel,No\n13,M03,Morning,213,49,Aluminum,No\n14,M02,Morning,NaN,50,Steel,Yes\n15,M01,Night,215,48,Steel,No", "model_steps": ["Load the dataset and identify target variable 'Defect_Flag'", "Clean data by imputing missing numeric values (Temperature_C and Pressure_psi) with median values", "Normalize inconsistent capitalization in categorical columns: 'Machine_ID', 'Operator_Shift', and 'Material_Type'", "Encode categorical variables using one-hot encoding", "Split the dataset into training (80%) and testing (20%) sets", "Standardize numeric features (Temperature_C, Pressure_psi) to zero mean and unit variance", "Train a RandomForestClassifier to predict 'Defect_Flag'", "Perform hyperparameter tuning on number of trees (n_estimators) and maximum tree depth (max_depth) via grid search with 5-fold cross-validation", "Evaluate the final model on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix and extract feature importances"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.9, "f1": 0.87, "confusion_matrix": {"true_positive": 9, "true_negative": 12, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Pressure_psi": 0.32, "Temperature_C": 0.28, "Material_Type_Steel": 0.15, "Machine_ID_M02": 0.1, "Operator_Shift_Morning": 0.08}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Predict whether a movie will be a box office hit based on its characteristics before release.", "raw_table": "MovieID,Genre,DirectorExperience,BudgetMillions,LeadActorPopularity,ReleaseMonth,MarketingSpendMillions,IsBoxOfficeHit\n1,Action,High,150,8.5,July,30,Yes\n2,comedy,Medium,40,6.0,december,15,No\n3,Drama,Low,20,7.2,March,5,No\n4,Action,HIGH,200,9.0,July,40,Yes\n5,Thriller,Medium,60,5.5,October,12,No\n6,Comedy,low,45,6.8,August,20,Yes\n7,Drama,Medium,35,7.0,April,8,No\n8,thriller,High,80,8.1,October,18,Yes\n9,Action,Medium,120,7.9,July,25,Yes\n10,Comedy,Medium,50,,June,10,No\n11,Drama,Low,25,6.7,March,6,No\n12,Action,High,180,9.2,July,35,Yes\n13,Comedy,Medium,55,6.3,June,14,No", "model_steps": ["Load the CSV data into a DataFrame", "Standardize capitalization in 'Genre' and 'DirectorExperience' columns and fill missing values in 'LeadActorPopularity' with the median", "Convert categorical columns 'Genre', 'DirectorExperience', and 'ReleaseMonth' using one-hot encoding", "Split data into train and test sets with an 80/20 ratio, stratifying on the target 'IsBoxOfficeHit'", "Standardize numeric features: BudgetMillions, LeadActorPopularity, MarketingSpendMillions", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search over max_depth values [5, 10, 15] using 5-fold cross-validation", "Select the best model based on cross-validation F1 score", "Evaluate the selected model on the test set computing accuracy, precision, recall, and F1 score", "Generate and display the confusion matrix", "Identify and report the top 3 feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.88, "recall": 0.82, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 8, "false_positive": 1, "false_negative": 2}, "top_feature_importances": {"MarketingSpendMillions": 0.28, "BudgetMillions": 0.25, "LeadActorPopularity": 0.2}, "best_max_depth": 10}}
{"purpose": "Predict whether a wheat field will have a high yield based on soil and weather conditions.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Avg_Temperature_C,Previous_Crop,Soil_pH,Fertilizer_Used,High_Yield\n1,Loam,200,22.5,Wheat,6.5,Yes,Yes\n2,Clay,180,21.0,Corn,7.0,no,No\n3,SAND,150,23.4,Barley,5.8,Yes,Yes\n4,Loam,,20.1,Wheat,6.7,No,No\n5,clay,170,19.5,Corn,7.1,YES,No\n6,Silt,190,22.0,Wheat,6.9,No,Yes\n7,Loam,210,22.8,Corn,6.4,Yes,Yes\n8,Loam,195,23.0,barley,6.6,no,No\n9,Silt,NaN,21.5,Corn,6.7,Yes,Yes\n10,Clay,175,20.8,Wheat,7.0,No,No\n11,Loam,185,22.3,Corn,,Yes,Yes\n12,Sand,160,21.9,Barley,6.0,No,No\n13,Clay,180,20.5,Corn,6.8,Yes,No\n14,Loam,200,22.1,Wheat,6.5,Yes,Yes", "model_steps": ["Load the dataset and identify missing values", "Standardize capitalization and spelling in categorical columns such as Soil_Type, Previous_Crop, and Fertilizer_Used", "Impute missing numeric values in Rainfall_mm and Soil_pH using median values", "One-hot encode categorical features Soil_Type, Previous_Crop, and Fertilizer_Used", "Split the data into training and testing sets with an 80/20 ratio", "Standardize numeric features Rainfall_mm, Avg_Temperature_C, and Soil_pH", "Train a RandomForestClassifier to predict High_Yield", "Perform grid search over max_depth and n_estimators hyperparameters", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix and extract feature importances"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "confusion_matrix": {"true_positive": 6, "false_positive": 1, "true_negative": 5, "false_negative": 1}, "top_feature_importances": {"Rainfall_mm": 0.3, "Soil_Type_Loam": 0.25, "Fertilizer_Used_Yes": 0.2, "Soil_pH": 0.15, "Previous_Crop_Wheat": 0.1}, "best_hyperparameters": {"max_depth": 7, "n_estimators": 100}}}
{"purpose": "Predict whether a government employee is at risk of leaving their position within the next year based on demographic and job-related factors.", "raw_table": "EmployeeID,Age,Department,YearsAtAgency,JobSatisfaction,LastPerformanceRating,EducationLevel,Attrition\nE001,34,Health,5,4,ExceLLent,Bachelor,No\nE002,29,Education,2,,Good,Master,Yes\nE003,45,Transport,12,3,Average,BACHELOR,No\nE004,38,Health,7,5,Excellent,Master,No\nE005,50,Finance,N/A,2,poor,Highschool,Yes\nE006,27,Education,1,4,Good,Master,No\nE007,33,Transport,4,3,Good,Bachelor,No\nE008,41,Health,9,2,Average,HighSchool,Yes\nE009,36,Finance,6,3,,Bachelor,No\nE010,30,Education,3,4,Good,Master,No\nE011,48,Transport,15,1,Poor,Highschool,Yes\nE012,39,Finance,8,3,Average,Bachelor,No", "model_steps": ["Load the CSV data into a DataFrame", "Handle missing values: impute 'YearsAtAgency' missing with median, 'JobSatisfaction' missing with mode, and 'LastPerformanceRating' missing with most frequent", "Standardize capitalization in categorical variables like 'LastPerformanceRating' and 'EducationLevel'", "Convert 'Attrition' target column to binary (Yes=1, No=0)", "Encode categorical features 'Department', 'LastPerformanceRating', and 'EducationLevel' using one-hot encoding", "Standardize numeric features 'Age' and 'YearsAtAgency'", "Split data into train and test sets (80% train, 20% test)", "Train a RandomForestClassifier with default parameters on training data", "Evaluate model performance using accuracy, precision, recall, and F1 score on the test set", "Extract and report feature importances from the trained model"], "model_results": {"accuracy": 0.83, "precision": 0.8, "recall": 0.67, "f1": 0.73, "feature_importances": {"YearsAtAgency": 0.31, "JobSatisfaction": 0.25, "LastPerformanceRating_Good": 0.12, "Age": 0.1, "Department_Health": 0.08, "EducationLevel_Master": 0.06, "Department_Education": 0.04, "Other_Categories": 0.04}}}
{"purpose": "Build a regression model to estimate house prices based on property features and location.", "raw_table": "HouseID,Bedrooms,Bathrooms,SquareFeet,Neighborhood,YearBuilt,HasGarage,Price\n1,3,2,1500,Downtown,1998,Yes,350000\n2,4,3,2500,Suburb,2005,NO,450000\n3,2,1,900,Urban,1980,Yes,200000\n4,3,,1600,Suburb,2010,yes,375000\n5,5,4,3000,downtown,2018,Yes,600000\n6,3,2,1400,Urban,1975,No,230000\n7,4,3,2200,Suburb,2000,yes,420000\n8,2,1,1100,urban,1990,No,210000\n9,,2,1800,Downtown,2003,Yes,390000\n10,3,2,1700,Suburb,,No,360000\n11,4,3,2500,Suburb,2012,Yes,480000\n12,3,2,1600,Suburb,2007,Yes,395000\n13,2,1,900,Urban,1985,No,215000\n14,3,2,1500,Downtown,1999,yes,365000\n15,4,3,2400,suburb,2008,Yes,460000", "model_steps": ["Load the CSV data into a DataFrame", "Identify and clean messy values: standardize 'HasGarage' to boolean and fix capitalization in 'Neighborhood'", "Impute missing values in 'Bedrooms', 'Bathrooms', and 'YearBuilt' using median values", "One-hot encode the 'Neighborhood' categorical variable", "Split the dataset into train and test sets (80/20 split)", "Standardize numeric features: Bedrooms, Bathrooms, SquareFeet, YearBuilt", "Train a RandomForestRegressor model on the training data", "Perform grid search to tune number of estimators and max_depth", "Evaluate model performance on test data using RMSE, MAE, and R2 metrics", "Analyze and report top 5 feature importances from the model"], "model_results": {"rmse": 24000, "mae": 18000, "r2": 0.87, "top_feature_importances": {"SquareFeet": 0.42, "Neighborhood_Downtown": 0.18, "HasGarage": 0.12, "Bedrooms": 0.1, "YearBuilt": 0.08}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 15}}}
{"purpose": "Predict customer churn probability in a telecommunications company based on usage patterns and customer demographics.", "raw_table": "customer_id,tenure,monthly_charges,internet_service,contract,multiple_lines,tech_support,churn\n001,12,70.35,Fiber optic,Month-to-month,No,Yes,No\n002,5,45.50,dsl,One year,Yes,No,Yes\n003,24,89.10,Fiber Optic,Two year,No,No,No\n004,NaN,60.00,dsl,Month-to-month,No,Yes,Yes\n005,8,75.25,Fiber optic,Month-to-month,Yes,No,No\n006,18,55.00,DSL,One year,No,Yes,No\n007,3,NaN,Fiber Optic,Month-to-month,No,No,Yes\n008,30,80.00,fiber optic,Two Year,Yes,Yes,No\n009,15,65.75,Dsl,One Year,No,No,No\n010,10,59.95,Fiber optic,Month-to-month,Yes,NaN,Yes\n011,20,85.10,Fiber Optic,Two year,No,Yes,No\n012,7,49.90,dsl,Month-to-month,Yes,No,Yes\n013,NaN,NaN,DSL,One year,No,Yes,No\n014,25,90.50,Fiber optic,Two year,No,Yes,No", "model_steps": ["Load the CSV data into a DataFrame", "Clean data by correcting inconsistent capitalization in categorical columns (e.g., 'fiber optic', 'Fiber Optic', 'Fiber optic' unified to 'Fiber Optic')", "Impute missing numeric values (tenure and monthly_charges) using median values", "Impute missing categorical values (e.g., 'multiple_lines', 'tech_support') with the most frequent category", "Convert categorical variables (internet_service, contract, multiple_lines, tech_support) into one-hot encoded features", "Split the dataset into 80% training and 20% testing sets stratified on the 'churn' target", "Standardize numeric features (tenure, monthly_charges) using the training set statistics", "Train a RandomForestClassifier with 100 trees to predict churn", "Perform hyperparameter tuning with grid search over max_depth values of [5, 10, 15]", "Evaluate the best model on the test set reporting accuracy, precision, recall, and F1-score", "Generate the confusion matrix for test predictions"], "model_results": {"accuracy": 0.82, "precision": 0.79, "recall": 0.75, "f1": 0.77, "confusion_matrix": [[18, 5], [6, 19]], "top_feature_importances": {"contract_One year": 0.21, "internet_service_Fiber Optic": 0.18, "monthly_charges": 0.15, "tech_support_Yes": 0.12, "tenure": 0.1}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict if a taxi trip will exceed 20 minutes in duration.", "raw_table": "trip_id,passenger_count,trip_distance,rate_code,payment_type,pickup_location,dropoff_location,trip_duration_over_20min\n1,1,2.5,1,Credit card,Downtown,Downtown,Yes\n2,3,0.8,1,cash,Uptown,Midtown,No\n3,2,5.0,2,Credit Card,Downtown,Uptown,Yes\n4,1,1.2,1,credit card,midtown,Downtown,No\n5,NaN,3.0,1,CASH,Midtown,Downtown,Yes\n6,1,7.5,2,Credit card,Downtown,Uptown,Yes\n7,2,1.1,1,Cash,Uptown,Midtown,No\n8,1,0.5,1,Credit Card,Downtown,Uptown,No\n9,4,4.0,2,Credit card,,Midtown,Yes\n10,1,3.3,1,Credit card,Midtown,Downtown,Yes\n11,2,NaN,2,Credit card,Uptown,Downtown,No\n12,1,2.8,1,Credit Card,Downtown,Midtown,No\n13,3,1.5,1,Cash,Midtown,Uptown,No\n14,2,6.0,2,Credit Card,Downtown,Uptown,Yes", "model_steps": ["Load the CSV data into a dataframe", "Handle missing values: impute missing passenger_count with median and trip_distance with median", "Standardize capitalization in payment_type and pickup_location columns", "Encode categorical variables: one-hot encode payment_type, pickup_location, dropoff_location, and rate_code", "Split the dataset into training (80%) and testing (20%) sets stratified by target variable trip_duration_over_20min", "Standardize numeric features: passenger_count and trip_distance", "Train a RandomForestClassifier to predict trip_duration_over_20min", "Perform grid search over n_estimators and max_depth parameters using cross-validation", "Evaluate the model on the test set using accuracy, F1 score, precision, and recall", "Compute the confusion matrix", "Extract and rank feature importances"], "model_results": {"accuracy": 0.85, "f1": 0.82, "precision": 0.8, "recall": 0.85, "confusion_matrix": {"true_positive": 30, "false_positive": 7, "true_negative": 40, "false_negative": 5}, "top_feature_importances": {"trip_distance": 0.35, "passenger_count": 0.15, "payment_type_Credit card": 0.12, "pickup_location_Downtown": 0.1, "dropoff_location_Uptown": 0.08}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Build a classification model to predict whether a student will pass the final exam based on demographic and study habit data.", "raw_table": "StudentID,Age,Gender,StudyHoursPerWeek,AttendanceRate,PreviousGrade,ParentalEducation,PassedFinal\n1,17,Male,12,0.95,B,College,Yes\n2,16,Female,8,0.85,c,High School,No\n3,18,Male,15,0.90,A,College,Yes\n4,17,FEMALE,7,,B,High School,No\n5,16,Male,5,0.80,C,Some college,No\n6,17,Female,10,0.88,B,College,Yes\n7,16,male,9,0.92,b,high school,Yes\n8,18,Female,14,0.98,A,College,Yes\n9,17,Female,,0.70,D,Some College,No\n10,16,Male,6,0.75,C,High School,No\n11,17,Female,11,0.89,B,High School,Yes\n12,16,Male,4,0.60,D,High school,No\n13,18,Female,13,0.94,A,College,Yes\n14,17,Male,7,0.85,C,College,No", "model_steps": ["Load CSV data and inspect for missing and inconsistent values", "Standardize capitalization of categorical fields (e.g., Gender, ParentalEducation, PreviousGrade)", "Impute missing values in StudyHoursPerWeek and AttendanceRate with median values", "Encode categorical variables: Gender (Male/Female), PreviousGrade (A-D), ParentalEducation (High School, Some College, College) using one-hot encoding", "Convert target variable 'PassedFinal' to binary (Yes=1, No=0)", "Split dataset into training (80%) and test (20%) sets", "Standardize numeric features StudyHoursPerWeek, AttendanceRate, and Age", "Train a RandomForestClassifier on the training data with default parameters", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix to analyze classification errors", "Identify top 3 feature importances from the trained Random Forest model"], "model_results": {"accuracy": 0.85, "f1": 0.86, "precision": 0.88, "recall": 0.84, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"StudyHoursPerWeek": 0.35, "AttendanceRate": 0.3, "PreviousGrade_B": 0.12}}}
{"purpose": "Predict whether a manufactured part will fail quality inspection based on sensor readings and operational settings.", "raw_table": "Part_ID,Machine_Type,Operator_Experience,Temperature_C,Pressure_psi,Humidity_pct,Shift,Failure\n001,TypeA,5,75.2,30,45,Day,No\n002,Typeb,3,78.5,32,47,Night,Yes\n003,TypeA,,74.8,29,44,Day,No\n004,TypeC,7,77.0,35,50,Night,Yes\n005,TYPEB,2,76.3,31,46,Day,No\n006,TypeA,4,NaN,30,43,Night,No\n007,TypeA,6,75.5,28,44,Day,No\n008,TypeC,5,79.1,36,51,Night,Yes\n009,TypeB,3,78.0,34,48,day,Yes\n010,TypeA,5,75.0,30,45,Day,No\n011,TypeC,7,77.5,33,52,Night,Yes\n012,TypeB,4,76.8,32,47,Night,No\n013,,3,75.3,31,46,Day,No\n014,TypeA,5,74.9,29,44,Day,No", "model_steps": ["Normalize inconsistent capitalization in Machine_Type and Shift columns", "Impute missing values in Operator_Experience and Temperature_C with median values", "Convert categorical variables Machine_Type and Shift into one-hot encoded features", "Split data into training (80%) and testing (20%) sets maintaining class distribution", "Standardize numeric features: Temperature_C, Pressure_psi, Humidity_pct, Operator_Experience", "Train a RandomForestClassifier to predict Failure", "Perform grid search tuning max_depth and n_estimators parameters", "Evaluate model on test set computing accuracy, precision, recall, and F1 score", "Generate confusion matrix and feature importance plot"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"True_Positive": 7, "True_Negative": 10, "False_Positive": 2, "False_Negative": 1}, "top_feature_importances": {"Temperature_C": 0.32, "Pressure_psi": 0.25, "Machine_Type_TypeC": 0.15, "Operator_Experience": 0.1, "Shift_Night": 0.08}, "best_hyperparameters": {"max_depth": 6, "n_estimators": 100}}}
{"purpose": "Predict whether a student will pass the final exam based on attendance, study hours, and prior grades.", "raw_table": "StudentID,AttendanceRate,StudyHoursPerWeek,PreviousGrade,Gender,SchoolType,PassFinal\nS001,0.95,12,A,Male,Public,Yes\nS002,0.80,8,B,Female,Private,Yes\nS003,0.60,6,C,Female,Public,No\nS004,0.85,NaN,B,Male,public,Yes\nS005,0.50,4,D,Female,Private,No\nS006,0.90,10,A,Male,Public,Yes\nS007,0.40,3,,Female,Public,No\nS008,0.75,7,B,Male,Private,Yes\nS009,0.55,5,C,Female,Private,No\nS010,missing,9,B,Male,Public,Yes\nS011,0.88,11,A,Female,Public,Yes\nS012,0.65,6,C,Male,Private,No\nS013,0.70,7,B,Female,Public,Yes\nS014,0.45,2,D,Male,Private,No", "model_steps": ["Load the CSV data and inspect for missing and inconsistent values", "Clean 'AttendanceRate' by replacing 'missing' with the median attendance rate", "Impute missing values in 'StudyHoursPerWeek' and 'PreviousGrade' using median and mode respectively", "Normalize inconsistent capitalization in 'SchoolType' to uniform casing", "Encode categorical variables: 'Gender', 'SchoolType', and 'PreviousGrade' using one-hot encoding", "Convert target variable 'PassFinal' to binary (Yes=1, No=0)", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features: 'AttendanceRate' and 'StudyHoursPerWeek'", "Train a RandomForestClassifier with default hyperparameters", "Evaluate the model using accuracy, precision, recall, and F1 score on the test set", "Generate a confusion matrix for test set predictions"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"AttendanceRate": 0.32, "StudyHoursPerWeek": 0.28, "PreviousGrade_B": 0.12, "PreviousGrade_A": 0.1, "SchoolType_Public": 0.08, "Gender_Male": 0.05, "PreviousGrade_C": 0.05}}}
{"purpose": "Build a classification model to predict whether a ride will be delayed by more than 10 minutes.", "raw_table": "ride_id,driver_experience_years,vehicle_type,trip_distance_km,weather_condition,pickup_hour,traffic_level,delayed\nR001,5,SUV,12.5,Clear,9,Medium,No\nR002,2,sedan,5.3,Rain,17,Heavy,Yes\nR003,10,SUV,22,Snow,22,Light,No\nR004,3,sedan,7.8,clear,14,Heavy,Yes\nR005,1,Truck,15.0,Fog,7,medium,No\nR006,8,suv,20.2,Clear,19,Heavy,Yes\nR007,4,Sedan,3.1,Fog,6,Light,No\nR008,6,Truck,11.0,Rain,13,Medium,Yes\nR009,NaN,SUV,9.7,Clear,8,Medium,No\nR010,7,sedan,18.4,Snow,21,Heavy,Yes\nR011,5,Sedan,14.3,CLEAR,16,Medium,No\nR012,3,truck,10.5,Fog,5,Light,No\nR013,2,SUV,4.9,Rain,18,Heavy,Yes\nR014,9,Sedan,16.7,Clear,12,medium,No", "model_steps": ["Load the CSV data into a DataFrame", "Inspect and clean data: normalize capitalization in 'vehicle_type' and 'weather_condition', and impute missing 'driver_experience_years' using median", "Convert target variable 'delayed' to binary (Yes=1, No=0)", "Encode categorical variables ('vehicle_type', 'weather_condition', 'traffic_level') using one-hot encoding", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features ('driver_experience_years', 'trip_distance_km', 'pickup_hour')", "Train a RandomForestClassifier with default hyperparameters", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Extract and report feature importances from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.8, "recall": 0.87, "top_feature_importances": {"traffic_level_Heavy": 0.32, "trip_distance_km": 0.25, "weather_condition_Rain": 0.15, "driver_experience_years": 0.12, "pickup_hour": 0.1, "vehicle_type_SUV": 0.06}}}
{"purpose": "Predict whether a customer will make a purchase during a browsing session based on their session and demographic data.", "raw_table": "session_id,session_length_seconds,page_views,device_type,referrer,customer_age,customer_gender,purchase\nS001,300,5,Mobile,Google,25,Male,Yes\nS002,120,2,Desktop,Direct,NaN,Female,No\nS003,450,8,mobile,Facebook,35,Male,Yes\nS004,60,1,Tablet,Organic,29,Female,No\nS005,NaN,3,Desktop,google,22,Female,No\nS006,200,5,Mobile,Direct,NA,Male,Yes\nS007,180,4,desktop,Facebook,27,female,No\nS008,90,2,Tablet,Organic,31,Male,No\nS009,400,7,Mobile,Google,28,Male,Yes\nS010,150,3,Tablet,Direct,,Female,No\nS011,360,6,Desktop,facebook,33,Male,Yes\nS012,80,2,Mobile,organic,26,Female,No\nS013,210,4,Desktop,Direct,24,Male,Yes\nS014,300,5,Mobile,Google,29,Female,No", "model_steps": ["Load CSV data and parse into a DataFrame", "Identify and impute missing values in 'session_length_seconds' and 'customer_age' using median imputation", "Standardize capitalization and fix inconsistent values in 'device_type' and 'referrer' columns", "Convert 'purchase' target variable to binary numeric (Yes=1, No=0)", "One-hot encode categorical variables: 'device_type', 'referrer', 'customer_gender'", "Split the dataset into training (80%) and testing (20%) sets using a stratified split on the target", "Standardize numeric features: 'session_length_seconds', 'page_views', 'customer_age'", "Train a Logistic Regression classifier on the training data", "Evaluate the model on the test set computing accuracy, precision, recall, and F1 score", "Generate the confusion matrix for test predictions"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.78, "f1": 0.8, "confusion_matrix": {"true_positive": 20, "true_negative": 35, "false_positive": 5, "false_negative": 6}, "top_feature_importances": {"session_length_seconds": 0.45, "page_views": 0.3, "device_type_Mobile": 0.12, "referrer_Google": 0.08, "customer_age": 0.05}}}
{"purpose": "Predict hourly energy consumption of residential buildings based on weather and building characteristics.", "raw_table": "Building_ID,Building_Type,Hour,Outdoor_Temp,Humidity,Weekday,Energy_Consumption_kWh\nB001,Apartment,0,15.2,45,Monday,12.5\nB002,Detached,1,14.8,47,tuesday,18.3\nB003,Apartment,2,14.6,,Wednesday,NaN\nB004,SemiDetached,3,14.3,50,Thursday,21.1\nB005,Detached,4,14.0,52,Friday,20.2\nB006,Apartment,5,13.7,49,Saturday,13.7\nB007,SemiDetached,6,13.5,48,Sunday,19.0\nB008,Apartment,7,13.3,47,Monday,15.6\nB009,Detached,8,13.1,46,Tuesday,22.4\nB010,SemiDetached,9,13.0,45,Wednesday,20.0\nB011,Apartment,10,12.9,44,Thursday,14.1\nB012,Detached,11,12.7,43,Friday,23.0\nB013,Apartment,12,12.6,42,Saturday,15.5\nB014,SemiDetached,13,12.5,41,Sunday,19.3", "model_steps": ["Load dataset from CSV string and parse data types", "Identify and handle missing values: impute missing Humidity with median, drop rows with missing Energy_Consumption_kWh", "Normalize inconsistent categorical entries in 'Weekday' column to title case", "One-hot encode categorical variables: Building_Type and Weekday", "Split data into train and test sets with 80/20 ratio stratified by Building_Type", "Standardize numeric features: Hour, Outdoor_Temp, Humidity", "Train Gradient Boosting Regressor to predict Energy_Consumption_kWh", "Perform hyperparameter tuning on number of estimators and learning rate via grid search with 5-fold cross-validation", "Evaluate final model performance on test set using RMSE, MAE, and R-squared", "Analyze feature importances from the trained model"], "model_results": {"rmse": 1.75, "mae": 1.32, "r2": 0.87, "best_hyperparameters": {"n_estimators": 150, "learning_rate": 0.1}, "feature_importances": {"Outdoor_Temp": 0.38, "Hour": 0.25, "Building_Type_SemiDetached": 0.15, "Building_Type_Detached": 0.12, "Humidity": 0.07, "Weekday_Saturday": 0.03}}}
{"purpose": "Predict whether a social media post will go viral based on post attributes and user engagement features.", "raw_table": "post_id,post_length,num_hashtags,post_type,user_followers,user_verified,user_region,avg_engagement,target_viral\n1,120,3,Image,1500,Yes,North America,300,1\n2,85,1,video,800,No,Europe,120,0\n3,200,,Text,5000,yes,Asia,1500,1\n4,60,0,IMAGE,300,No,North America,30,0\n5,150,4,Video,2500,No,Europe,,1\n6,100,2,Text,700,No,Asia,70,0\n7,130,3,image,1800,Yes,Europe,400,1\n8,95,1,Video,NaN,No,North America,100,0\n9,110,2,Text,1200,No,Asia,250,1\n10,80,0,video,600,No,Europe,50,0\n11,210,5,Image,4000,YES,North America,1800,1\n12,55,0,text,200,No,South America,25,0\n13,140,3,Video,2200,No,Asia,500,1\n14,90,1,Image,NaN,Yes,Europe,80,0", "model_steps": ["Load CSV data and inspect for missing or inconsistent values", "Normalize 'user_verified' column to boolean (Yes/yes/YES -> True, No -> False)", "Impute missing numeric values in 'num_hashtags', 'user_followers', and 'avg_engagement' using median imputation", "Encode 'post_type' and 'user_region' categorical variables using one-hot encoding", "Split data into training and test sets with 80% for training and 20% for testing", "Scale numeric features using standard scaling", "Train a RandomForestClassifier to predict 'target_viral'", "Perform grid search over 'max_depth' and 'n_estimators' hyperparameters", "Evaluate model using accuracy, F1 score, precision, and recall on the test set", "Generate a confusion matrix to analyze prediction errors"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.82, "recall": 0.87, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"avg_engagement": 0.35, "user_followers": 0.25, "post_length": 0.15, "post_type_Image": 0.1, "user_verified": 0.07, "num_hashtags": 0.05, "user_region_Europe": 0.03}, "best_hyperparameters": {"max_depth": 7, "n_estimators": 100}}}
{"purpose": "Predict hourly energy consumption (kWh) for smart homes based on weather and occupancy data.", "raw_table": "HouseID,Hour,Temperature_C,Weather,Occupancy,DayType,Energy_kWh\nH01,0,18.5,sunny,3,weekday,1.2\nH02,1,17.8,Cloudy,2,weekday,1.0\nH01,2,16.0,rain,0,weekday,0.7\nH03,3,15.5,SUNNY,1,weekend,0.8\nH02,4,15.0,rain,missing,weekend,0.9\nH01,5,14.8,sunny,0,Weekend,0.5\nH03,6,15.1,cloudy,1,weekday,0.6\nH02,7,17.0,Rain,4,weekday,1.5\nH01,8,19.2,Sunny,5,weekday,2.0\nH03,9,20.0,Cloudy,3,weekday,1.8\nH01,10,21.5,sunny,2,weekday,2.3\nH02,11,22.0,cloudy,3,weekday,2.1\nH03,12,22.5,sunny,4,weekday,2.4\nH01,13,23.0,rain,3,weekday,2.2", "model_steps": ["Load data and identify target variable as Energy_kWh", "Fix missing values in Occupancy by imputing median occupancy", "Normalize categorical columns Weather and DayType by converting all text to lowercase", "One-hot encode categorical features Weather and DayType", "Split data into train and test sets with 80% for training and 20% for testing", "Standardize numeric features Temperature_C, Hour, and Occupancy", "Train a Gradient Boosting Regressor to predict Energy_kWh", "Tune hyperparameters max_depth and learning_rate using grid search with 5-fold cross-validation", "Evaluate model performance on test set using RMSE, MAE, and R2 metrics", "Identify and report top 3 feature importances from the trained model"], "model_results": {"rmse": 0.18, "mae": 0.13, "r2": 0.87, "top_feature_importances": {"occupancy": 0.41, "hour": 0.29, "temperature_c": 0.15}, "hyperparameters": {"max_depth": 4, "learning_rate": 0.1}}}
{"purpose": "Build a classification model to predict whether a taxi trip will have a tip above 20%.", "raw_table": "trip_id,passenger_count,trip_distance,payment_type,day_of_week,tip_percentage,tip_above_20\n1,1,3.2,Card,Monday,18.5,No\n2,2,7.5,CASH,Tuesday,22.0,Yes\n3,1,,Card,Wednesday,15.0,No\n4,3,5.1,card,Thursday,25.3,Yes\n5,1,2.7,Cash,Friday,19.0,No\n6,4,10.0,CARD,Saturday,30.2,Yes\n7,2,6.3,Cash,Sunday,missing,No\n8,,4.2,Card,Monday,20.1,Yes\n9,1,3.5,card,Tuesday,17.5,No\n10,3,8.2,CASH,Wednesday,28.0,Yes\n11,1,2.9,Cash,Thursday,16.0,No\n12,5,12.0,CARD,Friday,35.0,Yes\n13,2,5.5,Card,Saturday,21.0,Yes\n14,3,7.7,Cash,Sunday,19.5,No\n15,1,3.1,card,Monday,22.5,Yes", "model_steps": ["Load CSV data into a dataframe and inspect for missing and inconsistent values", "Standardize casing for 'payment_type' categorical variable to lowercase", "Impute missing numeric values in 'passenger_count' and 'trip_distance' with median values", "Impute missing 'tip_percentage' with median tip percentage", "Convert 'tip_above_20' target variable to binary (Yes=1, No=0)", "One-hot encode categorical variables 'payment_type' and 'day_of_week'", "Split data into training (80%) and test (20%) sets with stratification on target variable", "Standardize numeric features 'passenger_count' and 'trip_distance'", "Train a RandomForestClassifier with default hyperparameters", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Extract and rank feature importances from the trained model"], "model_results": {"accuracy": 0.87, "f1": 0.85, "precision": 0.83, "recall": 0.88, "top_feature_importances": {"tip_percentage": 0.45, "trip_distance": 0.2, "payment_type_card": 0.1, "day_of_week_Saturday": 0.07, "passenger_count": 0.05}}}
{"purpose": "Predict whether a customer will make a purchase during a promotional campaign based on their demographics and past interactions.", "raw_table": "CustomerID,Age,Gender,Annual_Income,Last_Purchase_Month,Membership_Type,Num_Website_Visits,Purchase_Made\n1,34,Male,55000,Jan,Gold,15,Yes\n2,28,female,48000,FEB,Silver,8,No\n3,45,Male,NaN,Mar,Platinum,20,Yes\n4,NaN,FEMALE,62000,Apr,Gold,12,No\n5,37,Male,58000,may,Silver,NaN,Yes\n6,50,Male,72000,Jun,Platinum,25,Yes\n7,29,Female,53000,jul,Gold,10,No\n8,41,male,60000,Aug,Silver,18,Yes\n9,35,Female,49000,Sep,Silver,11,No\n10,33,Male,56000,Oct,Gold,13,Yes\n11,47,Female,67000,Nov,,22,Yes\n12,38,Male,51000,Dec,Silver,7,No\n13,44,female,65000,Jan,Gold,19,Yes\n14,31,Male,NaN,Feb,Silver,9,No\n15,43,Male,70000,Mar,Platinum,21,Yes", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Standardize the 'Gender' and 'Last_Purchase_Month' columns to consistent casing", "Impute missing numeric values in 'Age', 'Annual_Income', and 'Num_Website_Visits' with the median of respective columns", "Fill missing 'Membership_Type' values with the mode category", "Encode categorical variables ('Gender', 'Last_Purchase_Month', 'Membership_Type') using one-hot encoding", "Convert target variable 'Purchase_Made' to binary labels (Yes=1, No=0)", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features ('Age', 'Annual_Income', 'Num_Website_Visits') using StandardScaler", "Train a RandomForestClassifier with default parameters on the training set", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Extract and report feature importances from the trained RandomForestClassifier"], "model_results": {"accuracy": 0.87, "precision": 0.84, "recall": 0.9, "f1": 0.87, "feature_importances": {"Num_Website_Visits": 0.32, "Membership_Type_Platinum": 0.21, "Annual_Income": 0.18, "Membership_Type_Gold": 0.1, "Age": 0.08, "Gender_Male": 0.05, "Last_Purchase_Month_Jan": 0.03, "Last_Purchase_Month_Feb": 0.02, "Membership_Type_Silver": 0.01}}}
{"purpose": "Predict whether a government grant application will be approved based on applicant and project characteristics.", "raw_table": "Applicant_ID,Applicant_Age,Applicant_Income_kUSD,Project_Type,Region,Previous_Grants_Approved,Application_Status\n1,45,55.5,Infrastructure,North,2,Approved\n2,39,NaN,education,South,1,Rejected\n3,52,70.2,Health,East,3,Approved\n4,29,40.0,Infrastructure,west,0,Rejected\n5,60,80.1,Health,North,5,Approved\n6,,65.3,Education,South,1,Rejected\n7,34,50.0,Infrastructure,East,1,Approved\n8,41,59.8,health,West,2,Rejected\n9,37,48.7,Education,North,NaN,Approved\n10,55,75.0,Infrastructure,South,3,Approved\n11,43,53.2,Health,East,1,Rejected\n12,31,45.5,Education,West,0,Rejected\n13,47,61.0,Infrastructure,North,2,Approved\n14,50,69.5,health,South,4,Approved", "model_steps": ["Load data and identify missing values in Applicant_Age, Applicant_Income_kUSD, and Previous_Grants_Approved", "Standardize capitalization in categorical columns Project_Type and Region to ensure consistency", "Impute missing numeric values with median for Applicant_Age and Applicant_Income_kUSD, and mode for Previous_Grants_Approved", "One-hot encode categorical variables Project_Type and Region", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features Applicant_Age, Applicant_Income_kUSD, and Previous_Grants_Approved using training set statistics", "Train a RandomForestClassifier to predict Application_Status", "Perform grid search over max_depth=[3,5,7] and n_estimators=[50,100]", "Evaluate model performance using accuracy, F1 score, precision, and recall on the test set", "Generate confusion matrix and extract top 3 important features"], "model_results": {"accuracy": 0.79, "f1": 0.77, "precision": 0.75, "recall": 0.8, "confusion_matrix": {"true_positive": 7, "false_positive": 3, "true_negative": 6, "false_negative": 2}, "top_feature_importances": {"Previous_Grants_Approved": 0.34, "Applicant_Income_kUSD": 0.27, "Project_Type_Infrastructure": 0.15}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict if a crop field will have high or low yield based on soil and weather conditions.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Temperature_C,Previous_Crop,Yield_Category\n1,Loam,120,22.5,Wheat,High\n2,Clay,85,25.0,Corn,Low\n3,Sandy,NaN,23.1,Barley,High\n4,Loam,110,20.0,wheat,High\n5,Clay,95,21.7,Corn,Low\n6,Sandy,78,19.5,Barley,Low\n7,loam,130,24.3,Wheat,High\n8,Clay,100,NaN,CORN,Low\n9,Sandy,90,22.0,Barley,Low\n10,Loam,115,23.5,Wheat,High\n11,Sandy,85,21.0,Barley,Low\n12,Clay,NaN,20.5,Corn,Low\n13,Loam,105,22.8,Wheat,High\n14,Clay,88,18.9,Corn,Low\n15,Sandy,92,21.9,barley,Low", "model_steps": ["Load the dataset and identify missing values and inconsistent capitalization in categorical columns", "Fill missing numeric values (Rainfall_mm, Temperature_C) using median imputation", "Normalize the capitalization of Soil_Type and Previous_Crop columns to title case", "Convert categorical columns (Soil_Type, Previous_Crop) to one-hot encoded variables", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features (Rainfall_mm, Temperature_C) using training set statistics", "Train a RandomForestClassifier to predict Yield_Category", "Perform grid search over number of trees and max_depth hyperparameters", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and extract top 3 feature importances"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.9, "f1": 0.87, "confusion_matrix": [[7, 1], [2, 5]], "top_feature_importances": {"Rainfall_mm": 0.35, "Soil_Type_Loam": 0.25, "Temperature_C": 0.2}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a tomato plant is likely to have high yield based on environmental and plant characteristics.", "raw_table": "PlantID,SoilType,SunlightHours,AvgTempC,WateringFrequency,LeafColor,HighYield\n1,Loam,8,25,3,Green,Yes\n2,Sandy,7,23,,Yellow,No\n3,Clay,6,NaN,4,green,Yes\n4,Loam,8,26,3,Green,Yes\n5,Sandy,5,20,2,Brown,No\n6,Clay,7,22,3,Yellow,No\n7,Loam,8,24,3,Green,Yes\n8,Sandy,6,23,2,Yellow,No\n9,Loam,,25,3,green,Yes\n10,Clay,7,21,3,Yellow,No\n11,Sandy,5,20,1,Brown,No\n12,Loam,8,26,3,Green,Yes\n13,Clay,6,22,3,Green,Yes\n14,Sandy,7,23,2,yellow,No", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values: impute missing SunlightHours and AvgTempC with median values, and WateringFrequency with mode", "Standardize capitalization in categorical columns like LeafColor and SoilType to ensure consistency", "Convert categorical columns SoilType and LeafColor into one-hot encoded variables", "Encode target variable HighYield as binary (Yes=1, No=0)", "Split data into train and test sets with 80% training and 20% testing", "Standardize numeric features SunlightHours, AvgTempC, and WateringFrequency", "Train a RandomForestClassifier with 100 trees on the training set", "Evaluate the model with accuracy, F1 score, precision, and recall on the test set", "Compute and analyze the confusion matrix", "Identify and report the top 3 feature importances from the model"], "model_results": {"accuracy": 0.86, "f1": 0.88, "precision": 0.9, "recall": 0.86, "confusion_matrix": [[5, 1], [1, 7]], "top_feature_importances": {"SunlightHours": 0.32, "WateringFrequency": 0.25, "LeafColor_Green": 0.18}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a movie will be a box office hit based on production details and genre.", "raw_table": "MovieID,Title,Genre,BudgetMillions,DirectorExperience,LeadActorPopularity,ReleaseMonth,BoxOfficeHit\n1,Starlight,Sci-Fi,150,10,85,July,Yes\n2,Laugh Riot,Comedy,50,5,70,March,No\n3,Haunted Nights,Horror,30,,90,october,No\n4,Love Actually,Romance,40,7,75,February,Yes\n5,Space Odyssey,Sci-fi,200,15,95,December,Yes\n6,Fast Wheels,Action,100,3,80,June,Yes\n7,Silent Whisper,Horror,25,11,65,October,No\n8,Comedy Hour,comedy,45,4,60,May,No\n9,Galactic Wars,Sci-Fi,180,20,88,July,Yes\n10,Romantic Escape,romance,35,6,72,April,No\n11,Thrill Ride,Action,90,8,85,August,Yes\n12,The Unknown,Thriller,60,9,78,September,No\n13,Fun Times,Comedy,55,2,55,March,No\n14,Nightmare Alley,horror,40,12,60,october,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Normalize capitalization in categorical columns: Genre and ReleaseMonth", "Handle missing values in DirectorExperience by imputing the median", "Convert target variable BoxOfficeHit to binary numeric format (Yes=1, No=0)", "One-hot encode categorical features Genre and ReleaseMonth", "Standardize numeric features BudgetMillions, DirectorExperience, LeadActorPopularity", "Split the dataset into train and test sets (80% train, 20% test)", "Train a RandomForestClassifier on the training data", "Perform grid search over max_depth with values [5, 10, 15]", "Evaluate the model on the test set calculating accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze prediction errors"], "model_results": {"accuracy": 0.83, "precision": 0.8, "recall": 0.85, "f1": 0.82, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"BudgetMillions": 0.35, "LeadActorPopularity": 0.25, "DirectorExperience": 0.15, "Genre_Sci-Fi": 0.1, "ReleaseMonth_July": 0.05}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict hourly energy consumption in a residential building based on weather and occupancy data.", "raw_table": "Hour,Outdoor_Temp_C,Occupancy_Status,Heating_Type,Day_of_Week,Energy_Consumption_kWh\n0,5.2,Occupied,Gas,Monday,12.5\n1,4.8,occupied,Electric,Monday,11.8\n2,4.5,,Gas,Monday,11.2\n3,4.3,Occupied,Natural gas,Monday,10.9\n4,4.1,Occupied,gas,Monday,10.7\n5,4.0,Occupied,Electric,Monday,11.0\n6,5.5,Occupied,Electric,Monday,12.3\n7,8.0,Occupied,Gas,Monday,14.1\n8,12.2,Unoccupied,Electric,Monday,9.5\n9,15.0,Unoccupied,Electric,Monday,8.2\n10,18.5,Occupied,Electric,Monday,13.4\n11,22.0,Occupied,Gas,Monday,16.0\n12,25.1,occupied,Gas,Monday,18.2\n13,26.3,Occupied,Electric,Monday,19.5\n14,27.0,Unoccupied,Natural Gas,Monday,14.3", "model_steps": ["Load CSV data into a DataFrame", "Identify and impute missing values in Occupancy_Status column with mode", "Standardize capitalization and unify Heating_Type categories (e.g., 'Gas', 'Natural gas', 'gas' all mapped to 'Gas')", "One-hot encode categorical variables: Occupancy_Status, Heating_Type, Day_of_Week", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features: Hour and Outdoor_Temp_C", "Train a Gradient Boosting Regressor model to predict Energy_Consumption_kWh", "Perform hyperparameter tuning on learning_rate and n_estimators using 5-fold cross-validation", "Evaluate model performance on test set using RMSE, MAE, and R2", "Analyze and report top 3 feature importances"], "model_results": {"rmse": 1.23, "mae": 0.95, "r2": 0.87, "top_feature_importances": {"Outdoor_Temp_C": 0.37, "Hour": 0.29, "Occupancy_Status_Occupied": 0.15}, "best_hyperparameters": {"learning_rate": 0.1, "n_estimators": 100}}}
{"purpose": "Predict the likelihood of hospital readmission within 30 days for diabetic patients.", "raw_table": "patient_id,age,gender,BMI,blood_glucose_level,smoking_status,discharge_disposition,readmitted_30days\n1,54,Male,32.5,180,Former smoker,Home,Yes\n2,67,Female,NaN,210,non-smoker,HOME,No\n3,45,Female,28.7,NaN,Smoker,Nursing home,Yes\n4,62,Male,30.1,195,smoker,Home,No\n5,50,,27.9,165,Former Smoker,Rehab,No\n6,70,Female,35.2,220,Non-Smoker,Rehab,Yes\n7,58,Male,29.8,200,smoker,Home,No\n8,40,Female,26.3,150,Non-smoker,HOME,No\n9,55,Male,31.0,,Former smoker,Nursing home,Yes\n10,48,Female,NaN,170,Smoker,home,No\n11,60,Female,33.5,205,,Rehab,Yes\n12,65,Male,34.0,215,Former smoker,Home,No\n13,53,Female,28.0,185,non-smoker,Rehab,Yes\n14,47,Male,27.5,160,smoker,Home,No", "model_steps": ["Load the dataset and identify missing values and inconsistent capitalization", "Fill missing numeric values (BMI and blood_glucose_level) using median imputation", "Normalize capitalization in categorical columns such as discharge_disposition and smoking_status", "Encode categorical features using one-hot encoding", "Split the data into training (80%) and testing sets (20%) stratified by the target variable readmitted_30days", "Standardize numeric features (age, BMI, blood_glucose_level)", "Train a RandomForestClassifier to predict readmission within 30 days", "Perform grid search cross-validation to tune max_depth parameter of the RandomForest", "Evaluate model performance on the test set using accuracy, F1 score, precision, and recall", "Generate and analyze the confusion matrix to understand false positives and false negatives"], "model_results": {"accuracy": 0.79, "f1": 0.75, "precision": 0.72, "recall": 0.78, "confusion_matrix": {"true_positive": 7, "false_positive": 3, "true_negative": 8, "false_negative": 2}, "top_feature_importances": {"blood_glucose_level": 0.32, "discharge_disposition_Rehab": 0.18, "BMI": 0.15, "smoking_status_Smoker": 0.12, "age": 0.1}, "best_hyperparameter": {"max_depth": 6}}}
{"purpose": "Build a classification model to predict whether a field's soil moisture level is suitable for planting.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Temperature_C,Soil_Moisture_Percent,Planting_Suitability\nF01,Loamy,120,22,35.5,Yes\nF02,Clay,85,19,28.1,No\nF03,SANDY,95,25,40.0,Yes\nF04,Loamy,missing,23,33.2,Yes\nF05,Silty,110,21,NaN,No\nF06,Clay,90,18,27.5,No\nF07,Loamy,105,20,38.9,Yes\nF08,Sandy,100,24,30.0,No\nF09,Silty,115,22,36.0,Yes\nF10,CLAY,80,17,25.0,No\nF11,loamy,130,20,41.2,Yes\nF12,Silty,100,21,29.5,No\nF13,Sandy,95,22,31.0,No\nF14,Loamy,125,23,39.8,Yes", "model_steps": ["Load the dataset and inspect for missing values and inconsistent capitalization", "Standardize categorical values in 'Soil_Type' to lowercase", "Impute missing numeric values in 'Rainfall_mm' and 'Soil_Moisture_Percent' using median imputation", "Encode 'Soil_Type' using one-hot encoding", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features: 'Rainfall_mm', 'Temperature_C', 'Soil_Moisture_Percent'", "Train a RandomForestClassifier to predict 'Planting_Suitability'", "Perform grid search over 'n_estimators' with values [50, 100] and 'max_depth' with values [3, 5]", "Evaluate model performance using accuracy, F1 score, precision, and recall on the test set", "Generate confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.86, "f1": 0.87, "precision": 0.89, "recall": 0.85, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "feature_importances": {"soil_moisture_percent": 0.42, "rainfall_mm": 0.28, "temperature_c": 0.15, "soil_type_clay": 0.07, "soil_type_loamy": 0.05, "soil_type_sandy": 0.03}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a crop yield will be high or low based on soil and weather conditions.", "raw_table": "FieldID,SoilType,Rainfall_mm,Temperature_C,PreviousCrop,YieldCategory\nF01,Loamy,120,22.5,Wheat,High\nF02,CLAYY,95,19.0,Corn,Low\nF03,Sandy,130,25.0,Rice,High\nF04,Loamy,,21.0,Barley,High\nF05,Clay,85,20.5,Wheat,Low\nF06,Sandy,140,26.5,Corn,High\nF07,loamy,110,23.0,Rice,High\nF08,Sandy,135,27.0,Corn,High\nF09,Clay,90,18.5,Barley,Low\nF10,Loamy,115,22.0,Rice,High\nF11,Clay,abc,19.5,Wheat,Low\nF12,Sandy,125,24.0,Barley,High\nF13,LOAMY,105,21.5,Corn,High\nF14,Clay,80,20.0,Rice,Low", "model_steps": ["Load the dataset and inspect for missing or messy values", "Clean and normalize the 'SoilType' categorical variable to lowercase consistent labels", "Impute missing numeric values in 'Rainfall_mm' with the median rainfall", "Convert 'YieldCategory' target variable to binary labels (High=1, Low=0)", "Split data into training (80%) and testing (20%) sets", "One-hot encode categorical variables 'SoilType' and 'PreviousCrop'", "Standardize numeric variables 'Rainfall_mm' and 'Temperature_C'", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search over max_depth=[5,10,15] and min_samples_split=[2,5]", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score", "Generate confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.86, "precision": 0.89, "recall": 0.83, "f1": 0.86, "confusion_matrix": [[5, 1], [2, 6]], "top_feature_importances": {"Rainfall_mm": 0.32, "Temperature_C": 0.25, "soiltype_loamy": 0.15, "previouscrop_wheat": 0.1, "soiltype_sandy": 0.08}, "best_hyperparameters": {"max_depth": 10, "min_samples_split": 2}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features and location.", "raw_table": "House_ID,Location,Size_sqft,Bedrooms,Bathrooms,Year_Built,Garage_Type,Price\n1,Suburban,2100,3,2,1998,Attached,350000\n2,urban,1600,2,,2005,None,280000\n3,Rural,2500,4,3,2010,Detached,420000\n4,Suburban,1800,3,2,na,attached,310000\n5,Urban,2300,4,3,2015,Attached,450000\n6,RURAL,1900,3,2,2000,detached,330000\n7,Suburban,1400,2,1,1995,None,250000\n8,Urban,1700,3,2,2008,Attached,300000\n9,suburban,2200,4,3,2012,Detached,400000\n10,Rural,2000,3,2,2003,none,340000\n11,Urban,NaN,3,2,2011,Attached,360000\n12,Suburban,2400,5,4,2018,Attached,520000\n13,Rural,2100,3,2,1999,Detached,370000\n14,urban,1800,3,2,2007,None,310000", "model_steps": ["Load CSV data into a DataFrame", "Standardize inconsistent capitalization in 'Location' and 'Garage_Type' columns", "Impute missing numeric values in 'Size_sqft' and 'Bathrooms' with median values", "Impute missing 'Year_Built' values with the mode year", "Encode categorical variables 'Location' and 'Garage_Type' using one-hot encoding", "Split data into training (80%) and test (20%) sets", "Standardize numeric features such as 'Size_sqft', 'Bedrooms', 'Bathrooms', and 'Year_Built'", "Train a RandomForestRegressor model to predict 'Price'", "Perform grid search to tune 'n_estimators' and 'max_depth'", "Evaluate model performance on test set using RMSE, MAE, and R2", "Identify and report top 3 feature importances from the trained model"], "model_results": {"rmse": 18500, "mae": 14000, "r2": 0.87, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}, "top_feature_importances": {"Size_sqft": 0.42, "Location_Suburban": 0.18, "Bedrooms": 0.12}}}
{"purpose": "Predict whether a social media post will go viral based on its characteristics.", "raw_table": "post_id,post_length,num_hashtags,post_type,user_followers,user_verified,post_time,target_viral\n1,120,3,Image,1500,Yes,Evening,Yes\n2,85,0,Text,300,No,Morning,No\n3,200,5,Video,5000,YES,Night,Yes\n4,NaN,2,Image,1200,No,Afternoon,No\n5,70,1,text,700,No,Morning,No\n6,300,7,Video,15000,Yes,NIGHT,Yes\n7,95,NaN,Image,900,No,Evening,No\n8,130,4,Video,8000,No,Afternoon,Yes\n9,110,2,Text,400,No,Morning,No\n10,80,1,Image,NaN,No,Evening,No\n11,250,6,Video,12000,YES,Night,Yes\n12,140,3,Image,2000,No,Afternoon,No\n13,100,0,Text,500,No,Morning,No\n14,220,5,Video,9000,Yes,NIGHT,Yes", "model_steps": ["Load the CSV data into a dataframe and inspect for missing values and inconsistencies", "Clean 'post_type' and 'user_verified' columns to standardize capitalization and handle missing values", "Impute missing numeric values in 'post_length', 'num_hashtags', and 'user_followers' using median values", "Encode categorical variables: one-hot encode 'post_type' and 'post_time', binary encode 'user_verified'", "Split data into training and test sets with 80% for training and 20% for testing", "Scale numeric features 'post_length', 'num_hashtags', and 'user_followers' using StandardScaler", "Train a RandomForestClassifier to predict 'target_viral'", "Perform grid search to tune 'n_estimators' and 'max_depth' hyperparameters", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Generate and analyze the confusion matrix to understand prediction errors", "Extract top feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.81, "recall": 0.88, "confusion_matrix": {"true_positive": 7, "true_negative": 12, "false_positive": 3, "false_negative": 1}, "top_feature_importances": {"user_followers": 0.32, "post_type_Video": 0.25, "num_hashtags": 0.15, "post_length": 0.12, "user_verified": 0.1, "post_time_Evening": 0.06}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Build a classification model to predict whether a taxi trip will exceed 20 minutes based on trip and passenger features.", "raw_table": "trip_id,passenger_count,pickup_location,dropoff_location,trip_distance,trip_time_minutes,payment_type,tip_amount,exceeds_20_min\n1,2,Downtown,Uptown,3.5,18,Cash,2.5,No\n2,1,downtown,Suburb,8.0,25,Card,3.0,Yes\n3,3,Suburb,Downtown,7.2,22,card,1.0,Yes\n4,1,Airport,Uptown,12.0,35,Cash,5.0,Yes\n5,,Suburb,Airport,15.5,40,Card,4.5,Yes\n6,2,Uptown,Downtown,4.0,15,Cash,0,No\n7,1,Airport,Downtown,11.0,30,Cash,3.5,Yes\n8,3,Suburb,Uptown,6.1,19,Card,1.5,No\n9,2,Downtown,Airport,14.0,38,Cash,5.0,Yes\n10,2,Uptown,Suburb,5.0,21,Card,2.0,Yes\n11,1,Downtown,Suburb,7.5,23,Cash,3.0,Yes\n12,3,Airport,Suburb,13.2,36,card,4.0,Yes\n13,2,Suburb,Uptown,6.0,20,Cash,2.0,No\n14,1,uPtown,Downtown,3.8,16,Card,,No", "model_steps": ["Load the dataset from CSV and inspect for missing or inconsistent data", "Fix inconsistent capitalization in categorical columns (e.g., payment_type and locations)", "Impute missing values in passenger_count with the median passenger count", "One-hot encode categorical variables: pickup_location, dropoff_location, payment_type", "Separate features and target variable 'exceeds_20_min' (binary classification)", "Split data into train and test sets (80/20 ratio)", "Standardize numeric features: trip_distance, trip_time_minutes, tip_amount", "Train a RandomForestClassifier on the training set", "Perform grid search cross-validation over 'max_depth' parameter", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and feature importances"], "model_results": {"accuracy": 0.86, "f1": 0.88, "precision": 0.84, "recall": 0.93, "confusion_matrix": [[4, 1], [1, 8]], "top_feature_importances": {"trip_distance": 0.35, "trip_time_minutes": 0.3, "pickup_location_Suburb": 0.1, "payment_type_Cash": 0.08, "dropoff_location_Downtown": 0.07}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a student will pass the final exam based on their study habits and demographics.", "raw_table": "StudentID,Hours_Studied,Attendance,Preferred_Learning_Style,Parent_Education_Level,Previous_Grades,Passed_Final\n1,15,90,Visual,College,85,Yes\n2,7,80,audio,Highschool,70,No\n3,12,,Visual,Graduate,88,Yes\n4,5,60,Kinesthetic,highschool,62,No\n5,20,95,Visual,College,92,Yes\n6,8,70,AudIo,,75,No\n7,16,85,Kinesthetic,College,89,Yes\n8,10,75,Visual,HighSchool,78,Yes\n9,,88,Audio,Graduate,84,Yes\n10,4,55,Kinesthetic,Highschool,60,No\n11,18,92,visual,College,90,Yes\n12,6,65,Audio,College,68,No\n13,14,85,Visual,,87,Yes\n14,9,78,Kinesthetic,Highschool,73,No\n15,11,80,Visual,College,80,Yes", "model_steps": ["Load CSV data into a DataFrame", "Identify and handle missing values in 'Hours_Studied', 'Attendance', and 'Parent_Education_Level' columns", "Normalize capitalization inconsistency in 'Preferred_Learning_Style' and 'Parent_Education_Level' columns", "Convert categorical variables ('Preferred_Learning_Style', 'Parent_Education_Level') using one-hot encoding", "Convert target variable 'Passed_Final' to binary (Yes=1, No=0)", "Split dataset into train (80%) and test (20%) sets", "Standardize numeric features 'Hours_Studied', 'Attendance', and 'Previous_Grades'", "Train a RandomForestClassifier on training data", "Perform grid search to tune 'max_depth' and 'n_estimators' hyperparameters", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate and analyze confusion matrix"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.9, "f1": 0.87, "confusion_matrix": [[5, 1], [1, 7]], "top_feature_importances": {"Hours_Studied": 0.35, "Previous_Grades": 0.3, "Attendance": 0.2, "Preferred_Learning_Style_Visual": 0.05, "Parent_Education_Level_College": 0.05, "Preferred_Learning_Style_Audio": 0.03, "Preferred_Learning_Style_Kinesthetic": 0.02}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict customer likelihood to upgrade their mobile plan within the next month.", "raw_table": "CustomerID, Age, MonthlyUsageMinutes, PlanType, CustomerSegment, PaymentMethod, HasComplaints, UpgradeNextMonth\n001, 34, 320, basic, Premium, CreditCard, yes, 0\n002, 45, 540, Premium, standard, DebitCard, no, 1\n003, 29, 280, BASIC, Standard, creditcard, no, 0\n004, 52, , Premium, Premium, PayPal, yes, 1\n005, 40, 510, Basic, standard, DebitCard, no, 1\n006, 36, 430, premium, Standard, CreditCard, NO, 0\n007, 23, 180, Basic, Premium, PayPal, yes, 0\n008, 48, 600, Premium, standard, DebitCard, no, 1\n009, 31, 300, Basic, Standard, CreditCard, yes, 0\n010, 50, 550, Premium, Standard, PayPal, no, 1\n011, 38, 400, Basic, PREMIUM, CreditCard, No, 0\n012, 28, 270, basic, standard, DebitCard, yes, 0\n013, 55, 580, Premium, Premium, Creditcard, no, 1\n014, 33, 350, Basic, STANDARD, PayPal, yes, 0", "model_steps": ["Load data and identify missing values in MonthlyUsageMinutes", "Impute missing MonthlyUsageMinutes with median value", "Normalize inconsistent capitalization in PlanType, CustomerSegment, PaymentMethod, and HasComplaints columns", "Convert HasComplaints to binary numeric (yes=1, no=0)", "One-hot encode categorical variables: PlanType, CustomerSegment, PaymentMethod", "Split data into training (80%) and testing (20%) sets", "Standardize numeric features: Age, MonthlyUsageMinutes", "Train a RandomForestClassifier to predict UpgradeNextMonth", "Perform grid search on number of estimators and max_depth parameters", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and determine top 3 important features"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": [[7, 1], [2, 4]], "top_feature_importances": {"MonthlyUsageMinutes": 0.34, "PlanType_Premium": 0.27, "HasComplaints": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Predict whether a customer will upgrade to a premium mobile data plan.", "raw_table": "CustomerID,Age,MonthlyUsageGB,ContractType,CustomerSupportCalls,Region,IncomeLevel,Upgrade\n1001,34,12.5,monthly,2,North,medium,No\n1002,45,8.2,Annual,1,South,HIGH,Yes\n1003,23,,Monthly,3,East,low,No\n1004,54,15.0,Monthly,,west,medium,Yes\n1005,37,10.1,annual,0,North,Medium,No\n1006,29,7.4,Monthly,1,East,low,No\n1007,41,20.3,Monthly,4,South,High,Yes\n1008,31,NaN,monthly,2,North,medium,No\n1009,50,13.7,ANNUAL,0,West,Medium,Yes\n1010,28,9.6,Monthly,1,East,Low,No\n1011,46,14.2,Monthly,3,South,high,Yes\n1012,38,11.3,Monthly,2,North,medium,No\n1013,44,16.8,Annual,2,West,High,Yes\n1014,27,8.9,monthly,1,East,low,No", "model_steps": ["Load dataset and parse CSV", "Fix inconsistent capitalization in ContractType, Region, and IncomeLevel columns", "Impute missing MonthlyUsageGB values with median usage", "Fill missing CustomerSupportCalls values with zero", "Convert Upgrade target column to binary (Yes=1, No=0)", "One-hot encode categorical features: ContractType, Region, IncomeLevel", "Split data into training (80%) and test (20%) sets with stratification on target", "Standardize numeric features: Age, MonthlyUsageGB, CustomerSupportCalls", "Train RandomForestClassifier with 100 trees", "Perform grid search over max_depth with values [5, 10, 15]", "Evaluate performance on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 important features"], "model_results": {"accuracy": 0.86, "precision": 0.82, "recall": 0.79, "f1": 0.805, "best_max_depth": 10, "confusion_matrix": {"true_positive": 11, "true_negative": 14, "false_positive": 3, "false_negative": 3}, "top_feature_importances": {"MonthlyUsageGB": 0.32, "ContractType_annual": 0.21, "CustomerSupportCalls": 0.18}}}
{"purpose": "Predict whether a social media post will go viral based on its attributes and posting time.", "raw_table": "post_id,post_length,content_type,post_hour,user_followers,is_verified,hashtags_count,viral\n1,120,text,14,1500,yes,3,yes\n2,80,Image,22,300,No,1,no\n3,45,video,9,NaN,yes,2,yes\n4,60,text,3,5000,no,0,no\n5,200,Text,18,1200,Yes,4,yes\n6,90,image,21,800,No,NaN,no\n7,110,video,15,2300,yes,3,yes\n8,NaN,text,10,1800,No,1,no\n9,130,video,20,2700,yes,2,yes\n10,75,image,12,400,No,1,no\n11,50,video,5,1000,yes,0,no\n12,85,text,25,900,no,2,no\n13,140,Image,13,3500,YES,3,yes\n14,100,video,16,NaN,No,2,no", "model_steps": ["Load dataset and identify missing values", "Impute missing numeric values with median (post_length, user_followers)", "Standardize numeric columns (post_length, post_hour, user_followers, hashtags_count)", "Normalize text in categorical columns and unify capitalization (content_type, is_verified)", "One-hot encode categorical variables (content_type, is_verified)", "Split data into training (80%) and testing (20%) sets stratified by target variable viral", "Train a RandomForestClassifier to predict viral (yes/no) outcome", "Perform grid search over number of trees (n_estimators) and max depth", "Evaluate model on test set calculating accuracy, F1 score, precision, and recall", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.82, "recall": 0.87, "confusion_matrix": {"true_positive": 6, "true_negative": 7, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"user_followers": 0.35, "post_hour": 0.22, "content_type_video": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 8}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features.", "raw_table": "HouseID,Neighborhood,Size_sqft,Bedrooms,Bathrooms,YearBuilt,Garage,Condition,SalePrice\n1,Northside,2100,3,2,1998,Yes,Good,350000\n2,southside,1800,3,2,2005,No,Fair,280000\n3,EastSide,2500,4,3,2010,Yes,Excellent,420000\n4,Westside,NaN,3,2,2000,Yes,good,330000\n5,Northside,1950,3,2,1995,No,Good,310000\n6,SouthSide,2200,4,3,,Yes,Fair,365000\n7,Eastside,2400,4,3,2012,Yes,Excellent,450000\n8,WestSide,1700,2,1,1990,No,Poor,210000\n9,Northside,2000,3,2,2003,Yes,Good,340000\n10,Southside,1900,3,four,2007,No,Fair,300000\n11,EastSide,2300,4,3,2011,Yes,excellent,440000\n12,Westside,1600,2,1,1985,No,Poor,200000\n13,Northside,2050,3,2,1999,YES,Good,355000\n14,Southside,1850,3,2,2006,No,Fair,290000", "model_steps": ["Load the dataset and identify the target variable SalePrice", "Clean data by fixing inconsistent capitalization in 'Neighborhood', 'Garage', and 'Condition' columns", "Convert 'Bathrooms' to numeric, correcting or imputing invalid entries (e.g., 'four' replaced with 4)", "Impute missing values in 'Size_sqft' and 'YearBuilt' using median values grouped by Neighborhood", "One-hot encode categorical variables: Neighborhood, Garage, and Condition", "Standardize numeric features: Size_sqft, Bedrooms, Bathrooms, YearBuilt", "Split data into training and testing sets with an 80/20 ratio", "Train a Gradient Boosting Regressor on the training set", "Tune hyperparameters using grid search over learning_rate and n_estimators", "Evaluate regression metrics (RMSE, MAE, R2) on the test set", "Extract and report top 5 feature importances from the trained model"], "model_results": {"rmse": 15000, "mae": 12000, "r2": 0.85, "top_feature_importances": {"Size_sqft": 0.35, "Neighborhood_Northside": 0.18, "Bathrooms": 0.15, "Condition_Excellent": 0.12, "YearBuilt": 0.1}, "best_hyperparameters": {"learning_rate": 0.1, "n_estimators": 100}}}
{"purpose": "Predict customer likelihood to upgrade their mobile plan within the next quarter.", "raw_table": "CustomerID,Age,MonthlyUsageMinutes,PlanType,ContractLengthMonths,CustomerSupportCalls,HasFiber,UpgradeNextQuarter\nC001,34,350,prepaid,12,1,Yes,No\nC002,45,720,Postpaid,24,three,No,Yes\nC003,29,480,PrePaid,6,0,Yes,Yes\nC004,NaN,300,postpaid,18,2,No,No\nC005,52,620,PostPaid,36,,YES,Yes\nC006,41,450,prepaid,12,1,no,No\nC007,38,NaN,Postpaid,24,1,Yes,No\nC008,27,390,prepaid,6,0,No,No\nC009,50,700,postpaid,30,4,Yes,Yes\nC010,33,410,Prepaid,12,1,yes,No\nC011,47,680,Postpaid,24,3,No,Yes\nC012,36,530,PREPAID,12,2,Yes,No\nC013,44,610,Postpaid,24,1,No,Yes\nC014,39,NaN,prepaid,NaN,2,No,No", "model_steps": ["Load the CSV data into a dataframe and inspect for missing or inconsistent values", "Normalize categorical values in 'PlanType' and 'HasFiber' columns to lowercase for consistency", "Impute missing numeric values (Age, MonthlyUsageMinutes, ContractLengthMonths) using median imputation", "Convert 'CustomerSupportCalls' from mixed types (strings and integers) to numeric, treating non-numeric as missing and imputing with median", "One-hot encode the categorical columns 'PlanType' and 'HasFiber'", "Split the dataset into training and test sets using an 80/20 stratified split based on the target 'UpgradeNextQuarter'", "Standardize numeric features: Age, MonthlyUsageMinutes, ContractLengthMonths, CustomerSupportCalls", "Train a RandomForestClassifier on the training set with default hyperparameters", "Evaluate model performance on the test set using accuracy, F1 score, precision, and recall", "Extract and rank feature importances from the trained RandomForestClassifier"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.82, "recall": 0.87, "top_feature_importances": {"MonthlyUsageMinutes": 0.32, "PlanType_postpaid": 0.25, "CustomerSupportCalls": 0.18, "ContractLengthMonths": 0.12, "HasFiber_yes": 0.08, "Age": 0.05}}}
{"purpose": "Build a regression model to predict hourly electricity consumption (kWh) based on environmental and operational features.", "raw_table": "Hour,Temperature_C,Day_Type,Weather,Previous_Hour_Consumption,Is_Holiday,Electricity_Consumption_kWh\n0,15.2,weekday,Clear,120.5,No,130.4\n1,14.8,weekday,Clear,130.4,No,125.7\n2,14.5,weekday,clear,125.7,No,123.9\n3,14.1,weekday,Cloudy,123.9,No,119.2\n4,13.9,Weekday,Foggy,119.2,No,117.5\n5,,weekend,Foggy,117.5,No,110.3\n6,16.0,weekend,Rain,110.3,NO,115.1\n7,18.3,Weekend,Rain,115.1,No,130.6\n8,20.2,weekday,Clear,130.6,No,145.2\n9,22.0,weekday,Clear,145.2,No,160.7\n10,23.5,weekday,Clear,160.7,Yes,170.3\n11,24.0,weekday,Clear,170.3,Yes,175.4\n12,22.8,Weekday,Clear,175.4,No,165.8\n13,21.3,Weekday,Clear,165.8,No,158.9\n14,19.9,weekday,Cloudy,158.9,No,150.2", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing or inconsistent values", "Correct inconsistent capitalization in 'Day_Type' and 'Weather' columns", "Impute missing numeric values in 'Temperature_C' using median temperature", "Convert categorical columns ('Day_Type', 'Weather', 'Is_Holiday') to one-hot encoded variables", "Split the dataset into training (80%) and test (20%) sets with a fixed random seed", "Standardize numeric features: 'Hour', 'Temperature_C', 'Previous_Hour_Consumption'", "Train a GradientBoostingRegressor to predict 'Electricity_Consumption_kWh'", "Perform 5-fold cross-validation to tune learning rate and number of estimators", "Evaluate final model on test set using RMSE, MAE, and R2", "Identify top 3 most important features based on feature importance scores"], "model_results": {"rmse": 4.8, "mae": 3.7, "r2": 0.92, "top_feature_importances": {"Previous_Hour_Consumption": 0.53, "Temperature_C": 0.27, "Hour": 0.12}, "hyperparameters": {"learning_rate": 0.05, "n_estimators": 150, "max_depth": 3}}}
{"purpose": "Predict whether a given agricultural field will experience pest infestation based on environmental and crop features.", "raw_table": "Field_ID,Soil_Type,Crop_Type,Avg_Temperature,Cumulative_Rainfall,Pesticide_Used,Pest_Infestation\nF001,Loamy,Corn,23.5,150,Yes,Yes\nF002,Sandy,Wheat,25.1,80,No,no\nF003,CLAY,soy,22.3,120,Yes,Yes\nF004,Loamy,Corn,missing,140,No,No\nF005,Sandy,Corn,26.8,90,NO,No\nF006,Loamy,Soy,24.0,130,Yes,Yes\nF007,Clay,Wheat,23.0,110,no,No\nF008,Sandy,CORN,27.2,85,Yes,Yes\nF009,Loamy,soy,22.7,missing,No,No\nF010,Sandy,Wheat,25.5,95,Yes,Yes\nF011,Clay,Corn,24.3,125,No,No\nF012,loamy,soy,23.9,135,Yes,Yes\nF013,Sandy,Corn,26.0,100,Yes,No\nF014,Clay,Wheat,22.5,115,No,No\nF015,,Soy,24.1,105,Yes,Yes", "model_steps": ["Load the dataset and identify missing or inconsistent values", "Standardize capitalization in categorical variables Soil_Type, Crop_Type, Pesticide_Used and target Pest_Infestation", "Impute missing numeric values in Avg_Temperature and Cumulative_Rainfall using median imputation", "Impute missing categorical Soil_Type with the mode", "Encode categorical variables Soil_Type, Crop_Type, Pesticide_Used using one-hot encoding", "Convert target variable Pest_Infestation to binary (Yes=1, No=0)", "Split data into training and test sets with 80% for training and 20% for testing", "Train a RandomForestClassifier with 100 trees on the training set", "Perform hyperparameter tuning on max_depth using 5-fold cross-validation", "Evaluate model on test set computing accuracy, precision, recall, and F1 score", "Generate confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.89, "f1": 0.87, "confusion_matrix": [[9, 2], [1, 11]], "top_feature_importances": {"Pesticide_Used_Yes": 0.28, "Crop_Type_Corn": 0.2, "Soil_Type_Loamy": 0.15, "Avg_Temperature": 0.12, "Cumulative_Rainfall": 0.1}, "best_max_depth": 7}}
{"purpose": "Build a classification model to predict 30-day hospital readmission among diabetic patients.", "raw_table": "PatientID,Age,Gender,AdmissionType,NumPreviousAdmissions,HbA1c,Smoker,Comorbidities,Readmitted\n1,65,Male,Emergency,2,8.1,Yes,Hypertension,Yes\n2,58,Female,Elective,0,6.5,No,none,No\n3,72,male,Urgent,1,9.0,yes,Hypertension;CKD,Yes\n4,45,FEMALE,Emergency,3,7.2,,None,No\n5,53,Male,Elective,0,NA,No,Hypertension,No\n6,60,Female,Emergency,4,8.9,Yes,CKD,Yes\n7,70,Male,Elective,1,7.5,No,none,No\n8,49,Female,Urgent,2,7.8,No,Hypertension;CKD,Yes\n9,55,Male,emergency,1,6.8,Yes,Hypertension,No\n10,61,Female,Elective,0,7.0,No,None,No\n11,67,Male,Urgent,5,8.2,Yes,Hypertension;CKD,Yes\n12,54,Female,Emergency,2,7.4,No,none,No\n13,63,Male,Elective,1,8.0,Yes,Hypertension,Yes\n14,59,Female,Emergency,,7.1,No,CKD,No\n15,50,Male,Urgent,0,6.9,No,None,No", "model_steps": ["Impute missing values in 'HbA1c' with the median value", "Standardize numeric features: Age, NumPreviousAdmissions, HbA1c", "Normalize and unify categorical variables: map all 'Gender' values to lowercase, unify 'AdmissionType' capitalization", "Fill missing 'Smoker' values with 'No' (assumed non-smoker)", "One-hot encode categorical variables: Gender, AdmissionType, Smoker, Comorbidities", "Split dataset into training and testing sets with an 80/20 ratio, stratified by target 'Readmitted'", "Train a RandomForestClassifier with 100 trees on the training data", "Perform 5-fold cross-validation to tune 'max_depth' hyperparameter", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score", "Generate confusion matrix and analyze false positives and false negatives"], "model_results": {"accuracy": 0.8, "precision": 0.78, "recall": 0.82, "f1": 0.8, "confusion_matrix": {"true_positive": 9, "false_positive": 3, "true_negative": 12, "false_negative": 2}, "top_feature_importances": {"HbA1c": 0.3, "NumPreviousAdmissions": 0.25, "AdmissionType_Emergency": 0.15, "Smoker_Yes": 0.1, "Comorbidities_Hypertension": 0.08, "Age": 0.07, "Gender_male": 0.05}, "hyperparameters": {"max_depth": 7, "n_estimators": 100}}}
{"purpose": "Predict whether a loan applicant will default on their loan based on financial and demographic data.", "raw_table": "ApplicantID,Age,Income,EmploymentStatus,CreditScore,LoanAmount,LoanPurpose,Defaulted\n1,35,58000,full-time,700,15000,car,No\n2,42,NaN,Part-Time,650,20000,Home Improvement,Yes\n3,28,45000,Full-Time,720,8000,credit card,no\n4,50,62000,full-time,,12000,Car,No\n5,NaN,54000,Self-Employed,690,10000,Debt consolidation,Yes\n6,31,47000,Unemployed,680,5000,Car,No\n7,45,58000,Full-Time,710,NaN,Home Improvement,No\n8,38,60000,part-time,640,13000,CAR,Yes\n9,27,52000,Full-Time,700,9000,Debt Consolidation,No\n10,33,56000,Full-time,730,11000,car,No\n11,40,49000,Full-time,670,15000,Car,Yes\n12,29,48000,,690,7000,Credit Card,No", "model_steps": ["Load raw CSV data into a dataframe", "Standardize column names and fix inconsistent capitalization in categorical columns", "Impute missing numeric values with median and missing categorical values with the mode", "One-hot encode the EmploymentStatus and LoanPurpose categorical variables", "Convert the target variable 'Defaulted' to binary (Yes=1, No=0)", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features: Age, Income, CreditScore, LoanAmount", "Train a Logistic Regression classifier on the training set", "Perform 5-fold cross-validation to tune the regularization parameter C", "Evaluate the model on the test set computing accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix"], "model_results": {"accuracy": 0.83, "precision": 0.79, "recall": 0.75, "f1": 0.77, "confusion_matrix": {"true_positive": 6, "false_positive": 2, "true_negative": 12, "false_negative": 2}, "top_feature_importances": {"CreditScore": 0.45, "LoanAmount": 0.3, "EmploymentStatus_Full-Time": 0.15, "Income": 0.1}, "best_hyperparameter_C": 0.1}}
{"purpose": "Build a regression model to predict house sale prices based on property features.", "raw_table": "PropertyID,Neighborhood,HouseStyle,LotArea,Bedrooms,Bathrooms,Garage,YearBuilt,SalePrice\n1,OldTown,2Story,8450,3,2,Attached,2005,208500\n2,Oldtown,1Story,9600,4,2,detached,1997,181500\n3,CollgCr,2Story,11250,3,2,MISSING,2001,223500\n4,Edwards,1Story,9550,2,1,Attached,1915,140000\n5,OldTown,2Story,14260,4,3,attached,2000,250000\n6,CollgCr,1Story,14115,3,,Detached,1993,143000\n7,Somerst,2Story,10084,,2,Attached,2004,307000\n8,NAmes,1Story,10382,3,2,attached,1970,200000\n9,OldTown,2Story,6120,3,2,Attached,1992,129900\n10,Somerst,2Story,7420,3,2,Attached,2007,345000\n11,CollgCr,1Story,11200,3,2,Attached,1999,185000\n12,Edwards,2Story,11924,4,3,Attached,1976,220000\n13,NAmes,1Story,12968,3,2,attached,2005,180000\n14,Somerst,2Story,10652,4,3,MISSING,2003,235000", "model_steps": ["Load the CSV data into a DataFrame", "Identify and impute missing values in 'Bedrooms', 'Bathrooms', and 'Garage' columns", "Standardize capitalization and fix inconsistent values in 'Neighborhood' and 'Garage' columns", "One-hot encode categorical variables: 'Neighborhood', 'HouseStyle', and 'Garage'", "Split data into training (80%) and test (20%) sets", "Standardize numeric features: 'LotArea', 'Bedrooms', 'Bathrooms', 'YearBuilt'", "Train a Gradient Boosting Regressor on the training set", "Perform 5-fold cross-validation to tune learning rate and number of estimators", "Evaluate model performance on the test set using RMSE, MAE, and R^2 metrics", "Extract and report top 5 feature importances"], "model_results": {"rmse": 17500, "mae": 13500, "r2": 0.82, "top_feature_importances": {"LotArea": 0.32, "YearBuilt": 0.25, "Neighborhood_OldTown": 0.15, "Bathrooms": 0.12, "Garage_Attached": 0.08}}}
{"purpose": "Predict whether a manufactured part will pass quality inspection based on production parameters and machine settings.", "raw_table": "Part_ID,Machine_ID,Operator_Shift,Temperature,Pressure,Material_Batch,Defect_Flag\n1,M01,Morning,350.5,1200,BatchA,Pass\n2,M02,Afternoon,348.7,1180,Batchb,Fail\n3,M02,MORNING,351,,BatchA,Pass\n4,M01,Night,349.2,1195,BatchC,Pass\n5,M03,Morning,NaN,1210,BatchA,Fail\n6,M01,Afternoon,347.8,1175,BatchB,Pass\n7,m03,Night,350.0,1205,BatchC,Fail\n8,M02,Morning,349.9,1185,BatchA,Pass\n9,M01,Afternoon,351.2,NaN,Batcha,Fail\n10,M03,Night,348.0,1190,BatchC,Pass\n11,M02,Morning,350.3,1215,BatchB,Pass\n12,M01,Morning,349.1,1200,BatchC,Fail\n13,M02,afternoon,352.0,1180,BatchA,Pass\n14,M03,Night,347.5,1170,BatchB,Fail", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Standardize categorical text data by converting all to consistent capitalization", "Impute missing numeric values using median imputation", "Encode categorical variables (Machine_ID, Operator_Shift, Material_Batch) using one-hot encoding", "Split data into training (80%) and test (20%) sets, stratified by Defect_Flag", "Standardize numeric features Temperature and Pressure with z-score normalization", "Train a RandomForestClassifier with default parameters on the training set", "Perform grid search to tune max_depth parameter between 3 and 10 using 5-fold cross-validation", "Select the best model based on F1 score and retrain on the full training set", "Evaluate the model on the test set computing accuracy, precision, recall, and F1 score", "Generate a confusion matrix and extract top 3 feature importances"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.81, "recall": 0.87, "confusion_matrix": {"true_positive": 24, "true_negative": 22, "false_positive": 5, "false_negative": 4}, "top_feature_importances": {"Pressure": 0.28, "Temperature": 0.22, "Material_Batch_BatchA": 0.15}, "best_hyperparameters": {"max_depth": 7}}}
{"purpose": "Predict whether a loan applicant will default on a loan based on their financial and demographic information.", "raw_table": "ApplicantID,Age,EmploymentStatus,AnnualIncome,LoanAmount,CreditScore,MaritalStatus,LoanDefault\n1,35,Full-time,55000,15000,720,Married,No\n2,42,Part-time,32000,8000,680,Single,No\n3,29,full-Time,47000,12000,NaN,Married,Yes\n4,51,Unemployed,40000,20000,600,Divorced,Yes\n5,38,Full-Time,62000,22000,710,Single,No\n6,45,Part-time,NaN,18000,670,Married,Yes\n7,33,full-time,48000,13000,690,Single,No\n8,40,Unemployed,50000,15000,710,,Yes\n9,27,Full-Time,31000,7000,650,Single,No\n10,30,Part-time,45000,11000,NaN,Married,Yes\n11,39,Full-time,58000,16000,720,Married,No\n12,44,Full-Time,60000,24000,730,Divorced,Yes\n13,36,part-time,49000,14000,680,Single,No\n14,29,Full-Time,43000,10000,690,Married,No", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values in AnnualIncome, CreditScore, and MaritalStatus by imputing median for numeric and mode for categorical", "Normalize inconsistent capitalization in EmploymentStatus to standardize categories", "Convert categorical variables EmploymentStatus and MaritalStatus to one-hot encoded features", "Split the dataset into 80% training and 20% testing sets preserving target distribution", "Standardize numeric features: Age, AnnualIncome, LoanAmount, CreditScore", "Train a LogisticRegression classifier to predict LoanDefault", "Perform hyperparameter tuning using grid search over regularization strength (C parameter)", "Evaluate model on test data using accuracy, F1 score, precision, and recall", "Generate and analyze confusion matrix", "Identify top 3 most important features from model coefficients"], "model_results": {"accuracy": 0.79, "f1": 0.76, "precision": 0.74, "recall": 0.78, "confusion_matrix": [[9, 3], [2, 6]], "top_features": {"CreditScore": 0.85, "LoanAmount": 0.62, "EmploymentStatus_Full-time": 0.45}, "best_hyperparameters": {"C": 0.1}}}
{"purpose": "Predict whether a retail customer will make a purchase during a promotional campaign.", "raw_table": "CustomerID,Age,Gender,Annual_Income_kUSD,Previous_Purchases,Membership_Status,Promo_Engaged,Purchase\n1,25,Male,45,3,Gold,Yes,1\n2,38,Female,NaN,7,Silver,No,0\n3,29,Male,54,NaN,Bronze,Yes,1\n4,47,Female,72,12,Gold,yes,1\n5,35,Male,61,8,Silver,No,0\n6,42,Female,58,6,Gold,YES,1\n7,,Male,40,2,Bronze,No,0\n8,31,Female,50,4,silver,Yes,1\n9,45,Male,68,10,Gold,No,0\n10,27,Female,49,NaN,Bronze,No,0\n11,40,Male,55,7,Gold,Yes,1\n12,34,Female,53,5,bronze,No,0\n13,39,Male,60,9,Gold,No,0\n14,28,Female,47,3,Silver,yes,1\n15,33,Male,52,4,Gold,No,0", "model_steps": ["Load dataset and inspect for missing or inconsistent values", "Impute missing Age and Previous_Purchases values using median", "Standardize inconsistent capitalization in Membership_Status and Promo_Engaged columns", "Convert Promo_Engaged and Membership_Status into categorical variables", "One-hot encode categorical variables: Gender, Membership_Status, Promo_Engaged", "Split data into training (80%) and test (20%) sets using stratified sampling on Purchase", "Standardize numeric features: Age, Annual_Income_kUSD, Previous_Purchases", "Train a RandomForestClassifier with 100 trees", "Perform grid search over max_depth values [5, 10, 15] using 5-fold cross-validation", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and feature importance plot"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.8, "f1": 0.82, "confusion_matrix": {"true_positive": 7, "true_negative": 17, "false_positive": 3, "false_negative": 2}, "top_feature_importances": {"Promo_Engaged_Yes": 0.31, "Membership_Status_Gold": 0.22, "Previous_Purchases": 0.17, "Annual_Income_kUSD": 0.13, "Age": 0.08, "Gender_Male": 0.04, "Membership_Status_Silver": 0.03, "Membership_Status_Bronze": 0.02}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Build a regression model to predict hourly electricity consumption based on environmental and operational factors.", "raw_table": "Hour,Temperature_C,Day_Type,Wind_Speed_mph,Humidity_Percent,Equipment_Status,Electricity_Consumption_kWh\n0,15.2,weekday,5.4,65,operational,120.5\n1,14.8,Weekday,5.1,67,Operational,118.3\n2,14.5,weekday,4.9,70,operational,115.7\n3,NaN,weekday,4.8,71,operational,113.2\n4,13.8,weekend,4.6,68,maintenance,110.0\n5,13.5,weekend,4.4,66,maintenance,108.5\n6,14.0,weekend,5.0,65,Operational,112.1\n7,16.2,Weekday,5.8,60,operational,130.7\n8,18.7,weekday,6.2,58,operational,145.3\n9,20.1,weekday,6.5,55,operational,160.0\n10,21.5,weekday,6.7,52,operational,170.8\n11,22.0,weekday,6.8,50,operational,175.4\n12,22.5,weekend,7.0,49,operational,178.0\n13,23.0,weekday,7.2,48,operational,180.3", "model_steps": ["Load the CSV data into a DataFrame", "Standardize inconsistent capitalization in categorical columns (Day_Type, Equipment_Status)", "Handle missing numeric values by imputing the mean for Temperature_C", "One-hot encode the categorical variables Day_Type and Equipment_Status", "Split data into train and test sets (80/20) preserving time order", "Standardize numeric features (Temperature_C, Wind_Speed_mph, Humidity_Percent, Hour)", "Train a Gradient Boosting Regressor on the training set", "Tune hyperparameters max_depth and learning_rate using cross-validation", "Evaluate the model on the test set using RMSE, MAE, and R2 metrics", "Analyze feature importances from the trained model"], "model_results": {"rmse": 5.3, "mae": 3.9, "r2": 0.92, "top_feature_importances": {"Hour": 0.35, "Temperature_C": 0.25, "Day_Type_weekday": 0.15, "Equipment_Status_operational": 0.12, "Humidity_Percent": 0.08, "Wind_Speed_mph": 0.05}, "best_hyperparameters": {"max_depth": 4, "learning_rate": 0.1}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features and location.", "raw_table": "PropertyID,Size_sqft,Bedrooms,Bathrooms,Neighborhood,Year_Built,Has_Garage,Sale_Price\n1,1500,3,2,Downtown,1995,Yes,350000\n2,2000,4,3,Uptown,2005,yes,480000\n3,1200,2,1,Suburb,,No,250000\n4,1800,3,,Downtown,2010,Yes,395000\n5,1600,3,2,suburb,2000,No,310000\n6,2100,4,3,Uptown,2018,YES,520000\n7,1400,3,2,Midtown,1990,No,330000\n8,1700,3,2,Midtown,1998,No,360000\n9,NaN,2,1,Downtown,1985,Yes,290000\n10,1900,4,3,Uptown,2015,Yes,475000\n11,1550,3,2,Suburb,2003,No,340000\n12,1750,3,2,Midtown,1999,Yes,380000\n13,1300,2,1,suburb,1992,No,275000\n14,1650,3,2,Uptown,2007,yes,400000", "model_steps": ["Load CSV data and inspect for missing and inconsistent values", "Normalize the 'Has_Garage' column to lowercase and convert to binary (Yes=1, No=0)", "Fill missing numeric values in 'Size_sqft' and 'Bathrooms' with median values", "Impute missing 'Year_Built' values with median year", "One-hot encode the 'Neighborhood' categorical variable after standardizing capitalization", "Split data into training and test sets (80/20 split)", "Standardize numeric features: Size_sqft, Bedrooms, Bathrooms, Year_Built", "Train a RandomForestRegressor using the training set", "Evaluate model performance on the test set using RMSE, MAE, and R2 metrics", "Extract and report top 5 feature importances from the trained model"], "model_results": {"rmse": 25000, "mae": 18000, "r2": 0.87, "top_feature_importances": {"Size_sqft": 0.41, "Neighborhood_Uptown": 0.19, "Bedrooms": 0.15, "Has_Garage": 0.12, "Year_Built": 0.08}}}
{"purpose": "Predict whether a customer will upgrade to a premium telecom plan within the next month.", "raw_table": "CustomerID,Age,Gender,MonthlyMinutes,ContractType,PaymentMethod,NumComplaints,HasFiber,PremiumUpgrade\n1001,34,Male,320,Month-to-month,Credit Card,0,Yes,No\n1002,45,female,450,One year,bank transfer,1,No,Yes\n1003,29,Male,NaN,Month-to-month,Electronic Check,2,yes,No\n1004,53,MALE,500,Two year,Credit Card,0,No,Yes\n1005,38,Female,390,month-to-month,Electronic check,1,Yes,No\n1006,,Female,420,One Year,Credit card,0,No,Yes\n1007,41,Male,430,Month-to-month,Electronic Check,NaN,Yes,No\n1008,36,FEMALE,400,One year,credit card,0,No,No\n1009,48,Male,480,Two Year,Bank Transfer,0,No,Yes\n1010,50,Female,460,One year,Electronic Check,1,Yes,Yes\n1011,33,Male,310,Month-to-Month,Credit Card,2,No,No\n1012,37,Female,385,One year,Electronic Check,1,Yes,No\n1013,42,Male,435,Month-to-month,Electronic check,0,Yes,Yes", "model_steps": ["Load the dataset and inspect for missing values and inconsistent capitalization in categorical columns.", "Fill missing numeric values (Age, MonthlyMinutes) with median values.", "Standardize capitalization in categorical columns (Gender, ContractType, PaymentMethod, HasFiber).", "Fill missing NumComplaints values with zero (assuming no complaints).", "Encode categorical variables using one-hot encoding.", "Standardize numeric features Age, MonthlyMinutes, NumComplaints.", "Split data into train and test sets with an 80/20 ratio.", "Train a RandomForestClassifier to predict PremiumUpgrade.", "Perform grid search with 5-fold cross-validation over max_depth and n_estimators hyperparameters.", "Evaluate final model on test set using accuracy, precision, recall, and F1 score.", "Generate and analyze the confusion matrix and identify top 3 important features."], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.78, "f1": 0.8, "confusion_matrix": [[14, 3], [4, 19]], "top_feature_importances": {"ContractType_One year": 0.28, "MonthlyMinutes": 0.22, "HasFiber_Yes": 0.18}, "best_hyperparameters": {"max_depth": 7, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict whether a loan application will be approved based on applicant financial and demographic information.", "raw_table": "ApplicantID,Income,Employment_Status,Credit_Score,Loan_Amount,Loan_Purpose,Approved\n1,55000,Full-time,720,15000,home improvement,Yes\n2,43000,part-time,680,12000,Debt consolidation,No\n3,NaN,Self-Employed,610,13000,Home Improvement,Yes\n4,75000,FULL-Time,780,20000,Debt Consolidation,Yes\n5,62000,Unemployed,NaN,9000,education,No\n6,48000,Part-Time,650,11000,Home improvement,No\n7,50000,full-time,700,14000,Education,Yes\n8,54000,Self-employed,710,16000,Debt Consolidation,Yes\n9,40000,Employed,600,10000,home Improvement,No\n10,68000,full-time,730,18000,Debt consolidation,Yes\n11,57000,Full-Time,625,11500,Education,No\n12,62000,Part-time,690,13000,Home Improvement,Yes", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing and inconsistent values", "Normalize categorical variables by correcting inconsistent capitalization and unify categories (e.g., Employment_Status and Loan_Purpose)", "Impute missing numeric values for Income and Credit_Score using median imputation", "Encode categorical features (Employment_Status, Loan_Purpose) with one-hot encoding", "Split the dataset into train (80%) and test (20%) sets, stratified on the target Approved", "Standardize numeric features (Income, Credit_Score, Loan_Amount) using StandardScaler", "Train a RandomForestClassifier with default parameters on the training data", "Perform grid search cross-validation over max_depth=[5,10,15] and n_estimators=[50,100]", "Evaluate the final model on the test set computing accuracy, precision, recall, and F1 score", "Generate the confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.83, "precision": 0.85, "recall": 0.8, "f1": 0.82, "confusion_matrix": [[4, 1], [2, 7]], "top_feature_importances": {"Credit_Score": 0.38, "Income": 0.27, "Loan_Amount": 0.15, "Employment_Status_Full-time": 0.1}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict the daily precipitation level category using meteorological and geographic features.", "raw_table": "Day,Temperature_C,Humidity_Percent,WindSpeed_kmh,Region,CloudCover,Precipitation_Level\n1,22.5,55,15,Coastal,Partly Cloudy,Low\n2,19.0,80,,Inland,CLEAR,Medium\n3,25.1,60,20,Coastal,Cloudy,High\n4,,75,13,Inland,cloudy,Medium\n5,18.3,65,12,Mountain,Clear,Low\n6,21.7,70,18,Coastal,Partly cloudy,Medium\n7,23.0,NaN,16,Mountain,Cloudy,High\n8,20.2,68,NaN,inland,Clear,Low\n9,24.5,60,17,Coastal,partly Cloudy,Medium\n10,17.8,72,14,Mountain,Cloudy,Low\n11,22.0,67,15,Coastal,CLEAR,Medium\n12,19.5,73,13,Inland,clear,Low\n13,21.0,66,14,Mountain,Cloudy,Medium\n14,20.0,70,15,Coastal,Partly Cloudy,High", "model_steps": ["Handle missing values in numeric columns by imputing with median values", "Standardize numeric features: Temperature_C, Humidity_Percent, WindSpeed_kmh", "Normalize and unify capitalization in categorical columns: Region and CloudCover", "One-hot encode categorical variables: Region and CloudCover", "Encode target variable Precipitation_Level as ordinal categories: Low=0, Medium=1, High=2", "Split data into train and test sets with 80% training and 20% testing", "Train a GradientBoostingClassifier on the training data", "Perform hyperparameter tuning with grid search over learning_rate and n_estimators", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.79, "f1": 0.77, "precision": 0.75, "recall": 0.78, "top_feature_importances": {"Humidity_Percent": 0.31, "CloudCover_Cloudy": 0.22, "Temperature_C": 0.18}, "hyperparameters": {"learning_rate": 0.1, "n_estimators": 100}}}
{"purpose": "Predict whether a telecom customer will subscribe to a premium data plan based on usage and demographics.", "raw_table": "CustomerID,Age,Gender,Monthly_Usage_GB,Contract_Type,Payment_Method,Customer_Satisfaction,Premium_Subscriber\n1001,34,Male,12.4,monthly,Credit Card,8,Yes\n1002,45,Female,NaN,Yearly,bank transfer,7,no\n1003,29,Female,5.3,Monthly,CASH,9,No\n1004,51,Male,20.1,yearly,Credit card,6,Yes\n1005,38,Female,7.8,Monthly,Credit Card,missing,No\n1006,42,Male,15.6,Monthly,Credit Card,7,Yes\n1007,23,female,3.2,Monthly,Cash,8,no\n1008,36,Male,9.0,Yearly,Bank Transfer,7,No\n1009,48,Male,14.5,Monthly,Credit card,5,Yes\n1010,31,Female,10.2,Monthly,Credit Card,8,No\n1011,27,Male,NaN,Monthly,Credit Card,9,No\n1012,50,Female,18.9,Yearly,Credit Card,6,Yes\n1013,39,Female,8.3,monthly,Credit Card,7,No\n1014,44,Male,12.1,Yearly,credit card,8,Yes", "model_steps": ["Load the CSV data into a dataframe and inspect for missing and inconsistent values", "Standardize column names and categorical values (e.g., unify 'monthly'/'Monthly', 'Credit card'/'credit card')", "Impute missing numeric values in Monthly_Usage_GB and Customer_Satisfaction with median values", "Convert target variable 'Premium_Subscriber' to binary (Yes=1, No=0)", "One-hot encode categorical variables: Gender, Contract_Type, Payment_Method", "Split data into training (80%) and test (20%) sets with stratification on target", "Standardize numeric features: Age, Monthly_Usage_GB, Customer_Satisfaction", "Train a RandomForestClassifier on the training set with default hyperparameters", "Perform grid search on max_depth and n_estimators with 5-fold cross-validation", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.86, "f1": 0.82, "precision": 0.8, "recall": 0.85, "confusion_matrix": [[7, 2], [1, 8]], "top_features": {"Monthly_Usage_GB": 0.32, "Contract_Type_Yearly": 0.25, "Customer_Satisfaction": 0.15, "Payment_Method_Credit Card": 0.12, "Age": 0.08}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a customer will make a purchase during a browsing session on an ecommerce platform.", "raw_table": "session_id,session_duration,device_type,browser,referral_source,pages_viewed,is_returning_customer,purchase\ns1,300,Mobile,chrome,Google,5,Yes,1\ns2,120,Desktop,firefox,Direct,2,No,0\ns3,45,mobile,Chrome,google,1,No,0\ns4,NA,Tablet,Edge,Facebook,3,Yes,1\ns5,200,Desktop,Firefox,Direct,4,Yes,1\ns6,180,Mobile,safari,Google,3,No,0\ns7,75,desktop,Chrome,Direct,2,No,0\ns8,240,Tablet,Edge,facebook,6,Yes,1\ns9,300,Mobile,Safari,Google,5,No,1\ns10,60,Mobile,chrome,,1,No,0\ns11,210,Desktop,firefox,Google,4,Yes,1\ns12,100,Mobile,Chrome,Direct,2,,0\ns13,90,Tablet,edge,Facebook,3,No,0\ns14,50,Desktop,Chrome,Google,1,Yes,0", "model_steps": ["Load the raw CSV data into a DataFrame", "Handle missing values by imputing session_duration with median and referral_source with mode", "Standardize inconsistent capitalization in categorical columns (device_type, browser, referral_source, is_returning_customer)", "Fill missing is_returning_customer values with 'No'", "Convert categorical variables (device_type, browser, referral_source, is_returning_customer) using one-hot encoding", "Split the dataset into training (80%) and testing (20%) sets", "Standardize numeric features: session_duration and pages_viewed", "Train a RandomForestClassifier to predict 'purchase'", "Evaluate model performance using accuracy, F1 score, precision, and recall on the test set", "Generate and analyze the confusion matrix", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.82, "precision": 0.79, "recall": 0.86, "confusion_matrix": [[6, 1], [2, 5]], "top_feature_importances": {"pages_viewed": 0.28, "session_duration": 0.25, "is_returning_customer_Yes": 0.18}}}
{"purpose": "Predict whether a customer will make a purchase during a website session based on their activity and demographics.", "raw_table": "session_id,user_age,user_gender,device_type,page_views,time_spent_minutes,referral_source,purchase\n1,25,MOBILE,Female,Mobile,5,12.5,Organic,Yes\n2,30,Desktop,male,Desktop,8,15.0,Social Media,No\n3,22,MOBILE,female,Mobile,3,,Ads,Yes\n4,45,Desktop,Female,Desktop,10,20.3,Organic,No\n5,38,mobile,Male,Mobile,7,17.2,Direct,Yes\n6,29,Tablet,,Tablet,6,10.1,organic,No\n7,35,Desktop,Female,Desktop,9,18.0,Social Media,Yes\n8,40,mobile,Male,Mobile,4,11.0,Direct,No\n9,28,tablet,Female,Tablet,5,13.5,Ads,Yes\n10,32,Desktop,Male,Desktop,8,16.3,Social media,No\n11,27,MOBILE,Female,Mobile,6,14.0,Direct,Yes\n12,31,Desktop,Male,Desktop,7,NaN,Organic,No\n13,26,mobile,male,Mobile,4,12.0,ads,Yes\n14,34,Tablet,Female,Tablet,5,15.5,Direct,No", "model_steps": ["Load the dataset and identify target variable 'purchase' as binary classification target", "Clean messy categorical values in 'user_gender', 'device_type', and 'referral_source' by standardizing capitalization and imputing missing genders as 'Unknown'", "Convert 'purchase' target to binary labels (Yes=1, No=0)", "One-hot encode categorical features: 'user_gender', 'device_type', and 'referral_source'", "Impute missing numeric values in 'time_spent_minutes' using median value", "Standardize numeric features 'user_age', 'page_views', and 'time_spent_minutes'", "Split data into training (80%) and test (20%) sets randomly", "Train a RandomForestClassifier with default parameters on training data", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score", "Compute confusion matrix to analyze false positives and false negatives"], "model_results": {"accuracy": 0.79, "precision": 0.81, "recall": 0.76, "f1": 0.78, "confusion_matrix": {"true_positive": 7, "false_positive": 2, "true_negative": 9, "false_negative": 3}, "top_feature_importances": {"time_spent_minutes": 0.32, "page_views": 0.25, "referral_source_Organic": 0.15, "device_type_Mobile": 0.1, "user_gender_Female": 0.08}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a manufactured widget will pass quality inspection based on process parameters.", "raw_table": "BatchID,Machine,Operator,Temperature,Pressure,CycleTime,MaterialLot,PassedQC\n101,A1,John Doe,220,5.1,30,Lot42,Yes\n102,a1,Jane Smith,215,5.0,28,lot42,No\n103,A2,Bob Lee,NaN,5.3,32,Lot43,Yes\n104,A2,John Doe,225,Five,29,Lot44,Yes\n105,A1,Jane Smith,218,5.2,NaN,Lot42,No\n106,A3,,222,5.1,31,Lot45,Yes\n107,A3,Bob Lee,219,5.0,30,Lot45,No\n108,a1,John Doe,217,5.1,27,lot42,Yes\n109,A2,Jane Smith,221,5.3,33,Lot44,Yes\n110,A3,Bob Lee,220,5.2,30,Lot45,No\n", "model_steps": ["Load CSV data into a DataFrame", "Identify and handle missing values in Temperature, Pressure, CycleTime, and Operator columns by imputation", "Standardize capitalization and whitespace inconsistencies in categorical columns (Machine, Operator, MaterialLot)", "Convert Pressure column from mixed types (numeric and string 'Five') to numeric, handling conversion errors", "Encode categorical variables using one-hot encoding", "Split data into training (80%) and test (20%) sets stratified by PassedQC", "Standardize numeric features: Temperature, Pressure, CycleTime", "Train a RandomForestClassifier with default hyperparameters", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate and analyze confusion matrix to understand error types"], "model_results": {"accuracy": 0.83, "precision": 0.8, "recall": 0.85, "f1": 0.82, "confusion_matrix": {"TP": 5, "FP": 1, "TN": 4, "FN": 1}, "top_feature_importances": {"Temperature": 0.32, "Pressure": 0.28, "CycleTime": 0.2, "Machine_A1": 0.1, "Operator_JohnDoe": 0.05, "MaterialLot_Lot42": 0.05}}}
{"purpose": "Predict the likelihood of hospital readmission within 30 days for diabetic patients.", "raw_table": "PatientID,Age,Gender,HbA1c_Level,Previous_Admissions,Smoker,Medication_Compliance,Readmitted_30Days\n1,58,Male,7.2,2,Yes,High,Yes\n2,65,Female,8.5,0,no,medium,No\n3,47,Male,6.1,1,Yes,low,No\n4,52,Female,NA,3,No,High,Yes\n5,39,Female,7.9,0,yes,Medium,No\n6,71,Male,8.3,4,No,high,Yes\n7,60,Female,6.8,2,No,Low,No\n8,50,Male,7.4,1,yes,Medium,Yes\n9,45,Female,6.5,0,No,HIGH,No\n10,55,Male,7.0,1,Yes,medium,Yes\n11,68,Female,8.8,3,No,Low,Yes\n12,62,Male,7.7,2,No,medium,No\n13,49,Female,7.1,0,yes,HIGH,No\n14,53,Male,NA,1,No,Low,Yes", "model_steps": ["Handle missing HbA1c_Level values by imputing with the median of available HbA1c_Level values", "Normalize capitalization inconsistencies in 'Smoker' and 'Medication_Compliance' columns and treat as categorical variables", "Encode 'Gender', 'Smoker', and 'Medication_Compliance' using one-hot encoding", "Split data into train and test sets with an 80/20 ratio, stratifying by Readmitted_30Days", "Standardize numeric features: Age, HbA1c_Level, and Previous_Admissions", "Train a RandomForestClassifier with default parameters to predict Readmitted_30Days", "Perform grid search over max_depth values [3, 5, 7] and n_estimators [50, 100]", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Compute confusion matrix for test predictions", "Identify top 3 feature importances from the best model"], "model_results": {"accuracy": 0.79, "f1": 0.76, "precision": 0.74, "recall": 0.78, "confusion_matrix": {"true_positive": 7, "true_negative": 8, "false_positive": 3, "false_negative": 2}, "top_feature_importances": {"HbA1c_Level": 0.32, "Previous_Admissions": 0.25, "Medication_Compliance_Medium": 0.15}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a student will pass the final exam based on their study habits and demographic information.", "raw_table": "StudentID,Hours_Studied,Attendance_Rate,Gender,Previous_Grade,Parental_Education,Extra_Curricular,Passed_Final\n1,15,0.95,Male,B,College,Yes,Yes\n2,7,0.80,Female,a,High School,No,No\n3,NA,0.88,female,C,College,yes,No\n4,12,0.92,Male,B,College,No,Yes\n5,5,0.50,Female,C,High school,No,No\n6,20,0.98,Male,A,Graduate,Yes,Yes\n7,8,0.85,Male,B,College,No,Yes\n8,10,missing,Female,b,High School,Yes,Yes\n9,3,0.40,Female,C,High school,No,No\n10,17,0.97,Male,A,Graduate,Yes,Yes\n11,6,0.75,Female,B,College,No,No\n12,NA,0.90,Male,b,College,Yes,Yes\n13,14,0.93,Female,B,Graduate,No,Yes\n14,4,0.55,Male,C,High School,No,No\n15,9,0.85,Female,B,College,Yes,Yes", "model_steps": ["Load the CSV data and inspect for missing or inconsistent values", "Impute missing numeric values (Hours_Studied) with median", "Impute missing categorical values (Attendance_Rate) with mode and standardize capitalization in Gender and Parental_Education", "Convert categorical variables (Gender, Parental_Education, Extra_Curricular) using one-hot encoding", "Encode target variable 'Passed_Final' as binary (Yes=1, No=0)", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features (Hours_Studied, Attendance_Rate) using StandardScaler", "Train a RandomForestClassifier with default parameters on the training set", "Perform grid search over n_estimators (50, 100) and max_depth (5, 10)", "Evaluate the best model on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Identify and report top feature importances"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.9, "f1": 0.87, "confusion_matrix": [[6, 1], [2, 6]], "top_feature_importances": {"Hours_Studied": 0.35, "Attendance_Rate": 0.3, "Previous_Grade_B": 0.15, "Parental_Education_College": 0.1, "Extra_Curricular_Yes": 0.1}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Predict whether a government grant application will be approved based on applicant and project details.", "raw_table": "Application_ID,Applicant_Age,Applicant_Gender,Project_Type,Requested_Amount,Previous_Grants,Submission_Month,Approved\n1,45,Male,Infrastructure,500000,2,January,Yes\n2,34,female,Education,150000,0,FebRUary,No\n3,29,Male,Health,300000,1,March,Yes\n4,54,Female,Infrastructure,700000,3,April,yes\n5,47,male,Education,,1,May,No\n6,38,Female,Health,250000,2,june,No\n7,41,Male,infrastructure,400000,0,July,Yes\n8,,Female,Education,200000,1,August,No\n9,50,male,Health,350000,2,September,Yes\n10,33,Female,Education,175000,1,october,No\n11,28,Male,Health,150000,0,November,Yes\n12,60,female,Infrastructure,800000,4,December,Yes\n13,42,Male,Education,225000,1,january,No\n14,39,Female,Health,300000,2,February,Yes", "model_steps": ["Identify target variable 'Approved' and encode as binary (Yes=1, No=0)", "Fill missing numeric values in 'Applicant_Age' and 'Requested_Amount' using median imputation", "Standardize numeric features 'Applicant_Age', 'Requested_Amount', and 'Previous_Grants'", "Normalize and unify categorical variables: standardize casing for 'Applicant_Gender', 'Project_Type', and 'Submission_Month'", "One-hot encode categorical variables 'Applicant_Gender', 'Project_Type', and 'Submission_Month'", "Split data into training and test sets (80/20 split)", "Train a RandomForestClassifier with default parameters on the training data", "Perform grid search over max_depth=[3,5,7] and n_estimators=[50,100]", "Evaluate best model on test set computing accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"Requested_Amount": 0.32, "Project_Type_Health": 0.18, "Previous_Grants": 0.15}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a movie will be a box office hit based on production and cast features.", "raw_table": "Movie_ID,Genre,Director,Lead_Actor,Production_Budget_Million,Runtime_Minutes,Release_Season,Box_Office_Hit\n1,Action,Jones,Smith,150,130,Summer,Yes\n2,Comedy,lee,Johnson,40,95,Winter,No\n3,Drama,Baker,Davis,30,120,SPRING,No\n4,Action,Smith,Walker,200,140,Summer,Yes\n5,Horror,lee,Smith,15,100,Fall,No\n6,Comedy,Baker,Wilson,50,,Winter,No\n7,Drama,Jones,Johnson,60,110,Spring,No\n8,Horror,Smith,Walker,25,90,Fall,No\n9,Action,Jones,Wilson,175,135,Summer,Yes\n10,Comedy,Baker,Davis,45,105,Winter,No\n11,Action,Lee,Smith,160,125,Summer,Yes\n12,Drama,Smith,Walker,55,115,Spring,No\n13,Horror,Baker,Wilson,20,85,Fall,No\n14,Comedy,Lee,Johnson,55,105,Winter,No", "model_steps": ["Load the data and handle missing values in the Runtime_Minutes column by imputing the median.", "Normalize capitalization inconsistencies in categorical columns such as Director and Release_Season.", "One-hot encode the categorical variables: Genre, Director, Lead_Actor, and Release_Season.", "Convert the target variable Box_Office_Hit to a binary label (Yes=1, No=0).", "Split data into training and test sets with an 80/20 ratio.", "Standardize numeric features: Production_Budget_Million and Runtime_Minutes.", "Train a RandomForestClassifier on the training set.", "Perform grid search cross-validation over max_depth values of [3, 5, 7].", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score.", "Generate a confusion matrix to analyze false positives and false negatives."], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"Production_Budget_Million": 0.35, "Genre_Action": 0.25, "Release_Season_Summer": 0.15, "Director_Smith": 0.1, "Runtime_Minutes": 0.08, "Lead_Actor_Smith": 0.07}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a movie will be a box office hit based on its production attributes and genre.", "raw_table": "MovieID,Genre,DirectorExperience,BudgetMillions,LeadActorPopularity,ReleaseMonth,ScreenCount,BoxOfficeHit\n1,Action,High,150,85,July,3200,Yes\n2,Comedy,medium,30,40,December,1500,No\n3,Drama,Low,25,55,May,800,No\n4,action,HIGH,200,90,July,3500,Yes\n5,Comedy,,45,50,October,1200,No\n6,Drama,Low,20,60,April,600,No\n7,Comedy,Medium,50,55,December,1300,No\n8,Action,High,180,88,July,3300,Yes\n9,Drama,low,15,30,March,400,No\n10,Action,High,170,95,July,3400,Yes\n11,Comedy,Medium,40,45,November,1100,No\n12,Drama,Low,22,58,May,700,No\n13,Action,High,160,92,July,3100,Yes\n14,Comedy,Medium,48,50,December,1400,No", "model_steps": ["Load the CSV data into a DataFrame", "Clean inconsistent capitalization in 'Genre' and 'DirectorExperience' columns", "Impute missing values in 'DirectorExperience' with the mode", "Convert 'BoxOfficeHit' target variable from Yes/No to binary 1/0", "One-hot encode categorical variables: Genre, DirectorExperience, ReleaseMonth", "Standardize numeric features: BudgetMillions, LeadActorPopularity, ScreenCount", "Split data into training and test sets with an 80/20 ratio", "Train a RandomForestClassifier on the training set", "Perform grid search over max_depth=[5,10,15] and n_estimators=[50,100]", "Evaluate model accuracy, F1 score, precision, and recall on the test set", "Extract and report feature importances from the trained model"], "model_results": {"accuracy": 0.93, "f1": 0.92, "precision": 0.9, "recall": 0.95, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}, "feature_importances": {"BudgetMillions": 0.35, "LeadActorPopularity": 0.3, "Genre_Action": 0.15, "DirectorExperience_High": 0.1, "ScreenCount": 0.07, "ReleaseMonth_July": 0.03}}}
{"purpose": "Build a classification model to predict hospital readmission risk within 30 days for diabetic patients.", "raw_table": "PatientID,Age,Gender,BMI,AdmissionType,HbA1c,PreviousAdmissions,DischargeDisposition,Readmitted\n1,65,Male,28.7,Emergency,7.8,2,Home,Yes\n2,50,Female,31.2,Elective,6.5,0,Home,No\n3,73,Male,missing,Emergency,8.2,5,Skilled Nursing,Yes\n4,60,female,29.1,Urgent,7.0,1,Home,No\n5,45,Male,27.3,Elective,6.1,0,home,No\n6,52,Female,32.0,Emergency,9.0,3,Rehab,Yes\n7,58,,30.5,Elective,7.4,1,Home,No\n8,67,Female,29.9,urgent,7.9,2,Home,Yes\n9,49,Male,26.8,Emergency,,0,Home,No\n10,55,Female,28.0,Elective,6.8,1,Skilled Nursing,No\n11,62,Male,33.3,Elective,8.5,4,Rehab,Yes\n12,70,Female,31.7,Emergency,7.7,3,Home,Yes\n13,53,Male,29.5,Urgent,6.9,0,Home,No\n14,61,Female,28.8,Emergency,8.1,2,Home,Yes", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Impute missing BMI values using median BMI grouped by Gender", "Normalize capitalization in categorical columns such as Gender, AdmissionType, and DischargeDisposition", "Fill missing HbA1c values with the median HbA1c value", "Convert target variable 'Readmitted' to binary (Yes=1, No=0)", "One-hot encode categorical variables: Gender, AdmissionType, DischargeDisposition", "Split data into train and test sets with an 80/20 ratio, stratifying on Readmitted", "Standardize numeric features: Age, BMI, HbA1c, PreviousAdmissions using training data statistics", "Train a RandomForestClassifier with 100 trees and max_depth=5", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.79, "precision": 0.75, "recall": 0.72, "f1": 0.74, "confusion_matrix": {"true_negative": 7, "false_positive": 2, "false_negative": 3, "true_positive": 6}, "top_feature_importances": {"HbA1c": 0.32, "PreviousAdmissions": 0.27, "DischargeDisposition_Rehab": 0.15}, "hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict hourly electricity consumption category (Low, Medium, High) based on weather and timestamp features.", "raw_table": "Hour,Temperature_C,Humidity_Percent,Day_of_Week,Is_Holiday,Wind_Speed_kmh,Electricity_Usage_Category\n0,15.2,82,Monday,No,5.2,Low\n1,14.8,85,Monday,no,4.8,Low\n2,14.5,87,Monday,No,5.0,Low\n3,14.0,90,Monday,No,missing,Low\n4,13.8,88,Monday,No,4.5,Low\n5,14.1,80,Monday,No,4.0,Medium\n6,16.5,70,Monday,No,5.5,Medium\n7,20.1,65,Monday,No,6.0,High\n8,23.0,60,Monday,No,7.0,High\n9,25.5,55,Monday,No,7.5,High\n10,27.0,50,monday,No,7.8,High\n11,28.5,48,Monday,NO,8.0,High\n12,29.0,45,Monday,No,8.2,High\n13,28.0,50,Monday,No,8.0,High", "model_steps": ["Load the dataset and identify missing and inconsistent values", "Impute missing Wind_Speed_kmh using median value", "Standardize capitalization and unify 'Is_Holiday' and 'Day_of_Week' categorical values", "One-hot encode 'Day_of_Week' and 'Is_Holiday' columns", "Split data into 80% training and 20% testing sets", "Standardize numeric features: Temperature_C, Humidity_Percent, Wind_Speed_kmh, Hour", "Train a RandomForestClassifier to predict Electricity_Usage_Category", "Perform grid search over number of trees (n_estimators) and max_depth", "Evaluate model using accuracy, F1 score, precision, and recall on the test set", "Generate and analyze confusion matrix"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.82, "recall": 0.84, "confusion_matrix": {"Low": {"Low": 5, "Medium": 0, "High": 0}, "Medium": {"Low": 1, "Medium": 3, "High": 1}, "High": {"Low": 0, "Medium": 1, "High": 3}}, "top_feature_importances": {"Hour": 0.35, "Temperature_C": 0.3, "Is_Holiday_No": 0.1, "Humidity_Percent": 0.08, "Day_of_Week_Monday": 0.06, "Wind_Speed_kmh": 0.05, "Day_of_Week_monday": 0.04, "Is_Holiday_no": 0.02}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a customer will make a purchase during a browsing session on an ecommerce platform.", "raw_table": "session_id,browser,device_type,page_views,time_spent_seconds,referral_source,cart_abandonment,purchase\nS001,Chrome,Mobile,5,300,Google,No,Yes\nS002,firefox,Desktop,3,180,Facebook,yes,No\nS003,Safari,Tablet,8,,Organic,No,Yes\nS004,chrome,Desktop,2,90,Google,No,No\nS005,Edge,Mobile,7,240,Facebook,No,Yes\nS006,,Desktop,4,150,Organic,No,No\nS007,Chrome,mobile,6,270,google,No,Yes\nS008,Firefox,Tablet,NaN,210,Direct,Yes,No\nS009,Safari,Desktop,5,200,organic,,Yes\nS010,chrome,Mobile,3,120,Facebook,No,No\nS011,Edge,tablet,7,260,Direct,No,Yes\nS012,Firefox,Desktop,4,NaN,Google,No,No\nS013,Chrome,Mobile,6,310,Facebook,No,Yes\nS014,Safari,Desktop,5,190,Facebook,No,No\nS015,Chrome,Mobile,4,150,,Yes,No", "model_steps": ["Load the CSV data and inspect for missing and inconsistent values", "Standardize categorical text data to lowercase (e.g., browser names and device types)", "Impute missing numeric values in 'time_spent_seconds' and 'page_views' using median imputation", "Fill missing categorical values in 'browser' and 'referral_source' with 'unknown'", "Convert 'cart_abandonment' and 'purchase' columns to binary (Yes=1, No=0)", "One-hot encode categorical variables: 'browser', 'device_type', and 'referral_source'", "Split data into 80% training and 20% testing sets, stratified on the target 'purchase'", "Standardize numeric features ('page_views', 'time_spent_seconds') using StandardScaler", "Train a RandomForestClassifier with 100 trees on the training set", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.8, "precision": 0.78, "recall": 0.83, "f1": 0.8, "top_feature_importances": {"time_spent_seconds": 0.32, "cart_abandonment": 0.25, "device_type_Mobile": 0.15}}}
{"purpose": "Predict whether a customer will purchase a recommended product based on browsing behavior and demographics.", "raw_table": "CustomerID,Age,Gender,Country,TimeOnSite_minutes,PagesViewed,PreviousPurchases,DeviceType,RecommendedPurchase\n1,25,Male,USA,12,5,2,Desktop,Yes\n2,37,Female,usa,7,,1,Mobile,No\n3,29,Female,Canada,15,8,3,Tablet,Yes\n4,,Male,UK,20,10,5,Desktop,Yes\n5,45,Female,Germany,5,3,0,mobile,No\n6,33,,France,11,6,2,Desktop,Yes\n7,31,Male,USA,9,4,1,Laptop,No\n8,27,Female,UK,14,7,4,Desktop,Yes\n9,22,Male,Canada,8,3,0,mobile,No\n10,39,Female,Germany,13,6,3,Tablet,Yes\n11,28,Male,France,,5,2,Desktop,No\n12,35,Female,UK,17,9,4,Mobile,Yes\n13,41,Male,USA,6,,1,desktop,No\n14,30,Female,Canada,10,5,2,Laptop,Yes", "model_steps": ["Load CSV data and inspect for missing or inconsistent values", "Standardize inconsistent capitalization in 'Country', 'DeviceType', and 'Gender' columns", "Impute missing values: median for numeric columns, mode for categorical columns", "Convert 'RecommendedPurchase' to binary target variable (Yes=1, No=0)", "One-hot encode categorical variables: Gender, Country, DeviceType", "Split dataset into training (80%) and testing (20%) sets", "Scale numeric features using StandardScaler", "Train a Logistic Regression classifier on the training data", "Perform 5-fold cross-validation to tune regularization strength", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score"], "model_results": {"accuracy": 0.82, "precision": 0.79, "recall": 0.85, "f1": 0.82, "top_feature_importances": {"PreviousPurchases": 0.42, "TimeOnSite_minutes": 0.28, "PagesViewed": 0.15, "DeviceType_Desktop": 0.08, "Country_USA": 0.07}, "hyperparameters": {"C": 1.0, "penalty": "l2", "solver": "liblinear"}}}
{"purpose": "Predict whether a retail customer will make a purchase based on their browsing behavior and demographics.", "raw_table": "CustomerID,Age,Gender,TimeOnSiteMinutes,PagesVisited,ReferralSource,DeviceType,Purchase\n1,34,Male,15,5,Google,Mobile,Yes\n2,28,Female,7,3,facebook,Desktop,No\n3,,Female,10,4,Direct,Mobile,Yes\n4,45,Male,25,,Google,Tablet,Yes\n5,22,Female,5,2,Facebook,desktop,No\n6,37,Male,12,6,Direct,Mobile,Yes\n7,29,Female,8,3,google,Mobile,No\n8,31,Unknown,16,7,Referral,Tablet,Yes\n9,40,Male,NaN,5,Direct,Desktop,No\n10,27,Female,20,8,Referral,Mobile,Yes\n11,35,Male,9,4,Google,Mobile,No\n12,38,Female,14,5,facebook,Desktop,Yes\n13,33,Male,11,6,Referral,Tablet,No\n14,30,female,7,3,Direct,Mobile,No", "model_steps": ["Load the dataset and identify missing and inconsistent values.", "Impute missing numeric values for Age and PagesVisited using median values.", "Standardize the Age, TimeOnSiteMinutes, and PagesVisited numeric features.", "Normalize inconsistent capitalization in categorical columns: Gender, ReferralSource, and DeviceType.", "Treat 'Unknown' in Gender as a separate category.", "One-hot encode categorical variables: Gender, ReferralSource, DeviceType.", "Convert target variable 'Purchase' to binary labels (Yes=1, No=0).", "Split data into training (80%) and test (20%) sets with stratification on Purchase.", "Train a RandomForestClassifier with default hyperparameters on the training data.", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score.", "Extract and rank feature importances from the trained Random Forest model."], "model_results": {"accuracy": 0.79, "precision": 0.81, "recall": 0.75, "f1": 0.78, "top_feature_importances": {"TimeOnSiteMinutes": 0.32, "PagesVisited": 0.27, "ReferralSource_Referral": 0.12, "Age": 0.1, "DeviceType_Mobile": 0.08}}}
{"purpose": "Build a classification model to predict whether a bus trip will be delayed by more than 10 minutes.", "raw_table": "trip_id,route_number,day_of_week,departure_time,num_passengers,weather_condition,driver_experience,delayed\n1,15,Monday,08:15,23,Clear,5,No\n2,7,Tuesday,14:30,15,Rain,3,Yes\n3,15,Wednesday,08:00,18,clear,4,No\n4,22,Thursday,18:45,30,Fog,7,Yes\n5,7,Friday,09:10,,Rain,2,Yes\n6,15,Saturday,20:00,12,Clear,5,No\n7,22,Sunday,07:50,25,Fog,6,Yes\n8,7,Monday,17:30,20,rain,3,Yes\n9,15,Tuesday,12:00,22,Clear,4,No\n10,22,Wednesday,19:15,28,Fog,8,Yes\n11,7,Thursday,08:45,16,Rain,3,Yes\n12,15,Friday,06:30,19,CLEAR,5,No\n13,22,Saturday,21:00,27,Fog,7,Yes\n14,7,Sunday,13:15,14,Rain,2,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Correct inconsistent capitalization in the 'weather_condition' column (e.g., 'Clear', 'clear', 'CLEAR')", "Fill missing values in 'num_passengers' with the median passenger count", "Convert 'departure_time' to numeric feature representing minutes from midnight", "One-hot encode categorical variables: 'route_number', 'day_of_week', and 'weather_condition'", "Split the dataset into training (80%) and test (20%) sets", "Standardize numeric features: 'departure_time', 'num_passengers', and 'driver_experience'", "Train a RandomForestClassifier to predict the 'delayed' target", "Perform grid search over number of trees and max_depth parameters", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix for the test set predictions"], "model_results": {"accuracy": 0.86, "precision": 0.84, "recall": 0.89, "f1": 0.86, "confusion_matrix": {"true_positive": 8, "true_negative": 7, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"departure_time": 0.28, "weather_condition_Rain": 0.22, "driver_experience": 0.15, "route_number_7": 0.12, "day_of_week_Monday": 0.09}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 6}}}
{"purpose": "Predict whether a movie will be a box office hit based on its production and genre features.", "raw_table": "MovieID,Genre,Director_Reputation,Budget_Million,Lead_Actor_Popularity,Release_Season,Duration_Minutes,BoxOffice_Hit\n1,Action,High,150,8.5,Summer,130,Yes\n2,Comedy,medium,30,7.0,winter,95,No\n3,Drama,High,45,6.5,Fall,110,Yes\n4,Action,Low,80,5.0,Summer,125,yes\n5,Comedy,,25,6.0,Spring,100,No\n6,Drama,Medium,50,7.5,Fall,115,Yes\n7,Thriller,High,70,8.0,Summer,105,no\n8,Comedy,Low,40,4.5,Winter,90,No\n9,Action,Medium,110,7.8,Summer,,Yes\n10,Drama,Medium,60,7.2,Fall,120,Yes\n11,Thriller,low,65,5.5,Summer,108,No\n12,Comedy,Medium,35,6.8,Spring,98,No\n13,Action,High,,8.2,Summer,135,Yes\n14,Drama,Medium,55,7.0,Fall,112,Yes", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Standardize capitalization in categorical columns like 'Director_Reputation' and target variable 'BoxOffice_Hit'", "Impute missing numeric values in 'Budget_Million' and 'Duration_Minutes' using median values", "Fill missing categorical values in 'Director_Reputation' with the mode", "Encode categorical variables 'Genre', 'Director_Reputation', and 'Release_Season' using one-hot encoding", "Convert target variable 'BoxOffice_Hit' to binary (Yes=1, No=0)", "Split data into train and test sets (80/20)", "Standardize numeric features: 'Budget_Million', 'Lead_Actor_Popularity', and 'Duration_Minutes'", "Train a RandomForestClassifier on the training set", "Tune hyperparameters using grid search over max_depth and number of estimators", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Determine top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.88, "recall": 0.83, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 1, "false_negative": 2}, "top_feature_importances": {"Budget_Million": 0.32, "Lead_Actor_Popularity": 0.25, "Director_Reputation_High": 0.18}, "best_hyperparameters": {"max_depth": 7, "n_estimators": 100}}}
{"purpose": "Predict whether a citizen will comply with a tax audit based on demographic and financial information.", "raw_table": "Citizen_ID,Age,Income_Level,Employment_Status,Previous_Audits,Reported_Income,Residence_Type,Complied\n001,45,Medium,Employed,2,55000,Urban,Yes\n002,34,High,Self-employed,0,120000,Suburban,No\n003,52,low,Unemployed,1,18000,Rural,yes\n004,28,,Employed,0,40000,urban,No\n005,40,Medium,Employed,3,65000,Suburban,Yes\n006,37,High,Self-Employed,2,110000,Rural,No\n007,60,Low,Retired,5,30000,Rural,Yes\n008,50,Medium,Employed,2,,Urban,No\n009,31,Medium,Unemployed,1,35000,Suburban,Yes\n010,44,High,Employed,4,90000,Urban,No\n011,29,Medium,Self-employed,0,48000,Suburban,yes\n012,55,Low,Retired,3,25000,Rural,No\n013,38,Medium,Employed,1,60000,Urban,Yes\n014,47,High,Employed,2,97000,Suburban,No", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Clean inconsistent capitalization in categorical columns (e.g., Residence_Type and Complied)", "Impute missing values in Income_Level and Reported_Income using median/mode as appropriate", "Encode categorical variables: Income_Level, Employment_Status, Residence_Type using one-hot encoding", "Convert target variable 'Complied' to binary format (Yes=1, No=0)", "Split data into training (80%) and testing (20%) sets, stratifying on the target variable", "Standardize numeric features: Age, Previous_Audits, Reported_Income", "Train a RandomForestClassifier with default hyperparameters on the training set", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Compute and analyze the confusion matrix", "Identify top 3 most important features from the trained model"], "model_results": {"accuracy": 0.79, "precision": 0.81, "recall": 0.75, "f1": 0.78, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 2, "false_negative": 3}, "top_features": ["Previous_Audits", "Reported_Income", "Employment_Status_Self-employed"]}}
{"purpose": "Predict whether a movie will be a box office hit based on its production and genre features.", "raw_table": "MovieID,Genre,DirectorExperienceYears,ProductionBudgetMillions,LeadActorFamous,ReleaseMonth,RuntimeMinutes,BoxOfficeHit\n1,Action,10,150,Yes,July,130,Yes\n2,Comedy,5,30,No,December,95,No\n3,Drama,15,,Yes,March,110,Yes\n4,action,7,55,No,June,105,No\n5,Horror,3,20,No,October,100,No\n6,Comedy,NaN,25,yes,August,90,No\n7,Drama,12,80,Yes,April,115,Yes\n8,Horror,8,40,No,october,98,No\n9,Action,20,200,Yes,July,140,Yes\n10,Comedy,6,35,No,November,100,No\n11,Drama,NaN,70,Yes,May,112,Yes\n12,Horror,5,15,No,October,95,No\n13,Action,9,120,Yes,August,125,Yes\n14,Comedy,4,28,No,December,92,No", "model_steps": ["Load CSV data and inspect for missing and inconsistent values", "Normalize categorical values (e.g., unify 'action' and 'Action', 'yes' and 'Yes')", "Impute missing numeric values in DirectorExperienceYears and ProductionBudgetMillions with median values", "Encode categorical variables: Genre, LeadActorFamous, and ReleaseMonth using one-hot encoding", "Split dataset into training (80%) and test (20%) sets with stratification on BoxOfficeHit", "Standardize numeric features: DirectorExperienceYears, ProductionBudgetMillions, RuntimeMinutes", "Train a RandomForestClassifier to predict BoxOfficeHit", "Perform grid search to tune max_depth and n_estimators hyperparameters", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze false positives and false negatives", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"ProductionBudgetMillions": 0.35, "Genre_Action": 0.22, "LeadActorFamous_Yes": 0.18}, "hyperparameters": {"max_depth": 8, "n_estimators": 100}}}
{"purpose": "Predict the likelihood of 30-day hospital readmission for patients with heart failure based on clinical and demographic features.", "raw_table": "PatientID,Age,Gender,AdmissionType,BMI,NumPrevAdmissions,Diabetes,SmokingStatus,DischargeDisposition,Readmitted\n1,68,M,Emergency,29.5,2,Yes,Current,Home,Yes\n2,54,F,Elective,27.0,0,No,never,Rehab,No\n3,77,M,Emergency,,1,YES,Former,Home,Yes\n4,63,F,Urgent,31.2,3,No,Current,SNFF,Yes\n5,59,M,Elective,28.4,1,No,Unknown,Home,No\n6,72,F,Emergency,26.7,2,Yes,Current,home,Yes\n7,65,F,Urgent,NaN,1,No,never,SNF,No\n8,70,M,Emergency,30.1,4,yes,Former,Home,Yes\n9,61,F,Elective,25.8,0,No,Current,Rehab,No\n10,55,M,Urgent,28.9,1,No,Current,SNF,No\n11,69,M,Emergency,27.5,3,Yes,former,Home,Yes\n12,58,F,Elective,29.0,0,No,Current,Home,No\n13,66,M,Emergency,29.8,2,Yes,,Home,Yes\n14,73,F,Urgent,26.5,1,No,Never,Rehab,No", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Impute missing BMI values using median BMI grouped by Gender and AdmissionType", "Standardize the numeric features: Age, BMI, and NumPrevAdmissions", "Normalize inconsistent categorical entries (e.g., 'yes', 'YES', 'Yes' unified to 'Yes'; 'home' and 'Home' unified to 'Home')", "One-hot encode categorical variables: Gender, AdmissionType, Diabetes, SmokingStatus, DischargeDisposition", "Split the dataset into 80% training and 20% test sets, stratified by the target Readmitted", "Train a RandomForestClassifier on the training data with 100 trees and max_depth=5", "Perform 5-fold cross-validation on the training set to tune max_depth (range 3 to 7)", "Evaluate the final model on the test set, computing accuracy, F1 score, precision, recall", "Generate the confusion matrix and identify top 5 feature importances"], "model_results": {"accuracy": 0.79, "f1": 0.75, "precision": 0.77, "recall": 0.74, "confusion_matrix": [[8, 2], [3, 11]], "top_feature_importances": {"NumPrevAdmissions": 0.28, "AdmissionType_Emergency": 0.22, "Diabetes_Yes": 0.15, "BMI": 0.13, "SmokingStatus_Current": 0.1}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Develop a classification model to predict the likelihood of hospital readmission within 30 days for diabetic patients.", "raw_table": "PatientID,Age,Gender,BMI,HbA1c,Medication,PreviousAdmissions,Readmitted\n1,54,Male,29.5,7.8,Metformin,2,Yes\n2,67,Female,NA,8.2,Insulin,1,No\n3,45,Male,33.1,6.9,metformin,0,No\n4,59,Female,28.4,9.1,Sulfonylurea,3,Yes\n5,72,Male,31.0,NA,Insulin,2,Yes\n6,38,Female,22.0,7.5,Metformin,0,No\n7,49,Male,27.8,7.2,insulin,1,No\n8,60,Female,30.2,8.7,Sulfonylurea,2,Yes\n9,55,Unknown,29.9,7.4,Metformin,1,No\n10,63,Female,31.5,8.5,Sulfonylurea,4,Yes\n11,48,Male,26.3,7.0,Metformin,0,No\n12,70,Female,NA,8.9,Insulin,3,Yes\n13,52,Female,28.7,7.6,Sulfonylurea,1,No\n14,46,Male,30.1,7.1,Metformin,0,No", "model_steps": ["Load dataset from CSV string into a DataFrame", "Identify and impute missing numeric values in BMI and HbA1c with median values", "Standardize numeric features: Age, BMI, HbA1c, PreviousAdmissions", "Normalize 'Medication' and 'Gender' categorical variables by converting to lowercase and handling unknown/missing values", "One-hot encode categorical variables: Gender and Medication", "Convert target variable 'Readmitted' to binary (Yes=1, No=0)", "Split data into training (80%) and testing (20%) sets with stratification on target", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search cross-validation to tune max_depth parameter (values: 5, 10, 15)", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score", "Generate and analyze confusion matrix to assess false positives and false negatives", "Identify top 3 feature importances from the trained Random Forest model"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.79, "f1": 0.81, "confusion_matrix": {"True_Positive": 19, "True_Negative": 31, "False_Positive": 4, "False_Negative": 5}, "top_feature_importances": {"PreviousAdmissions": 0.31, "HbA1c": 0.25, "Medication_insulin": 0.15}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a manufactured part will fail quality inspection based on sensor readings and production conditions.", "raw_table": "Part_ID,Temperature,Pressure,Material_Type,Operator_Shift,Machine_Age,Defect\n001,350,5.2,Steel,Day,3,No\n002,355,5.1,steel,Night,4,Yes\n003,370,,Aluminum,Day,2,No\n004,360,5.5,Aluminum,NIGHT,5,Yes\n005,345,5.0,Steel,Day,3,No\n006,NaN,4.9,Plastic,Night,1,No\n007,375,5.7,plastic,Day,2,Yes\n008,365,5.3,Aluminum,Night,3,No\n009,358,5.1,Steel,Day,4,No\n010,362,5.4,Steel,Night,3,Yes\n011,355,5.2,Aluminum,day,1,No\n012,370,5.6,Steel,Night,NaN,Yes\n013,365,5.3,Plastic,Day,2,No\n014,360,5.2,Steel,Night,3,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Clean data by correcting inconsistent capitalization in 'Material_Type' and 'Operator_Shift'", "Impute missing numeric values in 'Temperature', 'Pressure', and 'Machine_Age' using median imputation", "Encode 'Material_Type' and 'Operator_Shift' using one-hot encoding", "Split the dataset into train (80%) and test (20%) sets, stratifying on 'Defect'", "Standardize numeric features: 'Temperature', 'Pressure', and 'Machine_Age'", "Train a RandomForestClassifier on the training data with default parameters", "Perform grid search over 'max_depth' with values [3, 5, 7] using 5-fold cross-validation", "Evaluate the best model on the test set computing accuracy, precision, recall, and F1 score", "Generate and display the confusion matrix", "Extract and report top 3 feature importances"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.79, "f1": 0.81, "confusion_matrix": [[7, 1], [2, 4]], "top_feature_importances": {"Temperature": 0.32, "Material_Type_Steel": 0.25, "Operator_Shift_Night": 0.18}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features.", "raw_table": "HouseID,Location,Size_sqft,Bedrooms,Bathrooms,Year_Built,Garage,Condition,SalePrice\n1,Uptown,1800,3,2,1995,Yes,Good,350000\n2,Downtown,1500,2,1,,No,Excellent,310000\n3,Suburb,2200,4,3,2005,Yes,Fair,420000\n4,suburb,1700,3,2,1998,Yes,Good,365000\n5,Midtown,,3,2,2000,No,Poor,300000\n6,Uptown,1600,2,1,2010,No,good,330000\n7,Downtown,1400,2,1,1990,No,Excellent,295000\n8,Midtown,2000,3,2,2003,Yes,Fair,400000\n9,Suburb,2100,4,3,2007,Yes,Good,415000\n10,midtown,1750,3,2,2001,No,Poor,320000\n11,Uptown,1900,3,2,NA,Yes,Good,360000\n12,Downtown,1600,2,1,1994,No,Excellent,305000\n13,Suburb,2300,4,3,2012,yes,Fair,430000\n14,Uptown,1850,3,2,1999,Yes,Good,355000", "model_steps": ["Load data and parse CSV", "Standardize capitalization in 'Location' and 'Condition' columns", "Impute missing 'Size_sqft' and 'Year_Built' values with median values", "Convert 'Garage' from Yes/No to binary 1/0", "One-hot encode categorical variables: 'Location' and 'Condition'", "Split data into train and test sets (80/20)", "Standardize numeric features: 'Size_sqft', 'Year_Built', 'Bedrooms', 'Bathrooms'", "Train a RandomForestRegressor with default parameters", "Evaluate model using RMSE, MAE, and R2 on test set", "Extract and report top 3 feature importances"], "model_results": {"rmse": 18000, "mae": 13500, "r2": 0.87, "top_feature_importances": {"Size_sqft": 0.35, "Location_Uptown": 0.2, "Year_Built": 0.15}}}
{"purpose": "Build a regression model to predict average monthly temperature based on geographic and atmospheric factors.", "raw_table": "location,latitude,longitude,altitude_m,land_cover,month,avg_temp_C\nMountainView,37.4,-122.1,150,Forest,Jan,12.5\nCoastCity,36.7,-121.8,5,Coast,jan,15.3\nHighland,38.1,-123.2,850,Grassland,Feb,8.2\nValley,37.0,-121.9,120,Urban,Mar,20.1\nMountainview,37.4,-122.1,150,forest,Apr,16.0\nCoastCity,36.7,-121.8,5,Coast,May,17.8\nHighland,38.1,-123.2,,Grassland,Jun,14.5\nValley,37.0,-121.9,120,Urban,Jul,24.0\nMountainView,37.4,-122.1,150,Forest,Aug,23.5\nCoastcity,36.7,-121.8,5,Coast,Sep,21.0\nHighland,38.1,-123.2,850,Grassland,Oct,13.2\nValley,37.0,-121.9,120,Urban,Nov,16.3\nMountainView,37.4,-122.1,150,Forest,Dec,10.7", "model_steps": ["Load the CSV data into a DataFrame.", "Normalize inconsistent capitalization in 'land_cover' and 'location' columns.", "Impute missing 'altitude_m' value with median altitude.", "Convert 'month' to numerical month index.", "One-hot encode categorical variables: 'land_cover' and 'location'.", "Split data into training (80%) and testing (20%) sets.", "Standardize numeric features: 'latitude', 'longitude', 'altitude_m', and 'month'.", "Train a RandomForestRegressor to predict 'avg_temp_C'.", "Evaluate the model using RMSE, MAE, and R-squared on the test set.", "Extract and report feature importances from the trained model."], "model_results": {"rmse": 1.8, "mae": 1.4, "r2": 0.87, "top_feature_importances": {"month": 0.4, "altitude_m": 0.25, "land_cover_Forest": 0.15, "latitude": 0.1, "location_MountainView": 0.05, "longitude": 0.05}}}
{"purpose": "Predict whether a customer will upgrade their mobile data plan within the next month.", "raw_table": "CustomerID,Age,MonthlyUsageGB,PlanType,CustomerSegment,SupportCallsLastMonth,DataUpgrade\n001,45,12.5,Premium,Enterprise,2,Yes\n002,29,8.7,standard,Consumer,0,no\n003,38,,Standard,consumer,1,Yes\n004,50,20.0,Premium,Enterprise,5,YES\n005,22,5.4,Basic,Consumer,0,No\n006,35,10.0,Standard,Enterprise,1,No\n007,,7.8,basic,consumer,0,no\n008,41,15.2,Premium,Enterprise,3,Yes\n009,30,9.1,Standard,consumer,?,No\n010,27,6.5,Basic,Consumer,0,No\n011,48,17.3,Premium,Enterprise,4,YES\n012,33,11.0,Standard,Consumer,2,Yes\n013,39,13.5,Premium,enterprise,1,Yes\n014,26,4.7,Basic,Consumer,0,No\n015,44,16.1,Premium,Enterprise,3,Yes", "model_steps": ["Load the CSV data and parse into a DataFrame", "Standardize capitalization and trim whitespace in categorical columns (PlanType, CustomerSegment, DataUpgrade)", "Handle missing numeric values by imputing median for Age and MonthlyUsageGB", "Replace '?' in SupportCallsLastMonth with zero and convert to integer", "Convert target variable DataUpgrade to binary (Yes=1, No=0)", "One-hot encode categorical features PlanType and CustomerSegment", "Split data into train and test sets with an 80/20 ratio", "Scale numeric features (Age, MonthlyUsageGB, SupportCallsLastMonth) using StandardScaler", "Train a RandomForestClassifier on the training set with default hyperparameters", "Evaluate model performance on the test set calculating accuracy, precision, recall, and F1 score", "Extract feature importances from the trained model"], "model_results": {"accuracy": 0.87, "precision": 0.83, "recall": 0.79, "f1": 0.81, "top_feature_importances": {"MonthlyUsageGB": 0.34, "PlanType_Premium": 0.22, "SupportCallsLastMonth": 0.15, "Age": 0.12, "CustomerSegment_Enterprise": 0.09, "PlanType_Standard": 0.08}}}
{"purpose": "Predict whether a citizen will apply for government housing assistance based on demographic and socioeconomic factors.", "raw_table": "ApplicantID,Age,Income,EmploymentStatus,MaritalStatus,NumberOfChildren,HomeOwnership,AppliedForAssistance\n001,34,45000,Employed,Married,2,No,Yes\n002,28,32000,Unemployed,Single,0,yes,No\n003,45,58000,employed,Divorced,3,No,Yes\n004,52,Null,Retired,Married,1,No,No\n005,37,47000,Employed,Single,,Yes,No\n006,41,39000,Employed,Married,2,No,Yes\n007,29,27000,unemployed,Single,0,No,No\n008,50,62000,Employed,Married,4,No,Yes\n009,33,Null,Employed,Single,1,No,No\n010,26,35000,Unemployed,Single,0,Yes,No\n011,60,54000,Retired,Married,3,No,Yes\n012,38,48000,Employed,Married,2,Yes,No\n013,43,50000,Employed,Single,2,No,Yes\n014,,46000,Employed,Divorced,1,No,No\n015,31,31000,Unemployed,Single,0,No,No", "model_steps": ["Load the dataset and identify missing and inconsistent values", "Impute missing numeric values in 'Age' and 'Income' columns using median", "Standardize inconsistent capitalization in 'EmploymentStatus' and 'HomeOwnership' columns", "One-hot encode categorical variables: 'EmploymentStatus', 'MaritalStatus', and 'HomeOwnership'", "Fill missing values in 'NumberOfChildren' with zero", "Split data into training (80%) and testing (20%) sets stratified on the target 'AppliedForAssistance'", "Standardize numeric features 'Age' and 'Income'", "Train a RandomForestClassifier with default parameters to predict 'AppliedForAssistance'", "Evaluate the model using accuracy, precision, recall, and F1 score on the test set", "Generate confusion matrix and feature importance scores"], "model_results": {"accuracy": 0.87, "precision": 0.83, "recall": 0.78, "f1": 0.8, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 2}, "feature_importances": {"Income": 0.28, "Age": 0.22, "EmploymentStatus_Employed": 0.18, "MaritalStatus_Married": 0.12, "NumberOfChildren": 0.1, "HomeOwnership_Yes": 0.1}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Classify whether a given field is likely to have a pest infestation based on environmental and crop features.", "raw_table": "Field_ID,Soil_Type,Crop_Type,Avg_Temperature,Cumulative_Rainfall,Pesticide_Used,Pest_Infestation\nF01,Loamy,Wheat,22.5,300.2,Yes,Yes\nF02,Clay,Maize,18.0,450.1,no,No\nF03,Sandy,Rice,25.3,,YES,yes\nF04,loamy,Barley,20.1,320.0,No,No\nF05,Clay,maize,19.8,400.6,Yes,NO\nF06,Sandy,Wheat,23.5,290.3,yes,Yes\nF07,LoAmy,Rice,24.0,310.0,No,No\nF08,Clay,Barley,17.5,410.7,No,No\nF09,Sandy,Maize,26.1,350.5,Yes,yes\nF10,Loamy,Wheat,21.7,305.2,no,No\nF11,Clay,Rice,18.9,420.0,Yes,No\nF12,Sandy,Barley,25.0,330.3,No,Yes\nF13,Loamy,Maize,22.0,NaN,Yes,No\nF14,Clay,Wheat,19.5,400.0,Yes,Yes", "model_steps": ["Load CSV data and identify the target variable 'Pest_Infestation'", "Clean 'Soil_Type' and 'Crop_Type' columns by standardizing capitalization", "Handle missing values in 'Cumulative_Rainfall' by imputing median rainfall", "Convert 'Pesticide_Used' and 'Pest_Infestation' columns to binary categorical variables", "One-hot encode categorical variables: 'Soil_Type', 'Crop_Type', and 'Pesticide_Used'", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features: 'Avg_Temperature' and 'Cumulative_Rainfall'", "Train a RandomForestClassifier with default hyperparameters on the training set", "Evaluate model performance on the test set with accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.83, "precision": 0.79, "recall": 0.85, "f1": 0.82, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Cumulative_Rainfall": 0.31, "Pesticide_Used_Yes": 0.28, "Avg_Temperature": 0.22}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a student will pass the final exam based on attendance, prior grades, and study habits.", "raw_table": "StudentID,Attendance,PreviousGrade,StudyHours,Participation,Major,PassFinal\n1,85,78,12,High,Engineering,Yes\n2,90,82,15,medium,Biology,Yes\n3,70,65,,Low,engineering,No\n4,88,88,14,High,Mathematics,Yes\n5,60,55,5,low,Physics,No\n6,95,MISSING,20,High,Chemistry,Yes\n7,75,70,10,Medium,Biology,No\n8,80,75,9,Medium,PHYSICS,Yes\n9,,80,8,Medium,Mathematics,No\n10,65,60,6,Low,Chemistry,No\n11,92,85,16,,Engineering,Yes\n12,78,72,11,Medium,Biology,Yes\n13,83,77,13,Medium,Mathematics,Yes", "model_steps": ["Load dataset and identify target variable 'PassFinal'", "Handle missing values by imputing numeric columns with median and categorical columns with mode", "Normalize 'Attendance', 'PreviousGrade', and 'StudyHours' numeric features", "Standardize capitalization and one-hot encode categorical variables 'Participation' and 'Major'", "Encode target variable 'PassFinal' as binary (Yes=1, No=0)", "Split data into train and test sets (80/20 split)", "Train a RandomForestClassifier with 100 trees on training data", "Perform grid search to tune max_depth parameter over values [3, 5, 10]", "Evaluate model on test set computing accuracy, F1 score, precision, and recall", "Generate confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.81, "recall": 0.86, "confusion_matrix": [[6, 2], [1, 7]], "top_feature_importances": {"PreviousGrade": 0.35, "StudyHours": 0.25, "Attendance": 0.2, "Participation_High": 0.1, "Major_Engineering": 0.05}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a student will pass or fail the final exam based on study habits and demographic factors.", "raw_table": "StudentID,Hours_Studied,Gender,School_Type,Previous_Grades,Attendance_Rate,Participation,Final_Result\n1,15,Male,Public,88,0.95,High,Pass\n2,5,Female,Private,70,0.80,Medium,Fail\n3,12,M,public,85,0.90,Low,Pass\n4,8,Female,Private,81,missing,Medium,Fail\n5,,Male,Public,78,0.85,High,Fail\n6,20,Female,Public,92,0.98,High,Pass\n7,3,Male,private,60,0.75,Low,Fail\n8,10,Female,Public,83,0.88,Medium,Pass\n9,7,Male,Private,75,0.82,Medium,Fail\n10,14,Female,Public,90,0.93,High,Pass\n11,9,Male,Public,missing,0.89,Medium,Pass\n12,6,Female,Private,72,0.80,Low,Fail\n13,16,Male,public,89,0.94,High,Pass\n14,4,Female,Private,68,0.78,Low,Fail\n15,11,Male,Public,84,0.91,Medium,Pass", "model_steps": ["Load the dataset and identify missing and inconsistent values", "Normalize inconsistent capitalization in categorical columns (e.g., 'public' vs 'Public')", "Impute missing numeric values using median imputation", "Impute missing categorical values using the most frequent category", "Encode categorical variables using one-hot encoding", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features such as Hours_Studied, Previous_Grades, and Attendance_Rate", "Train a RandomForestClassifier to predict Final_Result", "Perform grid search over number of trees (n_estimators) and max_depth", "Evaluate the model on the test set using accuracy, F1 score, precision, and recall", "Generate and analyze the confusion matrix"], "model_results": {"accuracy": 0.87, "f1": 0.85, "precision": 0.88, "recall": 0.83, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"Hours_Studied": 0.35, "Previous_Grades": 0.3, "Attendance_Rate": 0.15, "Participation_High": 0.1, "School_Type_Private": 0.05}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Train a model to predict 30-day hospital readmission risk for diabetic patients.", "raw_table": "PatientID,Age,Gender,HbA1c_Level,Medication_Type,Previous_Admissions,Smoker,Readmitted\n1,65,Male,7.5,Insulin,2,No,Yes\n2,72,Female,8.2,Metformin,1,Yes,No\n3,58,Female,NaN,SULFONYLUREA,0,No,No\n4,45,Male,6.1,Metformin,1,No,No\n5,50,Male,7.9,insulin,3,Yes,Yes\n6,60,Female,8.0,Metformin,2,,Yes\n7,55,Male,7.3,Sulfonylurea,1,Yes,No\n8,62,Female,6.8,Metformin,NaN,No,No\n9,49,Male,7.0,Insulin,0,No,No\n10,70,Female,8.5,Metformin,4,Yes,Yes\n11,67,Male,7.8,,2,No,Yes\n12,53,Female,7.2,SULFONYLUREA,1,No,No\n13,59,Male,7.6,Metformin,1,Yes,Yes\n14,61,Female,6.5,insulin,0,No,No\n15,57,Female,7.1,Metformin,2,No,No", "model_steps": ["Handle missing values by imputing median for numeric and mode for categorical columns", "Normalize numeric features: Age, HbA1c_Level, Previous_Admissions", "Standardize inconsistent categorical values in Medication_Type (e.g., 'insulin' and 'Insulin')", "Convert categorical variables Gender, Medication_Type, and Smoker into one-hot encoding", "Split data into training (80%) and test (20%) sets with stratification on target Readmitted", "Train a RandomForestClassifier to predict readmission", "Perform grid search for hyperparameters: number of trees (100, 200) and max_depth (5, 10, None)", "Evaluate model performance on test set with accuracy, precision, recall, and F1 score", "Generate and analyze confusion matrix", "Identify top 3 feature importances"], "model_results": {"accuracy": 0.8, "precision": 0.75, "recall": 0.7, "f1": 0.72, "confusion_matrix": [[6, 2], [1, 5]], "top_feature_importances": {"HbA1c_Level": 0.35, "Previous_Admissions": 0.28, "Medication_Type_Insulin": 0.15}, "best_hyperparameters": {"n_estimators": 200, "max_depth": 10}}}
{"purpose": "Predict whether a student will pass the final exam based on study habits and demographic features.", "raw_table": "StudentID,Hours_Studied_per_Week,Attendance_Rate,Participation_Level,Previous_Grades,School_Type,Final_Exam_Result\n1,15,0.95,High,85,Public,Pass\n2,8,0.80,medium,75,private,pass\n3,12,0.90,Low,70,Public,Fail\n4,5,0.60,high,65,Private,fail\n5,0,,Medium,50,public,Fail\n6,20,0.98,High,90,Public,Pass\n7,7,0.70,LOW,60,Private,Fail\n8,10,0.85,Medium,72,public,Pass\n9,3,0.55,Low,,public,Fail\n10,18,0.92,High,88,Private,Pass\n11,9,0.78,medium,76,Public,Pass\n12,6,0.65,Medium,62,public,Fail\n13,11,0.88,Medium,80,Private,Pass\n14,4,0.50,low,58,public,Fail", "model_steps": ["Load the CSV data into a DataFrame", "Clean the data by correcting inconsistent capitalization in categorical columns (e.g., Participation_Level, School_Type, Final_Exam_Result)", "Impute missing numeric values in Hours_Studied_per_Week and Previous_Grades with median values", "Impute missing Attendance_Rate with mean value", "Encode categorical variables (Participation_Level and School_Type) using one-hot encoding", "Convert target variable Final_Exam_Result to binary label (Pass=1, Fail=0)", "Split data into training (80%) and testing (20%) sets with stratification on the target", "Standardize numeric features (Hours_Studied_per_Week, Attendance_Rate, Previous_Grades)", "Train a RandomForestClassifier with 100 trees and max_depth=5 on the training set", "Evaluate the model on the test set and compute accuracy, F1 score, precision, and recall", "Generate a confusion matrix for test set predictions"], "model_results": {"accuracy": 0.83, "f1": 0.84, "precision": 0.85, "recall": 0.83, "confusion_matrix": [[4, 1], [2, 8]], "top_feature_importances": {"Hours_Studied_per_Week": 0.38, "Attendance_Rate": 0.3, "Previous_Grades": 0.2, "Participation_Level_Medium": 0.07, "School_Type_Public": 0.05}}}
{"purpose": "Predict whether a retail customer will make a purchase during a promotional campaign.", "raw_table": "CustomerID,Age,Gender,Annual_Income,Store_Region,Last_Purchase_Amount,Promo_Response\n1,34,Male,55000,North,120.5,Yes\n2,29,Female,NaN,South,85,No\n3,42,FEMALE,72000,East,NaN,Yes\n4,23,Male,48000,west,40,No\n5,37,Female,61000,North,75.5,Yes\n6,31,Male,58000,South,NaN,No\n7,45,Female,69000,East,150.75,Yes\n8,38,male,65000,West,95,No\n9,50,Female,72000,north,200,Yes\n10,28,Male,54000,South,55,No\n11,35,Female,,East,130,Yes\n12,40,Male,60000,West,NaN,No\n13,33,Female,59000,South,100.25,Yes\n14,47,Male,70000,North,160,Yes", "model_steps": ["Load the dataset and identify missing values and inconsistent capitalizations", "Standardize casing for categorical variables 'Gender' and 'Store_Region'", "Impute missing numeric values ('Annual_Income' and 'Last_Purchase_Amount') using median imputation", "Encode categorical variables 'Gender' and 'Store_Region' using one-hot encoding", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features 'Age', 'Annual_Income', and 'Last_Purchase_Amount'", "Train a RandomForestClassifier to predict 'Promo_Response'", "Perform grid search for hyperparameters max_depth (5, 10, None) and n_estimators (50, 100)", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.81, "recall": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Last_Purchase_Amount": 0.35, "Annual_Income": 0.27, "Store_Region_North": 0.15}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict customer churn probability for a telecom provider to enable targeted retention efforts.", "raw_table": "customer_id,tenure_months,contract_type,internet_service,monthly_charges,total_charges,tech_support,churn\n1001,12,Month-to-month,Fiber optic,70.35,845.2,Yes,Yes\n1002,24,One year,DSL,45.60,1085.4,No,No\n1003,3,Month-to-month,fiber Optic,89.1,267.3,MISSING,Yes\n1004,48,Two year,DSL,35.4,1699.2,No,No\n1005,5,Month-to-month,DSL,NaN,160.5,No,Yes\n1006,36,One Year,Fiber optic,80.2,2887.5,yes,No\n1007,1,Month-to-month,No,20.0,20.0,No,Yes\n1008,60,Two year,DSL,34.9,2094.0,No,No\n1009,8,Month-to-month,Fiber Optic,75.5,604.0,No,Yes\n1010,18,One year,DSL,50.7,912.6,No,No\n1011,30,One year,Fiber optic,78.9,2367.0,Yes,No\n1012,2,Month-to-month,Fiber optic,,150.4,No,Yes\n1013,15,One year,DSL,44.4,MISSING,No,No\n1014,7,Month-to-month,Fiber optic,72.7,515.0,No,Yes", "model_steps": ["Load data and identify target variable 'churn' with binary classes Yes/No", "Handle missing values in 'monthly_charges', 'tech_support', and 'total_charges' by imputing median for numeric and mode for categorical", "Normalize inconsistent capitalization in categorical columns such as 'internet_service' and 'contract_type', standardize to lowercase", "One-hot encode categorical features: 'contract_type', 'internet_service', 'tech_support'", "Convert target variable 'churn' to binary (1 for Yes, 0 for No)", "Split data into training (80%) and testing (20%) sets with stratification on target", "Standardize numeric features 'tenure_months', 'monthly_charges', 'total_charges' using z-score normalization", "Train a RandomForestClassifier with 100 trees and max_depth=5 on the training set", "Perform 5-fold cross-validation on training set to tune max_depth parameter among [3,5,7]", "Evaluate final model on the test set computing accuracy, F1 score, precision, and recall", "Generate the confusion matrix for test predictions", "Identify top 3 most important features by mean decrease in impurity"], "model_results": {"accuracy": 0.79, "f1": 0.75, "precision": 0.72, "recall": 0.78, "confusion_matrix": {"true_positive": 7, "false_positive": 3, "true_negative": 15, "false_negative": 2}, "top_feature_importances": {"tenure_months": 0.31, "internet_service_fiber_optic": 0.25, "monthly_charges": 0.18}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a movie will be a box office hit based on its production and genre attributes.", "raw_table": "MovieID,Genre,BudgetMillion,DirectorExperienceYears,LeadActorPopularity,Country,RuntimeMinutes,Hit\n1,Action,150,10,85,USA,130,Yes\n2,comedy,30,4,60,UK,95,No\n3,Drama,45,,70,usa,110,No\n4,HORROR,20,2,40,Canada,90,No\n5,Action,200,15,95,USA,145,Yes\n6,Comedy,25,5,55,,100,No\n7,Drama,50,7,65,UK,115,No\n8,Action,180,12,80,Canada,140,Yes\n9,Horror,18,3,35,USA,85,No\n10,Comedy,,6,60,UK,98,No\n11,Drama,55,8,72,USA,120,Yes\n12,action,170,11,90,USA,135,Yes\n13,Comedy,28,4,58,Canada,100,No\n14,Drama,60,9,68,UK,125,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Normalize 'Genre' and 'Country' columns by converting all text to lowercase", "Fill missing values in 'BudgetMillion' and 'DirectorExperienceYears' with median values", "Convert target variable 'Hit' to binary (Yes=1, No=0)", "One-hot encode categorical variables 'Genre' and 'Country'", "Standardize numeric features: 'BudgetMillion', 'DirectorExperienceYears', 'LeadActorPopularity', 'RuntimeMinutes'", "Split data into training and test sets with an 80/20 ratio", "Train a RandomForestClassifier with 100 trees on the training data", "Evaluate the model on the test set calculating accuracy, precision, recall, and F1 score", "Identify and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.89, "f1": 0.86, "top_feature_importances": {"budgetmillion": 0.32, "leadactorpopularity": 0.25, "genre_action": 0.18}}}
{"purpose": "Build a regression model to estimate house sale prices based on property and neighborhood features.", "raw_table": "PropertyID,Neighborhood,Bedrooms,Bathrooms,Size_sqft,YearBuilt,Condition,SalePrice\n1,Greenwood,3,2,1500,1995,Good,350000\n2,Maple Ridge,4,,2000,2002,Fair,420000\n3,greenwood,2,1,850,1980,Excellent,210000\n4,Lakeside,5,3,3000,2015,Good,680000\n5,Maple Ridge,3,2,1800,NaN,Good,400000\n6,Greenwood,4,3,2200,2010,poor,450000\n7,Lakeside,3,2,1600,2008,Good,375000\n8,Maple Ridge,3,2,1700,2005,Excellent,430000\n9,Lakeside,4,3,2500,2012,,600000\n10,Greenwood,3,2,NaN,1998,Good,360000\n11,Maple Ridge,4,3,2100,2000,Fair,445000\n12,Lakeside,2,1,900,1995,Good,280000\n13,greenwood,3,2,1400,1999,Good,340000", "model_steps": ["Load the CSV data into a DataFrame", "Normalize neighborhood names to consistent capitalization", "Impute missing numeric values (Bathrooms, YearBuilt, Size_sqft) using median values per Neighborhood", "Fill missing categorical 'Condition' values with the mode", "Convert 'Condition' and 'Neighborhood' to categorical variables using one-hot encoding", "Split the data into train (80%) and test (20%) sets randomly", "Standardize numeric features (Bedrooms, Bathrooms, Size_sqft, YearBuilt)", "Train a Gradient Boosting Regressor on the training set", "Tune hyperparameters max_depth and learning_rate with cross-validation", "Evaluate the model on the test set calculating RMSE, MAE, and R2 score", "Identify top 3 feature importances from the trained model"], "model_results": {"rmse": 25000, "mae": 19000, "r2": 0.85, "top_feature_importances": {"Size_sqft": 0.45, "Neighborhood_Maple Ridge": 0.18, "YearBuilt": 0.12}, "hyperparameters": {"max_depth": 4, "learning_rate": 0.1}}}
{"purpose": "Predict whether a citizen will apply for government housing assistance based on demographic and financial data.", "raw_table": "Applicant_ID,Age,Income,Employment_Status,Household_Size,Region,Previous_Assistance,Applied\n1,45,55000,Employed,4,North,Yes,Yes\n2,32,NaN,Unemployed,2,south,No,No\n3,28,30000,employed,3,East,No,No\n4,NaN,45000,Unemployed,5,West,Yes,Yes\n5,51,70000,Employed,1,North,No,No\n6,37,40000,Employed,3,SOUTH,Yes,Yes\n7,43,62000,Unemployed,,East,No,No\n8,29,35000,Employed,2,West,Yes,No\n9,61,80000,Retired,1,North,No,No\n10,39,42000,employed,4,East,No,Yes\n11,34,38000,Unemployed,3,West,Yes,No\n12,48,NaN,Employed,2,North,No,Yes\n13,55,72000,Retired,2,South,No,No\n14,31,39000,Employed,3,East,Yes,Yes\n15,27,31000,unemployed,2,West,No,No", "model_steps": ["Load dataset and identify target variable 'Applied' as binary classification label", "Clean missing values: impute missing numeric values with median, and missing categorical values with mode", "Standardize capitalization in 'Employment_Status' and 'Region' columns to ensure consistency", "Convert 'Employment_Status', 'Region', and 'Previous_Assistance' categorical columns to one-hot encoded variables", "Split data into training (80%) and test (20%) sets with stratification on the target variable", "Standardize numeric features 'Age', 'Income', and 'Household_Size' using training set statistics", "Train a RandomForestClassifier with default parameters on the training data", "Perform grid search to tune 'max_depth' and 'n_estimators' hyperparameters using 5-fold cross-validation", "Evaluate the best model on the test set computing accuracy, precision, recall, and F1 score", "Analyze feature importances to understand key predictors", "Generate confusion matrix and report classification metrics"], "model_results": {"accuracy": 0.8, "precision": 0.78, "recall": 0.75, "f1": 0.76, "confusion_matrix": [[15, 4], [5, 20]], "top_feature_importances": {"Income": 0.28, "Employment_Status_Employed": 0.22, "Previous_Assistance_Yes": 0.18, "Household_Size": 0.12, "Region_North": 0.08, "Age": 0.06, "Region_South": 0.06}, "best_hyperparameters": {"max_depth": 7, "n_estimators": 150}}}
{"purpose": "Predict whether a citizen will apply for government housing assistance based on demographic and socioeconomic factors.", "raw_table": "Citizen_ID,Age,Income,Employment_Status,Marital_Status,Number_of_Dependants,Region,Housing_Assistance_Applied\n1,34,45000,Employed,Married,2,North,Yes\n2,45,55000,Unemployed,Single,,South,No\n3,29,30000,employed,Married,1,East,yes\n4,52,70000,Employed,Divorced,3,North,No\n5,37,,Self-Employed,Married,2,South,No\n6,41,40000,Employed,Single,0,East,No\n7,23,28000,Unemployed,Single,1,North,Yes\n8,36,47000,Employed,Married,2,West,No\n9,48,52000,Employed,Married,3,south,No\n10,39,60000,Self-employed,Divorced,2,East,Yes\n11,31,35000,Employed,Single,0,West,No\n12,44,49000,Employed,Married,2,North,Yes\n13,50,58000,Unemployed,Married,4,South,No\n14,28,33000,Employed,Single,1,East,No\n15,35,44000,,Married,2,West,Yes", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Clean Employment_Status and Region columns by standardizing capitalization and fixing typos", "Impute missing Income values with the median income", "Impute missing Number_of_Dependants with 0", "Impute missing Employment_Status value with the mode", "Encode categorical variables Employment_Status, Marital_Status, and Region using one-hot encoding", "Convert target variable Housing_Assistance_Applied to binary (Yes=1, No=0)", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features Age, Income, Number_of_Dependants using z-score normalization", "Train a RandomForestClassifier with 100 trees on the training set", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.8, "precision": 0.78, "recall": 0.75, "f1": 0.76, "top_feature_importances": {"Income": 0.33, "Employment_Status_Employed": 0.24, "Number_of_Dependants": 0.18}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict hourly electricity consumption category (Low, Medium, High) based on weather and time features.", "raw_table": "Hour,Temperature_C,Humidity,Weather_Condition,Day_Type,Electricity_Usage_Category\n0,15.2,80,Clear,weekday,Low\n1,14.8,82,clear,Weekday,Low\n2,14.5,85,Cloudy,weekend,Low\n3,14.0,88,Rain,Weekend,Low\n4,13.7,90,Rain,weekday,Medium\n5,14.1,87,Clear,weekDay,Medium\n6,16.5,75,Clear,weekday,High\n7,18.8,70,Clear,Weekday,High\n8,20.1,65,Cloudy,weekday,High\n9,21.0,60,Cloudy,Weekday,High\n10,22.3,58,Clear,Weekend,Medium\n11,23.0,55,Rain,weekend,Medium\n12,24.5,50,Clear,weekday,High\n13,24.0,,clear,Weekday,High", "model_steps": ["Identify and handle missing values in Humidity column by imputing with median", "Standardize numeric features: Hour, Temperature_C, Humidity", "Normalize inconsistent capitalization in Weather_Condition and Day_Type columns", "One-hot encode categorical variables: Weather_Condition, Day_Type", "Split data into training (80%) and testing (20%) sets using stratified sampling on target", "Train a RandomForestClassifier on the training set", "Perform grid search over max_depth and n_estimators hyperparameters", "Evaluate model on test set using accuracy, F1 score, precision, and recall metrics", "Generate confusion matrix to analyze misclassifications", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.84, "recall": 0.82, "confusion_matrix": {"Low": {"Low": 3, "Medium": 0, "High": 0}, "Medium": {"Low": 0, "Medium": 3, "High": 1}, "High": {"Low": 0, "Medium": 1, "High": 4}}, "top_feature_importances": {"Temperature_C": 0.35, "Hour": 0.3, "Weather_Condition_Clear": 0.15}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict whether a taxi trip will have a tip greater than 20% of the fare.", "raw_table": "trip_id,passenger_count,trip_distance,store_and_fwd_flag,payment_type,tip_percentage\n1,1,2.5,N,NOCASH,18\n2,2,5.7,Y,cash,22\n3,1,3.1,N,Cash,25\n4,3,10.2,N,CREDIT,15\n5,1,1.0,N,credit,30\n6,4,8.5,Y,Credit,NA\n7,2,4.7,N,CASH,19\n8,,5.0,N,CREDIT,21\n9,1,3.8,n,nocash,17\n10,1,2.0,N,Credit,20\n11,3,7.5,Y,CASH,23\n12,2,6.2,N,Credit,16\n13,1,2.8,N,NOCASH,24\n14,2,5.4,N,Cash,22\n15,4,9.1,N,cash,27", "model_steps": ["Load CSV data into a DataFrame", "Handle missing values: impute missing passenger_count with median, drop rows with missing tip_percentage", "Normalize inconsistent capitalization in categorical columns: convert store_and_fwd_flag and payment_type to lower case", "Create binary target variable 'high_tip' where tip_percentage > 20% is 1 else 0", "Drop original tip_percentage column after creating target", "Split data into train and test sets (80/20)", "One-hot encode categorical features: store_and_fwd_flag and payment_type", "Standardize numeric features: passenger_count and trip_distance", "Train a RandomForestClassifier with default parameters on training data", "Evaluate model on test data: compute accuracy, F1 score, precision, recall", "Generate confusion matrix and identify top 3 important features"], "model_results": {"accuracy": 0.87, "f1": 0.82, "precision": 0.79, "recall": 0.85, "confusion_matrix": {"true_positive": 7, "false_positive": 2, "true_negative": 9, "false_negative": 1}, "top_feature_importances": {"trip_distance": 0.38, "payment_type_cash": 0.24, "passenger_count": 0.18}}}
{"purpose": "Predict customer churn probability for a telecom provider to improve retention strategies.", "raw_table": "CustomerID,MonthlyCharges,ContractType,PaymentMethod,TenureMonths,InternetService,TechSupport,Churn\nC001,79.85,Month-to-month,Credit card,12,Fiber optic,Yes,No\nC002,56.3,One year,Electronic check,24,Fiber Optic,No,Yes\nC003,NaN,Month-to-month,mailed check,3,DSL,No,Yes\nC004,99.65,Two year,Credit Card,48,Fiber optic,Yes,No\nC005,34.15,Month-to-Month,Electronic Check,1,DSL,No,Yes\nC006,80.5,Month-to-month,Electronic check,5,Fiber optic,No,No\nC007,45.3,One year,Mailed Check,36,DSL,Yes,No\nC008,70.1,Month-to-month,credit card,10,Fiber optic,No,Yes\nC009,29.75,Two Year,ELECTRONIC CHECK,60,DSL,Yes,No\nC010,NaN,One year,Mailed check,28,Fiber optic,No,No\nC011,67.9,Month-to-month,Credit Card,6,Fiber optic,No,Yes\nC012,59.4,Month-to-month,Electronic check,2,DSL,Yes,Yes\nC013,48.2,One year,Mailed check,34,Fiber optic,No,No\nC014,83.1,Two year,Credit card,50,Fiber optics,Yes,No\nC015,41.5,Month-to-month,Credit card,7,DSL,No,Yes", "model_steps": ["Load data and identify target variable 'Churn'", "Handle missing values in 'MonthlyCharges' by imputing median values", "Normalize inconsistent capitalization and fix typos in categorical columns (e.g., 'Fiber Optic', 'Fiber optics' \u2192 'Fiber optic')", "Convert categorical variables 'ContractType', 'PaymentMethod', 'InternetService', and 'TechSupport' using one-hot encoding", "Split data into training and test sets with ratio 80/20", "Standardize numeric features 'MonthlyCharges' and 'TenureMonths' using StandardScaler", "Train a RandomForestClassifier with 100 trees on the training set", "Tune 'max_depth' hyperparameter via 5-fold cross-validation, selecting the best depth", "Evaluate model on test set and compute accuracy, F1 score, precision, and recall", "Generate a confusion matrix and identify top 3 important features"], "model_results": {"accuracy": 0.8, "f1": 0.78, "precision": 0.75, "recall": 0.82, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 2, "false_negative": 2}, "top_feature_importances": {"ContractType_Month-to-month": 0.32, "InternetService_Fiber optic": 0.28, "TenureMonths": 0.15}, "best_hyperparameters": {"max_depth": 6}}}
{"purpose": "Build a classification model to predict whether a taxi trip will exceed 20 minutes duration.", "raw_table": "trip_id,passenger_count,pickup_zone,dropoff_zone,payment_type,trip_distance_km,trip_duration_min,tip_amount,target_long_trip\n1,2,Downtown,Airport,Card,12.5,25,3.50,Yes\n2,1,Uptown,Downtown,cash,5.8,15,0.00,No\n3,3,Downtown,Midtown,Card,8.2,,1.20,No\n4,1,Midtown,Uptown,CARD,4.0,22,2.00,Yes\n5,2,Airport,Midtown,Cash,10.1,18,1.50,No\n6,1,Uptown,Airport,card,13.0,30,4.00,Yes\n7,,Downtown,Uptown,Cash,6.5,17,0.00,No\n8,2,Midtown,Downtown,Cash,7.3,21,2.50,Yes\n9,1,Uptown,Midtown,CARD,3.9,12,,No\n10,4,Downtown,Airport,Card,14.7,28,5.00,Yes\n11,1,Downtown,Midtown,Cash,NaN,20,1.00,Yes\n12,3,Uptown,Downtown,card,6.8,16,0.00,No\n13,2,Midtown,Airport,Cash,9.5,19,1.75,No\n14,1,Downtown,Uptown,cash,5.2,23,2.20,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values in 'passenger_count' and 'trip_duration_min' by imputing median values", "Correct inconsistent capitalization in 'payment_type' to standard lowercase", "Convert categorical columns 'pickup_zone', 'dropoff_zone', and 'payment_type' using one-hot encoding", "Split the dataset into train (80%) and test (20%) sets stratified by the target variable", "Standardize numeric features 'passenger_count', 'trip_distance_km', 'trip_duration_min', and 'tip_amount'", "Train a RandomForestClassifier to predict 'target_long_trip'", "Perform grid search with 5-fold cross-validation to tune 'max_depth' parameter over [3, 5, 7]", "Evaluate the final model on the test set calculating accuracy, F1 score, precision, and recall", "Generate the confusion matrix and extract top 3 feature importances"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.82, "recall": 0.87, "confusion_matrix": {"true_positive": 12, "true_negative": 10, "false_positive": 3, "false_negative": 2}, "top_feature_importances": {"trip_distance_km": 0.32, "trip_duration_min": 0.27, "pickup_zone_Downtown": 0.12}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a customer will make a purchase within the next month based on their recent browsing and engagement behavior.", "raw_table": "customer_id,age,gender,browse_time_minutes,num_sessions,last_purchase_days,device_type,region,purchase_next_month\n1,34,Male,45,3,15,Mobile,North,Yes\n2,29,female,30,2,,Desktop,South,No\n3,22,FEMALE,120,5,40,Mobile,East,Yes\n4,40,Male,15,N/A,10,Tablet,west,No\n5,31,male,60,4,5,Mobile,West,Yes\n6,28,Female,NaN,3,20,DESKTOP,North,No\n7,35,M,90,6,8,Mobile,South,Yes\n8,NaN,Female,20,1,30,Tablet,East,No\n9,27,Female,55,3,25,desktop,South,No\n10,33,Male,80,5,12,Mobile,North,Yes\n11,45,Male,25,2,50,Tablet,East,No\n12,38,female,70,4,NaN,Desktop,North,Yes\n13,26,Female,35,3,18,mobile,South,No\n14,30,Male,40,3,22,Tablet,West,Yes\n15,29,Female,50,4,7,Mobile,South,Yes", "model_steps": ["Load dataset and inspect for missing and inconsistent values", "Standardize 'gender' and 'device_type' columns to consistent lowercase categories", "Impute missing numeric values in 'age', 'browse_time_minutes', and 'last_purchase_days' with median values", "Convert target variable 'purchase_next_month' to binary encoding (Yes=1, No=0)", "One-hot encode categorical features: 'gender', 'device_type', and 'region'", "Split data into 80% training and 20% testing sets with stratification on the target", "Standardize numeric features: 'age', 'browse_time_minutes', 'num_sessions', 'last_purchase_days'", "Train a GradientBoostingClassifier on the training set", "Perform 5-fold cross-validation for hyperparameter tuning on n_estimators and learning_rate", "Evaluate model performance on the test set with accuracy, precision, recall, and F1 score", "Generate confusion matrix and analyze feature importances", "Export final model and preprocessing pipeline for deployment"], "model_results": {"accuracy": 0.87, "f1": 0.85, "precision": 0.83, "recall": 0.87, "confusion_matrix": {"true_negative": 12, "false_positive": 3, "false_negative": 2, "true_positive": 18}, "top_feature_importances": {"last_purchase_days": 0.32, "browse_time_minutes": 0.27, "num_sessions": 0.15, "device_type_mobile": 0.1, "region_south": 0.08}, "best_hyperparameters": {"n_estimators": 100, "learning_rate": 0.1, "max_depth": 3}}}
{"purpose": "Build a classification model to predict whether a bus trip will be delayed by more than 10 minutes.", "raw_table": "Trip_ID,Route,Day_of_Week,Start_Time,Traffic_Condition,Distance_km,Driver_Experience,Delay_Over_10min\n1,Route_A,Monday,08:15,Heavy,12.5,5,Yes\n2,route_b,Tuesday,09:00,Moderate,8.2,3,No\n3,Route_A,Wednesday,07:45,Heavy,15.0,10,Yes\n4,Route_C,Thursday,14:30,light,5.6,2,No\n5,Route_B,Friday,18:00,Moderate,9.8,NaN,Yes\n6,Route_C,Saturday,10:00,Heavy,7.7,4,No\n7,Route_A,Sunday,13:15,,11.0,7,Yes\n8,Route_B,Monday,17:45,Moderate,NaN,6,No\n9,Route_C,Tuesday,06:30,Light,6.1,8,No\n10,Route_A,Wednesday,12:00,Heavy,14.3,5,Yes\n11,route_b,Thursday,16:20,Moderate,10.0,3,No\n12,Route_C,Friday,09:50,Light,5.2,9,No\n13,Route_A,Saturday,08:10,HEAVY,13.1,11,Yes\n14,Route_B,Sunday,19:30,moderate,10.7,4,No", "model_steps": ["Load data and inspect for missing or inconsistent values", "Standardize categorical values for 'Route' and 'Traffic_Condition' (e.g., fix capitalization inconsistencies)", "Impute missing numeric values in 'Driver_Experience' and 'Distance_km' using median imputation", "One-hot encode categorical variables: 'Route', 'Day_of_Week', and 'Traffic_Condition'", "Convert target variable 'Delay_Over_10min' from Yes/No to binary 1/0", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features: 'Start_Time' converted to minutes after midnight, 'Distance_km', and 'Driver_Experience'", "Train a RandomForestClassifier with 100 trees", "Perform grid search to tune max_depth parameter over values [5, 10, 15]", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate and analyze confusion matrix", "Identify top 3 feature importances from the final model"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.78, "f1": 0.8, "confusion_matrix": [[8, 2], [3, 11]], "top_feature_importances": {"Traffic_Condition_Heavy": 0.31, "Distance_km": 0.22, "Start_Time_minutes": 0.18}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Build a regression model to estimate house prices based on property features.", "raw_table": "Property_ID,Bedrooms,Bathrooms,Size_sqft,Neighborhood,Year_Built,Has_Garage,Price\n1,3,2,1500,Downtown,1995,Yes,350000\n2,4,3,2000,suburb,2005,YES,420000\n3,2,1,,Midtown,1980,No,280000\n4,3,,1800,Suburb,2010,yes,390000\n5,5,4,3200,Downtown,2018,No,670000\n6,4,3,2500,midtown,2000,No,450000\n7,3,2,1700,Suburb,NaN,Yes,360000\n8,2,1,1200,Downtown,1990,No,310000\n9,4,,2300,Midtown,2003,Yes,480000\n10,3,2,1600,suburb,1998,No,340000\n11,,2,1400,Downtown,1992,Yes,330000\n12,3,2,1900,Midtown,2007,NO,410000\n13,4,3,2100,Suburb,2015,Yes,460000\n14,3,2,1800,downtown,2001,No,370000", "model_steps": ["Load the dataset from CSV format into a DataFrame", "Handle missing numeric values by imputing median values", "Standardize capitalization and fill missing values in categorical columns", "Convert 'Has_Garage' to binary feature (Yes=1, No=0)", "One-hot encode the 'Neighborhood' categorical variable", "Split data into training (80%) and testing (20%) sets", "Standardize numeric features: Bedrooms, Bathrooms, Size_sqft, Year_Built", "Train a Gradient Boosting Regressor on the training set", "Tune hyperparameters max_depth and learning_rate using 5-fold cross-validation", "Evaluate model performance on the test set using RMSE, MAE, and R2", "Analyze feature importances from the trained model"], "model_results": {"rmse": 28000, "mae": 21000, "r2": 0.87, "best_hyperparameters": {"max_depth": 4, "learning_rate": 0.1}, "top_feature_importances": {"Size_sqft": 0.35, "Neighborhood_Downtown": 0.2, "Bedrooms": 0.15, "Year_Built": 0.12, "Has_Garage": 0.08, "Bathrooms": 0.06, "Neighborhood_Suburb": 0.04}}}
{"purpose": "Predict hourly energy consumption of residential buildings based on weather and occupancy features.", "raw_table": "Hour,Temperature_C,Humidity,Day_Type,Occupants,Heating_Type,Energy_Consumption_kWh\n0,15.2,55,weekday,3,electric,1.2\n1,14.8,57,weekday,3,Electric,1.1\n2,14.5,58,weekday,2,electric,0.9\n3,14.0,60,weekday,2,electric,0.8\n4,13.8,62,weekday,,electric,0.7\n5,13.5,63,Weekday,1,electric,0.6\n6,14.0,61,weekday,2,gas,1.0\n7,16.5,55,weekend,4,gas,1.8\n8,19.0,50,weekend,5,Gas,2.3\n9,21.5,45,weekend,5,gas,2.7\n10,24.0,40,weekend,5,gas,3.0\n11,25.5,38,weekend,5,gas,3.2\n12,26.0,37,weekend,6,gas,3.5\n13,27.2,35,weekend,6,gas,3.7\n14,28.0,33,weekend,6,gas,3.9", "model_steps": ["Load CSV data into a DataFrame and inspect for missing or inconsistent values", "Impute missing 'Occupants' values using median occupancy grouped by 'Day_Type'", "Normalize inconsistent capitalization in 'Heating_Type' and 'Day_Type' columns", "One-hot encode categorical variables 'Day_Type' and 'Heating_Type'", "Split data into training (80%) and testing (20%) sets based on chronological hour order", "Standardize numeric features: 'Temperature_C' and 'Humidity'", "Train a RandomForestRegressor to predict 'Energy_Consumption_kWh'", "Tune hyperparameters 'n_estimators' and 'max_depth' using grid search with 5-fold cross-validation", "Evaluate the final model on the test set using RMSE, MAE, and R2 metrics", "Analyze feature importances from the trained model to understand key drivers of consumption"], "model_results": {"rmse": 0.18, "mae": 0.14, "r2": 0.92, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}, "top_feature_importances": {"Temperature_C": 0.35, "Occupants": 0.3, "Heating_Type_gas": 0.15, "Heating_Type_electric": 0.1, "Humidity": 0.07, "Day_Type_weekend": 0.03}}}
{"purpose": "Predict whether a student will pass the final exam based on study habits and demographic factors.", "raw_table": "StudentID,Hours_Studied,Attendance,Gender,Major,Previous_Grade,Pass_Final\n1,15,95,Male,Engineering,88,Yes\n2,8,85,FEMALE,Biology,76,No\n3,NaN,80,female,Engineering,82,Yes\n4,12,?,Male,Mathematics,90,Yes\n5,5,70,Male,biology,65,No\n6,20,100,Female,Engineering,92,Yes\n7,7,78,male,Mathematics,72,No\n8,10,88,Female,Engineering,81,Yes\n9,13,90,Female,Biology,?,Yes\n10,6,60,Male,Mathematics,68,No\n11,9,85,female,Engineering,79,Yes\n12,11,80,Male,Biology,73,No\n13,4,75,FEMALE,Mathematics,66,No\n14,14,95,Male,Engineering,85,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values in 'Hours_Studied', 'Attendance', and 'Previous_Grade' by imputing median values", "Standardize capitalization and unify 'Gender' and 'Major' categorical values to consistent lowercase", "Convert categorical variables 'Gender' and 'Major' into one-hot encoded features", "Split the dataset into training (80%) and testing (20%) sets stratified on the target 'Pass_Final'", "Standardize numeric features: 'Hours_Studied', 'Attendance', and 'Previous_Grade' using StandardScaler", "Train a RandomForestClassifier with default parameters on the training data", "Perform grid search over 'max_depth' [3, 5, 10] and 'n_estimators' [50, 100]", "Evaluate the best model on the test set reporting accuracy, precision, recall, and F1 score", "Generate the confusion matrix and identify top 3 most important features from the model"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "confusion_matrix": {"true_positive": 6, "false_positive": 1, "true_negative": 5, "false_negative": 2}, "top_feature_importances": {"Hours_Studied": 0.4, "Previous_Grade": 0.3, "Attendance": 0.2}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a student will pass a final exam based on study habits and demographic features.", "raw_table": "StudentID,Hours_Studied,Attendance_Percentage,Gender,Previous_Grade,Major,Passed\nS001,15,90,Male,B,Engineering,Yes\nS002,8,85,Female,C,science,No\nS003,20,92,Female,A,Engineering,Yes\nS004,5,70,Male,C,Business,No\nS005,12,80,Female,B,Science,Yes\nS006,9,85,Male,B,engineering,Yes\nS007,NA,75,Female,C,Business,No\nS008,18,95,Male,A,Science,Yes\nS009,7,65,Female,D,Business,No\nS010,14,88,Male,B,Science,Yes\nS011,6,80,Female,C,Engineering,No\nS012,16,92,Male,A,Engineering,Yes\nS013,4,60,male,D,Business,No\nS014,10,85,Female,B,Science,Yes", "model_steps": ["Load data from CSV and identify target variable 'Passed' as binary classification", "Handle missing values in 'Hours_Studied' by imputing median", "Standardize 'Hours_Studied' and 'Attendance_Percentage' numeric features", "Normalize inconsistent capitalization in 'Major' and 'Gender' categorical columns", "One-hot encode categorical variables 'Gender' and 'Major'", "Split data into training (80%) and testing (20%) sets with stratification on 'Passed'", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search over max_depth values [3,5,7] using 5-fold cross-validation", "Evaluate final model on test set using accuracy, precision, recall, and F1-score", "Compute confusion matrix and extract top 3 feature importances", "Generate predicted probabilities for the test set"], "model_results": {"accuracy": 0.85, "precision": 0.88, "recall": 0.82, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "false_positive": 1, "true_negative": 5, "false_negative": 2}, "top_feature_importances": {"Hours_Studied": 0.35, "Attendance_Percentage": 0.28, "Previous_Grade_B": 0.12}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict the likelihood of wheat crop disease presence based on environmental and field conditions.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Temperature_C,Previous_Crop,Disease_Present\nF001,Loam,120,22.5,Wheat,Yes\nF002,Clay,85,19.0,Barley,No\nF003,Silt,NaN,21.0,wheat,Yes\nF004,loam,100,23.5,Maize,No\nF005,Clay,95,,Barley,No\nF006,Silt,110,20.0,Wheat,Yes\nF007,Loam,130,25.0,Maize,Yes\nF008,Clay,90,18.5,barley,No\nF009,Silt,NaN,22.0,Wheat,Yes\nF010,Loam,115,23.0,Maize,No\nF011,loam,105,21.5,Wheat,Yes\nF012,Clay,88,19.5,Barley,No\nF013,Silt,115,20.5,Maize,Yes\nF014,loam,NaN,22.0,Wheat,No", "model_steps": ["Load data from CSV and inspect for missing or inconsistent values", "Standardize soil type names to consistent capitalization (e.g., 'Loam')", "Impute missing Rainfall_mm and Temperature_C values using median imputation", "Encode categorical variables: Soil_Type and Previous_Crop using one-hot encoding", "Convert target variable Disease_Present into binary labels (Yes=1, No=0)", "Split data into train and test sets with 80/20 ratio", "Train a RandomForestClassifier to predict Disease_Present", "Perform grid search over n_estimators [50, 100] and max_depth [3, 5, 7]", "Evaluate model using accuracy, precision, recall, and F1 score on test set", "Compute confusion matrix", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": [[5, 1], [2, 7]], "top_feature_importances": {"Rainfall_mm": 0.35, "Temperature_C": 0.3, "Soil_Type_Loam": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Build a classification model to predict whether a government grant application will be approved.", "raw_table": "Application_ID,Applicant_Age,Applicant_Education,Project_Type,Requested_Funding,Previous_Grants,Grant_Approved\n1,34,Bachelor,Healthcare,50000,2,Yes\n2,45,Master,Education,75000,1,No\n3,29,phd,Infrastructure,120000,,Yes\n4,41,Bachelor,Healthcare,60000,0,No\n5,,Master,education,85000,3,Yes\n6,37,Bachelor,Infrastructure,110000,1,No\n7,52,PhD,Healthcare,Invalid,2,Yes\n8,48,Master,Healthcare,70000,1,No\n9,33,Bachelor,Education,65000,0,Yes\n10,40,Master,Infrastructure,90000,2,No\n11,27,Bachelor,Healthcare,55000,0,Yes\n12,44,PhD,Education,80000,1,No\n13,30,Master,Infrastructure,100000,1,Yes\n14,36,Bachelor,education,72000,2,No", "model_steps": ["Load the CSV data into a dataframe and inspect for missing and inconsistent values", "Standardize categorical variables by normalizing case and fixing typos (e.g., 'phd' to 'PhD', 'education' to 'Education')", "Impute missing numeric values with median values (e.g., Applicant_Age)", "Handle invalid entries in numeric columns by treating them as missing (e.g., 'Invalid' in Requested_Funding) and imputing", "Encode categorical variables using one-hot encoding (Applicant_Education, Project_Type)", "Split data into training and test sets with an 80/20 ratio, stratified on the target variable Grant_Approved", "Train a RandomForestClassifier with default hyperparameters on the training data", "Perform grid search to tune max_depth and n_estimators with 5-fold cross-validation", "Evaluate the model on the test set using accuracy, F1 score, precision, and recall", "Generate and analyze the confusion matrix for test predictions", "Identify top feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.79, "f1": 0.81, "precision": 0.78, "recall": 0.85, "confusion_matrix": {"true_positive": 7, "false_positive": 2, "true_negative": 7, "false_negative": 2}, "top_feature_importances": {"Requested_Funding": 0.32, "Applicant_Education_PhD": 0.21, "Project_Type_Healthcare": 0.15, "Previous_Grants": 0.12, "Applicant_Age": 0.1}, "best_hyperparameters": {"max_depth": 6, "n_estimators": 100}}}
{"purpose": "Predict whether a student will pass or fail the final exam based on demographic and academic features.", "raw_table": "StudentID,Age,Gender,StudyHoursPerWeek,PreviousGrade,SchoolType,ParentalEducation,FinalExamResult\n1,17,Male,10,85,Public,Bachelor,Pass\n2,18,Female,5,70,Private,Master,Fail\n3,17,Male,15,,public,Highschool,Pass\n4,19,Female,8,60,Private,Bachelor,Fail\n5,18,Female,12,78,Public,Bachelor,Pass\n6,,Male,7,68,Public,Master,Fail\n7,17,FEMALE,9,80,Private,HighSchool,Pass\n8,18,Male,4,50,public,Bachelor,Fail\n9,19,Female,11,77,Public,master,Pass\n10,17,Male,3,55,Private,Highschool,Fail\n11,18,Male,14,82,Public,Bachelor,Pass\n12,17,Female,6,65,Private,bachelor,Fail\n13,18,Female,10,90,Public,Master,Pass\n14,19,Male,7,58,Public,Highschool,Fail\n15,17,Female,13,85,private,Bachelor,Pass", "model_steps": ["Load the CSV dataset into a DataFrame.", "Handle missing values by imputing the average Age and PreviousGrade.", "Standardize inconsistent capitalization in categorical columns: Gender, SchoolType, ParentalEducation.", "Encode categorical variables (Gender, SchoolType, ParentalEducation) using one-hot encoding.", "Split the dataset into training (80%) and test (20%) sets stratified by FinalExamResult.", "Scale numeric features: Age, StudyHoursPerWeek, PreviousGrade using StandardScaler.", "Train a RandomForestClassifier to predict FinalExamResult (Pass/Fail).", "Perform hyperparameter tuning with grid search over number of estimators (50, 100) and max_depth (5, 10).", "Evaluate the model on the test set using accuracy, F1 score, precision, and recall.", "Generate and analyze the confusion matrix.", "Identify top 3 feature importances from the trained model."], "model_results": {"accuracy": 0.87, "f1": 0.88, "precision": 0.9, "recall": 0.86, "confusion_matrix": {"TruePositive": 7, "TrueNegative": 6, "FalsePositive": 1, "FalseNegative": 2}, "top_feature_importances": {"PreviousGrade": 0.35, "StudyHoursPerWeek": 0.28, "ParentalEducation_Master": 0.12}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Build a classification model to predict whether a manufactured part will pass quality inspection based on sensor readings and production parameters.", "raw_table": "Part_ID,Machine_Type,Temperature,Pressure,Operator_Shift,Humidity,Defect\n001,A,75.2,101.5,Day,45,No\n002,B,,99.8,NIGHT,50,Yes\n003,a,76.0,100.2,Day,47,No\n004,C,74.5,NaN,Night,49,Yes\n005,B,75.8,101.0,Day,missing,No\n006,A,75.0,100.5,Day,46,No\n007,C,76.3,102.1,NIGHT,48,Yes\n008,b,74.8,100.0,Day,44,No\n009,A,missing,101.2,Night,47,Yes\n010,C,75.5,101.3,Day,46,No\n011,B,75.1,100.7,Night,50,Yes\n012,A,75.4,101.0,Day,45,No\n013,C,74.9,100.9,Day,48,No\n014,B,76.2,101.4,night,49,Yes", "model_steps": ["Load the CSV data and identify columns and missing values", "Clean the 'Humidity' column by replacing 'missing' with the median humidity", "Standardize capitalization in 'Machine_Type' and 'Operator_Shift' columns to a consistent format", "Impute missing numeric values in 'Temperature' and 'Pressure' with median values", "Encode categorical variables 'Machine_Type' and 'Operator_Shift' using one-hot encoding", "Split data into training (80%) and test (20%) sets with stratification on the target 'Defect'", "Standardize numeric features 'Temperature', 'Pressure', and 'Humidity' using StandardScaler", "Train a RandomForestClassifier with default parameters on the training set", "Perform grid search over 'n_estimators' [50,100] and 'max_depth' [5,10] using cross-validation", "Evaluate the final model on the test set computing accuracy, F1 score, precision, and recall", "Extract and report the top 3 feature importances", "Generate predictions on the test set and produce a confusion matrix"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.8, "recall": 0.87, "top_feature_importances": {"Pressure": 0.32, "Machine_Type_B": 0.2, "Temperature": 0.18}, "confusion_matrix": {"true_positive": 7, "true_negative": 10, "false_positive": 2, "false_negative": 1}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Predict customer churn within the next month based on usage patterns and account information.", "raw_table": "customer_id,account_length,voice_mail_plan,number_vmail_messages,total_day_minutes,total_day_calls,total_eve_minutes,total_eve_calls,total_night_minutes,total_night_calls,total_intl_minutes,total_intl_calls,customer_service_calls,churn\nC001,128,yes,25,265.1,110,197.4,99,244.7,91,10.0,3,1,No\nC002,107,No,0,161.6,123,195.5,103,254.4,103,13.7,3,4,Yes\nC003,137,YES,30,243.4,114,121.2,110,162.6,104,12.2,5,2,No\nC004,84,no,0,299.4,137,61.9,88,196.9,89,6.6,7,3,No\nC005,75,No,0,166.7,103,148.3,122,186.9,121,10.1,4,0,Yes\nC006,118,yes,24,223.4,108,220.6,104,203.9,118,7.6,6,1,No\nC007,121,No,0,218.2,111,348.5,121,212.6,112,8.7,5,5,Yes\nC008,147,Yes,28,157.0,99,103.1,101,211.8,97,12.4,2,1,No\nC009,117,No,0,184.5,113,351.6,125,215.8,98,9.3,4,3,Yes\nC010,141,yes,23,196.4,104,151.6,112,203.1,107,11.8,6,2,No\nC011,117,yes,27,230.2,118,202.4,110,213.3,120,9.9,3,0,No\nC012,100,No,0,148.3,108,123.6,105,152.1,90,7.2,3,4,Yes\nC013,118,no,0,215.8,122,165.7,112,203.7,110,7.3,6,1,No\nC014,109,YES,,188.5,107,89.2,100,176.2,111,9.5,4,3,Yes", "model_steps": ["Load data and identify target variable 'churn'", "Clean 'voice_mail_plan' column by standardizing capitalization and imputing missing values with 'No'", "Convert categorical features ('voice_mail_plan', 'churn') into numerical format using one-hot encoding or label encoding", "Split dataset into training (80%) and testing (20%) sets with stratification on target", "Standardize numeric features such as call minutes and number of calls", "Train a RandomForestClassifier with default hyperparameters on training data", "Perform grid search for max_depth and n_estimators to optimize model performance", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze classification errors", "Identify top 3 most important features based on feature importance scores"], "model_results": {"accuracy": 0.85, "precision": 0.8, "recall": 0.75, "f1": 0.77, "confusion_matrix": {"true_positive": 6, "true_negative": 7, "false_positive": 2, "false_negative": 2}, "top_features": ["customer_service_calls", "total_day_minutes", "number_vmail_messages"], "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a student will pass the final exam based on attendance, study hours, and demographic factors.", "raw_table": "StudentID,AttendanceRate,StudyHoursPerWeek,ParentalEducation,Gender,SchoolType,PassedFinal\nS01,0.85,10,highschool,Male,Public,yes\nS02,0.92,15,Bachelor,Female,Private,Yes\nS03,0.6,5,HighSchool,female,public,no\nS04,0.78,8,,Male,Public,no\nS05,0.95,20,Master,Male,Private,YES\nS06,0.5,3,Bachelor,Female,public,No\nS07,0.88,12,Highschool,Male,Private,Yes\nS08,0.7,6,bachelor,Female,Public,no\nS09,0.98,18,Master,Female,Public,yes\nS10,NaN,7,HighSchool,Male,Private,No\nS11,0.82,11,Master,Female,Public,Yes\nS12,0.9,14,master,Male,Private,yes\nS13,0.65,4,HighSchool,Female,Public,no", "model_steps": ["Load the raw CSV data into a DataFrame", "Handle missing values by imputing AttendanceRate with median and ParentalEducation with mode", "Normalize capitalization inconsistencies in categorical columns ParentalEducation and PassedFinal to lowercase", "Convert PassedFinal target variable into binary (yes=1, no=0)", "One-hot encode categorical features: ParentalEducation, Gender, and SchoolType", "Standardize numeric features: AttendanceRate and StudyHoursPerWeek", "Split data into train and test sets with 80% training and 20% testing", "Train a RandomForestClassifier on the training set", "Perform grid search to tune max_depth parameter with values [3, 5, 7]", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 important features"], "model_results": {"accuracy": 0.85, "precision": 0.88, "recall": 0.83, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 1, "false_negative": 2}, "top_feature_importances": {"AttendanceRate": 0.35, "StudyHoursPerWeek": 0.3, "ParentalEducation_Master": 0.15}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a citizen's social assistance application will be approved based on demographic and application details.", "raw_table": "Age,Gender,Employment_Status,Annual_Income,Marital_Status,Application_Type,Previous_Approvals,Application_Approved\n34,Male,employed,45000,married,New,2,Yes\n27,FEMALE,unemployed,0,Single,Renewal,0,No\n45,Male,Self-Employed,55000,married,New,1,Yes\nNaN,Female,employed,48000,Divorced,New,3,Yes\n52,Male,unemployed,,Married,Renewal,0,No\n29,Female,Employed,40000,Single,New,1,Yes\n38,Male,Employed,NaN,Single,renewal,2,Yes\n41,Female,unemployed,22000,Married,New,0,No\n35,male,Self-employed,53000,Married,Renewal,2,Yes\n48,Female,employed,47000,divorced,New,1,No\n31,Male,unemployed,15000,Single,Renewal,1,No\n55,Female,SELF-EMPLOYED,62000,Married,New,4,Yes\n40,Male,Employed,51000,Married,Renewal,N/A,Yes\n33,Female,employed,46000,Single,New,1,Yes", "model_steps": ["Load dataset and inspect for missing and inconsistent values", "Clean Employment_Status to consistent casing and standard categories", "Impute missing Age and Annual_Income values using median imputation", "Convert Previous_Approvals column 'N/A' and missing values to 0", "Encode categorical variables: Gender, Employment_Status, Marital_Status, Application_Type using one-hot encoding", "Split data into training set (80%) and test set (20%) with stratification on Application_Approved", "Standardize numeric features: Age, Annual_Income, Previous_Approvals", "Train a RandomForestClassifier to predict Application_Approved", "Tune number of trees (n_estimators) and max_depth using grid search with 5-fold cross-validation", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.87, "recall": 0.82, "f1": 0.84, "confusion_matrix": {"true_positive": 7, "true_negative": 9, "false_positive": 2, "false_negative": 2}, "top_feature_importances": {"Employment_Status_Self-Employed": 0.22, "Annual_Income": 0.2, "Application_Type_New": 0.18, "Previous_Approvals": 0.15, "Age": 0.1}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 8}}}
{"purpose": "Predict whether a social media user will click on an ad based on their profile and activity features.", "raw_table": "user_id,age,gender,device_type,daily_time_spent,min_since_last_click,ad_category,clicked_ad\n1,25,MOBILE,mobile,120,15,Sports,Yes\n2,34,Desktop,Desktop,95,NaN,TECH,No\n3,29,Female,mobile,85,5,Health,yes\n4,,Male,MOBILE,60,20,Sports,No\n5,42,Female,tablet,130,10,tech,Yes\n6,31,male,Desktop,NaN,25,Health,No\n7,28,FEMALE,mobile,110,7,Sports,Yes\n8,37,,desktop,100,12,TECH,No\n9,23,Male,Tablet,90,NaN,Health,No\n10,30,Female,Mobile,105,8,Sports,Yes\n11,45,Male,desktop,115,18,Tech,No\n12,27,Female,mobile,80,6,sports,Yes\n13,38,Male,DESKTOP,125,11,Health,No\n14,26,Female,Tablet,85,NaN,Tech,Yes", "model_steps": ["Impute missing numeric values using median for age and daily_time_spent", "Impute missing categorical data (gender, ad_category, device_type) with the mode", "Normalize capitalization in categorical columns (gender, device_type, ad_category, clicked_ad) to consistent format", "Convert clicked_ad target variable to binary (Yes=1, No=0)", "One-hot encode categorical variables: gender, device_type, ad_category", "Split data into train and test sets (80% train, 20% test)", "Standardize numeric features: age, daily_time_spent, min_since_last_click", "Train a RandomForestClassifier with 100 trees on the training data", "Evaluate the model on the test set calculating accuracy, F1-score, precision, and recall", "Extract and rank feature importances from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.82, "precision": 0.8, "recall": 0.85, "feature_importances": {"daily_time_spent": 0.32, "min_since_last_click": 0.24, "ad_category_Sports": 0.15, "age": 0.1, "device_type_Mobile": 0.08, "gender_Female": 0.06, "ad_category_Tech": 0.05}}}
{"purpose": "Predict whether a retail customer will make a purchase during a promotional campaign.", "raw_table": "CustomerID,Age,Gender,AnnualIncome,MembershipTier,LastPurchaseDaysAgo,UsedCoupon,MadePurchase\n1,34,Male,58000,Gold,12,Yes,1\n2,28,Female,NaN,Silver,45,No,0\n3,45,Female,72000,Platinum,7,YES,1\n4,23,Male,32000,Bronze,90,No,0\n5,37,Female,54000,Gold,,Yes,1\n6,30,Male,49000,Bronze,60,No,0\n7,40,FemAle,61000,Silver,30,yes,1\n8,50,Female,80000,Platinum,3,No,1\n9,29,Male,40000,Bronze,75,No,0\n10,33,Male,55000,Gold,20,Yes,1\n11,31,Female,NaN,Silver,40,No,0\n12,42,Male,67000,Gold,10,No,1\n13,38,Female,60000,Silver,25,YES,1", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Impute missing AnnualIncome values using median based on MembershipTier", "Standardize inconsistent capitalization in Gender and UsedCoupon columns", "Convert categorical variables Gender, MembershipTier, and UsedCoupon into one-hot encoded features", "Fill missing LastPurchaseDaysAgo values with the median value", "Split data into train and test sets with 80% for training and 20% for testing", "Standardize numeric features Age, AnnualIncome, and LastPurchaseDaysAgo", "Train a RandomForestClassifier to predict MadePurchase", "Perform hyperparameter tuning on number of trees and max_depth using grid search", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix to understand prediction errors"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 8, "true_negative": 7, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"UsedCoupon_Yes": 0.32, "LastPurchaseDaysAgo": 0.25, "MembershipTier_Platinum": 0.15, "AnnualIncome": 0.12, "Age": 0.08}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 6}}}
{"purpose": "Predict whether a manufactured part will be defective based on sensor readings and machine settings.", "raw_table": "part_id,machine_id,temperature,pressure,operator_shift,material_batch,defect\n1,MachA,350,5.1,Day,Batch1,No\n2,MachB,370,5.3,Night,batch2,Yes\n3,machA,NaN,5.0,day,Batch1,No\n4,MACHC,360,missing,Morning,Batch3,No\n5,MachB,355,5.2,Night,Batch2,Yes\n6,MachA,365,5.4,Day,Batch1,No\n7,MachC,375,5.5,Morning,Batch3,yes\n8,MachB,NaN,5.3,Night,Batch2,No\n9,MachA,360,5.1,Day,Batch1,No\n10,MachC,380,5.6,morning,batch3,Yes\n11,MachB,370,5.4,Night,Batch2,No\n12,MachB,365,5.3,Night,Batch2,Yes\n13,MachA,355,5.2,Day,Batch1,no\n14,MachC,375,5.5,Morning,Batch3,Yes", "model_steps": ["Load dataset and inspect for missing and inconsistent values", "Standardize capitalization in categorical columns (machine_id, operator_shift, material_batch, defect)", "Impute missing numeric values for temperature with median value", "Impute missing numeric values for pressure with median value", "Encode categorical variables using one-hot encoding", "Convert target variable 'defect' to binary label (Yes=1, No=0)", "Split data into training and test sets (80/20 split)", "Standardize numeric features (temperature, pressure) using training set mean and std", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search over max_depth values [3, 5, 7] with 5-fold cross-validation", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Compute and display confusion matrix"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.79, "f1": 0.81, "confusion_matrix": [[7, 2], [2, 7]], "top_feature_importances": {"temperature": 0.32, "pressure": 0.25, "machine_id_MachB": 0.15, "operator_shift_Night": 0.12, "material_batch_Batch3": 0.1}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a customer will make a purchase during their visit to an ecommerce website.", "raw_table": "session_id,device_type,browser,pages_visited,time_spent_minutes,referral_source,ad_click,target_purchase\n1,Mobile,Chrome,5,12.5,Organic,Yes,1\n2,Desktop,firefox,3,5.0,Paid,No,0\n3,Mobile,Safari,NA,8.2,organic,No,0\n4,Tablet,Chrome,7,15.1,Paid,Yes,1\n5,Desktop,Edge,4,7.8,Referral,No,0\n6,Mobile,ChromE,6,10.3,PAID,Yes,1\n7,Desktop,Firefox,2,,Referral,No,0\n8,Tablet,Safari,8,20.5,Organic,yes,1\n9,Mobile,Chrome,3,6.7,Paid,No,0\n10,Desktop,EDGE,5,9.0,Referral,No,0\n11,Tablet,Firefox,7,14.0,Paid,Yes,1\n12,Mobile,Safari,4,11.3,organic,No,0\n13,Desktop,Chrome,NA,5.5,Referral,No,0\n14,Tablet,Edge,6,13.2,Paid,Yes,1", "model_steps": ["Load data and identify missing or inconsistent values", "Fill missing numeric values (pages_visited, time_spent_minutes) using median imputation", "Normalize inconsistent capitalization in categorical columns (device_type, browser, referral_source, ad_click)", "One-hot encode categorical variables: device_type, browser, referral_source, ad_click", "Split data into training set (80%) and test set (20%)", "Standardize numeric features: pages_visited, time_spent_minutes", "Train a Logistic Regression classifier to predict target_purchase", "Tune regularization parameter C using 5-fold cross-validation", "Evaluate model on test set computing accuracy, precision, recall, and F1 score", "Generate confusion matrix and analyze feature coefficients for interpretability"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.8, "f1": 0.81, "confusion_matrix": [[7, 1], [2, 4]], "top_feature_importances": {"ad_click_Yes": 1.25, "time_spent_minutes": 0.85, "referral_source_Paid": 0.65, "pages_visited": 0.6, "device_type_Mobile": 0.45}, "best_hyperparameters": {"C": 1.0}}}
{"purpose": "Predict hourly electricity consumption in megawatts to help optimize power grid management.", "raw_table": "Hour,DayOfWeek,TemperatureC,WeatherCondition,IsHoliday,PreviousHourLoadMW,Region,ElectricityLoadMW\n0,Monday,15,Clear,no,120.5,North,118.2\n1,Monday,14,clear,No,118.2,north,115.8\n2,Monday,13,,no,115.8,North,113.5\n3,Monday,12,Cloudy,no,113.5,North,110.1\n4,Monday,12,Cloudy,No,110.1,North,109.0\n5,Monday,11,Rain,no,109.0,North,107.3\n6,Monday,10,Rain,No,107.3,North,108.8\n7,Monday,11,Cloudy,No,108.8,North,115.2\n8,Monday,14,Clear,no,115.2,North,125.4\n9,Monday,18,Sunny,No,125.4,North,134.9\n10,Monday,20,Sunny,No,134.9,North,140.8\n11,Monday,22,Sunny,No,140.8,North,145.1\n12,Monday,24,Sunny,No,145.1,North,147.3\n13,Monday,25,Sunny,No,147.3,North,150.0", "model_steps": ["Load data from CSV string into a DataFrame", "Handle missing values in WeatherCondition by imputing the mode", "Standardize capitalization in WeatherCondition and IsHoliday columns", "Convert IsHoliday from yes/no to binary 1/0", "One-hot encode WeatherCondition and Region categorical variables", "Split data into train and test sets using an 80/20 ratio, stratified by DayOfWeek", "Standardize numeric features: Hour, TemperatureC, PreviousHourLoadMW", "Train a Gradient Boosting Regressor to predict ElectricityLoadMW", "Tune hyperparameters max_depth and learning_rate via grid search with 5-fold cross-validation", "Evaluate the model on the test set calculating RMSE, MAE, and R2", "Analyze feature importances from the trained model"], "model_results": {"rmse": 3.2, "mae": 2.5, "r2": 0.89, "best_hyperparameters": {"max_depth": 4, "learning_rate": 0.1}, "top_feature_importances": {"PreviousHourLoadMW": 0.52, "TemperatureC": 0.22, "Hour": 0.14, "WeatherCondition_Clear": 0.06, "IsHoliday": 0.03, "Region_North": 0.03}}}
{"purpose": "Predict whether a student will pass the final exam based on demographic and academic performance data.", "raw_table": "StudentID,Age,Gender,StudyHoursPerWeek,AttendanceRate,PreviousGrade,ExtraCurricular,PassFinal\n1,17,Male,15,0.9,B,Yes,Yes\n2,18,Female,8,0.85,C,No,No\n3,17,Female,12,0.95,A,Yes,Yes\n4,18,Male,5,0.7,D,No,No\n5,17,F,10,0.8,c,yes,Yes\n6,16,male,7,0.65,C,No,No\n7,19,Female,16,0.98,A,Yes,Yes\n8,18,Male,,0.75,B,No,No\n9,17,Female,14,missing,B,Yes,Yes\n10,16,Male,9,0.9,B,No,Yes\n11,18,Female,11,0.88,C,No,No\n12,17,Male,13,0.92,B,Yes,Yes\n13,16,Female,6,0.6,D,No,No\n14,19,Female,17,0.99,A,Yes,Yes", "model_steps": ["Load the CSV data into a dataframe", "Identify and correct messy values: standardize gender (e.g., 'F' to 'Female', 'male' to 'Male'), fix lowercase grades, and impute missing values in StudyHoursPerWeek and AttendanceRate columns with median values", "Encode categorical variables: One-hot encode Gender, PreviousGrade, and ExtraCurricular", "Split data into train and test sets with an 80/20 ratio, stratifying on PassFinal", "Standardize numeric features: Age, StudyHoursPerWeek, AttendanceRate", "Train a RandomForestClassifier on the training set", "Perform grid search over number of estimators [50,100] and max_depth [3,5,7]", "Evaluate the final model on the test set using accuracy, precision, recall, and F1 score", "Generate and display the confusion matrix", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.88, "recall": 0.83, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 1, "false_negative": 2}, "top_feature_importances": {"StudyHoursPerWeek": 0.35, "AttendanceRate": 0.3, "PreviousGrade_B": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Build a classification model to predict whether a field's crop yield will be high or low based on soil and weather attributes.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Temperature_C,Previous_Crop,Yield_Category\n1,Loamy,120,22,Maize,High\n2,Clay,85,19,Wheat,Low\n3,Sandy,NaN,23,Barley,High\n4,loamy,95,20,maize,High\n5,Clay,100,18,,Low\n6,Sandy,80,21,Rice,Low\n7,Loamy,110,22,Wheat,High\n8,SANDY,90,NaN,Barley,Low\n9,Clay,105,20,Wheat,High\n10,Loamy,115,22,Maize,High\n11,Clay,100,19,Rice,Low\n12,loamy,NaN,21,Maize,High\n13,Sandy,88,20,Barley,Low\n14,Clay,95,18,rice,Low", "model_steps": ["Load the dataset and inspect for missing values and inconsistent capitalization", "Standardize categorical values for Soil_Type and Previous_Crop columns to ensure consistency", "Impute missing numeric values (Rainfall_mm, Temperature_C) using median values per Soil_Type", "Impute missing Previous_Crop values with the mode of the dataset", "One-hot encode categorical variables Soil_Type and Previous_Crop", "Encode target variable Yield_Category into binary labels (High=1, Low=0)", "Split the dataset into training (80%) and testing sets (20%) with stratification on the target", "Standardize numeric features Rainfall_mm and Temperature_C", "Train a RandomForestClassifier with 100 trees on the training set", "Evaluate model performance on the test set by computing accuracy, precision, recall, and F1 score", "Extract and analyze feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "feature_importances": {"Rainfall_mm": 0.35, "Temperature_C": 0.25, "Soil_Type_Loamy": 0.15, "Soil_Type_Clay": 0.1, "Soil_Type_Sandy": 0.05, "Previous_Crop_Maize": 0.05, "Previous_Crop_Wheat": 0.03, "Previous_Crop_Rice": 0.02}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a field's crop yield will be above average based on soil and environmental factors.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Avg_Temperature_C,Crop_Variety,Fertilizer_Used_YN,Yield_Above_Avg\nF01,Loam,450,22.5,VarietyA,Yes,Yes\nF02,Clay,380,19.8,VarietyB,No,No\nF03,sand,510,23.2,VarietyA,yes,Yes\nF04,LoAm,,21.0,VarietyC,No,No\nF05,Clay,400,20.1,VarietyB,,No\nF06,Sand,470,24.5,VarietyA,Yes,Yes\nF07,Loam,430,22.0,VarietyC,Yes,Yes\nF08,Clay,390,19.5,VarietyB,no,No\nF09,loam,460,22.8,VarietyA,Yes,Yes\nF10,Sand,480,23.7,VarietyC,Yes,No\nF11,Clay,405,20.0,VarietyB,Yes,No\nF12,Loam,455,21.9,VarietyA,yes,Yes\nF13,Sand,500,24.0,VarietyC,No,No", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Standardize capitalization and spelling inconsistencies in categorical columns 'Soil_Type' and 'Fertilizer_Used_YN'", "Impute missing 'Rainfall_mm' value with the median rainfall", "Encode categorical variables 'Soil_Type', 'Crop_Variety', and 'Fertilizer_Used_YN' using one-hot encoding", "Split data into training (80%) and testing (20%) sets with stratification on the target variable 'Yield_Above_Avg'", "Scale numeric features 'Rainfall_mm' and 'Avg_Temperature_C' using standard scaling", "Train a RandomForestClassifier with 100 trees on the training data", "Perform hyperparameter tuning over max_depth with values [3, 5, 7] using 3-fold cross-validation", "Evaluate the final model on the test set calculating accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix to assess classification errors", "Identify and report top 3 feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"Rainfall_mm": 0.32, "Fertilizer_Used_YN_Yes": 0.27, "Soil_Type_Loam": 0.18}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict whether a wheat field is at high risk of pest infestation.", "raw_table": "FieldID,SoilType,Rainfall_mm,AvgTemp_C,PreviousCrop,PesticideUsed,InfestationRisk\n1,Loam,120,22.5,Wheat,Yes,High\n2,Clay,85,19.8,Corn,no,Low\n3,Sand,100,21.0,Wheat,Yes,Medium\n4,Loam,Missing,23.1,Barley,Yes,High\n5,Clay,90,20.2,Wheat,No,Low\n6,Loam,110,22.0,Corn,yes,High\n7,Sand,95,21.5,Barley,No,Medium\n8,Clay,100,19.9,Corn,No,Low\n9,Loam,105,22.3,Wheat,Yes,High\n10,Sand,88,20.8,Corn,No,Low\n11,CLay,92,21.1,Wheat,Yes,Medium\n12,Loam,115,22.7,Wheat,Yes,High\n13,Sand,97,21.3,Barley,No,Medium\n14,Loam,103,22.4,Corn,Yes,High\n15,Clay,Missing,20.0,Wheat,No,Low", "model_steps": ["Load CSV data and identify target column 'InfestationRisk'", "Handle missing values in 'Rainfall_mm' by imputing median rainfall", "Standardize capitalization inconsistencies in 'SoilType' and 'PesticideUsed' columns", "Encode categorical variables 'SoilType', 'PreviousCrop', and 'PesticideUsed' using one-hot encoding", "Split data into training (80%) and test (20%) sets with stratification on target", "Standardize numeric features 'Rainfall_mm' and 'AvgTemp_C' using z-score normalization", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search on max_depth parameter over values [5, 10, 15]", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.87, "f1": 0.85, "precision": 0.83, "recall": 0.88, "confusion_matrix": {"High": {"High": 5, "Medium": 1, "Low": 0}, "Medium": {"High": 1, "Medium": 4, "Low": 0}, "Low": {"High": 0, "Medium": 1, "Low": 6}}, "top_feature_importances": {"PesticideUsed_Yes": 0.29, "SoilType_Loam": 0.22, "Rainfall_mm": 0.18, "PreviousCrop_Wheat": 0.15, "AvgTemp_C": 0.1}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a citizen will default on a government loan based on application and demographic data.", "raw_table": "ApplicantID,Age,Income,EmploymentStatus,LoanAmount,CreditScore,MaritalStatus,Defaulted\n1,34,55000,Employed,15000,680,Married,No\n2,28,NaN,Self-employed,10000,720,Single,No\n3,45,72000,Employed,20000,NA,divorced,Yes\n4,52,48000,Unemployed,12000,650,Married,No\n5,23,40000,employed,8000,700,Single,No\n6,38,61000,Self-Employed,17000,690,Married,Yes\n7,41,59000,Employed,,640,Single,No\n8,30,47500,Unemployed,11000,670,Married,No\n9,,53000,Employed,13000,660,Single,Yes\n10,50,81000,Employed,25000,710,married,No\n11,29,47000,Self-employed,9000,680,Single,No\n12,47,68000,Employed,22000,NA,Married,Yes\n13,35,56000,Employed,16000,700,Single,No\n14,33,45000,unemployed,14000,690,Married,No", "model_steps": ["Load dataset and inspect for missing and inconsistent values", "Standardize capitalization in categorical columns (EmploymentStatus, MaritalStatus)", "Impute missing numeric values (Age, Income, LoanAmount, CreditScore) using median imputation", "Encode categorical variables EmploymentStatus and MaritalStatus using one-hot encoding", "Split data into train and test sets with an 80/20 ratio ensuring class balance", "Standardize numeric features Age, Income, LoanAmount, and CreditScore using z-score scaling", "Train a RandomForestClassifier to predict Defaulted status", "Perform hyperparameter tuning on number of estimators and max_depth using grid search with 5-fold cross-validation", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Analyze feature importances from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.8, "recall": 0.75, "f1": 0.77, "feature_importances": {"CreditScore": 0.32, "LoanAmount": 0.21, "Income": 0.18, "EmploymentStatus_Employed": 0.1, "Age": 0.08, "MaritalStatus_Married": 0.06, "EmploymentStatus_Self-employed": 0.05}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Predict the likelihood of a manufactured part failing quality inspection based on sensor readings and production conditions.", "raw_table": "Part_ID,Machine_Type,Operator_Shift,Temperature_C,Pressure_bar,Humidity_pct,Defect\n001,A,Day,75.2,1.2,45,No\n002,b,Night,80.1,1.1,47,Yes\n003,A,day,78.5,1.3,missing,No\n004,C,Day,77.0,NaN,44,No\n005,B,Night,79.5,1.0,49,Yes\n006,a,Night,missing,1.2,46,No\n007,C,Day,76.2,1.3,45,No\n008,B,Day,81.0,1.1,48,Yes\n009,C,Night,80.5,1.2,47,Yes\n010,A,Day,75.0,1.0,44,No\n011,B,Day,missing,1.1,missing,Yes\n012,C,Night,77.8,1.3,46,No\n013,A,Night,79.0,1.2,45,Yes\n014,B,Day,78.1,1.0,43,No", "model_steps": ["Load the CSV data and identify missing values and inconsistent capitalization in categorical columns.", "Normalize capitalization in 'Machine_Type' and 'Operator_Shift' columns to ensure consistency.", "Impute missing numeric values (Temperature_C, Pressure_bar, Humidity_pct) using median values grouped by Machine_Type.", "Encode categorical variables 'Machine_Type' and 'Operator_Shift' using one-hot encoding.", "Convert target variable 'Defect' to binary format (Yes=1, No=0).", "Split the dataset into training (80%) and testing (20%) sets with stratification on the target.", "Standardize numeric features: Temperature_C, Pressure_bar, Humidity_pct.", "Train a RandomForestClassifier with 100 trees on the training data.", "Perform grid search over max_depth values [5, 10, 15] and select the best model based on F1 score using 5-fold cross-validation.", "Evaluate the final model on the test set and compute accuracy, precision, recall, and F1 score.", "Generate a confusion matrix and extract feature importances from the model."], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.82, "recall": 0.87, "confusion_matrix": {"true_positive": 8, "true_negative": 13, "false_positive": 2, "false_negative": 1}, "feature_importances": {"Temperature_C": 0.29, "Pressure_bar": 0.21, "Humidity_pct": 0.15, "Machine_Type_A": 0.12, "Machine_Type_B": 0.11, "Machine_Type_C": 0.08, "Operator_Shift_Day": 0.04, "Operator_Shift_Night": 0.0}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a government grant application will be approved based on applicant and project details.", "raw_table": "ApplicationID,ApplicantAge,ApplicantExperienceYears,ProjectType,RequestedAmountUSD,SubmissionMonth,PreviousGrants,Approved\n1,45,10,Infrastructure,500000,March,2,Yes\n2,33,5,Health,missing,July,0,No\n3,29,3,education,200000,November,1,No\n4,52,20,Infrastructure,750000,May,3,Yes\n5,40,,Health,300000,August,1,No\n6,37,7,Infrastructure,450000,June,missing,Yes\n7,50,15,Education,600000,April,2,Yes\n8,28,2,health,150000,December,0,No\n9,36,8,Infrastructure,NaN,January,1,Yes\n10,42,12,Education,550000,September,2,Yes\n11,31,4,Health,250000,October,0,No\n12,47,14,Infrastructure,700000,March,3,Yes\n13,35,6,Education,480000,missing,1,Yes\n14,39,9,Health,350000,July,1,No", "model_steps": ["Load the CSV dataset into a DataFrame", "Identify and handle missing values: replace 'missing' and 'NaN' with appropriate imputation (median for numeric, mode for categorical)", "Standardize capitalization in categorical columns to ensure consistency (e.g., 'education' to 'Education', 'health' to 'Health')", "Convert categorical variables (ProjectType, SubmissionMonth) into one-hot encoded features", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features such as ApplicantAge, ApplicantExperienceYears, RequestedAmountUSD, and PreviousGrants", "Train a RandomForestClassifier to predict 'Approved' status", "Perform grid search to optimize max_depth parameter of RandomForest between values 3, 5, and 7", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"RequestedAmountUSD": 0.32, "ApplicantExperienceYears": 0.25, "ProjectType_Infrastructure": 0.18}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a loan application will be approved based on applicant financial and demographic data.", "raw_table": "Applicant_ID,Age,Income,Employment_Status,Credit_Score,Loan_Amount,Loan_Purpose,Approved\n1,35,58000,Full-time,720,15000,Home Improvement,Yes\n2,42,NaN,part-time,680,9000,Debt Consolidation,No\n3,29,45000,Full-time,690,,Car,Yes\n4,NaN,52000,Contractor,710,12000,Education,No\n5,50,83000,Full-Time,780,20000,Home Improvement,Yes\n6,38,49000,full-time,NaN,14000,Car,Yes\n7,27,35000,Unemployed,640,8000,Debt consolidation,No\n8,45,72000,Contractor,700,18000,Home Improvement,Yes\n9,34,60000,Part-Time,680,10000,Education,No\n10,31,40000,Full-time,675,11000,Car,Yes\n11,39,65000,Full-time,705,13000,Debt Consolidation,No\n12,48,72000,Full-time,725,15000,Home improvement,Yes\n13,44,NaN,Contractor,690,14000,Education,No\n14,36,56000,Full-time,NaN,9000,Car,Yes", "model_steps": ["Impute missing numeric values using median for Income, Age, Credit_Score, and Loan_Amount", "Standardize numeric features: Age, Income, Credit_Score, Loan_Amount", "Normalize inconsistent capitalization in Employment_Status and Loan_Purpose columns", "One-hot encode categorical variables: Employment_Status and Loan_Purpose", "Split data into training and testing sets with 80/20 ratio", "Train a RandomForestClassifier with default hyperparameters to predict Approved", "Perform grid search over max_depth with values [3, 5, 7] and n_estimators [50, 100]", "Select best model based on validation F1 score", "Evaluate final model on test set calculating accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze model performance"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.81, "recall": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 5, "false_positive": 2, "false_negative": 1}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}, "top_feature_importances": {"Credit_Score": 0.35, "Income": 0.25, "Loan_Amount": 0.15, "Employment_Status_Contractor": 0.1, "Loan_Purpose_Home Improvement": 0.08}}}
{"purpose": "Build a classification model to predict whether a manufactured product will pass quality control based on sensor readings and production parameters.", "raw_table": "BatchID,MachineID,OperatorShift,Temperature,Pressure,MaterialGrade,DefectDetected\n1001,MachA,Morning,215.4,30.2,A,No\n1002,MachA,Night,217.1,29.8,b,Yes\n1003,MachB,Afternoon,NaN,30.0,A,No\n1004,machB,Morning,213.5,31.1,A,No\n1005,MachC,Afternoon,218.0,abc,B,Yes\n1006,MachA,Night,216.3,29.7,B,No\n1007,MachC,MORNING,214.8,30.5,B,Yes\n1008,MachB,Night,215.9,30.3,a,No\n1009,MachC,Afternoon,217.4,30.0,B,Yes\n1010,MachA,Morning,215.2,29.9,A,No\n1011,MachB,Night,214.3,30.1,B,Yes\n1012,MachC,Morning,NaN,30.2,B,Yes\n1013,MachA,Afternoon,216.0,29.6,A,No\n1014,MachB,Morning,215.5,30.4,a,No", "model_steps": ["Load the dataset and inspect for missing and inconsistent values.", "Impute missing numeric values in Temperature with the median temperature.", "Convert Pressure column to numeric, coerce errors to NaN, then impute with median pressure.", "Standardize the capitalization of categorical variables: MachineID, OperatorShift, MaterialGrade.", "One-hot encode categorical variables: MachineID, OperatorShift, MaterialGrade.", "Split data into training and test sets using stratified sampling on the target (DefectDetected) with 80/20 split.", "Standardize numeric features Temperature and Pressure using training set statistics.", "Train a RandomForestClassifier with 100 trees on the training set.", "Perform grid search over max_depth values [5, 10, None] using 5-fold cross-validation optimizing F1 score.", "Evaluate the best model on the test set and compute accuracy, precision, recall, and F1 score.", "Generate and analyze the confusion matrix and identify top 3 feature importances."], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.81, "recall": 0.87, "confusion_matrix": {"true_negative": 12, "false_positive": 2, "false_negative": 3, "true_positive": 15}, "top_feature_importances": {"Pressure": 0.32, "MaterialGrade_B": 0.25, "Temperature": 0.18}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict the likelihood of extreme heat events based on weather and geographic features.", "raw_table": "Region,AvgTempC,HumidityPercent,SoilType,ElevationM,Season,ExtremeHeatEvent\nNorth,32.5,45,Sandy,1200,Summer,Yes\nsouth,29.0,55,Clay,850,Summer,No\nEast,34.0,,Loam,600,summer,Yes\nWest,28.5,60,Clay,NA,Spring,No\nNorth,35.2,48,SANDY,1300,Summer,Yes\nSouth,27.8,50,Loam,900,Spring,No\nEast,33.5,53,Sandy,620,Summer,Yes\nWest,29.1,58,,700,Summer,No\nNorth,,47,Loam,1250,Autumn,Yes\nSouth,30.0,52,Clay,880,Summer,No\nEast,34.8,49,Loam,610,Summer,Yes\nWest,28.9,59,Sandy,720,Summer,No\nnorth,31.7,46,Sandy,1220,SUMMER,Yes", "model_steps": ["Load the CSV dataset into a DataFrame", "Clean data by standardizing categorical values in 'Region', 'SoilType', and 'Season' columns to consistent capitalization", "Impute missing numeric values in 'AvgTempC', 'HumidityPercent', and 'ElevationM' using median imputation", "Impute missing categorical values in 'SoilType' using the most frequent category", "Encode categorical variables 'Region', 'SoilType', and 'Season' using one-hot encoding", "Convert target variable 'ExtremeHeatEvent' to binary labels (Yes=1, No=0)", "Split data into training and test sets (80/20 split) with stratification on the target", "Standardize numeric features ('AvgTempC', 'HumidityPercent', 'ElevationM') using StandardScaler fitted on training data", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search over max_depth values [5, 10, 15] using 5-fold cross-validation to select best hyperparameters", "Evaluate final model on the test set calculating accuracy, F1 score, precision, and recall", "Generate confusion matrix and extract the top 3 feature importances from the model"], "model_results": {"accuracy": 0.92, "f1": 0.91, "precision": 0.9, "recall": 0.92, "confusion_matrix": {"true_positive": 10, "true_negative": 11, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"AvgTempC": 0.35, "Season_Summer": 0.22, "ElevationM": 0.18}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict the risk of hospital readmission within 30 days for diabetic patients.", "raw_table": "Patient_ID,Age,Gender,Admission_Type,Blood_Glucose_mg_dl,HbA1c_Percentage,Previous_Readmissions,Discharge_Disposition,Readmitted\n001,55,Male,Emergency,180,8.2,2,Home,Yes\n002,62,Female,Elective,110,6.5,0,home,No\n003,47,Male,Emergency,NaN,7.1,1,Skilled Nursing Facility,Yes\n004,70,Female,Emergency,200,9.0,3,Home,Yes\n005,34,Female,Elective,95,5.8,,Home,No\n006,58,Male,Emergency,150,7.5,1,HOSPITAL,Yes\n007,40,Female,Elective,85,5.6,0,Home,No\n008,63,,Emergency,165,8.0,2,Home,Yes\n009,50,Male,Elective,120,7.0,1,Home,No\n010,46,Female,emergency,NaN,7.3,1,home,Yes\n011,59,Male,Emergency,160,8.1,2,Home,No\n012,72,Female,Elective,140,6.9,1,home,No\n013,65,Male,Emergency,170,7.8,2,Home,Yes", "model_steps": ["Load CSV data and inspect for missing and inconsistent values", "Correct inconsistent capitalization in categorical fields (e.g., 'home', 'Home', 'HOSPITAL', 'emergency')", "Impute missing numeric values (Blood_Glucose_mg_dl) using median per Admission_Type", "Impute missing categorical values (Gender) using mode", "Encode categorical variables: Gender, Admission_Type, Discharge_Disposition using one-hot encoding", "Standardize numeric features: Age, Blood_Glucose_mg_dl, HbA1c_Percentage, Previous_Readmissions", "Split data into training (80%) and testing (20%) sets stratified by target variable Readmitted", "Train a RandomForestClassifier with 100 trees to predict Readmitted", "Perform grid search over max_depth parameter with values [3, 5, 7]", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Compute confusion matrix and analyze false positives and false negatives", "Generate and store test set predictions"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 8, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Previous_Readmissions": 0.32, "HbA1c_Percentage": 0.25, "Blood_Glucose_mg_dl": 0.18, "Discharge_Disposition_Home": 0.1, "Admission_Type_Emergency": 0.07}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a manufactured part will fail quality inspection based on sensor readings and production parameters.", "raw_table": "Part_ID,Machine_ID,Operator_Shift,Temperature_C,Pressure_kPa,Material_Type,Inspection_Result\n001,M01,Morning,75.2,101.3,Steel,Pass\n002,m02,afternoon,78.6,NA,Aluminum,Fail\n003,M01,Night,74.1,99.8,steel,Pass\n004,M03,Morning,80.0,102.5,Aluminum,Fail\n005,M02,Afternoon,77.5,100.7,Steel,Pass\n006,m01,Night,missing,98.9,Aluminum,Fail\n007,M03,Morning,79.3,103.1,Steel,Pass\n008,M02,Afternoon,76.8,100.5,Aluminum,Fail\n009,M01,Night,75.9,101.0,Steel,Pass\n010,M03,Morning,78.0,NA,steel,Fail\n011,M02,Afternoon,missing,99.9,Aluminum,Pass\n012,m01,Night,74.5,100.2,Steel,Pass", "model_steps": ["Load data and identify missing values in Temperature_C and Pressure_kPa columns", "Impute missing numeric values using median imputation", "Standardize numeric features: Temperature_C and Pressure_kPa", "Normalize categorical variables: unify casing in Machine_ID, Operator_Shift, and Material_Type", "One-hot encode categorical variables: Machine_ID, Operator_Shift, Material_Type", "Split data into train and test sets (80% train, 20% test) with stratification on Inspection_Result", "Train a RandomForestClassifier to predict Inspection_Result", "Perform grid search on max_depth and n_estimators hyperparameters with 5-fold cross-validation", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score", "Analyze feature importances from the trained model"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.79, "recall": 0.83, "top_feature_importances": {"Temperature_C": 0.34, "Material_Type_Steel": 0.22, "Pressure_kPa": 0.18, "Operator_Shift_Morning": 0.13, "Machine_ID_M02": 0.08, "Material_Type_Aluminum": 0.05}, "best_hyperparameters": {"max_depth": 6, "n_estimators": 100}}}
{"purpose": "Predict whether a movie will be a box office hit based on its production attributes and genre.", "raw_table": "MovieID,Genre,DirectorExperience,ProductionBudgetMillions,LeadActorPopularity,ReleaseMonth,RuntimeMinutes,BoxOfficeHit\n1,Action,5,150,8,July,130,Yes\n2,Comedy,2,30,6,December,95,No\n3,Drama,10,50,7,March,110,No\n4,action,7,200,9,July,140,Yes\n5,Comedy,missing,40,5,August,100,No\n6,Horror,3,15,4,October,90,No\n7,Drama,6,60,7,may,115,No\n8,Action,8,170,9,July,135,Yes\n9,Comedy,4,35,6,December,105,No\n10,Drama,9,55,8,March,120,Yes\n11,Horror,1,10,3,october,85,No\n12,Action,5,180,8,july,128,Yes\n13,Comedy,3,45,5,August,98,No\n14,Drama,missing,65,7,March,118,No", "model_steps": ["Load the dataset and identify the target variable 'BoxOfficeHit' as a binary classification task", "Clean and standardize categorical values, e.g., normalize 'Genre' and 'ReleaseMonth' capitalization", "Impute missing values in 'DirectorExperience' with median experience", "Encode categorical features 'Genre' and 'ReleaseMonth' using one-hot encoding", "Convert target 'BoxOfficeHit' to binary format (Yes=1, No=0)", "Split the data into train (80%) and test (20%) sets", "Standardize numeric features: 'DirectorExperience', 'ProductionBudgetMillions', 'LeadActorPopularity', 'RuntimeMinutes'", "Train a RandomForestClassifier with default parameters on the training set", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Extract feature importances from the trained Random Forest model", "Generate and display the confusion matrix"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.82, "f1": 0.85, "confusion_matrix": [[7, 1], [2, 8]], "top_feature_importances": {"ProductionBudgetMillions": 0.35, "LeadActorPopularity": 0.25, "Genre_Action": 0.15, "DirectorExperience": 0.1, "ReleaseMonth_July": 0.08, "RuntimeMinutes": 0.07}}}
{"purpose": "Predict whether a social media post will go viral based on post characteristics and user engagement metrics.", "raw_table": "post_id,user_type,post_length,num_images,post_time,topic,previous_engagement,viral\n1,Influencer,120,2,18,Technology,350,Yes\n2,regular,45,0,9,Lifestyle,23,no\n3,Regular,78,1,21,Technology,87,Yes\n4,INFLUENCER,200,3,14,Health,470,Yes\n5,regular,NaN,0,11,Sports,15,no\n6,Influencer,150,2,20,Lifestyle,400,Yes\n7,regular,65,1,7,health,20,No\n8,Regular,97,NaN,19,Technology,120,Yes\n9,influencer,180,2,22,Sports,520,Yes\n10,regular,50,0,6,Lifestyle,18,no\n11,regular,55,0,12,Health,25,No\n12,Influencer,130,1,16,Technology,300,Yes\n13,regular,40,0,10,Sports,10,No\n14,INFLUENCER,175,3,17,Technology,450,Yes", "model_steps": ["Correct inconsistent capitalization in 'user_type' and 'topic' columns", "Impute missing values in 'post_length' and 'num_images' with median values", "Encode 'user_type' and 'topic' categorical columns using one-hot encoding", "Convert target variable 'viral' to binary labels (Yes=1, No=0)", "Split data into training (80%) and testing (20%) sets", "Standardize numeric features: 'post_length', 'num_images', 'post_time', 'previous_engagement'", "Train a RandomForestClassifier with default parameters", "Perform grid search over number of estimators [50, 100, 150]", "Evaluate model using accuracy, F1 score, precision, and recall on test set", "Compute confusion matrix on test predictions"], "model_results": {"accuracy": 0.86, "f1": 0.88, "precision": 0.85, "recall": 0.91, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"previous_engagement": 0.32, "user_type_Influencer": 0.25, "post_length": 0.15, "num_images": 0.12, "topic_Technology": 0.1, "post_time": 0.06}, "best_hyperparameters": {"n_estimators": 100}}}
{"purpose": "Predict hourly energy consumption category (Low, Medium, High) based on weather and operational factors.", "raw_table": "Hour,TemperatureC,HumidityPercent,WindSpeed_mps,DayOfWeek,Holiday,EnergyConsumptionCategory\n0,15.2,68,3.5,Monday,no,Medium\n1,14.8,70,,Monday,No,low\n2,14.5,65,4.0,Monday,No,Low\n3,14.0,64,3.8,Monday,No,Medium\n4,13.8,missing,3.2,Tuesday,No,Medium\n5,13.5,60,3.0,Tuesday,NO,Medium\n6,14.5,62,3.5,Tuesday,No,High\n7,16.0,59,4.1,Wednesday,No,High\n8,18.2,55,4.8,Wednesday,No,High\n9,20.3,50,5.0,Wednesday,No,High\n10,22.1,48,5.5,Wednesday,No,High\n11,23.5,45,5.8,Thursday,no,High\n12,24.0,43,6.0,Thursday,No,High\n13,23.8,44,5.9,Thursday,No,Medium\n14,22.5,46,5.3,Friday,No,Medium", "model_steps": ["Load dataset and identify target variable EnergyConsumptionCategory", "Handle missing values in HumidityPercent and WindSpeed_mps by imputing median values", "Standardize capitalization and unify Holiday column values to lowercase 'yes' or 'no'", "Encode categorical variables DayOfWeek and Holiday using one-hot encoding", "Split data into training (80%) and testing (20%) sets with stratification on target", "Scale numeric features TemperatureC, HumidityPercent, WindSpeed_mps using StandardScaler", "Train a RandomForestClassifier with 100 trees on training data", "Evaluate model performance using accuracy, F1-score, precision, and recall on test set", "Generate confusion matrix to analyze classification errors", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.87, "f1": 0.85, "precision": 0.86, "recall": 0.84, "confusion_matrix": {"Low": {"Low": 3, "Medium": 0, "High": 0}, "Medium": {"Low": 0, "Medium": 4, "High": 1}, "High": {"Low": 0, "Medium": 1, "High": 5}}, "top_feature_importances": {"TemperatureC": 0.34, "Hour": 0.28, "HumidityPercent": 0.15}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict the risk of hospital readmission within 30 days for diabetic patients.", "raw_table": "PatientID,Age,Gender,HbA1c_Level,Blood_Pressure,Smoker,Previous_Readmission,Medication_Compliance,Readmitted\n1,65,Male,8.2,140/90,Yes,Yes,High,Yes\n2,53,FEMALE,7.1,130/85,No,No,Medium,No\n3,47,Male,NaN,125/80,No,No,Low,No\n4,72,Female,9.0,150/95,YES,Yes,High,Yes\n5,59,male,6.5,135/88,No,No,Medium,No\n6,61,Female,7.8,142/92,No,Yes,,Yes\n7,55,Female,8.0,138/91,Yes,No,High,Yes\n8,68,Male,7.3,NaN,No,No,Medium,No\n9,50,Female,6.9,128/82,No,No,Low,No\n10,58,Male,8.5,145/94,YES,Yes,High,Yes\n11,62,Female,7.2,132/85,No,No,Medium,No\n12,49,Male,7.6,135/87,No,No,Low,No\n13,70,Female,8.8,148/90,Yes,Yes,High,Yes\n14,54,Female,6.8,130/80,No,No,Medium,No", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Standardize capitalization for the 'Gender' and 'Smoker' columns", "Parse 'Blood_Pressure' into two numeric columns: Systolic and Diastolic", "Impute missing numeric values (HbA1c_Level, Blood_Pressure) using median values", "Impute missing categorical values (Medication_Compliance) with the mode", "Encode categorical features: Gender, Smoker, Previous_Readmission, and Medication_Compliance using one-hot encoding", "Convert target variable 'Readmitted' to binary (Yes=1, No=0)", "Split data into training (80%) and test (20%) sets", "Standardize numeric features: Age, HbA1c_Level, Systolic, Diastolic", "Train a RandomForestClassifier with default hyperparameters", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Calculate feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.87, "f1": 0.85, "top_feature_importances": {"HbA1c_Level": 0.29, "Previous_Readmission_Yes": 0.25, "Medication_Compliance_High": 0.15, "Smoker_Yes": 0.12, "Age": 0.1, "Blood_Pressure_Systolic": 0.09}}}
{"purpose": "Predict whether a credit card transaction is fraudulent based on transaction details and user information.", "raw_table": "TransactionID,UserAge,TransactionAmount,MerchantCategory,TransactionTime,IsFraud\n1,34,120.50,Electronics,14:05,No\n2,28,250.00,Groceries,09:20,No\n3,45,89.99,Electronics,21:15,No\n4,NaN,300.00,Travel,08:30,Yes\n5,52,15.75,groceries,23:40,No\n6,23,500.00,Travel,02:05,Yes\n7,37,NaN,Clothing,19:00,No\n8,29,75.00,Clothing,18:45,No\n9,41,1000.00,Electronics,11:10,Yes\n10,30,40,CLOTHING,07:55,No\n11,55,1200.00,Travel,16:25,Yes\n12,33,NaN,groceries,12:00,No\n13,38,110,Electronics,20:00,No\n14,NaN,60.50,Clothing,09:00,No", "model_steps": ["Handle missing numeric values in UserAge and TransactionAmount using median imputation", "Normalize TransactionTime by converting to minutes since midnight and scaling between 0 and 1", "Standardize numeric features UserAge and TransactionAmount", "Normalize inconsistent capitalization in MerchantCategory and apply one-hot encoding", "Split data into training (80%) and testing (20%) sets, stratified by IsFraud", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search over max_depth parameter with values [5, 10, 15]", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze false positives and false negatives"], "model_results": {"accuracy": 0.89, "precision": 0.85, "recall": 0.78, "f1": 0.81, "confusion_matrix": {"true_positive": 7, "true_negative": 30, "false_positive": 3, "false_negative": 2}, "top_feature_importances": {"TransactionAmount": 0.35, "MerchantCategory_Electronics": 0.25, "UserAge": 0.15, "MerchantCategory_Travel": 0.1, "TransactionTime": 0.08}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a customer will default on their credit card payment next month based on their financial and demographic information.", "raw_table": "CustomerID,Age,Gender,Education,MaritalStatus,MonthlyIncome,CreditLimit,PaymentHistory,DefaultNextMonth\n1,34,Male,Bachelor,Single,4500,12000,Good,No\n2,58,Female,Master,Married,8500,25000,Poor,Yes\n3,45,Female,PhD,Married,,30000,Good,No\n4,22,Male,High School,Single,3200,8000,average,No\n5,39,Male,Bachelor,Divorced,5400,15000,Poor,Yes\n6,29,Female,Bachelor,Single,4700,11000,Good,No\n7,50,Male,Master,Married,7000,20000,Poor,YES\n8,41,,Bachelor,Married,6000,18000,Average,No\n9,36,Female,High School,Single,4000,13000,good,No\n10,27,Male,Master,Single,4200,9000,poor,Yes\n11,33,Female,Bachelor,Married,4800,14000,Average,No\n12,60,Male,PhD,Married,9000,28000,Good,No\n13,44,Female,Bachelor,Divorced,5000,16000,Good,No\n14,38,Male,Master,Single,5300,17000,Poor,Yes", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing or inconsistent values", "Standardize capitalization and spelling inconsistencies in categorical columns such as PaymentHistory and DefaultNextMonth", "Impute missing values in MonthlyIncome with median and Gender with mode", "Encode categorical variables (Gender, Education, MaritalStatus, PaymentHistory) using one-hot encoding", "Convert target variable DefaultNextMonth to binary (Yes=1, No=0)", "Split the dataset into training (80%) and test (20%) sets with stratification on the target variable", "Standardize numeric features (Age, MonthlyIncome, CreditLimit) using z-score normalization", "Train a RandomForestClassifier with 100 trees on the training data", "Perform a grid search over max_depth with values [5, 10, 15] to optimize model performance using 5-fold cross-validation", "Evaluate the final model on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix to understand prediction errors", "Extract and report top 5 feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.86, "precision": 0.8, "recall": 0.75, "f1": 0.77, "confusion_matrix": {"true_positive": 9, "true_negative": 18, "false_positive": 3, "false_negative": 3}, "top_feature_importances": {"PaymentHistory_Good": 0.28, "MonthlyIncome": 0.23, "CreditLimit": 0.19, "PaymentHistory_Poor": 0.15, "Age": 0.1}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Build a regression model to predict hourly electricity consumption for residential buildings.", "raw_table": "Hour,Temperature_C,Humidity_pct,Day_of_Week,Weather_Condition,Is_Holiday,Building_Type,Electricity_Consumption_kWh\n0,22.5,45,Monday,Clear,No,Apartment,15.2\n1,21.8,47,Monday,clear,No,apartment,14.8\n2,20.1,50,Monday,Cloudy,No,Apartment,13.5\n3,19.5,55,tuesday,Rain,No,House,12.1\n4,19.0,,Tuesday,Rain,No,House,11.8\n5,18.7,60,Tuesday,Rain,No,House,N/A\n6,19.2,58,Wednesday,Cloudy,No,Apartment,16.0\n7,20.3,53,Wednesday,Clear,No,Apartment,18.3\n8,23.4,48,Wednesday,Clear,No,Apartment,21.0\n9,25.0,44,Thursday,Clear,No,House,25.4\n10,26.1,41,Thursday,Clear,No,House,27.3\n11,27.0,40,Thursday,Sunny,No,House,29.1\n12,28.4,38,Thursday,Sunny,Yes,House,30.0\n13,29.0,35,Friday,Sunny,No,Apartment,28.5\n14,28.0,36,Friday,sunny,No,Apartment,26.7\n", "model_steps": ["Load the dataset and handle missing values in Humidity_pct and Electricity_Consumption_kWh columns by median imputation", "Normalize the capitalization of categorical variables Weather_Condition and Day_of_Week for consistency", "Convert categorical features Day_of_Week, Weather_Condition, Is_Holiday, and Building_Type into one-hot encoded vectors", "Split data into training (80%) and test (20%) sets preserving time order to prevent data leakage", "Standardize numeric features Hour, Temperature_C, and Humidity_pct to zero mean and unit variance", "Train a Gradient Boosting Regressor model on the training data to predict Electricity_Consumption_kWh", "Perform hyperparameter tuning on number of estimators and learning rate using 3-fold cross-validation", "Evaluate the model on the test set using RMSE, MAE, and R-squared metrics", "Analyze feature importances from the trained model to identify key drivers of electricity consumption"], "model_results": {"rmse": 1.45, "mae": 1.1, "r2": 0.87, "top_feature_importances": {"Temperature_C": 0.42, "Hour": 0.3, "Building_Type_House": 0.12, "Weather_Condition_Clear": 0.08, "Is_Holiday_Yes": 0.05}, "hyperparameters": {"n_estimators": 150, "learning_rate": 0.05}}}
{"purpose": "Predict the likelihood of patient readmission within 30 days after hospital discharge.", "raw_table": "PatientID,Age,Gender,Diagnosis,LengthOfStay,PreviousAdmissions,DischargeDisposition,Readmitted\n1,65,Male,Diabetes,5,2,Home,Yes\n2,72,Female,Hypertension,3,1,home,No\n3,58,Male,Heart Failure,10,,Rehab,Yes\n4,45,Female,diabetes,4,0,Home,No\n5,83,Male,Stroke,7,3,Skilled Nursing,Yes\n6,70,Female,Hypertension,3,1,,No\n7,60,Male,Heart failure,8,2,Rehab,Yes\n8,50,Female,Stroke,6,1,Skilled nursing,No\n9,67,Male,Diabetes,5,2,Home,Yes\n10,55,Female,Hypertension,4,1,Home,No\n11,63,male,Heart Failure,9,2,Rehab,Yes\n12,48,Female,Stroke,5,0,Skilled Nursing,No\n13,74,Male,Diabetes,6,3,Home,Yes\n14,59,Female,Hypertension,3,1,Home,No", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Standardize capitalization and correct inconsistent entries in 'Diagnosis' and 'DischargeDisposition' columns", "Impute missing values in 'PreviousAdmissions' with median", "Encode categorical variables 'Gender', 'Diagnosis', and 'DischargeDisposition' using one-hot encoding", "Split data into training (80%) and test (20%) sets", "Standardize numeric features 'Age' and 'LengthOfStay'", "Train a RandomForestClassifier to predict 'Readmitted'", "Perform grid search to tune 'max_depth' and 'n_estimators' hyperparameters", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze prediction errors"], "model_results": {"accuracy": 0.85, "f1": 0.82, "precision": 0.79, "recall": 0.85, "confusion_matrix": {"true_positive": 6, "true_negative": 8, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"PreviousAdmissions": 0.28, "LengthOfStay": 0.22, "Diagnosis_Heart Failure": 0.15, "Age": 0.12, "DischargeDisposition_Rehab": 0.1, "Gender_Male": 0.05}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a manufactured part will fail quality inspection based on sensor readings and production parameters.", "raw_table": "PartID,Temperature,Pressure,MaterialType,OperatorShift,SurfaceFinish,Defect\n001,350,5.2,Steel,Morning,good,No\n002,365,5.5,steel,Evening,Excellent,No\n003,NaN,5.1,Aluminum,Morning,fair,Yes\n004,355,5.3,Aluminum,Night,Good,No\n005,360,5.7,steel,Morning,poor,Yes\n006,348,5.0,ALUMINUM,Evening,fair,No\n007,370,5.8,Steel,Night,Excellent,Yes\n008,362,5.4,Steel,Morning,good,No\n009,359,5.6,Aluminum,Evening,poor,Yes\n010,NaN,5.2,steel,Morning,Good,No\n011,366,5.9,Steel,Night,excellent,Yes\n012,355,5.3,Aluminum,Morning,Good,No\n013,362,NaN,Steel,Evening,fair,Yes\n014,365,5.5,Steel,Night,GOOD,No\n015,358,5.4,aluminum,Morning,Fair,No", "model_steps": ["Load the raw data CSV into a DataFrame", "Standardize inconsistent capitalization in categorical columns: MaterialType, SurfaceFinish, OperatorShift", "Impute missing numeric values in Temperature and Pressure columns using median values", "Encode categorical variables MaterialType, OperatorShift, and SurfaceFinish using one-hot encoding", "Split the dataset into training (80%) and test (20%) sets, stratifying by the target Defect", "Standardize numeric features Temperature and Pressure to zero mean and unit variance", "Train a RandomForestClassifier with 100 trees on the training set", "Perform hyperparameter tuning on max_depth with values [5, 10, 15]", "Evaluate the final model on the test set computing accuracy, precision, recall, and F1 score", "Generate the confusion matrix and analyze feature importances", "Save the trained model and preprocessing pipeline for deployment"], "model_results": {"accuracy": 0.87, "precision": 0.82, "recall": 0.78, "f1": 0.8, "confusion_matrix": [[12, 3], [4, 11]], "top_feature_importances": {"SurfaceFinish_good": 0.22, "MaterialType_Steel": 0.19, "Temperature": 0.17, "Pressure": 0.15, "OperatorShift_Morning": 0.1, "SurfaceFinish_poor": 0.08, "MaterialType_Aluminum": 0.06, "OperatorShift_Night": 0.03}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict customer likelihood to churn within the next month for a telecommunications company.", "raw_table": "customer_id,contract_type,monthly_charges,tenure_months,internet_service,tech_support,churn\nC001,Month-to-month,75.35,2,Fiber optic,Yes,Yes\nC002,One year,56.75,12,DSL,No,No\nC003,month-to-month,82.50,1,Fiber optic,No,Yes\nC004,Two Year,45.00,24,DSL,Yes,No\nC005,Month-to-month,NaN,3,Fiber optic,no,Yes\nC006,One year,69.99,11,Fiber Optic,Yes,No\nC007,Month-to-Month,80.00,1,None,No,Yes\nC008,Two year,54.50,20,DSL,Yes,No\nC009,One Year,88.10,6,Fiber optic,No,Yes\nC010,Month-to-month,72.40,4,Fiber optic,No,Yes\nC011,One year,70.00,10,DSL,Yes,No\nC012,Month-to-month,68.00,5,Fiber optic,No,Yes\nC013,Two Year,40.30,30,DSL,Yes,No\nC014,One year,52.75,9,DSL,No,No\nC015,Month-to-month,NaN,2,Fiber optic,No,Yes", "model_steps": ["Load raw data and identify target variable 'churn'", "Clean 'monthly_charges' column by imputing missing values with the median", "Standardize capitalization and unify categories in 'contract_type' and 'tech_support' columns", "One-hot encode categorical features: 'contract_type', 'internet_service', 'tech_support'", "Split data into train and test sets using an 80/20 ratio with stratification on 'churn'", "Standardize numeric features: 'monthly_charges' and 'tenure_months' using training set statistics", "Train a RandomForestClassifier with default hyperparameters on the training data", "Perform grid search over 'max_depth' parameter with values [3, 5, 10] using 5-fold cross-validation", "Evaluate the best model on the test set calculating accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 important features", "Save final model and preprocessing pipeline for deployment"], "model_results": {"accuracy": 0.87, "precision": 0.83, "recall": 0.89, "f1": 0.86, "confusion_matrix": [[10, 2], [3, 15]], "top_features": {"contract_type_Month-to-month": 0.34, "tenure_months": 0.27, "internet_service_Fiber optic": 0.2}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Build a regression model to estimate property sale prices based on property features and location.", "raw_table": "PropertyID,Location,Size_sqft,Bedrooms,Bathrooms,Year_Built,Type,Sale_Price\n1,Downtown,1500,3,2,1990,Condo,450000\n2,Suburb,2000,4,3,2005,House,550000\n3,downtown,1300,2,1.5,1985,condo,420000\n4,Suburb,NA,3,2,2010,House,600000\n5,Rural,1800,3,2,1970,House,300000\n6,Rural,1600,2,1,1965,Townhouse,310000\n7,Suburb,2200,4,3,2015,house,620000\n8,Downtown,1400,3,2,2000,Condo,480000\n9,Suburb,1900,,2,2008,House,580000\n10,Downtown,1350,2,2,1995,Condo,455000\n11,Rural,1700,3,2,1975,Townhouse,320000\n12,Suburb,2100,4,3,2012,House,610000\n13,Downtown,1250,2,1.5,1988,condo,430000\n14,Suburb,2050,4,3,2011,House,590000", "model_steps": ["Load the dataset and identify target variable as Sale_Price", "Handle missing values: impute missing Size_sqft and Bedrooms with median values", "Standardize capitalization inconsistencies in Location and Type columns (e.g., 'downtown' to 'Downtown')", "Convert categorical variables Location and Type into one-hot encoded features", "Split data into training (80%) and testing (20%) sets randomly", "Standardize numeric features: Size_sqft, Bedrooms, Bathrooms, Year_Built", "Train a GradientBoostingRegressor model on the training data", "Perform hyperparameter tuning on learning_rate and n_estimators with 5-fold cross-validation", "Evaluate model performance on the test set using RMSE, MAE, and R-squared metrics", "Identify and report top 3 most important features based on feature importances"], "model_results": {"rmse": 27000, "mae": 21000, "r2": 0.85, "top_feature_importances": {"Size_sqft": 0.43, "Location_Downtown": 0.21, "Year_Built": 0.12}, "best_hyperparameters": {"learning_rate": 0.05, "n_estimators": 200}}}
{"purpose": "Classify whether a given corn field plot shows signs of nitrogen deficiency based on soil and environmental features.", "raw_table": "PlotID,SoilType,Rainfall_mm,Temperature_C,PesticideUsed,NDVI,Deficiency\n1,Loam,200,25,yes,0.72,No\n2,Clay,150,28,No,0.65,Yes\n3,Sand,NaN,27,YES,0.60,yes\n4,Loam,180,26,No,0.68,No\n5,clay,210,29,yes,0.55,Yes\n6,Sand,170,22,No,NA,No\n7,Loam,160,24,YES,0.70,No\n8,Loam,185,27,Yes,0.58,yes\n9,Sand,200,25,No,0.63,No\n10,Clay,190,26,Yes,0.50,Yes\n11,Loam,NaN,23,no,0.67,No\n12,Clay,175,28,no,0.52,Yes\n13,Sand,165,27,No,0.59,No\n14,Loam,155,24,yes,0.71,No", "model_steps": ["Load the CSV dataset and inspect for missing and inconsistent values", "Standardize capitalization for categorical columns SoilType, PesticideUsed, and Deficiency", "Impute missing numeric values (Rainfall_mm and NDVI) using median values for their SoilType group", "Convert categorical features SoilType and PesticideUsed into one-hot encoded variables", "Encode target variable Deficiency as binary (Yes=1, No=0)", "Split data into training (80%) and testing (20%) sets with stratification on Deficiency", "Standardize numeric features Rainfall_mm, Temperature_C, and NDVI using training set statistics", "Train a RandomForestClassifier with 100 trees", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Extract top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.78, "f1": 0.8, "confusion_matrix": {"true_negative": 7, "false_positive": 1, "false_negative": 2, "true_positive": 4}, "top_feature_importances": {"NDVI": 0.35, "Rainfall_mm": 0.25, "PesticideUsed_yes": 0.15}, "model_hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Build a regression model to predict monthly average surface temperature based on environmental features.", "raw_table": "Region,Month,CO2_ppm,Precipitation_mm,Vegetation_Index,Urbanization_Level,Avg_Temperature_C\nNorth,Jan,412.5,78,0.45,High,2.3\nSouth,Feb,INVALID,120,0.67,Medium,3.1\nEast,Mar,410.2,85,,Low,1.8\nWest,Apr,415.0,NaN,0.50,MEDIUM,2.5\nnorth,May,413.8,70,0.48,High,2.9\nSouth,Jun,414.1,95,0.52,Medium,3.5\nEast,Jul,411.9,110,0.60,Low,2.0\nWest,Aug,412.7,75,0.55,medium,2.7\nNorth,Sep,413.2,88,0.49,High,2.4\nsouth,Oct,414.0,NaN,0.51,Medium,3.0\nEast,Nov,412.0,92,0.58,Low,1.9\nWest,Dec,415.3,80,0.53,Medium,2.6\n", "model_steps": ["Correct inconsistent capitalization in 'Region' and 'Urbanization_Level' columns", "Handle missing values in 'CO2_ppm', 'Precipitation_mm', and 'Vegetation_Index' by imputing with median values", "One-hot encode the categorical variables 'Region' and 'Urbanization_Level'", "Split data into training (80%) and test (20%) sets", "Standardize numeric features: 'CO2_ppm', 'Precipitation_mm', and 'Vegetation_Index'", "Train a RandomForestRegressor to predict 'Avg_Temperature_C'", "Perform grid search to tune 'n_estimators' and 'max_depth'", "Evaluate model performance using RMSE, MAE, and R2 on test set", "Identify top 3 feature importances from the trained model"], "model_results": {"rmse": 0.35, "mae": 0.28, "r2": 0.87, "top_feature_importances": {"CO2_ppm": 0.42, "Precipitation_mm": 0.35, "Urbanization_Level_Medium": 0.12}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 6}}}
{"purpose": "Predict whether a government grant application will be approved based on applicant and project characteristics.", "raw_table": "ApplicationID,ApplicantIncome,ProjectType,Region,PreviousGrants,ApplicationStatus\n1,55000,Infrastructure,North,2,Approved\n2,46000,Healthcare,South,1,Rejected\n3,72000,Education,East,3,Approved\n4,NaN,Infrastructure,west,0,Rejected\n5,68000,HEALTHCARE,North,2,Approved\n6,50000,Education,East,1,Approved\n7,48000,Infrastructure,South,,Rejected\n8,53000,Education,East,1,Approved\n9,60000,Healthcare,North,1,Rejected\n10,45000,Infrastructure,West,0,Rejected\n11,70000,Education,East,3,Approved\n12,NaN,Healthcare,North,,Rejected\n13,62000,Infrastructure,South,2,Approved\n14,58000,Education,east,1,Approved", "model_steps": ["Load CSV data and parse with appropriate data types", "Identify and fill missing numeric values in ApplicantIncome and PreviousGrants with median values", "Standardize capitalization in categorical columns ProjectType and Region to ensure consistency", "One-hot encode categorical variables ProjectType and Region", "Split data into training (80%) and testing (20%) sets with stratification on ApplicationStatus", "Standardize numeric features ApplicantIncome and PreviousGrants using training set statistics", "Train a RandomForestClassifier to predict ApplicationStatus", "Perform grid search on number of estimators and max_depth hyperparameters using 5-fold cross-validation", "Evaluate model on test set computing accuracy, F1 score, precision, and recall", "Generate confusion matrix to analyze prediction errors"], "model_results": {"accuracy": 0.86, "f1": 0.87, "precision": 0.85, "recall": 0.89, "confusion_matrix": {"True_Positive": 6, "True_Negative": 5, "False_Positive": 1, "False_Negative": 1}, "top_feature_importances": {"ApplicantIncome": 0.35, "ProjectType_Infrastructure": 0.25, "PreviousGrants": 0.15, "Region_North": 0.1, "ProjectType_Healthcare": 0.08, "Region_East": 0.07}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Build a classification model to predict whether a household will exceed peak electricity usage during summer afternoons.", "raw_table": "Household_ID,Region,Num_Occupants,Home_Age,Has_Solar,Avg_Daily_Usage_kWh,Peak_Usage_Exceeded\nH001,North,4,15,Yes,23.5,Yes\nH002,south,2,30,no,12.7,No\nH003,East,3,NA,No,16.3,Yes\nH004,West,5,20,YES,25.1,Yes\nH005,North,1,5,No,9.8,No\nH006,East,3,12,No,17.2,Yes\nH007,South,4,18,No,20.4,Yes\nH008,West,2,25,No,11.1,No\nH009,North,3,10,yes,14.9,No\nH010,East,NA,22,No,19.6,Yes\nH011,South,2,8,No,13.3,No\nH012,West,6,35,No,27.8,Yes\nH013,North,4,20,No,21.0,Yes\nH014,East,3,5,Yes,15.5,No", "model_steps": ["Load dataset from CSV string into a DataFrame", "Identify and handle missing values: Impute missing 'Num_Occupants' and 'Home_Age' with median values", "Normalize inconsistent capitalization in 'Region' and 'Has_Solar' columns", "Convert 'Has_Solar' and 'Peak_Usage_Exceeded' to binary categorical variables", "One-hot encode 'Region' categorical variable", "Split data into train (80%) and test (20%) sets with stratification on the target variable", "Standardize numeric features: 'Num_Occupants', 'Home_Age', and 'Avg_Daily_Usage_kWh'", "Train a RandomForestClassifier with 100 trees", "Perform a grid search over max_depth values [5, 10, 15]", "Evaluate model on the test set, computing accuracy, precision, recall, and F1 score", "Extract top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.88, "f1": 0.85, "top_features": {"Avg_Daily_Usage_kWh": 0.35, "Has_Solar_Yes": 0.22, "Num_Occupants": 0.18}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Build a regression model to estimate monthly average surface temperature based on environmental factors.", "raw_table": "month,region,humidity_percent,wind_speed_kmh,precipitation_mm,soil_type,avg_surface_temp_C\nJanuary,North,85,12,120,Sandy,2.3\nFebruary,South,78,15,85,Clay,4.1\nMarch,East,88,NaN,95,Loam,7.5\nApril,West,75,10,missing,SANDY,12.2\nMay,North,80,8,60,Clay,16.8\nJune,South,70,20,55,loam,20.4\nJuly,East,65,18,40,Clay,23.1\nAugust,West,60,16,50,Sandy,22.8\nSeptember,North,72,14,70,Loam,18.7\nOctober,South,79,13,80,clay,13.0\nNovember,East,82,9,115,Loam,7.8\nDecember,West,85,11,130,Sandy,3.4\nJanuary,South,NaN,17,90,Clay,2.9\nFebruary,East,77,14,85,Loam,5.2", "model_steps": ["Replace missing values in numeric columns with column medians", "Normalize text in soil_type column to lowercase and standardize categories", "One-hot encode the soil_type and region categorical variables", "Split data into train (80%) and test (20%) sets by random sampling", "Standardize numeric features: humidity_percent, wind_speed_kmh, precipitation_mm", "Train a RandomForestRegressor model to predict avg_surface_temp_C", "Perform 5-fold cross-validation to tune number of trees and max_depth", "Evaluate model performance on the test set using RMSE, MAE, and R2", "Identify and report top 3 feature importances from the trained model"], "model_results": {"rmse": 1.45, "mae": 1.1, "r2": 0.87, "top_feature_importances": {"humidity_percent": 0.32, "precipitation_mm": 0.28, "soil_type_loam": 0.15}, "hyperparameters": {"n_estimators": 100, "max_depth": 8}}}
{"purpose": "Predict whether a government grant application will be approved based on application details and applicant background.", "raw_table": "Application_ID,Applicant_Age,Applicant_Education,State,Requested_Amount,Previous_Grants,Agency,Application_Status\n1,45,Bachelor,CA,50000,2,Health,Approved\n2,29,Master,ny,120000,0,Education,Denied\n3,,High School,TX,30000,1,Health,Approved\n4,52,PhD,FL,70000,3,Transport,Approved\n5,40,master,CA,100000,1,Education,Denied\n6,37,Bachelor,TX,missing,0,Health,Denied\n7,33,Bachelor,CA,45000,2,Transport,Approved\n8,28,High School,FL,35000,0,Education,Denied\n9,61,PhD,NY,90000,4,Health,Approved\n10,54,Master,TX,80000,3,Transport,Denied\n11,47,Bachelor,CA,60000,2,Education,Approved\n12,35,High school,NY,,1,Health,Denied\n13,42,Master,FL,70000,missing,Transport,Approved\n14,30,Bachelor,TX,55000,0,Education,Denied", "model_steps": ["Load dataset and inspect for missing and inconsistent values", "Standardize categorical values for 'Applicant_Education' and 'State' (e.g., normalize capitalization)", "Impute missing numeric values in 'Applicant_Age' and 'Requested_Amount' with median values", "Fill missing values in 'Previous_Grants' with zero", "Encode categorical variables 'Applicant_Education', 'State', and 'Agency' using one-hot encoding", "Split the data into training (80%) and test (20%) sets stratified by 'Application_Status'", "Scale numeric features using standardization (mean=0, std=1)", "Train a RandomForestClassifier to predict 'Application_Status' (Approved vs Denied)", "Tune hyperparameters max_depth and n_estimators using grid search with 5-fold cross-validation", "Evaluate the model on test data using accuracy, precision, recall, and F1 score", "Generate a confusion matrix to analyze prediction errors", "Report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.79, "precision": 0.81, "recall": 0.75, "f1": 0.78, "confusion_matrix": {"True_Positive": 9, "True_Negative": 7, "False_Positive": 2, "False_Negative": 3}, "top_feature_importances": {"Requested_Amount": 0.28, "Previous_Grants": 0.22, "Applicant_Education_Master": 0.15}, "best_hyperparameters": {"max_depth": 6, "n_estimators": 100}}}
{"purpose": "Predict whether a movie will be a box office hit based on its attributes before release.", "raw_table": "MovieID,Genre,Director,BudgetMillions,RuntimeMinutes,LeadActorPopularity,ReleaseMonth,IsHit\n1,Action,Smith,150,130,85,July,Yes\n2,Comedy,jones,40,95,70,December,No\n3,Drama,Lee,30,120,65,May,No\n4,Action,smith,200,140,90,June,Yes\n5,Horror,King,15,100,50,October,No\n6,Comedy,Adams,60,105,75,November,No\n7,Drama,Lee,80,125,80,April,Yes\n8,Action,Smith,NaN,135,88,July,Yes\n9,Horror,King,20,98,55,Oct,No\n10,Comedy,Adams,55,102,72,,No\n11,Drama,Lee,70,110,78,March,Yes\n12,Action,Smith,180,145,92,July,Yes\n13,Comedy,Jones,45,90,68,December,No", "model_steps": ["Load the CSV data and parse into a DataFrame", "Standardize capitalization in categorical columns such as 'Director', 'Genre', and 'ReleaseMonth'", "Impute missing numeric values in 'BudgetMillions' with median budget", "Impute missing categorical values in 'ReleaseMonth' with the mode", "Encode categorical variables using one-hot encoding", "Convert target variable 'IsHit' into binary labels (Yes=1, No=0)", "Split data into 80% training and 20% testing sets", "Standardize numeric features 'BudgetMillions', 'RuntimeMinutes', and 'LeadActorPopularity'", "Train a RandomForestClassifier with 100 trees", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score", "Extract feature importances from the trained model"], "model_results": {"accuracy": 0.83, "precision": 0.8, "recall": 0.85, "f1": 0.82, "top_feature_importances": {"BudgetMillions": 0.32, "LeadActorPopularity": 0.25, "Genre_Action": 0.15, "ReleaseMonth_July": 0.1, "RuntimeMinutes": 0.08}}}
{"purpose": "Predict hourly electricity consumption of residential buildings based on weather and occupancy features.", "raw_table": "BuildingID,DayOfWeek,TemperatureC,Humidity,OccupancyStatus,EnergyConsumption_kWh\nB01,Monday,22.5,45,Occupied,15.2\nB02,Tuesday,19.0,50,occupied,13.5\nB03,Wednesday,25.1,55,Vacant,8.7\nB04,Thursday,NaN,48,Occupied,14.1\nB05,Friday,21.3,missing,occupied,16.0\nB06,Saturday,24.8,60,Vacant,9.2\nb07,Sunday,23.0,58,Occupied,15.5\nB08,Monday,20.5,53,Occupied,14.0\nB09,Tuesday,18.7,49,Vacant,7.9\nB10,Wednesday,22.0,52,Occupied,15.8\nB11,Thursday,21.9,50,occupied,14.3\nB12,Friday,23.5,55,Vacant,9.5", "model_steps": ["Load data and inspect for missing and inconsistent values", "Normalize categorical data in OccupancyStatus to consistent capitalization", "Impute missing numeric values in TemperatureC and Humidity using median values", "One-hot encode OccupancyStatus and DayOfWeek categorical variables", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features TemperatureC and Humidity", "Train a RandomForestRegressor to predict EnergyConsumption_kWh", "Perform grid search cross-validation over number of estimators and max_depth", "Evaluate model on test set using RMSE, MAE, and R2 metrics", "Extract and report top feature importances from the trained model"], "model_results": {"rmse": 1.25, "mae": 0.98, "r2": 0.87, "top_feature_importances": {"TemperatureC": 0.38, "OccupancyStatus_Occupied": 0.27, "Humidity": 0.18, "DayOfWeek_Monday": 0.05, "DayOfWeek_Tuesday": 0.04, "DayOfWeek_Friday": 0.03, "OccupancyStatus_Vacant": 0.05}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 8}}}
{"purpose": "Predict customer churn probability for a telecommunications service provider.", "raw_table": "CustomerID,MonthlyCharges,ContractType,PaymentMethod,TenureMonths,InternetService,TechSupport,Churn\nC001,70.35,Month-to-month,Credit card,3,Fiber optic,No,Yes\nC002,85.5,One year,bank transfer,12,Fiber optic,Yes,No\nC003,56.75,Month-to-month,Electronic check,,DSL,No,Yes\nC004,45.10,Two year,mailed check,24,DSL,Yes,No\nC005,NaN,Month-to-month,Credit Card,1,Fiber optic,No,Yes\nC006,90.0,Month-to-month,Electronic check,5,Fiber Optic,NO,Yes\nC007,42.0,Two Year,bank transfer,36,DSL,Yes,No\nC008,60.25,Month-to-month,Credit card,2,Fiber optic,No,Yes\nC009,75.0,One Year,Electronic Check,10,Fiber optic,Yes,No\nC010,50.5,Month-to-month,mailed check,4,DSL,No,Yes\nC011,55.0,Month-to-month,Credit card,7,Fiber Optic,No,Yes\nC012,48.3,Two year,bank transfer,20,DSL,Yes,No", "model_steps": ["Load the CSV data into a DataFrame", "Clean missing values in TenureMonths and MonthlyCharges by imputing median values", "Standardize capitalization and correct inconsistent categorical values (e.g., 'Fiber Optic' to 'Fiber optic')", "Convert the target variable 'Churn' to binary (Yes=1, No=0)", "Split data into train and test sets with an 80/20 ratio", "One-hot encode categorical variables: ContractType, PaymentMethod, InternetService, TechSupport", "Standardize numeric features: MonthlyCharges and TenureMonths", "Train a RandomForestClassifier on the training set", "Perform grid search over n_estimators and max_depth parameters using 5-fold cross-validation", "Evaluate the final model on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.83, "f1": 0.79, "precision": 0.75, "recall": 0.84, "confusion_matrix": [[7, 2], [3, 8]], "top_feature_importances": {"ContractType_Month-to-month": 0.22, "TechSupport_No": 0.18, "InternetService_Fiber optic": 0.15, "MonthlyCharges": 0.13, "TenureMonths": 0.11}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict if a given day will experience high air pollution levels (PM2.5) based on weather and traffic conditions.", "raw_table": "Date,Temperature_C,Humidity_Percent,Wind_Speed_kmh,Traffic_Level,Weather_Condition,PM2.5_High\n2024-01-01,5.2,80,15,High,Clear,Yes\n2024-01-02,3.8,85,8,medium,Cloudy,Yes\n2024-01-03,NaN,78,12,Low,Fog,No\n2024-01-04,7.1,,10,High,Rain,Yes\n2024-01-05,6.4,82,7,High,clear,Yes\n2024-01-06,2.9,90,5,low,Snow,No\n2024-01-07,4.0,88,20,Medium,Fog,No\n2024-01-08,3.5,80,18,HIGH,Cloudy,Yes\n2024-01-09,1.2,85,9,Medium,Rain,No\n2024-01-10,NaN,75,14,Low,Clear,No\n2024-01-11,5.0,78,13,Medium,cloudy,Yes\n2024-01-12,6.8,82,11,High,Fog,Yes\n2024-01-13,3.3,NaN,16,Medium,Rain,No\n2024-01-14,4.5,83,NaN,Low,Clear,No\n", "model_steps": ["Parse dates and set 'Date' column as index", "Impute missing numeric values (Temperature_C, Humidity_Percent, Wind_Speed_kmh) using median imputation", "Standardize numeric features: Temperature_C, Humidity_Percent, Wind_Speed_kmh", "Normalize casing and one-hot encode categorical variables: Traffic_Level, Weather_Condition", "Convert target 'PM2.5_High' to binary label (Yes=1, No=0)", "Split data into train and test sets with 80/20 ratio stratified on target", "Train a RandomForestClassifier with 100 trees", "Perform grid search for max_depth in [5, 10, None]", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Extract and rank top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "top_feature_importances": {"Traffic_Level_High": 0.32, "Humidity_Percent": 0.23, "Weather_Condition_Fog": 0.18}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a taxi trip will have a long wait time (over 10 minutes) based on trip and driver characteristics.", "raw_table": "trip_id,driver_experience,car_type,trip_distance_km,trip_start_hour,weather_condition,passenger_count,long_wait\n1,5,Sedan,3.2,8,Clear,1,No\n2,2,SUV,1.5,22,Rain,2,Yes\n3,10,sedan,7.8,14,Fog,1,No\n4,,SUV,4.0,18,clear,3,Yes\n5,7,Truck,2.9,9,Snow,1,No\n6,3,suv,5.5,20,Rain,NaN,Yes\n7,8,Sedan,6.1,15,Clear,2,No\n8,1,Truck,3.3,23,Snow,1,Yes\n9,6,Sedan,4.7,11,Fog,2,No\n10,4,SUV,2.8,19,Rain,1,Yes\n11,9,Sedan,5.0,13,Clear,3,No\n12,NaN,Truck,3.7,7,Snow,2,No\n13,5,SUV,6.4,21,Rain,1,Yes\n14,3,Sedan,2.2,6,Clear,1,No", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing values and inconsistencies", "Impute missing numeric values for 'driver_experience' and 'passenger_count' using median", "Standardize capitalization in categorical columns 'car_type' and 'weather_condition' to have consistent formatting", "One-hot encode categorical variables 'car_type' and 'weather_condition'", "Split the dataset into train (80%) and test (20%) sets with stratification on the target variable 'long_wait'", "Standardize numeric features 'driver_experience', 'trip_distance_km', and 'trip_start_hour'", "Train a RandomForestClassifier to predict 'long_wait'", "Perform grid search cross-validation to tune 'max_depth' and 'n_estimators'", "Evaluate the trained model on the test set computing accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Extract and report feature importances from the final model"], "model_results": {"accuracy": 0.82, "precision": 0.79, "recall": 0.75, "f1": 0.77, "confusion_matrix": {"true_positive": 6, "true_negative": 8, "false_positive": 2, "false_negative": 2}, "top_feature_importances": {"trip_distance_km": 0.31, "driver_experience": 0.25, "weather_condition_Rain": 0.15, "car_type_SUV": 0.1, "trip_start_hour": 0.08, "passenger_count": 0.06, "weather_condition_Snow": 0.05}, "best_hyperparameters": {"max_depth": 6, "n_estimators": 100}}}
{"purpose": "Predict whether a loan applicant will default on their loan based on financial and demographic features.", "raw_table": "ApplicantID,Age,Income,EmploymentStatus,CreditScore,LoanAmount,LoanPurpose,Defaulted\n1,45,54000,Employed,700,15000,HomeImprovement,No\n2,29,NaN,SELF-employed,620,12000,Debt Consolidation,Yes\n3,34,72000,employed,680,20000,Car,No\n4,52,48000,Unemployed,590,7000,HomeImprovement,Yes\n5,23,31000,Employed,NaN,5000,Education,No\n6,40,61000,Employed,710,13000,Car,No\n7,35,58000,Employed,675,18000,Debt Consolidation,No\n8,28,40000,Self-Employed,640,9000,Education,Yes\n9,50,60000,Employed,720,15000,Home Improvement,No\n10,38,45000,Unemployed,600,11000,Car,Yes\n11,31,53000,employed,655,14000,Debt Consolidation,No\n12,48,67000,Employed,690,21000,HomeImprovement,No\n13,27,39000,Employed,630,7000,Education,Yes\n14,44,NaN,Self-employed,700,16000,Car,No\n15,36,52000,Employed,660,12000,HomeImprovement,No", "model_steps": ["Load raw CSV data into a DataFrame", "Clean 'EmploymentStatus' column by standardizing capitalization and correcting inconsistent entries", "Impute missing numeric values in 'Income' and 'CreditScore' using median imputation", "Convert 'LoanPurpose' and 'EmploymentStatus' categorical columns into one-hot encoded features", "Split dataset into training (80%) and testing (20%) subsets randomly", "Standardize numeric features: Age, Income, CreditScore, LoanAmount using z-score normalization", "Train a RandomForestClassifier to predict 'Defaulted' using training data", "Perform grid search to tune 'n_estimators' and 'max_depth' hyperparameters", "Evaluate model performance on test set by calculating accuracy, F1 score, precision, and recall", "Generate a confusion matrix to analyze prediction errors"], "model_results": {"accuracy": 0.8, "f1": 0.78, "precision": 0.75, "recall": 0.82, "confusion_matrix": {"true_positive": 9, "true_negative": 12, "false_positive": 4, "false_negative": 2}, "top_feature_importances": {"CreditScore": 0.35, "LoanAmount": 0.22, "Income": 0.15, "EmploymentStatus_Employed": 0.1, "LoanPurpose_HomeImprovement": 0.08}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features and location.", "raw_table": "HouseID,Size_sqft,Bedrooms,Bathrooms,Neighborhood,YearBuilt,HasGarage,SalePrice\n1,2100,3,2,NorthSide,1995,Yes,350000\n2,1800,2,1.5,southside,1988,no,280000\n3,2500,4,3,Eastville,2005,Yes,420000\n4,1900,3,,WestEnd,1990,YES,300000\n5,1700,3,2,NorthSide,NaN,No,310000\n6,2300,4,2.5,Eastville,2008,yes,450000\n7,1600,2,1,SOUTHside,1985,No,260000\n8,2000,3,2,WestEnd,1992,No,330000\n9,2200,3,2.5,NorthSide,1998,Yes,380000\n10,NaN,3,2,Eastville,2003,Yes,400000\n11,2100,3,2,WestEnd,1997,No,340000\n12,1750,2,1.5,SouthSide,1987,No,275000\n13,2050,4,3,NorthSide,2000,YES,395000\n14,1900,3,2.5,Eastville,2006,yes,415000", "model_steps": ["Load dataset and inspect for missing and inconsistent values", "Standardize capitalization for categorical columns like Neighborhood and HasGarage", "Impute missing numeric values (e.g., YearBuilt and Size_sqft) with median values", "Convert HasGarage from Yes/No to binary 1/0", "One-hot encode Neighborhood categorical variable", "Split data into training (80%) and testing sets (20%)", "Standardize numeric features: Size_sqft, Bedrooms, Bathrooms, YearBuilt", "Train a RandomForestRegressor model to predict SalePrice", "Tune max_depth parameter of RandomForest using grid search with cross-validation", "Evaluate model on test data calculating RMSE, MAE, and R2 score", "Identify top 3 feature importances from the trained model"], "model_results": {"rmse": 21000, "mae": 16500, "r2": 0.87, "top_feature_importances": {"Size_sqft": 0.42, "Neighborhood_NorthSide": 0.19, "Bathrooms": 0.15}, "best_hyperparameters": {"max_depth": 8}}}
{"purpose": "Predict hourly energy consumption in kilowatt-hours based on weather conditions and time features.", "raw_table": "hour_of_day,day_type,temperature_c,humidity_pct,wind_speed_kmh,energy_consumption_kwh\n0,weekday,15.2,55,12,150.5\n1,Weekday,14.8,58,10,145.7\n2,weekday,14.5,60,8,140.3\n3,weekday,,62,7,138.9\n4,weekday,13.9,65,6,135.1\n5,weekday,13.7,63,5,133.8\n6,weekend,14.0,60,4,160.2\n7,Weekend,16.5,55,10,180.5\n8,weekend,18.0,50,12,210.7\n9,Weekend,20.2,45,14,230.1\n10,weekday,22.5,40,15,250.3\n11,weekday,24.0,42,16,265.7\n12,Weekday,25.0,43,18,275.4\n13,weekend,26.3,41,20,280.0\n14,weekend,27.1,39,22,290.8", "model_steps": ["Load the CSV dataset into a DataFrame", "Identify 'energy_consumption_kwh' as the target variable", "Handle missing values in 'temperature_c' by imputing with median", "Normalize inconsistent capitalization in 'day_type' to lowercase", "One-hot encode the 'day_type' categorical variable", "Split data into training (80%) and testing (20%) sets randomly", "Standardize numeric features: 'hour_of_day', 'temperature_c', 'humidity_pct', 'wind_speed_kmh'", "Train a Gradient Boosting Regressor on the training set", "Perform grid search to tune 'n_estimators' and 'max_depth'", "Evaluate model performance on test set using RMSE, MAE, and R2", "Analyze feature importances from the trained model"], "model_results": {"rmse": 8.7, "mae": 6.3, "r2": 0.92, "best_hyperparameters": {"n_estimators": 100, "max_depth": 4}, "top_feature_importances": {"hour_of_day": 0.35, "temperature_c": 0.3, "day_type_weekend": 0.2, "humidity_pct": 0.1, "wind_speed_kmh": 0.05}}}
{"purpose": "Predict whether a taxi trip will exceed 20 minutes based on trip and passenger characteristics.", "raw_table": "trip_id,passenger_count,trip_distance,rate_code,payment_type,trip_start_hour,weather_condition,trip_duration_over_20_min\n1,1,3.2,1,Cash,9,Clear,No\n2,2,7.5,1,card,18,Rain,Yes\n3,3,2.1,2,Cash,23,clear,No\n4,,5.0,1,Credit Card,14,Snow,Yes\n5,1,12.3,1,Cash,7,Fog,Yes\n6,2,3.7,3,CARD,20,Rain,No\n7,1,1.5,1,Cash,2,CLEAR,No\n8,4,8.0,1,Cash,17,Fog,Yes\n9,2,4.2,2,Cash,11,Clear,No\n10,1,6.5,1,Credit card,22,Snow,Yes\n11,3,9.1,1,Cash,15,Rain,Yes\n12,1,2.7,1,Cash,10,Clear,No\n13,2,4.0,1,Cash,16,Fog,Yes\n14,1,5.3,1,Cash,19,Rain,Yes", "model_steps": ["Load the CSV data into a dataframe and inspect for missing and inconsistent values", "Impute missing passenger_count with the median value", "Standardize capitalization in payment_type and weather_condition columns", "One-hot encode categorical variables: rate_code, payment_type, weather_condition", "Create a binary target variable from trip_duration_over_20_min ('Yes' as 1, 'No' as 0)", "Split data into training and testing sets with an 80/20 ratio", "Standardize numeric features: passenger_count, trip_distance, trip_start_hour", "Train a RandomForestClassifier on the training data", "Perform grid search over max_depth [3, 5, 7] and n_estimators [50, 100]", "Evaluate accuracy, precision, recall, and F1 score on the test set", "Generate and inspect the confusion matrix", "Identify and rank feature importances from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"trip_distance": 0.34, "trip_start_hour": 0.21, "passenger_count": 0.15, "payment_type_Cash": 0.1, "weather_condition_Rain": 0.08}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a customer will make a purchase during a website visit based on user and session features.", "raw_table": "session_id,age,user_type,device,browser_time_spent,referral_source,pages_visited,purchase\ns001,25,New,Mobile,5.4,Google,3,Yes\ns002,thirty,Returning,Desktop,12.3,Direct,7,No\ns003,45,new,Tablet,3.8,facebook,2,No\ns004,38,Returning,Mobile,missing,Google,5,Yes\ns005,29,New,desktop,8.1,Direct,4,No\ns006,52,Returning,Tablet,15.0,Google,11,Yes\ns007,,New,Mobile,6.5,Facebook,3,No\ns008,41,Returning,Mobile,9.7,Direct,6,Yes\ns009,33,new,Desktop,7.2,google,5,No\ns010,27,Returning,Tablet,4.6,Direct,3,No\ns011,50,NEW,Mobile,10.5,Facebook,8,Yes\ns012,36,Returning,Desktop,6.8,Direct,4,No\ns013,42,New,Mobile,7.9,google,5,Yes\ns014,39,Returning,Tablet,11.3,Direct,9,No", "model_steps": ["Load CSV data and parse columns", "Clean 'age' column: convert to numeric, impute missing and fix 'thirty' entry as 30", "Standardize capitalization in 'user_type' and 'referral_source' to consistent lowercase", "Impute missing values in 'browser_time_spent' with median value", "One-hot encode categorical variables: user_type, device, referral_source", "Split data into training (80%) and test (20%) sets with stratification on 'purchase'", "Standardize numeric features: age, browser_time_spent, pages_visited", "Train a RandomForestClassifier to predict 'purchase'", "Evaluate model performance on test set using accuracy, F1 score, precision, and recall", "Analyze feature importances from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.82, "precision": 0.8, "recall": 0.85, "feature_importances": {"browser_time_spent": 0.31, "pages_visited": 0.25, "user_type_New": 0.15, "device_Mobile": 0.1, "referral_source_Google": 0.08, "age": 0.07, "referral_source_Direct": 0.04}}}
{"purpose": "Build a regression model to predict hourly electricity consumption in residential buildings.", "raw_table": "Building_ID,Hour_of_Day,Day_Type,Outdoor_Temperature_C,Occupancy_Status,Heating_Type,Previous_Hour_Consumption_kWh,Electricity_Consumption_kWh\nB001,0,weekday,5.2,Occupied,electric,0.5,0.6\nB002,1,Weekday,6.1,unoccupied,Gas,0.4,0.5\nB003,2,weekend,4.8,occupied,electric,0.6,0.7\nB004,3,weekend,5.0,,Electric,0.7,0.8\nB005,4,weekday,5.5,Occupied,gas,0.5,0.6\nB006,5,Weekday,missing,Unoccupied,Electric,0.3,0.4\nB007,6,Weekday,6.3,Occupied,electric,0.6,0.7\nB008,7,Holiday,5.7,Occupied,Electric,0.7,0.9\nB009,8,holiday,5.8,Occupied,gas,0.8,0.85\nB010,9,weekday,6.0,Unoccupied,Gas,0.5,0.55\nB011,10,Weekday,6.2,occupied,Electric,0.6,0.65\nB012,11,Weekend,5.9,unoccupied,Gas,0.4,0.45\nB013,12,weekday,6.1,Occupied,Electric,0.7,0.75\nB014,13,weekday,5.8,Occupied,electric,0.6,0.7\nB015,14,weekday,6.0,Occupied,Electric,0.7,0.8", "model_steps": ["Identify and correct inconsistent capitalization in 'Day_Type' and 'Heating_Type' columns.", "Impute missing values in 'Outdoor_Temperature_C' and 'Occupancy_Status' columns using median and mode respectively.", "One-hot encode categorical variables: 'Day_Type', 'Occupancy_Status', and 'Heating_Type'.", "Standardize numeric features: 'Hour_of_Day', 'Outdoor_Temperature_C', and 'Previous_Hour_Consumption_kWh'.", "Split data into training (80%) and testing (20%) sets randomly.", "Train a Gradient Boosting Regressor on the training data.", "Perform grid search over 'n_estimators' (50, 100) and 'max_depth' (3, 5) hyperparameters.", "Evaluate the model on the test set using RMSE, MAE, and R2 score.", "Extract and analyze feature importances from the trained model."], "model_results": {"rmse": 0.08, "mae": 0.06, "r2": 0.92, "best_hyperparameters": {"n_estimators": 100, "max_depth": 3}, "top_feature_importances": {"Previous_Hour_Consumption_kWh": 0.45, "Hour_of_Day": 0.25, "Outdoor_Temperature_C": 0.15, "Occupancy_Status_Occupied": 0.1, "Heating_Type_Electric": 0.05}}}
{"purpose": "Predict hourly energy consumption for a residential building based on weather and occupancy data.", "raw_table": "Hour,Temperature_C,Humidity_Percent,Day_Type,Occupancy_Status,Energy_Consumption_kWh\n0,15.2,45,weekday,Occupied,1.2\n1,14.8,48,Weekday,occupied,1.1\n2,14.5,NaN,weekday,Unoccupied,0.8\n3,14.0,50,Weekend,Unoccupied,0.7\n4,13.8,52,weekend,unoccupied,0.6\n5,13.5,49,Weekend,Occupied,1.0\n6,14.0,46,Weekday,Occupied,1.4\n7,16.5,44,weekday,Occupied,2.2\n8,18.0,43,weekday,Occupied,3.1\n9,20.0,40,Weekday,occupied,4.0\n10,21.5,38,weekday,Occupied,4.5\n11,22.0,37,Weekday,Occupied,4.8\n12,23.0,35,Weekend,Occupied,5.0\n13,23.5,34,weekend,occupied,5.2\n14,24.0,33,Weekend,Occupied,5.1", "model_steps": ["Load CSV data into a DataFrame", "Standardize capitalization in categorical columns 'Day_Type' and 'Occupancy_Status'", "Impute missing humidity values with the median humidity", "One-hot encode 'Day_Type' and 'Occupancy_Status' categorical columns", "Split data into train (80%) and test (20%) sets", "Standardize numeric features 'Temperature_C' and 'Humidity_Percent' using training set statistics", "Train a Gradient Boosting Regressor on the training data to predict 'Energy_Consumption_kWh'", "Perform 5-fold cross-validation to tune the number of estimators and learning rate", "Evaluate model performance on test set using RMSE, MAE, and R2 metrics", "Plot predicted vs actual energy consumption for test set"], "model_results": {"rmse": 0.25, "mae": 0.18, "r2": 0.92, "best_hyperparameters": {"n_estimators": 100, "learning_rate": 0.1}, "top_feature_importances": {"Temperature_C": 0.45, "Occupancy_Status_Occupied": 0.3, "Humidity_Percent": 0.15, "Day_Type_Weekday": 0.1}}}
{"purpose": "Predict the likelihood of hospital readmission within 30 days for diabetic patients based on clinical and demographic data.", "raw_table": "PatientID,Age,Gender,BMI,HbA1c_Level,Medication_Type,Previous_Admissions,Smoker,Readmitted\n1,58,Male,31.2,8.1,insulin,2,Yes,Yes\n2,72,Female,28.5,7.4,Metformin,1,No,No\n3,65,FEMALE,NaN,9.0,Insulin,3,yes,Yes\n4,54,Male,26.9,6.8,Sulfonylurea,0,No,No\n5,45,Female,NaN,7.2,metformin,2,No,No\n6,60,Male,29.8,8.5,Insulin,1,YES,Yes\n7,70,Female,27.5,7.0,Sulfonylurea,NaN,No,No\n8,62,male,32.1,8.7,Metformin,3,No,Yes\n9,50,Female,30.0,7.8,Insulin,1,No,No\n10,59,Male,29.3,8.2,sulfonylurea,2,No,Yes\n11,67,Female,28.0,NaN,Metformin,1,No,No\n12,55,male,31.5,7.9,Insulin,2,Yes,Yes\n13,49,Female,26.7,7.1,Metformin,0,No,No", "model_steps": ["Load the dataset and identify missing and inconsistent values", "Standardize capitalization in categorical columns like Gender, Medication_Type, and Smoker", "Impute missing BMI and HbA1c_Level values with median values grouped by Gender", "Encode categorical variables Gender, Medication_Type, and Smoker using one-hot encoding", "Split the dataset into train and test sets with an 80/20 ratio maintaining class distribution", "Standardize numeric features such as Age, BMI, HbA1c_Level, and Previous_Admissions", "Train a RandomForestClassifier on the training data to predict Readmitted", "Perform a grid search over max_depth and n_estimators hyperparameters using 5-fold cross-validation", "Evaluate the best model on the test set computing accuracy, precision, recall, and F1 score", "Generate a confusion matrix and identify top 3 important features", "Save the trained model and preprocessing pipeline for deployment"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.79, "recall": 0.84, "confusion_matrix": {"true_negative": 14, "false_positive": 3, "false_negative": 4, "true_positive": 23}, "top_feature_importances": {"HbA1c_Level": 0.32, "Previous_Admissions": 0.25, "Medication_Type_Insulin": 0.18}, "best_hyperparameters": {"max_depth": 7, "n_estimators": 100}}}
{"purpose": "Build a regression model to estimate monthly average surface temperature based on environmental factors.", "raw_table": "Region,Month,Precipitation_mm,Humidity_pct,Elevation_m,Vegetation_Type,Avg_Temperature_C\nNorth,Jan,50,80,300,Forest,2.3\nSouth,FEB,20,,150,Grassland,15.1\nEast,Mar,30,65,200,Shrubland,10.5\nWest,Apr,\"NA\",70,400,forest,8.0\nNorth,May,45,85,300,Forest,12.8\nSouth,Jun,25,60,150,Grassland,20.2\nEast,Jul,35,70,200,Shrubland,22.5\nWest,Aug,40,75,400,Forest,19.3\nNorth,Sep,55,NULL,300,Forest,14.7\nSouth,Oct,15,55,150,Grassland,11.0\nEast,Nov,28,68,200,Shrubland,7.8\nWest,Dec,38,72,400,Forest,3.5\n", "model_steps": ["Load the CSV dataset into a dataframe", "Standardize the capitalization in the 'Region' and 'Vegetation_Type' categorical columns", "Impute missing numeric values in 'Precipitation_mm' and 'Humidity_pct' columns using median values", "One-hot encode the 'Region' and 'Vegetation_Type' categorical variables", "Split data into training (80%) and test sets (20%) randomly", "Standardize numeric features: 'Precipitation_mm', 'Humidity_pct', 'Elevation_m', and 'Month' encoded numerically", "Train a RandomForestRegressor model to predict 'Avg_Temperature_C'", "Perform grid search to tune number of trees (n_estimators) and max_depth", "Evaluate the model on the test set using RMSE, MAE, and R-squared metrics", "Identify and report the top 3 most important features from the trained model"], "model_results": {"rmse": 1.85, "mae": 1.43, "r2": 0.87, "top_feature_importances": {"Elevation_m": 0.38, "Month": 0.27, "Humidity_pct": 0.15}, "hyperparameters": {"n_estimators": 100, "max_depth": 8}}}
{"purpose": "Predict whether a customer will make a purchase in their current session based on session and user features.", "raw_table": "session_id,user_id,device_type,browser,time_on_site_seconds,page_views,referrer,is_returning,cart_value,target_purchase\ns001,u1001,Mobile,Chrome,320,5,Google,True,0,1\ns002,u1002,Desktop,firefox,150,3,Direct,false,20,0\ns003,u1003,Mobile,Safari,missing,4,google,True,N/A,1\ns004,u1004,Tablet,Chrome,400,6,Facebook,TRUE,35,0\ns005,u1005,Desktop,Edge,210,2,Direct,False,0,0\ns006,u1006,mobile,Chrome,180,,Google,True,15,1\ns007,u1007,Desktop,,230,3,Direct,FALSE,25,0\ns008,u1008,Tablet,Safari,300,5,Facebook,True,50,1\ns009,u1009,Mobile,Chrome,275,4,google,True,10,1\ns010,u1010,Desktop,Chrome,190,3,Direct,False,5,0\ns011,u1011,Tablet,Firefox,missing,6,Facebook,true,40,1\ns012,u1012,Mobile,chrome,360,5,Google,TRUE,30,1", "model_steps": ["Load CSV data into a DataFrame", "Clean and standardize 'device_type' and 'browser' categorical columns (e.g., lowercase all entries)", "Impute missing numeric values in 'time_on_site_seconds' and 'page_views' using median imputation", "Convert 'is_returning' and 'target_purchase' columns to boolean and integer types respectively", "One-hot encode categorical variables: 'device_type', 'browser', and 'referrer'", "Split dataset into train (80%) and test (20%) sets stratified by 'target_purchase'", "Standardize numeric features: 'time_on_site_seconds', 'page_views', and 'cart_value'", "Train a RandomForestClassifier on the training set", "Perform grid search over n_estimators and max_depth hyperparameters using 5-fold cross-validation", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate feature importance rankings from the best model"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.78, "f1": 0.8, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}, "top_feature_importances": {"cart_value": 0.25, "time_on_site_seconds": 0.2, "is_returning_True": 0.15, "device_type_Mobile": 0.1, "page_views": 0.08}}}
{"purpose": "Predict whether a manufactured part will pass quality inspection based on sensor readings and production parameters.", "raw_table": "Part_ID,Machine_ID,Operator_Shift,Temperature,Pressure,Material_Type,Defect\n1,M01,Day,75.2,30.1,TypeA,Pass\n2,M02,Night,77.5,29.8,typea,Pass\n3,M01,Day,?,30.5,TypeB,Fail\n4,M03,Day,74.8,28.7,TypeA,Pass\n5,M02,Night,76.1,err,TypeB,Fail\n6,M03,Day,75.0,30.0,TypeC,Pass\n7,M01,Night,75.3,29.9,typec,Fail\n8,M02,Day,74.9,30.2,TypeB,Pass\n9,M03,Night,75.5,29.7,TypeA,Fail\n10,M01,Day,75.1,30.3,TypeB,Pass\n11,M02,Night,missing,30.0,TypeC,Fail\n12,M03,Day,74.7,28.9,TypeA,Pass\n13,M01,Night,75.4,29.5,TypeA,Fail\n14,M02,Day,76.0,30.4,TypeB,Pass\n15,M03,Night,74.6,29.8,TypeC,Pass", "model_steps": ["Replace missing and erroneous values in numeric columns (Temperature, Pressure) with column median", "Normalize Temperature and Pressure features using standard scaling", "Standardize capitalization of Material_Type entries and one-hot encode Material_Type and Operator_Shift categorical variables", "Split data into train and test sets with 80% training data and 20% test data", "Train a RandomForestClassifier using training data to predict Defect (Pass/Fail)", "Perform grid search over number of trees (n_estimators) and max_depth hyperparameters", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix for test set predictions", "Identify top 3 important features from the trained RandomForest model"], "model_results": {"accuracy": 0.87, "f1": 0.85, "precision": 0.83, "recall": 0.88, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"Pressure": 0.34, "Material_Type_TypeA": 0.22, "Temperature": 0.18}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Build a regression model to estimate house prices based on property features and location.", "raw_table": "id,area_sqft,num_bedrooms,num_bathrooms,neighborhood,year_built,has_garage,flooring_type,price_usd\n1,1400,3,2,Midtown,2005,Yes,Hardwood,350000\n2,850,1,1,Downtown,1998,No,carpet,220000\n3,2000,4,3,suburb,2015,yes,Tile,480000\n4,1100,2,1,Midtown,missing,No,Hardwood,290000\n5,950,2,1,Suburb,2010,No,Carpet,310000\n6,1800,3,2,Midtown,2007,Yes,hardwood,410000\n7,1300,3,2,Downtown,2000,No,Tile,365000\n8,missing,2,1,Suburb,2012,No,Carpet,340000\n9,1700,3,2,midtown,2011,yes,Tile,455000\n10,1000,2,1,Downtown,2003,No,Hardwood,280000\n11,1500,3,2,Suburb,2009,Yes,Carpet,390000\n12,900,1,1,Downtown,2001,No,carpet,240000\n13,1600,4,3,Midtown,2014,Yes,Hardwood,500000\n14,1200,2,2,Suburb,2006,No,Tile,360000", "model_steps": ["Load the dataset and examine it for missing and inconsistent values", "Impute missing numeric values (area_sqft, year_built) using median imputation", "Standardize column names and categorical values (e.g., normalize 'neighborhood' casing, 'has_garage' to boolean)", "One-hot encode categorical variables: neighborhood, flooring_type, has_garage", "Split data into training and testing sets with 80/20 ratio", "Standardize numeric features: area_sqft, num_bedrooms, num_bathrooms, year_built", "Train a RandomForestRegressor on the training set", "Perform grid search over number of estimators [50, 100] and max_depth [5, 10]", "Evaluate model performance on test set using RMSE, MAE, and R-squared", "Extract and analyze top feature importances from the trained model"], "model_results": {"rmse": 21000, "mae": 16000, "r2": 0.85, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}, "top_feature_importances": {"area_sqft": 0.35, "neighborhood_Suburb": 0.2, "year_built": 0.15, "num_bedrooms": 0.1, "has_garage_Yes": 0.08, "flooring_type_Hardwood": 0.07, "num_bathrooms": 0.05}}}
{"purpose": "Build a classification model to predict whether a given day will experience heavy rainfall based on weather and environmental features.", "raw_table": "Date,TemperatureC,HumidityPercent,WindSpeedKmh,WeatherCondition,SoilMoisture,HeavyRain\n2024-04-01,18,65,15,Clear,0.20,No\n2024-04-02,20,70,12,Rain,0.35,Yes\n2024-04-03,22,?,10,RAIN,0.40,yes\n2024-04-04,19,68,8,Cloudy,0.25,No\n2024-04-05,17,72,20,clear,NA,No\n2024-04-06,16,75,18,Rain,0.50,Yes\n2024-04-07,21,60,14,Cloudy,0.30,No\n2024-04-08,23,65,13,Clear,0.22,No\n2024-04-09,24,70,15,Rain,0.45,Yes\n2024-04-10,25,68,16,Clear,0.28,No\n2024-04-11,20,64,12,Cloudy,0.27,No\n2024-04-12,18,69,17,Rain,0.47,Yes\n2024-04-13,19,?,15,cloudy,0.31,No\n2024-04-14,21,73,14,Rain,?,Yes\n", "model_steps": ["Parse Date column and drop it since it's not directly useful for modeling.", "Handle missing values in HumidityPercent and SoilMoisture by imputing median values.", "Normalize capitalization and unify categories in WeatherCondition column (e.g., 'RAIN' and 'Rain' to 'Rain').", "Convert WeatherCondition categorical column into one-hot encoded features.", "Encode target variable HeavyRain: 'Yes' as 1, 'No' as 0 (case insensitive).", "Split dataset into training (80%) and test sets (20%) randomly.", "Standardize numeric features: TemperatureC, HumidityPercent, WindSpeedKmh, SoilMoisture.", "Train a RandomForestClassifier on training data with 100 trees.", "Evaluate model on test set calculating accuracy, F1 score, precision, and recall.", "Extract and rank feature importances from the trained RandomForest model."], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.81, "recall": 0.88, "top_feature_importances": {"HumidityPercent": 0.32, "SoilMoisture": 0.28, "WeatherCondition_Rain": 0.2, "WindSpeedKmh": 0.12, "TemperatureC": 0.08}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features and location.", "raw_table": "PropertyID,Location,Size_sqft,Bedrooms,Bathrooms,Year_Built,Has_Garage,Condition,SalePrice\n1,downtown,850,2,1,1995,yes,Good,350000\n2,Suburbs,1200,3,2,2005,No,excellent,450000\n3,Rural,950,,1,1980,yes,Fair,220000\n4,Downtown,1100,3,2,2010,yes,Good,480000\n5,suburbs,1300,4,2,2018,no,Excellent,520000\n6,Rural,800,2,1,1975,Yes,Poor,180000\n7,Suburbs,1400,4,3,2020,YES,good,600000\n8,DOWNTOWN,1000,3,2,1990,no,Fair,400000\n9,Suburbs,NaN,3,2,2015,No,Good,475000\n10,rural,900,2,1,1985,Yes,poor,210000\n11,Downtown,1150,four,2,2000,yes,Good,490000\n12,Suburbs,1250,3,2,2012,No,Excellent,530000\n13,Rural,850,2,1,1988,yes,Fair,230000\n14,Downtown,1050,3,2,1998,Yes,Good,410000", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Standardize capitalization in categorical columns 'Location', 'Condition', and 'Has_Garage'", "Impute missing numeric values in 'Size_sqft' and 'Bedrooms' using median values", "Convert 'Bedrooms' column entries like 'four' to numeric values", "One-hot encode categorical variables: 'Location', 'Condition', and 'Has_Garage'", "Split the data into training and testing sets with an 80/20 ratio", "Standardize numeric features: 'Size_sqft', 'Bedrooms', 'Bathrooms', 'Year_Built'", "Train a Gradient Boosting Regressor on the training data", "Perform hyperparameter tuning over learning_rate and n_estimators via grid search", "Evaluate the model on test data calculating RMSE, MAE, and R2 score", "Identify and report top 5 feature importances from the model"], "model_results": {"rmse": 28000, "mae": 21000, "r2": 0.85, "top_feature_importances": {"Size_sqft": 0.35, "Location_Suburbs": 0.2, "Condition_Good": 0.15, "Bedrooms": 0.12, "Has_Garage_Yes": 0.08}, "best_hyperparameters": {"learning_rate": 0.1, "n_estimators": 150}}}
{"purpose": "Predict whether a social media post will go viral based on post characteristics and user engagement metrics.", "raw_table": "post_id,post_length,num_hashtags,user_followers,user_verified,post_time,content_type,engagement_score,viral\n1,120,3,1500,True,morning,Image,350,Yes\n2,45,0,230,False,afternoon,Text,40,No\n3,300,5,5000,TRUE,evening,Video,1200,Yes\n4,85,2,800,false,MORNING,Image,200,No\n5,200,1,3000,True,night,Video,800,Yes\n6,110,4,150,false,afternoon,text,50,No\n7,95,NaN,1200,False,morning,Image,180,No\n8,60,1,700,TRUE,evening,Text,90,No\n9,250,6,4500,Yes,night,Video,1300,Yes\n10,70,0,400,False,afternoon,Text,30,No\n11,400,7,10000,True,morning,Video,2500,Yes\n12,80,2,600,False,evening,IMAGE,170,No\n13,150,3,2500,True,Night,Video,900,Yes", "model_steps": ["Load raw CSV data into a DataFrame", "Standardize capitalization and convert 'user_verified' and 'viral' to boolean labels", "Fill missing 'num_hashtags' value with median", "Convert 'post_time' and 'content_type' categorical columns using one-hot encoding", "Split data into train (80%) and test (20%) sets", "Standardize numeric features: post_length, num_hashtags, user_followers, engagement_score", "Train a RandomForestClassifier to predict 'viral' (Yes/No)", "Perform grid search over n_estimators and max_depth parameters using cross-validation", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate and analyze feature importance scores from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.88, "f1": 0.85, "top_feature_importances": {"engagement_score": 0.45, "user_followers": 0.25, "post_length": 0.15, "num_hashtags": 0.08, "content_type_Video": 0.05, "user_verified": 0.02}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Build a regression model to estimate monthly average surface temperature based on climate indicators.", "raw_table": "Region,Month,CO2_Level_ppm,Humidity_Percent,Vegetation_Cover,Precipitation_mm,Avg_Temperature_C\nNorth,Jan,415,45,High,78,2.3\nSouth,Feb,418,55,medium,102,5.1\nEast,Mar,412,NaN,Low,85,7.8\nWest,Apr,420,60,Medium,NaN,12.0\nNorth,May,419,50,High,95,15.2\nSouth,Jun,414,48,high,110,18.5\nEast,Jul,NaN,52,Low,99,21.3\nWest,Aug,417,58,Medium,105,23.7\nNorth,Sep,416,47,High,87,19.8\nSouth,Oct,418,50,Medium,93,14.1\nEast,Nov,413,53,low,80,8.5\nWest,Dec,419,59,Medium,85,4.2\nNorth,Jan,415,44,High,77,2.5\nSouth,Feb,NaN,54,Medium,100,5.0", "model_steps": ["Identify 'Avg_Temperature_C' as the target variable for regression.", "Handle missing values by imputing CO2_Level_ppm with median and Precipitation_mm with mean; impute Humidity_Percent missing values with median.", "Normalize categorical 'Vegetation_Cover' by converting all to lowercase and mapping {'low':0,'medium':1,'high':2}.", "One-hot encode 'Region' and 'Month' categorical variables.", "Split data into training (80%) and test (20%) sets randomly.", "Standardize numeric features: CO2_Level_ppm, Humidity_Percent, Vegetation_Cover, Precipitation_mm.", "Train a RandomForestRegressor model with 100 trees on the training data.", "Perform hyperparameter tuning on max_depth with grid search over [5,10,15].", "Evaluate model performance on test set using RMSE, MAE, and R-squared metrics.", "Extract feature importances from the best model and analyze top 3 features."], "model_results": {"rmse": 1.85, "mae": 1.3, "r2": 0.82, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}, "top_feature_importances": {"CO2_Level_ppm": 0.35, "Precipitation_mm": 0.25, "Month_Mar": 0.15}}}
{"purpose": "Build a classification model to predict the risk of hospital readmission within 30 days for diabetic patients.", "raw_table": "PatientID,Age,Gender,Hemoglobin_A1C,DiabetesType,PreviousAdmissions,MedicationCompliance,Smoker,Readmitted\nP001,54,Male,7.8,Type2,2,High,No,Yes\nP002,67,Female,9.1,type1,1,Medium,Yes,No\nP003,45,Female,8.3,Type2,0,low,No,No\nP004,72,Male,,Type2,3,High,No,Yes\nP005,60,Female,10.0,Type1,4,Medium,Yes,Yes\nP006,59,Male,7.5,Type2,1,High,No,No\nP007,50,Male,8.0,TYPE1,2,Low,No,No\nP008,61,Female,8.8,Type2,NaN,Medium,Yes,Yes\nP009,47,Male,7.2,Type2,1,High,yes,No\nP010,55,Female,9.5,Type1,3,Medium,No,Yes\nP011,65,Male,7.9,Type2,2,High,No,No\nP012,58,Female,8.4,type1,1,Low,No,No", "model_steps": ["Load the dataset and parse CSV data", "Clean the dataset by standardizing 'DiabetesType' values to consistent capitalization", "Impute missing numeric values in 'Hemoglobin_A1C' and 'PreviousAdmissions' with median values", "Convert 'MedicationCompliance', 'Smoker', 'Gender', and 'DiabetesType' to categorical variables and one-hot encode them", "Encode target variable 'Readmitted' as binary (Yes=1, No=0)", "Split data into training (80%) and testing (20%) sets with stratification on target", "Standardize numeric features: 'Age', 'Hemoglobin_A1C', and 'PreviousAdmissions'", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search on max_depth parameter with values [3,5,7]", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.83, "precision": 0.79, "recall": 0.85, "f1": 0.82, "confusion_matrix": [[7, 2], [1, 8]], "top_feature_importances": {"Hemoglobin_A1C": 0.32, "PreviousAdmissions": 0.27, "MedicationCompliance_High": 0.15}, "best_max_depth": 5}}
{"purpose": "Predict whether a customer will make a purchase during a browsing session.", "raw_table": "session_id,customer_age,device_type,browser,session_duration_minutes,num_products_viewed,purchase\n1001,34,Mobile,Chrome,5.5,3,Yes\n1002,27,desktop,Firefox,3.2,1,No\n1003,45,Mobile,Safari,12.0,8,yes\n1004,NaN,Tablet,Chrome,7.1,5,No\n1005,22,Desktop,Edge,2.5,0,No\n1006,38,mobile,chrome,9.3,6,Yes\n1007,29,Tablet,Firefox,4.0,2,No\n1008,41,Desktop,Safari,10.5,7,Yes\n1009,33,Desktop,,8.0,4,No\n1010,50,Mobile,Chrome,15.0,10,YES\n1011,44,Mobile,Firefox,6.5,3,No\n1012,28,tablet,Edge,3.8,1,No\n1013,36,Desktop,Chrome,5.9,2,Yes\n1014,30,Mobile,Safari,11.5,8,yes\n1015,39,Desktop,chrome,7.0,5,Yes", "model_steps": ["Load the CSV dataset and inspect for missing and inconsistent values", "Standardize capitalization in categorical columns 'device_type', 'browser', and 'purchase' to ensure uniformity", "Impute missing customer_age values using the median age from the dataset", "Fill missing 'browser' entries with the mode browser 'Chrome'", "Convert the target variable 'purchase' to binary format: 1 for 'Yes', 0 for 'No'", "One-hot encode categorical variables: 'device_type' and 'browser'", "Split data into training (80%) and testing (20%) sets with stratification on the target variable", "Scale numeric features 'customer_age', 'session_duration_minutes', and 'num_products_viewed' using StandardScaler", "Train a RandomForestClassifier with 100 trees on the training set", "Evaluate model performance on the test set computing accuracy, precision, recall, and F1 score", "Generate a confusion matrix for detailed error analysis"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.82, "f1": 0.83, "confusion_matrix": [[14, 3], [4, 18]], "top_feature_importances": {"num_products_viewed": 0.32, "session_duration_minutes": 0.25, "device_type_Mobile": 0.15, "browser_Chrome": 0.12, "customer_age": 0.08, "device_type_Desktop": 0.05, "browser_Firefox": 0.03}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a field will have high soil moisture levels given weather and soil conditions.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Temperature_C,Humidity_pct,Soil_pH,Soil_Moisture_High\nF01,Loamy,12.5,22,85,6.5,Yes\nF02,Clay,8.0,25,78,5.8,No\nF03,sandy,15.3,20,90,,Yes\nF04,Loamy,NaN,23,88,6.7,yes\nF05,Clay,7.4,24,83,5.9,No\nF06,SANDY,16.0,19,92,6.2,Yes\nF07,Loamy,11.0,22,80,6.6,No\nF08,clay,9.1,26,75,6.0,No\nF09,Sandy,14.8,21,89,6.3,Yes\nF10,Loamy,10.5,23,84,6.4,No\nF11,Clay,8.5,24,79,5.7,No\nF12,Sandy,13.7,20,91,6.1,Yes", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Standardize capitalization in 'Soil_Type' and 'Soil_Moisture_High' columns", "Impute missing Soil_pH value with median Soil_pH", "Convert target variable 'Soil_Moisture_High' to binary (Yes=1, No=0)", "One-hot encode the 'Soil_Type' categorical variable", "Split data into train and test sets with 80% training and 20% testing", "Standardize numeric features: Rainfall_mm, Temperature_C, Humidity_pct, Soil_pH", "Train a RandomForestClassifier with 100 trees on the training set", "Evaluate model performance using accuracy, precision, recall, and F1 score on the test set", "Generate and analyze the confusion matrix", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.83, "precision": 0.86, "recall": 0.8, "f1": 0.83, "confusion_matrix": {"true_positive": 4, "false_positive": 1, "true_negative": 5, "false_negative": 1}, "top_feature_importances": {"Humidity_pct": 0.38, "Rainfall_mm": 0.27, "Soil_Type_Sandy": 0.15}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Build a classification model to predict if a taxi ride will have a surge price applied.", "raw_table": "ride_id,passenger_count,pickup_hour,weather_condition,day_of_week,surge_applied\n1,2,17,Clear,Monday,No\n2,1,8,Rainy,Tuesday,Yes\n3,3,22,clear,Friday,No\n4,2,15,,Wednesday,No\n5,4,9,Cloudy,Thursday,Yes\n6,1,6,Rainy,Saturday,Yes\n7,2,13,Clear,Sunday,No\n8,3,18,cloudy,Monday,No\n9,1,20,Foggy,Friday,Yes\n10,2,23,Foggy,Tuesday,No\n11,2,11,Clear,Wednesday,Yes\n12,3,7,RainY,Thursday,No\n13,1,16,Cloudy,Sunday,Yes\n14,2,14,Clear,Saturday,No", "model_steps": ["Load the dataset from CSV format.", "Identify and handle missing values in the weather_condition column by imputing the most frequent category.", "Standardize inconsistent capitalization in the weather_condition column to lowercase.", "Encode categorical variables: weather_condition and day_of_week using one-hot encoding.", "Convert surge_applied target variable to binary format (Yes=1, No=0).", "Split data into train and test sets with an 80/20 ratio.", "Scale numeric features passenger_count and pickup_hour using standard scaling.", "Train a RandomForestClassifier on the training set.", "Perform grid search over max_depth [3, 5, 7] and n_estimators [50, 100].", "Evaluate the model on the test set by computing accuracy, precision, recall, and F1 score.", "Generate the confusion matrix for test predictions."], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.78, "f1": 0.8, "confusion_matrix": [[7, 2], [1, 4]], "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}, "top_feature_importances": {"pickup_hour": 0.3, "weather_condition_rainy": 0.22, "day_of_week_friday": 0.15, "passenger_count": 0.13, "weather_condition_clear": 0.1}}}
{"purpose": "Predict whether a citizen will comply with tax filing requirements based on demographic and socio-economic data.", "raw_table": "Age,Income,Employment_Status,Education_Level,Marital_Status,Number_of_Dependents,Previous_Compliance,City,Tax_Compliance\n45,55000,Employed,Bachelor,Married,2,Yes,New York,Yes\n38,43000,Self-employed,Master,Single,0,No,los angeles,No\n27,32000,employed,High School,Single,1,Yes,Chicago,Yes\n52,78000,Unemployed,Bachelor,Married,,No,Houston,No\n33,48000,Employed,Master,Single,0,Yes,Phoenix,Yes\n61,62000,Retired,Bachelor,Divorced,3,Yes,Philadelphia,No\n29,40000,Employed,Bachelor,Single,0,No,San Antonio,Yes\n,36000,Employed,High school,Married,1,Yes,San Diego,No\n41,59000,Employed,Master,Married,2,,Dallas,Yes\n35,47000,Self-employed,Bachelor,Single,1,No,San Jose,Yes\n55,67000,Unemployed,High School,Married,2,No,Austin,No\n48,52000,Employed,Master,Divorced,1,Yes,Jacksonville,Yes\n30,45000,Employed,High School,Single,0,No,Ft. Worth,No", "model_steps": ["Load data and identify missing and inconsistent values", "Standardize city names to title case", "Fill missing Age and Number_of_Dependents with median values", "Convert Employment_Status and Education_Level to consistent categories", "Encode categorical variables using one-hot encoding", "Convert Previous_Compliance and Tax_Compliance to binary labels", "Split data into training (80%) and test (20%) sets", "Standardize numeric features like Age and Income", "Train a RandomForestClassifier to predict Tax_Compliance", "Perform grid search over number of trees and max_depth", "Evaluate model performance using accuracy, precision, recall, and F1 score", "Generate confusion matrix on test set"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.79, "recall": 0.84, "confusion_matrix": {"true_positive": 8, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Previous_Compliance_Yes": 0.27, "Income": 0.22, "Employment_Status_Employed": 0.15, "Education_Level_Master": 0.13, "Age": 0.1}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a manufactured part will pass quality inspection based on process parameters and operator details.", "raw_table": "Part_ID, Machine_Temp, Operator_Shift, Material_Type, Cycle_Time, Defect_Flag\n001, 350, Morning, Steel, 45.2, Pass\n002, 360, evening, Aluminum, 47.8, Fail\n003, 355, Morning, steel, 46.1, Pass\n004, missing, Night, Aluminum, 50.5, Fail\n005, 348, Night, Copper, 44.3, Pass\n006, 362, Evening, Copper, 51.0, Fail\n007, 349, Morning, Steel, 44.9, Pass\n008, 351, evening, aluminum, 46.7, Fail\n009, 353, Night, Copper, 45.5, Pass\n010, 359, Morning, Steel, 47.1, Pass\n011, 347, Evening, Copper, 44.0, Fail\n012, 354, Morning, Steel, , Pass\n013, 356, Night, Aluminum, 48.2, Fail\n014, 350, Morning, Steel, 45.0, Pass", "model_steps": ["Identify 'Defect_Flag' as the target variable with classes Pass and Fail", "Handle missing values: impute missing 'Machine_Temp' with median temperature and missing 'Cycle_Time' with median cycle time", "Normalize capitalization inconsistencies in 'Operator_Shift' and 'Material_Type' to ensure standard categories", "One-hot encode categorical variables: 'Operator_Shift' and 'Material_Type'", "Split data into training (80%) and test sets (20%) stratified on 'Defect_Flag'", "Standardize numeric features 'Machine_Temp' and 'Cycle_Time'", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search over max_depth values [5, 10, None] with 5-fold cross-validation", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix to analyze prediction errors"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.79, "recall": 0.83, "confusion_matrix": {"Pass": {"Pass": 7, "Fail": 1}, "Fail": {"Pass": 2, "Fail": 4}}, "top_feature_importances": {"Machine_Temp": 0.35, "Cycle_Time": 0.3, "Material_Type_Steel": 0.15, "Operator_Shift_Morning": 0.1, "Material_Type_Aluminum": 0.1}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict customer churn probability in a telecommunications company.", "raw_table": "CustomerID,Tenure,MonthlyCharges,Contract,PaymentMethod,InternetService,Churn\n001,12,29.85,Month-to-month,Electronic check,Fiber optic,Yes\n002,34,56.95,One year,Mailed check,Dsl,No\n003,1,53.85,Month-to-month,Electronic Check,Fiber optic,yes\n004,45,42.30,Two year,Bank transfer (automatic),DSL,No\n005,2,70.70,Month-to-month,Electronic check,Fiber optic,Yes\n006,8,99.65,Month-to-month,Mailed check,Fiber optic,No\n007,22,89.10,One year,Electronic Check,,No\n008,10,29.75,Month-to-month,Credit card (automatic),Dsl,No\n009,28,104.80,Two year,Electronic check,Fiber optic,No\n010,,56.15,One year,Mailed Check,Dsl,No\n011,5,49.95,Month-to-month,Credit Card (automatic),Fiber optic,Yes\n012,36,42.00,Two Year,Bank Transfer (Automatic),Dsl,No\n013,3,65.65,Month-to-month,Electronic check,Fiber Optic,Yes\n014,60,99.85,Two year,Mailed check,Fiber optic,No", "model_steps": ["Load the dataset from CSV string into a DataFrame", "Clean inconsistent capitalization in categorical columns: normalize 'Electronic check', 'Electronic Check', etc.", "Impute missing values in Tenure with median tenure value", "Impute missing InternetService value with the mode 'DSL'", "Convert target variable 'Churn' to binary numeric (Yes=1, No=0)", "One-hot encode categorical variables: Contract, PaymentMethod, InternetService", "Standardize numeric features: Tenure and MonthlyCharges", "Split the dataset into training (80%) and test sets (20%)", "Train a RandomForestClassifier with default hyperparameters on the training set", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.8, "recall": 0.75, "f1": 0.77, "top_feature_importances": {"Contract_Month-to-month": 0.32, "MonthlyCharges": 0.25, "InternetService_Fiber optic": 0.18}}}
{"purpose": "Predict whether a manufactured part will pass quality inspection based on sensor readings and machine settings.", "raw_table": "PartID,Temperature,Pressure,MachineType,OperatorShift,Defect\n1,350,5.1,TypeA,Morning,No\n2,360,5.5,TypeB,Afternoon,Yes\n3,355,NaN,TypeA,Morning,No\n4,370,5.7,TypeC,morning,Yes\n5,NaN,5.2,TypeB,Night,No\n6,365,5.3,TypeA,Night,Yes\n7,360,5.6,TypeC,Afternoon,No\n8,355,5.4,TYPEB,Morning,Yes\n9,358,5.1,TypeA,Morning,No\n10,362,5.5,TypeC,Night,No\n11,NaN,5.8,TypeB,Afternoon,Yes\n12,368,5.7,TypeA,morning,No\n13,361,NaN,TypeC,Night,Yes\n14,355,5.3,TypeB,Morning,No", "model_steps": ["Load the dataset and identify the target variable 'Defect'.", "Handle missing values in 'Temperature' and 'Pressure' by imputing with column means.", "Normalize capitalization inconsistencies in 'MachineType' and 'OperatorShift' categorical columns.", "One-hot encode 'MachineType' and 'OperatorShift' categorical variables.", "Split data into train (80%) and test sets (20%).", "Standardize numeric features 'Temperature' and 'Pressure' using training data statistics.", "Train a RandomForestClassifier with 100 trees on the training set.", "Perform grid search over max_depth values [5, 10, 15] using 5-fold cross-validation.", "Select the best model based on highest F1 score from cross-validation.", "Evaluate the selected model on the test set to compute accuracy, precision, recall, and F1 score.", "Generate the confusion matrix for test set predictions."], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.81, "recall": 0.85, "confusion_matrix": [[8, 2], [2, 7]], "top_feature_importances": {"Temperature": 0.35, "Pressure": 0.3, "MachineType_TypeA": 0.15, "OperatorShift_Morning": 0.1, "MachineType_TypeC": 0.1}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a bank customer will default on their loan based on demographic and financial features.", "raw_table": "CustomerID,Age,Income,EmploymentStatus,CreditScore,LoanAmount,LoanPurpose,Defaulted\n1,45,55000,Employed,720,15000,Home Improvement,No\n2,32,NaN,Self-Employed,680,12000,Debt Consolidation,Yes\n3,28,42000,employed,700,8000,Car,No\n4,54,80000,Retired,730,20000,Home Improvement,No\n5,38,50000,Employed,,10000,Education,Yes\n6,47,62000,Self-employed,690,13000,Car,No\n7,29,48000,Employed,710,9000,Debt Consolidation,Yes\n8,41,57000,Employed,NaN,15000,Home Improvement,No\n9,36,53000,Employed,705,11000,Education,No\n10,50,62000,,720,14000,Car,Yes\n11,33,49000,Self-employed,695,10000,Debt consolidation,No\n12,44,58000,Employed,715,12000,Home improvement,Yes\n13,31,47000,Employed,690,9000,Car,No\n14,39,,Employed,700,15000,Education,No", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values in Income, CreditScore, and EmploymentStatus by imputing median for numeric and mode for categorical", "Standardize numeric features: Age, Income, CreditScore, and LoanAmount", "Normalize inconsistent capitalization and spelling in EmploymentStatus and LoanPurpose columns", "One-hot encode categorical variables: EmploymentStatus and LoanPurpose", "Split data into training and testing sets with 80% train and 20% test", "Train a RandomForestClassifier to predict 'Defaulted' status", "Perform hyperparameter tuning on number of estimators and max_depth using grid search with cross-validation", "Evaluate model performance using accuracy, precision, recall, and F1 score on the test set", "Generate and display the confusion matrix and feature importance scores"], "model_results": {"accuracy": 0.79, "precision": 0.75, "recall": 0.68, "f1": 0.71, "confusion_matrix": {"true_negatives": 7, "false_positives": 2, "false_negatives": 3, "true_positives": 5}, "top_feature_importances": {"CreditScore": 0.34, "LoanAmount": 0.22, "Income": 0.18, "EmploymentStatus_Self-employed": 0.12, "LoanPurpose_Home Improvement": 0.08}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Predict whether a given day will experience extreme heat conditions based on weather and environmental factors.", "raw_table": "Date,Region,Avg_Temperature_C,Humidity_Percentage,Wind_Speed_kmh,Soil_Type,Precipitation_mm,Extreme_Heat\n2024-06-01,North,32.5,45,12,Sandy,0.0,Yes\n2024-06-02,South,28.1,NaN,8,Loamy,2.5,no\n2024-06-03,East,35.2,40,15,Clay,0,None\n2024-06-04,West,30.0,50,10,loamy,1.0,Yes\n2024-06-05,North,33.8,43,11,SANDY,0.0,YES\n2024-06-06,South,27.5,55,5,Clay,4.0,No\n2024-06-07,East,31.0,48,7,Loamy,0.0,No\n2024-06-08,West,36.5,38,20,Sandy,0.0,yes\n2024-06-09,North,29.0,50,12,Clay,NaN,No\n2024-06-10,South,34.0,44,18,Sandy,0.0,YES\n2024-06-11,East,26.5,60,6,Loamy,5.5,no\n2024-06-12,West,33.0,42,14,Clay,0.0,Yes\n2024-06-13,North,NaN,47,9,Loamy,1.2,No", "model_steps": ["Load the raw CSV data into a DataFrame", "Handle missing values: impute Avg_Temperature_C with median, Humidity_Percentage and Precipitation_mm with mean", "Standardize capitalization and unify 'Extreme_Heat' labels to binary (Yes=1, No=0)", "Convert Soil_Type and Region categorical variables using one-hot encoding", "Split data into training and testing sets with an 80/20 ratio", "Standardize numeric features: Avg_Temperature_C, Humidity_Percentage, Wind_Speed_kmh, and Precipitation_mm", "Train a RandomForestClassifier to predict Extreme_Heat", "Perform grid search over n_estimators and max_depth hyperparameters", "Evaluate model performance with accuracy, precision, recall, and F1 score on test data", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.8, "recall": 0.87, "top_feature_importances": {"Avg_Temperature_C": 0.38, "Humidity_Percentage": 0.22, "Wind_Speed_kmh": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 8}}}
{"purpose": "Predict whether a customer will make a purchase during a visit based on session and user data.", "raw_table": "session_duration,device_type,referrer,page_views,cart_additions,visitor_type,purchase_made\n12.5,mobile,Google,5,1,new,yes\n7.8,Desktop,facebook,3,0,returning,No\n9.0,Mobile,,4,1,New,yes\n15.2,tablet,Direct,8,2,returning,YES\n3.3,desktop,google,1,0,new,no\n8.7,mobile,Facebook,5,0,returning,No\n,desktop,Direct,2,0,new,No\n11.4,Mobile,google,6,1,Returning,yes\n6.1,tablet,facebook,3,0,new,no\n10.0,mobile,Direct,7,2,Returning,Yes\n14.3,desktop,google,8,3,new,yes\n5.5,,Facebook,4,0,returning,no\n7.9,mobile,Google,4,1,New,No\n13.2,tablet,Direct,7,2,returning,Yes\n4.8,desktop,facebook,2,0,new,no", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing and inconsistent values", "Standardize categorical entries in 'device_type', 'referrer', 'visitor_type', and 'purchase_made' columns (e.g., unify capitalization)", "Impute missing values in 'session_duration' with median value and fill missing 'device_type' with mode", "Convert 'purchase_made' to a binary target variable (yes=1, no=0)", "One-hot encode categorical features: 'device_type', 'referrer', 'visitor_type'", "Split data into training (80%) and test sets (20%) preserving class distribution", "Standardize numeric features: 'session_duration', 'page_views', 'cart_additions'", "Train a RandomForestClassifier on the training set", "Perform grid search over 'n_estimators' and 'max_depth' hyperparameters", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.9, "f1": 0.87, "confusion_matrix": {"true_negative": 7, "false_positive": 1, "false_negative": 1, "true_positive": 6}, "top_feature_importances": {"cart_additions": 0.32, "session_duration": 0.28, "visitor_type_returning": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict the likelihood of loan default for customers applying for personal loans.", "raw_table": "CustomerID,Age,Income,EmploymentStatus,LoanAmount,CreditScore,MaritalStatus,LoanDefault\n001,45,55000,Full-time,15000,720,Married,No\n002,32,62000,part-time,12000,680,Single,Yes\n003,29,48000,Full-Time,10000,NaN,Single,No\n004,58,72000,Unemployed,20000,650,married,Yes\n005,41,NaN,Full-time,15000,700,Divorced,No\n006,36,58000,Full-time,13000,710,Single,No\n007,50,67000,Full-Time,22000,690,Married,Yes\n008,27,45000,Part-Time,9000,640,Single,No\n009,39,60000,full-time,14000,675,widowed,No\n010,44,59000,Full-time,NaN,700,Married,No\n011,31,53000,Full-Time,11000,660,Single,No\n012,55,80000,Unemployed,25000,630,Married,Yes\n013,48,72000,Full-time,18000,710,Divorced,No", "model_steps": ["Load dataset and identify columns with missing or inconsistent values", "Impute missing numeric values (Income, CreditScore, LoanAmount) using median imputation", "Normalize inconsistent capitalization in categorical columns EmploymentStatus and MaritalStatus", "One-hot encode categorical variables EmploymentStatus and MaritalStatus", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features Age, Income, LoanAmount, CreditScore", "Train a RandomForestClassifier to predict LoanDefault", "Perform grid search for hyperparameters n_estimators and max_depth using 5-fold cross-validation", "Evaluate model on test set computing accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.8, "recall": 0.75, "f1": 0.77, "confusion_matrix": {"true_negatives": 7, "false_positives": 1, "false_negatives": 2, "true_positives": 5}, "top_feature_importances": {"CreditScore": 0.34, "LoanAmount": 0.25, "EmploymentStatus_Full-time": 0.17}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Build a classification model to predict whether a social media post will go viral based on post features and user engagement metrics.", "raw_table": "post_id,post_length,user_followers,user_verified,post_type,time_of_day,hashtags_count,viral\n1,120,1500,Yes,Image,morning,3,Yes\n2,85,300,No,Video,Afternoon,1,No\n3,NaN,2500,yes,text,NIGHT,2,Yes\n4,200,1000,No,Text,morning,,No\n5,65,5000,No,Video,Afternoon,4,Yes\n6,130,NA,Yes,Image,Night,2,No\n7,95,1800,No,video,Morning,3,No\n8,80,2200,No,text,afternoon,1,No\n9,110,1600,Yes,Image,Night,NaN,Yes\n10,150,2100,No,Text,morning,5,Yes\n11,100,1300,No,Video,Afternoon,2,No\n12,75,1900,No,Image,Night,1,No\n13,140,1700,yes,Video,Morning,3,Yes\n14,NaN,2000,No,Text,afternoon,2,No", "model_steps": ["Load dataset and identify missing values and inconsistent capitalization", "Clean 'user_verified' and 'post_type' columns by standardizing capitalization", "Impute missing numeric values in 'post_length' and 'user_followers' with median values", "Fill missing 'hashtags_count' with zero", "Encode 'user_verified' as binary (Yes=1, No=0)", "One-hot encode 'post_type' and 'time_of_day' categorical variables", "Split data into training (80%) and test (20%) sets with stratification on 'viral'", "Standardize numeric features: 'post_length', 'user_followers', 'hashtags_count'", "Train a Random Forest Classifier with 100 trees on the training data", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Extract feature importances from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.8, "f1": 0.815, "top_feature_importances": {"user_followers": 0.32, "post_length": 0.2, "user_verified": 0.15, "hashtags_count": 0.12, "post_type_Video": 0.08, "time_of_day_Morning": 0.05, "time_of_day_Night": 0.04, "post_type_Image": 0.04}}}
{"purpose": "Predict whether a customer will make a purchase during a visit based on their session attributes and demographics.", "raw_table": "session_id,session_duration,country,device,browser,referral_source,num_pages_visited,purchase_made\ns001,300,USA,Mobile,Chrome,Google,5,Yes\ns002,120,uk,Desktop,Firefox,Direct,3,No\ns003,450,,Tablet,chrome,Google,7,Yes\ns004,200,Canada,Mobile,Safari,Facebook,4,No\ns005,abc,USA,Desktop,Chrome,Google,6,Yes\ns006,180,UK,Mobile,,Google,5,No\ns007,220,USA,Laptop,Edge,Direct,3,Yes\ns008,150,Canada,Mobile,Chrome,Facebook,4,No\ns009,300,Usa,Desktop,chrome,Google,8,Yes\ns010,90,UK,Tablet,Firefox,,2,No\ns011,330,Canada,Mobile,Safari,Google,7,Yes\ns012,250,,Desktop,Chrome,Direct,6,No\ns013,400,USA,Mobile,Chrome,Google,7,Yes\ns014,210,UK,Mobile,Firefox,Facebook,5,No\ns015,100,USA,Tablet,Chrome,Google,3,Yes", "model_steps": ["Load the CSV data into a dataframe", "Correct inconsistent capitalization in 'country' and 'browser' columns", "Convert 'session_duration' to numeric, handling or imputing non-numeric or missing values", "Impute missing values in 'country' and 'browser' with the mode", "Encode categorical variables: 'country', 'device', 'browser', 'referral_source' using one-hot encoding", "Convert target variable 'purchase_made' to binary (Yes=1, No=0)", "Split data into training (80%) and testing (20%) sets with stratification on the target", "Standardize numeric features: 'session_duration' and 'num_pages_visited'", "Train a RandomForestClassifier with 100 trees on the training set", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.87, "precision": 0.84, "recall": 0.9, "f1": 0.87, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"num_pages_visited": 0.28, "session_duration": 0.22, "referral_source_Google": 0.15}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a manufactured part will be defective based on sensor readings and production conditions.", "raw_table": "Part_ID,Machine_ID,Operator_Shift,Temperature_C,Pressure_psi,Material_Grade,Defect_Status\nP001,M01,Morning,75.2,1020,Grade A,No\nP002,m02,Evening,78.5,1008,grade b,Yes\nP003,M01,Morning,NaN,1015,Grade A,No\nP004,M03,Night,77.0,995,Grade C,Yes\nP005,m02,evening,80.1,1002,Grade B,No\nP006,M01,MORNING,74.9,1018,Grade A,No\nP007,M03,Night,76.3,NaN,Grade C,Yes\nP008,M02,Evening,79.0,1005,Grade B,No\nP009,M01,Morning,75.5,1012,grade a,No\nP010,m03,Night,77.2,998,Grade C,Yes\nP011,M02,Evening,78.8,1003,Grade B,No\nP012,M01,Morning,75.1,1017,Grade A,No\nP013,m02,EVENING,79.5,1007,Grade B,Yes\nP014,M03,Night,76.5,999,Grade C,Yes\nP015,M02,Evening,NaN,1006,Grade B,No", "model_steps": ["Load dataset and identify target variable as Defect_Status", "Clean and standardize categorical variables (e.g., unify capitalization in Machine_ID, Operator_Shift, Material_Grade)", "Impute missing numeric values (Temperature_C and Pressure_psi) using median imputation", "Convert Defect_Status to binary labels (Yes=1, No=0)", "One-hot encode categorical features (Machine_ID, Operator_Shift, Material_Grade)", "Split data into training (80%) and testing (20%) sets with stratification on Defect_Status", "Standardize numeric features (Temperature_C, Pressure_psi) using training set statistics", "Train RandomForestClassifier with 100 trees and max_depth=5 on training data", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Extract and rank feature importances from the trained model"], "model_results": {"accuracy": 0.87, "precision": 0.83, "recall": 0.79, "f1": 0.81, "feature_importances": {"Material_Grade_Grade_C": 0.28, "Pressure_psi": 0.22, "Temperature_C": 0.18, "Operator_Shift_Morning": 0.12, "Machine_ID_M03": 0.1, "Material_Grade_Grade_B": 0.1}}}
{"purpose": "Predict whether a social media post will go viral based on its characteristics and author profile.", "raw_table": "post_id,author_followers,post_length,sentiment,posted_hour,media_type,hashtags_count,viral\n1,1500,120,Positive,14,Image,3,Yes\n2,2300,85,negative,9,video,1,No\n3,800,200,Neutral,20,text,0,No\n4,NaN,60,Positive,18,Image,2,Yes\n5,1200,NaN,NEGATIVE,11,Video,1,No\n6,5000,300,Positive,8,Text,5,Yes\n7,2300,150,,21,image,2,No\n8,1800,110,Neutral,7,Video,NaN,No\n9,NaN,95,Positive,15,Image,3,Yes\n10,3000,130,positive,19,Video,4,Yes\n11,2500,100,Neutral,13,text,2,No\n12,1600,140,Negative,10,Image,3,No\n13,900,80,Positive,22,video,1,No\n14,4000,250,Neutral,16,Image,2,Yes", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing and inconsistent values", "Fill missing numeric values (author_followers, post_length, hashtags_count) using median imputation", "Normalize capitalization in categorical columns such as sentiment and media_type", "Fill missing sentiment values with the mode ('Neutral')", "Convert target variable 'viral' into binary labels (Yes=1, No=0)", "One-hot encode categorical features: sentiment and media_type", "Split the dataset into training (80%) and testing (20%) sets with stratification on the target", "Standardize numeric features: author_followers, post_length, hashtags_count, and posted_hour", "Train a RandomForestClassifier with 100 trees on the training data", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Extract and report the top 3 feature importances from the trained model", "Generate and display the confusion matrix for the test set predictions"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.79, "f1": 0.81, "top_feature_importances": {"author_followers": 0.35, "hashtags_count": 0.22, "sentiment_Positive": 0.18}, "confusion_matrix": {"true_positive": 19, "true_negative": 27, "false_positive": 4, "false_negative": 5}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features.", "raw_table": "HouseID,Bedrooms,Bathrooms,Neighborhood,SizeSqFt,YearBuilt,GarageType,Condition,SalePrice\n1,3,2,OldTown,1500,1995,attached,good,250000\n2,4,3,Northside,2000,2005,Detached,Excellent,320000\n3,2,1,oldtown,900,1980,None,Fair,140000\n4,3,,EastEnd,1300,2010,Attached,good,270000\n5,5,4,NorthSide,2500,2015,detached,Excellent,400000\n6,3,2,EastEnd,NaN,2000,Attached,Good,280000\n7,4,3,OldTown,1800,1990,Attached,Good,310000\n8,3,2,EastEnd,1600,1985,None,Fair,230000\n9,2,1,Northside,1100,1975,none,poor,120000\n10,4,3,OldTown,1900,2000,Detached,Good,305000\n11,3,2,EastEnd,1400,2003,Attached,Good,275000\n12,NaN,1,Northside,1000,1988,Attached,Poor,150000\n13,3,2,OldTown,1700,1998,Attached,Good,290000\n14,4,3,EastEnd,2100,2012,Detached,Excellent,360000\n15,3,2,OldTown,1600,NaN,attached,Fair,265000", "model_steps": ["Load the dataset from CSV string and parse into tabular format", "Standardize capitalization in categorical columns (e.g., convert Neighborhood and GarageType to lowercase)", "Handle missing values: impute numeric missing values with median, categorical missing values with mode", "Convert categorical variables Neighborhood, GarageType, and Condition to one-hot encoded features", "Split the data into training and test sets with 80% training and 20% test", "Standardize numeric features Bedrooms, Bathrooms, SizeSqFt, YearBuilt using z-score normalization", "Train a Gradient Boosting Regressor on the training data", "Tune hyperparameters max_depth and learning_rate using grid search with 5-fold cross-validation", "Evaluate the final model on the test set computing RMSE, MAE, and R-squared", "Extract feature importances from the trained model", "Generate predictions on the test set and compare actual vs predicted SalePrice"], "model_results": {"rmse": 21000, "mae": 16000, "r2": 0.87, "hyperparameters": {"max_depth": 4, "learning_rate": 0.1}, "top_features": {"SizeSqFt": 0.35, "Neighborhood_oldtown": 0.18, "YearBuilt": 0.15, "Bedrooms": 0.12, "Condition_excellent": 0.1}, "predicted_vs_actual_summary": {"mean_actual": 275000, "mean_predicted": 272500, "std_actual": 70000, "std_predicted": 65000}}}
{"purpose": "Predict the probability of extreme heat event occurrence based on daily weather and environmental conditions.", "raw_table": "Date,Region,Avg_Temperature_C,Humidity_Percent,Wind_Speed_kmh,Soil_Moisture_Index,Extreme_Heat_Event\n2024-06-01,North,35.6,45,12.5,0.23,Yes\n2024-06-02,South,28.4,60,8.1,0.47,No\n2024-06-03,East,NaN,55,15.2,0.35,Yes\n2024-06-04,West,32.1,40,9.4,missing,No\n2024-06-05,North,36.3,42,11.7,0.20,Yes\n2024-06-06,south,29.0,58,7.8,0.44,No\n2024-06-07,East,31.5,50,10.3,0.30,Yes\n2024-06-08,West,33.0,48,12.0,0.28,No\n2024-06-09,North,37.2,43,13.5,0.18,Yes\n2024-06-10,South,27.8,62,6.9,0.50,no\n2024-06-11,East,30.2,53,11.1,0.33,Yes\n2024-06-12,West,31.8,,10.7,0.27,No\n2024-06-13,North,35.0,46,12.3,0.22,Yes\n2024-06-14,South,28.7,59,8.3,0.45,No", "model_steps": ["Parse 'Date' column and drop it as it is not needed for modeling", "Standardize capitalization in 'Region' column and treat as categorical variable", "Impute missing numeric values in 'Avg_Temperature_C', 'Humidity_Percent', 'Soil_Moisture_Index' with median values", "Convert target variable 'Extreme_Heat_Event' to binary labels (Yes=1, No=0), handling inconsistent capitalization", "One-hot encode the 'Region' categorical variable", "Split the dataset into train and test sets with an 80/20 ratio", "Standardize numeric features: 'Avg_Temperature_C', 'Humidity_Percent', 'Wind_Speed_kmh', 'Soil_Moisture_Index' using training set statistics", "Train a RandomForestClassifier with 100 trees on the training data", "Perform a grid search to optimize max_depth parameter over values [3, 5, 7]", "Evaluate the model on the test set by computing accuracy, precision, recall, and F1 score", "Generate a confusion matrix for test predictions"], "model_results": {"accuracy": 0.85, "precision": 0.88, "recall": 0.82, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "false_positive": 1, "true_negative": 6, "false_negative": 2}, "top_feature_importances": {"Avg_Temperature_C": 0.45, "Soil_Moisture_Index": 0.22, "Humidity_Percent": 0.18, "Region_North": 0.08, "Wind_Speed_kmh": 0.07}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Build a classification model to predict whether a given day will experience extreme heat based on weather and environmental factors.", "raw_table": "Date,TemperatureC,HumidityPercent,WindSpeedKmh,WeatherCondition,Region,ExtremeHeat\n2024-06-01,35,45,15,Sunny,North,true\n2024-06-02,28,50,10,Cloudy,south,false\n2024-06-03,33,NaN,20,Sunny,North,true\n2024-06-04,30,55,12,Rainy,East,false\n2024-06-05,37,60,8,Sunny,West,true\n2024-06-06,29,65,12,cloudy,South,false\n2024-06-07,40,40,15,Sunny,North,TRUE\n2024-06-08,27,50,5,Rainy,East,false\n2024-06-09,,55,10,Sunny,West,true\n2024-06-10,31,NA,14,Cloudy,North,false\n2024-06-11,36,48,20,Sunny,East,true\n2024-06-12,34,52,18,rainy,West,true\n2024-06-13,26,60,7,Cloudy,South,false\n2024-06-14,38,55,15,Sunny,North,TRUE", "model_steps": ["Parse the Date column to datetime but drop it from modeling as it is not predictive.", "Identify 'ExtremeHeat' as the binary target variable with values true/false (consider case-insensitivity).", "Clean and impute missing numeric values in TemperatureC and HumidityPercent using median imputation.", "Standardize numeric features: TemperatureC, HumidityPercent, WindSpeedKmh.", "Normalize the case of categorical variables WeatherCondition and Region to lowercase.", "One-hot encode categorical variables WeatherCondition and Region.", "Split data into training (80%) and testing (20%) sets stratified on target ExtremeHeat.", "Train a RandomForestClassifier with 100 trees on the training data.", "Perform grid search over max_depth values [5, 10, 15] with 3-fold cross-validation.", "Evaluate the best model on the test set using accuracy, precision, recall, and F1 score.", "Extract and report the top 3 feature importances from the trained model."], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.89, "f1": 0.86, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}, "top_feature_importances": {"temperaturec": 0.42, "humiditypercent": 0.25, "weathercondition_sunny": 0.15}, "confusion_matrix": {"true_positive": 8, "true_negative": 6, "false_positive": 2, "false_negative": 1}}}
{"purpose": "Predict if a given day will experience extreme heat based on weather and environmental factors.", "raw_table": "Date,Temperature_C,Humidity_Percent,Wind_Speed_kmh,Region,Weather_Condition,Soil_Moisture,Extreme_Heat\n2024-06-01,35.2,45,15,North,Sunny,0.12,Yes\n2024-06-02,28.5,60,10,South,Cloudy,0.20,No\n2024-06-03,NaN,55,12,East,Rainy,0.18,No\n2024-06-04,40.1,30,20,west,Sunny,missing,Yes\n2024-06-05,33.3,50,8,North,Sunny,0.10,No\n2024-06-06,36.7,40,14,South,Sunny,0.11,Yes\n2024-06-07,29.0,65,9,East,Rainy,0.19,no\n2024-06-08,38.5,,16,West,Cloudy,0.15,Yes\n2024-06-09,25.4,70,7,North,Rainy,0.22,No\n2024-06-10,41.0,35,18,south,Sunny,0.09,Yes\n2024-06-11,27.8,58,11,East,Cloudy,0.17,No\n2024-06-12,39.2,33,20,West,Sunny,0.13,Yes\n2024-06-13,30.1,55,10,south,Cloudy,0.16,No\n2024-06-14,34.7,48,15,North,Sunny,0.14,Yes", "model_steps": ["Convert 'Date' to datetime and extract day of year as a numeric feature", "Impute missing values in 'Temperature_C' and 'Humidity_Percent' using median values", "Standardize numeric features: Temperature_C, Humidity_Percent, Wind_Speed_kmh, Soil_Moisture, day_of_year", "Normalize capitalization in 'Region' and 'Weather_Condition' columns", "One-hot encode categorical variables: Region and Weather_Condition", "Convert target variable 'Extreme_Heat' to binary (Yes=1, No=0)", "Split data into training (80%) and test sets (20%) with stratification on the target", "Train a RandomForestClassifier with 100 trees on training data", "Perform grid search on RandomForest max_depth with values [5,10,15]", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Output feature importances from the best RandomForest model"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.81, "recall": 0.87, "top_feature_importances": {"Temperature_C": 0.35, "Humidity_Percent": 0.22, "Region_North": 0.1, "Wind_Speed_kmh": 0.08, "Weather_Condition_Sunny": 0.07, "Soil_Moisture": 0.06, "Day_of_Year": 0.05, "Region_South": 0.04, "Weather_Condition_Cloudy": 0.03}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a given day will experience extreme heat conditions based on weather and environmental features.", "raw_table": "Day,Temperature_C,Humidity_Percent,Wind_Speed_kmh,Region,Is_Weekend,Weather_Condition,Extreme_Heat\n1,34.5,40,15,north,Yes,Sunny,No\n2,38.2,35,10,South,No,Sunny,Yes\n3,29.8,50,5,East,No,Rainy,No\n4,42.1,30,12,west,Yes,Sunny,Yes\n5,NaN,45,20,North,No,Cloudy,No\n6,36.0,?,18,East,Yes,sunny,Yes\n7,33.5,55,8,South,No,Rainy,No\n8,41.3,33,14,West,No,Sunny,Yes\n9,28.9,60,7,East,Yes,Cloudy,No\n10,37.7,38,16,South,No,Sunny,Yes\n11,35.5,47,13,north,No,Rainy,No\n12,40.0,31,11,West,Yes,Sunny,Yes\n13,30.2,50,9,East,No,Cloudy,No\n14,39.5,34,15,South,Yes,Sunny,Yes", "model_steps": ["Replace missing numeric values in Temperature_C and Humidity_Percent columns with the column median", "Normalize Temperature_C, Humidity_Percent, and Wind_Speed_kmh features using standard scaling", "Convert Region, Is_Weekend, and Weather_Condition columns to consistent lowercase and fill missing or inconsistent values", "One-hot encode categorical columns: Region, Is_Weekend, Weather_Condition", "Split data into training (80%) and testing (20%) subsets randomly", "Train a RandomForestClassifier to predict Extreme_Heat", "Perform hyperparameter tuning on number of trees (n_estimators) and max_depth using grid search with 5-fold cross-validation", "Evaluate the best model on the test set computing accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Identify and report top 3 feature importances"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": [[5, 1], [2, 6]], "top_feature_importances": {"Temperature_C": 0.42, "Humidity_Percent": 0.25, "Weather_Condition_sunny": 0.15}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Predict likelihood of hospital readmission within 30 days for diabetic patients.", "raw_table": "PatientID,Age,Gender,HbA1c,MedicationType,PreviousAdmissions,Smoker,Readmitted\n001,54,Female,7.5,oral,2,Yes,Yes\n002,67,Male,8.2,Insulin,1,No,No\n003,45,female,6.8,oral,0,yes,No\n004,NaN,Male,9.1,insulin,3,No,Yes\n005,72,FEMALE,7.0,oral,1,No,Yes\n006,39,Male,6.5,Oral,0,Yes,No\n007,50,Male,8.7,Insulin,2,No,Yes\n008,60,Female,missing,oral,1,No,No\n009,58,Female,7.9,Insulin,2,,Yes\n010,66,Male,7.1,Oral,1,Yes,No\n011,47,Male,8.0,oral,1,No,No\n012,53,Female,7.4,INSULIN,0,Yes,Yes\n013,61,Male,8.3,oral,3,No,Yes\n014,65,Female,7.2,oral,2,Yes,No", "model_steps": ["Load data and identify missing values in 'Age' and 'HbA1c' columns", "Impute missing numeric values using median imputation", "Normalize 'HbA1c' values to float and handle 'missing' string as NaN before imputation", "Standardize numeric features 'Age', 'HbA1c', and 'PreviousAdmissions'", "Normalize inconsistent capitalization in categorical columns 'Gender', 'MedicationType', and 'Smoker'", "Encode categorical variables using one-hot encoding", "Split data into train and test sets with an 80/20 ratio", "Train a RandomForestClassifier to predict 'Readmitted'", "Perform grid search cross-validation over max_depth and n_estimators hyperparameters", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 important features"], "model_results": {"accuracy": 0.79, "f1": 0.78, "precision": 0.75, "recall": 0.82, "confusion_matrix": [[6, 2], [3, 9]], "top_feature_importances": {"HbA1c": 0.35, "PreviousAdmissions": 0.28, "MedicationType_Insulin": 0.15}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Predict whether a loan application will be approved based on applicant financial and demographic data.", "raw_table": "Applicant_ID,Age,Income,Credit_Score,Employment_Status,Loan_Amount,Loan_Purpose,Loan_Approved\n1,34,75000,680,Employed,15000,Home Improvement,Yes\n2,45,54000,720,Self-employed,20000,Debt consolidation,YES\n3,29,NaN,610,employed,12000,Car Purchase,No\n4,52,67000,580,Unemployed,5000,home improvement,No\n5,40,83000,NaN,Employed,25000,Debt Consolidation,Yes\n6,37,78000,690,Employed,18000,Car purchase,Yes\n7,30,62000,640,Employed,15000,Car Purchase,No\n8,47,90000,700,Self-Employed,22000,Debt consolidation,Yes\n9,28,48000,600,Unemployed,7000,Home Improvement,No\n10,33,72000,655,Employed,16000,Debt Consolidation,YES\n11,50,NaN,670,Self-employed,21000,Car Purchase,No\n12,42,85000,690,Employed,23000,Home Improvement,Yes\n13,38,71000,710,Employed,19000,home improvement,Yes\n14,35,66000,625,Self-Employed,14000,Debt Consolidation,No", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Standardize capitalization and spelling in categorical columns (Employment_Status, Loan_Purpose, Loan_Approved)", "Impute missing numeric values (Income, Credit_Score) using median imputation", "Convert target variable Loan_Approved into binary labels (Yes=1, No=0)", "One-hot encode categorical features Employment_Status and Loan_Purpose", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features (Age, Income, Credit_Score, Loan_Amount) using z-score normalization", "Train a RandomForestClassifier to predict loan approval", "Perform grid search over number of estimators and max_depth hyperparameters", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and feature importance ranking"], "model_results": {"accuracy": 0.79, "precision": 0.82, "recall": 0.75, "f1": 0.78, "confusion_matrix": {"true_positive": 9, "false_positive": 2, "true_negative": 8, "false_negative": 3}, "top_feature_importances": {"Credit_Score": 0.32, "Income": 0.24, "Loan_Amount": 0.15, "Employment_Status_Employed": 0.1, "Loan_Purpose_Debt Consolidation": 0.08}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Predict whether a given day will experience extreme heat based on weather and environmental features.", "raw_table": "Day,Temperature_C,Humidity_Percent,Wind_Speed_kmh,Cloud_Cover,Month,Region,Extreme_Heat\n1,35.2,45,12,Low,July,North,Yes\n2,28.5,60,8,medium,July,South,No\n3,40.1,38,15,Low,August,North,YES\n4,22.3,75,5,High,June,South,No\n5,NaN,50,10,Medium,July,North,Yes\n6,37.4,55,NaN,Low,August,East,yes\n7,30.0,65,9,Medium,August,South,No\n8,33.5,58,11,Medium,July,East,No\n9,41.2,40,14,low,August,North,Yes\n10,29.7,70,7,High,June,East,No\n11,36.0,43,13,Low,July,South,Yes\n12,27.5,68,8,Medium,June,North,No\n13,34.8,52,12,Medium,August,East,Yes\n14,31.0,60,9,Medium,July,South,No", "model_steps": ["Split data into train (80%) and test (20%) sets", "Fill missing numeric values with column median", "Normalize numeric features (Temperature_C, Humidity_Percent, Wind_Speed_kmh)", "Convert 'Cloud_Cover' and 'Month' categorical columns to lowercase and one-hot encode", "Standardize region names to consistent capitalization and one-hot encode", "Convert target variable 'Extreme_Heat' to binary (Yes=1, No=0)", "Train a GradientBoostingClassifier with default parameters", "Perform 5-fold cross-validation to tune learning rate and number of estimators", "Evaluate model on test set using accuracy, F1 score, precision, and recall", "Extract top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.81, "recall": 0.87, "top_features": {"temperature_c": 0.42, "humidity_percent": 0.25, "cloud_cover_medium": 0.12}, "best_hyperparameters": {"learning_rate": 0.1, "n_estimators": 100}}}
{"purpose": "Predict whether a customer will make a purchase during their visit to an ecommerce website.", "raw_table": "session_id,device_type,country,page_views,time_on_site_seconds,referral_source,previous_purchases,purchase\n1,Mobile,US,5,300,Google,2,Yes\n2,Desktop,uk,10,600,Direct,0,No\n3,Tablet,,3,180,Facebook,1,Yes\n4,Mobile,US,NaN,240,google,0,No\n5,Desktop,CA,8,480,Direct,3,Yes\n6,desktop,US,7,500,Google,1,No\n7,Tablet,UK,4,NaN,facebook,0,No\n8,Mobile,CA,6,350,Direct,2,Yes\n9,Mobile,US,9,700,Direct,,Yes\n10,Desktop,US,NaN,400,Facebook,1,No\n11,Tablet,CA,5,NaN,Google,2,Yes\n12,Mobile,us,7,450,Direct,1,No\n13,Desktop,UK,NaN,550,Direct,0,No\n14,Mobile,CA,8,620,Google,3,Yes\n15,Tablet,US,6,300,google,1,No", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Standardize categorical values (e.g., make 'google' and 'Google' consistent, fix country capitalization)", "Impute missing numeric values with median for 'page_views' and 'time_on_site_seconds'", "Impute missing categorical values in 'country' with the mode", "Convert 'purchase' target variable to binary labels (Yes=1, No=0)", "One-hot encode categorical variables: 'device_type', 'country', 'referral_source'", "Split the dataset into training (80%) and testing (20%) sets", "Standardize numeric features: 'page_views', 'time_on_site_seconds', 'previous_purchases'", "Train a RandomForestClassifier with default hyperparameters on the training set", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Extract feature importances from the trained model", "Generate predictions on the test set and compute a confusion matrix"], "model_results": {"accuracy": 0.8, "precision": 0.78, "recall": 0.75, "f1": 0.765, "confusion_matrix": {"true_positive": 9, "true_negative": 7, "false_positive": 3, "false_negative": 3}, "feature_importances": {"time_on_site_seconds": 0.3, "page_views": 0.25, "previous_purchases": 0.15, "device_type_Mobile": 0.1, "referral_source_Google": 0.08, "country_US": 0.07, "device_type_Desktop": 0.05}}}
{"purpose": "Predict whether a day will experience extreme heat based on atmospheric and surface conditions.", "raw_table": "Day,Temperature_C,Humidity_Percent,Wind_Speed_kmh,Cloud_Cover,Region,Extreme_Heat\n1,35,45,15,Clear,north,Yes\n2,28,55,20,partial,South,No\n3,31,NaN,25,CLEAR,south,Yes\n4,22,60,5,Overcast,North,No\n5,39,40,,Clear,East,Yes\n6,27,50,18,partial,East,No\n7,33,48,22,Clear,north,Yes\n8,25,55,10,overcast,West,No\n9,36,42,20,CLEAR,west,Yes\n10,29,50,15,Partial,East,No\n11,38,43,19,Clear,North,Yes\n12,23,60,12,Overcast,South,No\n13,30,53,17,Partial,west,No\n14,34,47,21,clear,East,Yes", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Standardize capitalization of categorical columns (Cloud_Cover and Region) and impute missing Wind_Speed_kmh values with median", "Convert target variable Extreme_Heat to binary (Yes=1, No=0)", "One-hot encode categorical features: Cloud_Cover and Region", "Standardize numeric features: Temperature_C, Humidity_Percent, Wind_Speed_kmh", "Split data into training (80%) and test sets (20%) maintaining class balance", "Train a RandomForestClassifier to predict Extreme_Heat", "Perform grid search over n_estimators and max_depth hyperparameters", "Evaluate model performance with accuracy, F1 score, precision, and recall on test set", "Generate confusion matrix and identify top 3 important features"], "model_results": {"accuracy": 0.86, "f1": 0.87, "precision": 0.89, "recall": 0.85, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_features": ["Temperature_C", "Cloud_Cover_Clear", "Humidity_Percent"], "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Develop a classification model to predict the likelihood of hospital readmission within 30 days for diabetic patients.", "raw_table": "Patient_ID,Age,Sex,Admission_Type,Num_Previous_Admissions,HbA1c,Diabetes_Type,Readmitted\n1,65,M,ELECTIVE,2,7.5,Type 2,Yes\n2,54,F,emergency,1,8.2,Type 1,No\n3,72,F,Emergency,3,NA,Type 2,Yes\n4,60,M,Elective,0,6.9,type 2,No\n5,46,F,URGENT,1,7.1,Type 1,No\n6,59,M,urgent,2,7.9,Type 2,Yes\n7,68,F,Elective,4,8.0,Type 2,Yes\n8,55,M,Emergency,1,7.2,Type 1,No\n9,63,F,elective,2,7.8,Type 2,Yes\n10,70,M,Emergency,3,NA,Type 1,No\n11,49,F,urgent,0,6.8,Type 2,No\n12,75,M,Emergency,5,8.5,Type 2,Yes\n13,58,F,Emergency,2,7.0,type 1,No\n14,62,M,Elective,3,7.6,Type 2,Yes\n15,53,F,URGENT,1,7.3,Type 1,No", "model_steps": ["Handle missing HbA1c values by imputing with the median value.", "Standardize numeric features: Age, Num_Previous_Admissions, HbA1c.", "Normalize categorical variables Admission_Type and Diabetes_Type by converting to lowercase to correct inconsistent capitalization.", "One-hot encode categorical variables: Sex, Admission_Type, Diabetes_Type.", "Split data into training and testing sets with an 80/20 ratio, stratifying by target variable Readmitted.", "Train a RandomForestClassifier to predict hospital readmission.", "Perform hyperparameter tuning using grid search over n_estimators (50, 100) and max_depth (5, 10).", "Evaluate the model using accuracy, precision, recall, and F1 score on the test set.", "Generate and analyze the confusion matrix to assess false positives and false negatives."], "model_results": {"accuracy": 0.8, "precision": 0.78, "recall": 0.85, "f1": 0.81, "confusion_matrix": {"true_positive": 17, "true_negative": 13, "false_positive": 5, "false_negative": 3}, "top_feature_importances": {"HbA1c": 0.35, "Num_Previous_Admissions": 0.25, "Admission_Type_emergency": 0.15, "Age": 0.1, "Diabetes_Type_type 2": 0.08}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Predict customer churn probability for a telecommunications provider to reduce attrition.", "raw_table": "CustomerID,MonthlyCharges,Contract,PaymentMethod,Tenure,TotalCharges,TechSupport,Churn\nC001,29.85,Month-to-month,Electronic check,1,29.85,No,Yes\nC002,56.95,One year,Mailed check,34,1889.5,Yes,No\nC003,53.85,Month-to-month,Electronic Check,2,108.15,No,Yes\nC004,42.30,Two year,Bank transfer,45,1840.75,Yes,No\nC005,,Month-to-month,Credit card (automatic),5,NaN,No,Yes\nC006,70.70,One year,Credit card (automatic),24,1510.2,No,No\nC007,99.65,Two Year,Electronic check,72,6999.3,Yes,No\nC008,89.10,Month-to-month,Bank transfer,3,267.3,No,Yes\nC009,29.75,month-to-month,Electronic Check,2,59.5,No,Yes\nC010,49.95,One year,Mailed Check,28,1398.6,Yes,No\nC011,105.65,Two year,Credit card (automatic),80,8399.2,Yes,No\nC012,81.35,Month-to-month,Electronic check,6,488.1,No,Yes\nC013,39.65,One Year,Mailed check,12,475.8,No,No\nC014,NaN,Month-to-month,Bank Transfer,4,NaN,No,Yes\nC015,29.60,Two year,Electronic check,60,1776.0,Yes,No", "model_steps": ["Load the dataset and identify target variable as 'Churn'.", "Handle missing values in 'MonthlyCharges' and 'TotalCharges' by imputing median values.", "Normalize inconsistent capitalization in categorical columns such as 'Contract', 'PaymentMethod', and 'TechSupport'.", "Encode categorical variables using one-hot encoding.", "Split the data into training and test sets with an 80/20 ratio.", "Standardize numeric features: 'MonthlyCharges', 'Tenure', and 'TotalCharges'.", "Train a RandomForestClassifier model on the training data.", "Perform grid search to tune 'max_depth' and 'n_estimators' hyperparameters.", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score.", "Generate confusion matrix and analyze feature importances."], "model_results": {"accuracy": 0.87, "f1": 0.82, "precision": 0.79, "recall": 0.85, "confusion_matrix": [[12, 3], [4, 16]], "top_feature_importances": {"Contract_Two year": 0.24, "Tenure": 0.21, "TechSupport_Yes": 0.15, "MonthlyCharges": 0.13, "PaymentMethod_Electronic check": 0.1}, "best_hyperparameters": {"max_depth": 7, "n_estimators": 100}}}
{"purpose": "Predict whether a customer will make a purchase during their store visit based on demographics and shopping behavior.", "raw_table": "CustomerID,Age,Gender,Income,StoreVisitDurationMinutes,DayOfWeek,PreviousPurchases,MadePurchase\n1,34,Male,55000,35,Monday,5,Yes\n2,29,Female,NaN,20,Tuesday,2,No\n3,42,Female,72000,45,Wednesday,,Yes\n4,23,Male,31000,30,thursday,0,No\n5,37,Male,48000,15,Friday,3,No\n6,52,Female,61000,50,Monday,7,Yes\n7,47,Male,59000,NaN,Tuesday,4,Yes\n8,31,FeMale,53000,25,Wednesday,1,No\n9,,Female,44000,40,Thursday,3,Yes\n10,28,Male,50000,22,Friday,2,No\n11,36,Male,65000,55,saturday,6,Yes\n12,40,Female,58000,33,Sunday,4,No\n13,45,Male,60000,38,Monday,5,Yes\n14,33,female,49000,27,Tuesday,2,No\n15,50,Male,62000,48,Wednesday,6,Yes", "model_steps": ["Load the CSV data into a dataframe", "Identify and handle missing values in 'Age', 'Income', 'StoreVisitDurationMinutes', and 'PreviousPurchases' columns by median imputation for numeric fields and mode for categorical if needed", "Standardize inconsistent capitalization in 'Gender' and 'DayOfWeek' columns (e.g., 'FeMale', 'female', 'thursday', 'saturday')", "Convert 'MadePurchase' target variable from Yes/No to binary 1/0", "One-hot encode categorical variables 'Gender' and 'DayOfWeek'", "Split data into training (80%) and testing (20%) sets with stratification on the target", "Standardize numeric features: Age, Income, StoreVisitDurationMinutes, PreviousPurchases", "Train a RandomForestClassifier with 100 trees", "Perform grid search over 'max_depth' with values [5, 10, 15]", "Evaluate model on test set by computing accuracy, F1 score, precision, and recall", "Generate confusion matrix for the test predictions"], "model_results": {"accuracy": 0.87, "f1": 0.85, "precision": 0.83, "recall": 0.87, "confusion_matrix": {"true_negative": 12, "false_positive": 3, "false_negative": 2, "true_positive": 18}, "top_feature_importances": {"StoreVisitDurationMinutes": 0.28, "PreviousPurchases": 0.22, "Income": 0.17, "DayOfWeek_Monday": 0.1, "Age": 0.08}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a taxi trip will have a high tip amount (tip >= 20% of fare).", "raw_table": "trip_distance,fare_amount,passenger_count,payment_type,day_of_week,tip_amount_pct,high_tip\n2.5,12.50,1,credit_card,Monday,0.22,1\n1.2,6.00,1,Cash,Tuesday,0.10,0\n3.1,15.00,2,CREDIT_card,Wednesday,0.25,1\n0.9,5.00,1,cash,Thursday,0.15,0\n4.0,20.00,3,Credit_card,Friday,0.30,1\n2.0,10.00,1,Cash,Saturday,missing,0\n5.2,25.00,2,Credit_Card,Sunday,0.18,0\n3.7,18.00,2,cash,Monday,0.20,1\n1.5,7.50,1,Credit_card,Tuesday,0.12,0\n2.9,12.00,1,,Wednesday,0.22,1\n3.3,15.50,2,cash,thursday,0.14,0\n2.7,13.00,1,Credit_card,friday,0.21,1\n1.0,6.00,1,cash,Saturday,0.09,0", "model_steps": ["Load dataset from CSV string into a DataFrame", "Identify and handle missing values in 'tip_amount_pct' and 'payment_type' columns by imputing median and mode respectively", "Normalize inconsistent capitalization in 'payment_type' and 'day_of_week' columns", "One-hot encode categorical variables: 'payment_type' and 'day_of_week'", "Split data into training and testing sets with 80/20 ratio", "Standardize numeric features: 'trip_distance', 'fare_amount', 'passenger_count', and 'tip_amount_pct'", "Train a RandomForestClassifier to predict the binary target 'high_tip'", "Perform grid search cross-validation over 'max_depth' parameter with values [3, 5, 7]", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix for test predictions"], "model_results": {"accuracy": 0.85, "precision": 0.8, "recall": 0.78, "f1": 0.79, "confusion_matrix": [[8, 2], [3, 7]], "top_feature_importances": {"tip_amount_pct": 0.45, "fare_amount": 0.2, "payment_type_credit_card": 0.15, "trip_distance": 0.1, "day_of_week_Friday": 0.05, "passenger_count": 0.05}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Build a classification model to predict whether a day will have extreme heat conditions based on meteorological and environmental data.", "raw_table": "Date,Temperature_C,Humidity_Percent,Wind_Speed_kmh,Region,Weather,Extreme_Heat\n2024-06-01,35.6,45,15,North,Sunny,Yes\n2024-06-02,28.4,55,12,South,Cloudy,No\n2024-06-03,40.2,30,20,North,Sunny,YES\n2024-06-04,33.1,50,,East,Rainy,No\n2024-06-05,NaN,48,10,West,sunny,No\n2024-06-06,38.0,40,18,East,Sunny,Yes\n2024-06-07,29.5,,14,South,Cloudy,No\n2024-06-08,42.3,35,22,North,Sunny,Yes\n2024-06-09,31.0,60,13,West,Rainy,No\n2024-06-10,36.5,45,17,East,Sunny,yes\n2024-06-11,27.8,55,11,South,cloudy,No\n2024-06-12,39.1,38,19,North,Sunny,Yes\n2024-06-13,30.0,50,15,West,Rainy,No", "model_steps": ["Parse the raw CSV data into a DataFrame", "Handle missing values by imputing mean temperature and wind speed, and mode for humidity", "Standardize capitalization in 'Weather' and 'Extreme_Heat' columns to ensure consistency", "Convert 'Extreme_Heat' target variable to binary labels (Yes=1, No=0)", "One-hot encode categorical variables: 'Region' and 'Weather'", "Split the data into train (80%) and test (20%) sets with stratification on target", "Standardize numeric features: Temperature_C, Humidity_Percent, Wind_Speed_kmh", "Train a RandomForestClassifier with 100 trees on the training set", "Tune max_depth parameter using grid search over [5, 10, 15]", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score", "Generate and display the confusion matrix", "Identify and report the top 3 feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": [[7, 2], [1, 8]], "top_feature_importances": {"Temperature_C": 0.45, "Humidity_Percent": 0.2, "Region_North": 0.15}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a social media post will go viral based on post characteristics and user engagement features.", "raw_table": "post_id,post_length,post_type,user_followers,user_verified,day_posted,avg_engagement,last_post_viral\n1,120,Image,1500,Yes,Monday,0.05,Yes\n2,85,video,800,no,Tuesday,0.02,No\n3,200,Text,2300,Yes,Wednesday,0.07,Yes\n4,150,IMAGE,NaN,No,thursday,0.03,No\n5,95,text,1200,Yes,Friday,0.04,No\n6,130,Video,950,Yes,Saturday,0.06,Yes\n7,NaN,Text,1100,No,Sunday,0.01,No\n8,80,video,1050,No,Monday,0.02,No\n9,140,image,2150,Yes,Tuesday,0.08,Yes\n10,100,Text,NaN,No,Wednesday,0.03,No\n11,170,Video,2250,yes,Thursday,0.07,Yes\n12,110,text,980,No,Friday,missing,No\n13,75,image,700,No,Saturday,0.01,No\n14,155,Video,1800,Yes,Sunday,0.06,Yes", "model_steps": ["Identify 'last_post_viral' as the target variable and encode it as binary (Yes=1, No=0)", "Clean data by filling missing numeric values with median and standardize inconsistent capitalization in categorical columns", "Convert 'post_type', 'user_verified', and 'day_posted' into one-hot encoded variables", "Impute missing values in 'avg_engagement' with the mean", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features: 'post_length', 'user_followers', and 'avg_engagement'", "Train a GradientBoostingClassifier on the training set", "Perform grid search to tune learning_rate and n_estimators", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate and analyze confusion matrix", "Extract and report top 3 feature importances"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.8, "f1": 0.81, "confusion_matrix": [[9, 2], [3, 10]], "top_feature_importances": {"avg_engagement": 0.35, "post_type_Video": 0.22, "user_followers": 0.18}, "best_hyperparameters": {"learning_rate": 0.1, "n_estimators": 100}}}
{"purpose": "Build a classification model to predict whether a taxi trip will incur a surge pricing multiplier.", "raw_table": "trip_id,passenger_count,trip_distance,payment_type,pickup_hour,weather,trip_duration,surge_multiplier\n1,2,3.5,card,8,Clear,12,1.0\n2,1,1.2,cash,22,Rain,7,1.5\n3,3,5.1,Card,18,clear,20,1.0\n4,1,0.8,cash,6,Fog,5,1.0\n5,2,2.3,CARD,14,Cloudy,11,1.0\n6,NaN,4.0,card,20,Rain,15,2.0\n7,2,3.7,cash,16,Rain,13,1.0\n8,1,0.5,cash,9,Clear,6,1.0\n9,5,8.4,card,19,Cloudy,25,2.5\n10,2,2.9,CASH,21,fog,10,1.5\n11,1,1.1,card,17,Clear,8,1.0\n12,4,6.2,card,23,Rain,22,3.0\n13,3,3.3,cash,18,Clear,14,1.0\n14,2,2.0,card,7,Clear,10,1.0", "model_steps": ["Load dataset and identify target variable as 'surge_multiplier' with values >1 indicating surge pricing.", "Convert surge_multiplier into binary target: 0 for 1.0, 1 for >1.0", "Handle missing values in 'passenger_count' by imputing with median", "Normalize case inconsistencies in 'payment_type' and 'weather' categorical columns", "One-hot encode 'payment_type' and 'weather'", "Split data into train and test sets with 80% train and 20% test", "Standardize numeric features: 'passenger_count', 'trip_distance', 'pickup_hour', and 'trip_duration'", "Train a RandomForestClassifier with default parameters on the training data", "Perform grid search over n_estimators (50, 100) and max_depth (5, 10)", "Evaluate model on test set measuring accuracy, precision, recall, and F1 score", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.8, "recall": 0.75, "f1": 0.77, "confusion_matrix": {"true_positive": 6, "true_negative": 10, "false_positive": 2, "false_negative": 2}, "top_feature_importances": {"trip_distance": 0.35, "pickup_hour": 0.25, "trip_duration": 0.2}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 10}}}
{"purpose": "Predict the likelihood of extreme rainfall events in a region based on meteorological and geographic features.", "raw_table": "region,avg_temperature,humidity,wind_speed,soil_type,month,extreme_rainfall\nNorth,22.5,80,12,Clay,Jan,Yes\nsouth,25.3,65,8,Sand,Feb,No\nEast,19.8,90,15,Loam,Mar,Yes\nWest,21.0,,10,Clay,Apr,No\nNorth,23.1,78,11,loam,May,Yes\nSouth,24.7,70,9,Sand,,No\nEast,20.5,85,14,Clay,Jul,Yes\nwest,22.0,82,13,Clay,Aug,No\nNorth,21.8,79,12,Sand,Sep,Yes\nSouth,26.1,68,7,Loam,Oct,No\nEast,19.9,88,16,Loam,Nov,Yes\nWest,21.4,81,12,Clay,Dec,No\nNorth,22.7,77,11,Sand,Jan,Yes", "model_steps": ["Standardize capitalization for the 'region' and 'soil_type' categorical columns", "Impute missing humidity values using median humidity of the corresponding region", "Fill missing 'month' entries with the mode month", "One-hot encode categorical variables: region, soil_type, month", "Split data into training and test sets using 80/20 stratified split on target 'extreme_rainfall'", "Standardize numeric features: avg_temperature, humidity, wind_speed", "Train a RandomForestClassifier to predict 'extreme_rainfall'", "Perform grid search over number of estimators [50, 100] and max_depth [3, 5]", "Evaluate model on test set computing accuracy, F1 score, precision, and recall", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.83, "f1": 0.81, "precision": 0.79, "recall": 0.84, "top_feature_importances": {"humidity": 0.31, "month_Jan": 0.18, "soil_type_Clay": 0.14}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a given day will experience extreme heat conditions based on meteorological factors.", "raw_table": "day,temp_max,temp_min,humidity,wind_direction,precipitation,region,extreme_heat\n1,35.2,22.1,45,NE,0.0,North,Yes\n2,28.5,19.0,55,SW,0.2,South,No\n3, ,18.5,60,N,0.0,East,No\n4,40.1,25.3,40,NE,0.0,North,Yes\n5,33.0,20.0,NaN,E,0.0,west,Yes\n6,29.5,21.2,50,SW,0.0,South,No\n7,31.2,20.9,48,north,0.1,North,Yes\n8,26.7,17.8,65,SE,0.5,East,No\n9,37.5,23.0,42,NE,0.0,North,Yes\n10,30.0,19.5,55,SW,0.0,South,No\n11,34.0,22.0,47,NE,0.0,North,Yes\n12,27.5,18.0,58,S,0.3,East,No\n13,36.0,24.0,40,NE,0.0,North,Yes\n14,29.0,19.0,52,SW,0.0,south,No", "model_steps": ["Load the CSV data into a DataFrame", "Handle missing values by imputing temp_max with the column median and humidity with the mean", "Normalize capitalization and inconsistent entries in categorical columns 'wind_direction' and 'region'", "One-hot encode the categorical variables 'wind_direction' and 'region'", "Split data into 80% training and 20% testing sets, preserving the distribution of the target variable 'extreme_heat'", "Standardize numeric features: temp_max, temp_min, humidity, precipitation", "Train a RandomForestClassifier on the training set with 100 trees", "Perform grid search to tune max_depth parameter over values [5, 10, 15]", "Evaluate the final model on the test set calculating accuracy, F1-score, precision, and recall", "Generate and analyze the confusion matrix", "Extract and report top 3 feature importances"], "model_results": {"accuracy": 0.86, "f1": 0.88, "precision": 0.91, "recall": 0.85, "confusion_matrix": {"true_negative": 5, "false_positive": 1, "false_negative": 2, "true_positive": 6}, "top_feature_importances": {"temp_max": 0.42, "humidity": 0.25, "region_North": 0.15}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a social media post will go viral based on post and user attributes.", "raw_table": "post_id,user_followers,user_verified,post_length,post_time,post_type,hashtags_count,engagement_viral\n1,1500,True,120,morning,text,3,Yes\n2,300,False,45,Afternoon,Image,0,no\n3,12000,true,300,EVENING,Video,5,YES\n4,5000,False,200,night,Text,2,No\n5,850,,150,morning,image,1,yes\n6,20000,True,400,afternoon,Video,8,Yes\n7,700,False,,evening,text,0,No\n8,4300,False,180,Morning,Image,2,No\n9,10000,TRUE,250,Night,Video,4,YES\n10,5600,false,100,Afternoon,Text,1,no\n11,3200,True,90,morning,image,0,Yes\n12,150,False,70,morning,text,0,No\n13,9000,True,350,evening,Video,6,Yes\n14,2700,False,130,Night,image,2,No", "model_steps": ["Load the dataset and inspect for missing and inconsistent values", "Standardize capitalization in categorical fields like 'user_verified', 'post_time', 'post_type', and 'engagement_viral'", "Impute missing values in 'post_length' with median value", "Convert 'user_verified' and 'engagement_viral' to boolean labels (target: 'Yes'->1, 'No'->0)", "One-hot encode categorical variables 'post_time' and 'post_type'", "Split data into training and testing sets with 80% for training and 20% for testing", "Standardize numeric features: 'user_followers', 'post_length', 'hashtags_count'", "Train a RandomForestClassifier to predict 'engagement_viral'", "Perform grid search over 'max_depth' parameter with values [3, 5, 7]", "Evaluate model performance using accuracy, precision, recall, and F1 score on test set", "Compute confusion matrix to analyze false positives and false negatives"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.8, "recall": 0.87, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"user_followers": 0.35, "post_length": 0.25, "post_type_Video": 0.15, "user_verified_True": 0.1, "hashtags_count": 0.08, "post_time_Morning": 0.07}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a government grant application will be approved based on applicant and project characteristics.", "raw_table": "Applicant_ID,Applicant_Age,Applicant_Gender,Project_Type,Requested_Amount,Previous_Grants,Approval_Status\n1,45,Male,Education,50000,2,Approved\n2,38,Female,Health,75000,0,Denied\n3,29,FEMALE,Infrastructure,100000,1,Approved\n4,50,male,Education,,3,Approved\n5,34,Female,Health,60000,0,Denied\n6,47,Male,Environment,85000,1,Approved\n7,42,Female,Education,70000,2,Approved\n8,,Male,Infrastructure,90000,1,Denied\n9,36,female,Environment,65000,0,Denied\n10,40,Male,Health,80000,1,Approved\n11,44,female,Education,55000,2,Approved\n12,39,Male,Health,72000,1,Denied\n13,31,Female,Infrastructure,88000,0,Denied\n14,37,Male,Environment,93000,2,Approved", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values in Applicant_Age and Requested_Amount by imputing with median values", "Standardize capitalization in Applicant_Gender and Project_Type columns", "Convert categorical variables Applicant_Gender and Project_Type to one-hot encoded features", "Split the dataset into training (80%) and test (20%) sets using stratified sampling on Approval_Status", "Standardize numeric features Applicant_Age, Requested_Amount, and Previous_Grants", "Train a RandomForestClassifier to predict Approval_Status", "Perform grid search cross-validation to tune max_depth and n_estimators parameters", "Evaluate the model on the test set calculating accuracy, precision, recall, and F1 score", "Generate a confusion matrix to analyze classification errors", "Identify top 3 most important features from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.83, "f1": 0.85, "confusion_matrix": {"true_positive": 6, "true_negative": 5, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"Requested_Amount": 0.32, "Previous_Grants": 0.25, "Project_Type_Health": 0.18}, "best_hyperparameters": {"max_depth": 7, "n_estimators": 100}}}
{"purpose": "Predict crop yield category (High, Medium, Low) based on soil and weather conditions.", "raw_table": "FarmID,SoilType,Rainfall_mm,Temperature_C,PreviousCrop,YieldCategory\n1,Loam,200,22,Corn,High\n2,Clay,150,25,Wheat,Medium\n3,SAND,180,23,Soybean,Low\n4,Loam,,21,Corn,High\n5,Clay,170,27,Rice,Medium\n6,Loam,190,22,Corn,High\n7,Clay,160,26,Wheat,medium\n8,Sand,175,24,Soybean,Low\n9,Loam,185,23,Corn,High\n10,Clay,150,25,Wheat,Medium\n11,LoaM,195,22,Corn,High\n12,Sand,180,23,Soybean,Low\n13,Clay,165,26,Rice,Medium\n14,Loam,200,21,Corn,High", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing Rainfall_mm value by imputing with median rainfall", "Standardize capitalization and spelling in SoilType and YieldCategory columns to ensure consistency", "Convert YieldCategory to a categorical target variable with classes: High, Medium, Low", "One-hot encode SoilType and PreviousCrop categorical features", "Split data into train and test sets with 80% training and 20% testing", "Standardize numeric features Rainfall_mm and Temperature_C", "Train a RandomForestClassifier to predict YieldCategory", "Perform grid search over max_depth values [3, 5, 7] and n_estimators [50, 100]", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score for each class", "Generate a confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.86, "f1": {"High": 0.9, "Medium": 0.82, "Low": 0.83}, "precision": {"High": 0.88, "Medium": 0.8, "Low": 0.85}, "recall": {"High": 0.92, "Medium": 0.85, "Low": 0.8}, "confusion_matrix": {"High": {"High": 5, "Medium": 0, "Low": 0}, "Medium": {"High": 1, "Medium": 5, "Low": 0}, "Low": {"High": 0, "Medium": 1, "Low": 3}}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}, "top_feature_importances": {"Rainfall_mm": 0.35, "SoilType_Loam": 0.22, "Temperature_C": 0.18, "PreviousCrop_Corn": 0.12, "SoilType_Clay": 0.08}}}
{"purpose": "Predict whether a customer will default on a loan based on their financial and demographic attributes.", "raw_table": "CustomerID,Age,EmploymentStatus,AnnualIncome,LoanAmount,CreditScore,LoanPurpose,Defaulted\n1,45,Employed,55000,15000,720,Home Improvement,No\n2,32,Self-employed,NaN,12000,690,Debt Consolidation,Yes\n3,28,employed,48000,8000,650,Car Purchase,No\n4,54,Unemployed,30000,5000,600,Medical,Yes\n5,40,Employed,62000,20000,NaN,Home Improvement,No\n6,37,employed,58000,NaN,710,Debt consolidation,No\n7,NaN,Self-employed,45000,10000,680,Car purchase,Yes\n8,29,Employed,52000,9000,700,Medical,No\n9,50,Unemployed,40000,7000,640,Home Improvement,Yes\n10,33,Employed,60000,15000,720,Debt Consolidation,No\n11,41,employed,58000,13000,690,Car Purchase,No\n12,36,Self-Employed,53000,11000,710,medical,No\n13,47,Employed,62000,19000,730,Home Improvement,Yes\n14,39,Employed,56000,14000,700,Debt Consolidation,No", "model_steps": ["Load data and identify target variable as 'Defaulted'.", "Clean categorical columns by standardizing capitalization (e.g., EmploymentStatus, LoanPurpose).", "Impute missing numeric values (Age, AnnualIncome, LoanAmount, CreditScore) using median imputation.", "Encode categorical variables EmploymentStatus and LoanPurpose using one-hot encoding.", "Split data into training and test sets using 80/20 split with stratification on the target variable.", "Standardize numeric features to zero mean and unit variance.", "Train a RandomForestClassifier with default hyperparameters on the training data.", "Perform grid search over max_depth in [3, 5, 7] and n_estimators in [50, 100] using 5-fold cross-validation.", "Evaluate the best model on the test set calculating accuracy, precision, recall, and F1 score.", "Generate confusion matrix and plot feature importances."], "model_results": {"accuracy": 0.79, "precision": 0.75, "recall": 0.7, "f1": 0.72, "confusion_matrix": [[7, 2], [3, 5]], "top_feature_importances": {"CreditScore": 0.32, "LoanAmount": 0.22, "AnnualIncome": 0.15, "EmploymentStatus_Employed": 0.1, "LoanPurpose_Home Improvement": 0.08}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
{"purpose": "Build a classifier to predict whether a day will experience high air pollution levels (PM2.5 > 35 \u00b5g/m3) based on weather and traffic conditions.", "raw_table": "Date,Temperature_C,Humidity_percent,Wind_Speed_kmh,Traffic_Volume,Weather_Condition,PM2.5_Level_High\n2024-04-01,18.5,55,12,High,Sunny,No\n2024-04-02,20.1,60,15,Medium,Cloudy,No\n2024-04-03,16.0,58,8,high,Rain,Yes\n2024-04-04,NaN,62,10,Low,Fog,No\n2024-04-05,22.3,55,11,Medium,sunny,No\n2024-04-06,19.8,NA,9,High,Cloudy,Yes\n2024-04-07,17.5,57,7,Low,Rain,No\n2024-04-08,21.0,61,20,Medium,Cloudy,Yes\n2024-04-09,20.5,59,16,HIGH,Fog,Yes\n2024-04-10,18.3,54,14,Low,Sunny,No\n2024-04-11,19.0,56,13,Medium,cloudy,No\n2024-04-12,15.7,60,8,Low,Rain,Yes\n2024-04-13,17.9,58,7,Medium,Fog,No\n2024-04-14,NaN,63,18,High,Sunny,Yes", "model_steps": ["Convert 'Date' to datetime type and drop it from features", "Clean 'Traffic_Volume' categorical column by standardizing capitalization (e.g., 'high' and 'HIGH' to 'High')", "Impute missing values in 'Temperature_C' and 'Humidity_percent' columns using median values", "One-hot encode 'Traffic_Volume' and 'Weather_Condition' categorical variables", "Split dataset into training (80%) and test sets (20%) with stratification on target variable", "Standardize numeric features: 'Temperature_C', 'Humidity_percent', and 'Wind_Speed_kmh'", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search over 'max_depth' parameter with values [5, 10, 15]", "Evaluate the model on test set using accuracy, F1 score, precision, and recall", "Generate confusion matrix and identify top 3 most important features"], "model_results": {"accuracy": 0.83, "f1": 0.79, "precision": 0.75, "recall": 0.84, "confusion_matrix": {"true_positive": 21, "true_negative": 36, "false_positive": 7, "false_negative": 4}, "top_feature_importances": {"Traffic_Volume_High": 0.32, "Humidity_percent": 0.21, "Wind_Speed_kmh": 0.15}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict customer churn probability for a telecom company based on service usage and customer demographics.", "raw_table": "CustomerID,MonthlyCharges,ContractType,TenureMonths,PaymentMethod,InternetService,TechSupport,Churn\n001,79.85,Month-to-month,12,Electronic check,Fiber optic,Yes,Yes\n002,56.95,One year,24,Mailed check,Dsl,No,No\n003,NaN,Month-to-month,3,Bank transfer,No,No,Yes\n004,99.65,Two year,36,Credit card (automatic),Fiber optic,yes,No\n005,89.10,month-to-month,2,Electronic Check,Fiber optic,No,Yes\n006,29.75,One Year,18,Bank transfer,Dsl,No,No\n007,70.35,Month-to-month,NaN,Electronic check,Fiber optic,No,Yes\n008,49.95,One year,30,Mailed Check,Dsl,Yes,No\n009,105.25,Two Year,48,Credit Card (automatic),Fiber Optic,Yes,No\n010,60.00,Month-to-month,5,Electronic Check,Dsl,No,Yes\n011,85.50,One Year,15,Bank Transfer,Fiber Optic,No,No\n012,65.00,Month-to-month,10,Credit card (Automatic),No,No,Yes\n013,45.75,Two year,40,Mailed check,Dsl,Yes,No\n014,NaN,One Year,22,Bank Transfer,Fiber optic,No,No", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing and inconsistent values", "Clean the 'TechSupport' column by standardizing capitalization and filling missing values with 'No'", "Impute missing 'MonthlyCharges' and 'TenureMonths' with median values", "Normalize 'ContractType', 'PaymentMethod', and 'InternetService' categorical values to consistent capitalization", "One-hot encode the categorical variables: ContractType, PaymentMethod, InternetService, TechSupport", "Split data into training (80%) and testing (20%) sets with stratification on the target variable 'Churn'", "Standardize numeric features: MonthlyCharges and TenureMonths using training set statistics", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search over max_depth values [5, 10, 15] using 5-fold cross-validation", "Select the best model based on F1 score and retrain on the full training set", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix and list the top 3 feature importances"], "model_results": {"accuracy": 0.86, "f1": 0.78, "precision": 0.75, "recall": 0.82, "confusion_matrix": {"true_positive": 9, "false_positive": 3, "true_negative": 15, "false_negative": 2}, "top_feature_importances": {"ContractType_Month-to-month": 0.32, "TenureMonths": 0.25, "TechSupport_Yes": 0.18}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a customer will make a repeat purchase within 30 days based on their first order and demographic features.", "raw_table": "CustomerID,Age,Gender,City,OrderAmount,PaymentMethod,DaysSinceSignup,RepeatPurchase\n1,34,Male,New York,120.50,Credit Card,15,Yes\n2,29,Female,los angeles,85.00,PayPal,NaN,No\n3,42,Female,Chicago,NaN,Credit Card,45,No\n4,23,Male,Houston,50.75,Credit card,5,Yes\n5,31,,Seattle,110.00,Debit Card,20,Yes\n6,38,Female,New York,NaN,Credit Card,30,No\n7,28,Male,Los Angeles,95.00,PayPal,18,Yes\n8,35,Female,Chicago,75.25,credit Card,25,No\n9,40,Male,Houston,,Debit Card,10,Yes\n10,27,Female,Seattle,60.00,PayPal,12,No\n11,33,Male,New York,130.00,Credit Card,8,Yes\n12,45,Female,Los Angeles,NaN,PayPal,40,No\n13,22,Male,Chicago,70.00,Credit Card,3,Yes\n14,,Female,Houston,55.50,Debit Card,27,No", "model_steps": ["Load the dataset and identify missing values", "Impute missing numeric values (OrderAmount, Age, DaysSinceSignup) using median", "Standardize the numeric features: Age, OrderAmount, DaysSinceSignup", "Normalize capitalization in categorical columns (Gender, City, PaymentMethod)", "Impute missing categorical Gender values with mode", "One-hot encode categorical variables: Gender, City, PaymentMethod", "Encode target variable RepeatPurchase as binary (Yes=1, No=0)", "Split the data into train and test sets with 80/20 ratio", "Train a Logistic Regression model on the training set", "Evaluate model performance using accuracy, precision, recall, and F1 score on the test set", "Extract feature coefficients to interpret important predictors"], "model_results": {"accuracy": 0.79, "precision": 0.81, "recall": 0.76, "f1": 0.78, "top_features": {"OrderAmount": 0.45, "DaysSinceSignup": -0.3, "Gender_Male": 0.2, "PaymentMethod_Credit Card": 0.15, "City_New York": 0.1}}}
{"purpose": "Build a classification model to predict whether a day will experience extreme heat based on weather and environmental factors.", "raw_table": "Date,TemperatureMaxC,HumidityPercent,WindSpeedKmph,WeatherCondition,Region,ExtremeHeat\n2024-06-01,35.2,45,12,Sunny,North,true\n2024-06-02,28.5,55,8,Rainy,South,false\n2024-06-03,37.8,40,15,Sunny,East,True\n2024-06-04,33.0,,10,Cloudy,west,false\n2024-06-05,40.1,30,5,Sunny,North,TRUE\n2024-06-06,27.4,60,7,Rainy,East,false\n2024-06-07,36.5,42,20,sunny,South,true\n2024-06-08,31.2,50,12,Cloudy,North,false\n2024-06-09,39.0,35,3,Sunny,East,TRUE\n2024-06-10,29.8,55,9,Rainy,South,false\n2024-06-11,38.2,38,11,Sunny,West,true\n2024-06-12,34.9,48,14,Cloudy,North,false\n2024-06-13,25.0,65,6,rainy,East,false\n2024-06-14,41.0,28,4,Sunny,South,True", "model_steps": ["Parse the CSV data and load into a DataFrame", "Clean the 'ExtremeHeat' target column to be boolean (case insensitive conversion)", "Handle missing values by imputing missing HumidityPercent with the median", "Standardize capitalization in categorical columns 'WeatherCondition' and 'Region'", "One-hot encode 'WeatherCondition' and 'Region' categorical variables", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features: TemperatureMaxC, HumidityPercent, WindSpeedKmph", "Train a RandomForestClassifier with default parameters on the training set", "Evaluate model on the test set computing accuracy, F1 score, precision, and recall", "Extract and rank feature importances from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.81, "recall": 0.85, "top_feature_importances": {"TemperatureMaxC": 0.42, "HumidityPercent": 0.25, "WeatherCondition_Sunny": 0.15, "Region_North": 0.1, "WindSpeedKmph": 0.08}}}
{"purpose": "Predict whether a loan applicant will default on their loan within 12 months.", "raw_table": "ApplicantID,Age,Income,EmploymentStatus,CreditScore,LoanAmount,LoanPurpose,Defaulted\n1,34,58000,Full-Time,700,15000,Car,No\n2,45,72000,Part-Time,650,20000,Home Improvement,No\n3,29,54000,full-time,NaN,12000,Car,Yes\n4,54,NaN,Unemployed,600,25000,Medical,Yes\n5,40,66000,Self-Employed,720,18000,Education,No\n6,31,48000,Full-Time,690,13000,Car,No\n7,38,NaN,Full-Time,710,NaN,Vacation,No\n8,27,52000,Part-time,680,11000,Car,Yes\n9,50,75000,Full-Time,740,22000,Home Improvement,No\n10,43,70000,Self-Employed,NaN,20000,Medical,No\n11,35,62000,Full-Time,690,16000,Car,No\n12,28,50000,Unemployed,580,10000,Education,Yes\n13,46,68000,Full-Time,720,21000,Vacation,No\n14,33,59000,Part-Time,670,14000,Car,No", "model_steps": ["Load data and inspect for missing values and inconsistent capitalization", "Correct inconsistent capitalization in EmploymentStatus column", "Impute missing numeric values with median of respective columns", "Encode categorical variables LoanPurpose and EmploymentStatus using one-hot encoding", "Convert target variable Defaulted to binary labels (Yes=1, No=0)", "Split data into training (80%) and testing (20%) sets with stratification on target", "Standardize numeric features: Age, Income, CreditScore, LoanAmount", "Train a RandomForestClassifier with 100 trees on the training set", "Perform grid search over max_depth parameter with values [5, 10, 15]", "Evaluate model on test set computing accuracy, precision, recall, and F1 score", "Generate confusion matrix and extract top 3 feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.8, "recall": 0.75, "f1": 0.77, "confusion_matrix": [[7, 2], [1, 4]], "top_feature_importances": {"CreditScore": 0.32, "LoanAmount": 0.25, "Income": 0.18}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict whether a government grant application will be approved based on application details and applicant profile.", "raw_table": "ApplicationID,ApplicantAge,ApplicantIncome,ProjectType,PreviousGrants,ApplicationStatus\n1,45,55000,Infrastructure,2,Approved\n2,NaN,62000,education,0,Denied\n3,38,48000,Healthcare,1,Approved\n4,50,52000,Infrastructure,3,approved\n5,29,43000,Education,0,Denied\n6,41,not disclosed,Healthcare,2,Approved\n7,36,49000,Healthcare,,Denied\n8,33,47000,Infrastructure,1,Denied\n9,60,52000,education,3,Approved\n10,47,58000,Healthcare,2,Denied\n11,52,60000,Infrastructure,4,Approved\n12,44,54000,education,2,Denied\n13,39,50000,Healthcare,1,Approved\n14,NaN,53000,Infrastructure,2,Denied", "model_steps": ["Load the dataset and examine for missing or inconsistent values", "Standardize capitalization in categorical columns (e.g., ProjectType and ApplicationStatus)", "Impute missing numeric values in ApplicantAge and ApplicantIncome using median values", "Fill missing PreviousGrants with zero as no prior grants", "Encode categorical variables using one-hot encoding (ProjectType)", "Convert target variable ApplicationStatus to binary: Approved=1, Denied=0", "Split data into training and test sets (80% train, 20% test)", "Scale numeric features ApplicantAge, ApplicantIncome, and PreviousGrants using standardization", "Train a RandomForestClassifier on the training set", "Perform hyperparameter tuning on number of trees and max_depth with cross-validation", "Evaluate model performance on the test set using accuracy, F1 score, precision, and recall", "Generate and analyze the confusion matrix to understand types of errors"], "model_results": {"accuracy": 0.82, "f1": 0.79, "precision": 0.81, "recall": 0.77, "confusion_matrix": {"true_positive": 9, "false_positive": 2, "true_negative": 7, "false_negative": 3}, "top_feature_importances": {"PreviousGrants": 0.35, "ApplicantIncome": 0.25, "ApplicantAge": 0.15, "ProjectType_Infrastructure": 0.12, "ProjectType_Healthcare": 0.08, "ProjectType_Education": 0.05}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 7}}}
{"purpose": "Build a regression model to estimate monthly average surface temperature based on climate indicators.", "raw_table": "region,month,co2_level,solar_radiation,precipitation,vegetation_index,temp_avg\nNorth,Jan,415.2,1360,78.5,0.34,2.1\nSouth,Feb,412.8,1345,65.2,0.45,5.8\nEast,Mar,418.0,1380,90.1,0.5,8.2\nWest,Apr,missing,1402,85.3,0.48,11.4\nNorth,May,420.5,NaN,70.0,0.52,15.3\nSouth,Jun,421.7,1415,60.5,0.55,19.6\nEast,Jul,419.3,1420,100.7,NaN,22.3\nwest,Aug,422.1,1398,88.0,0.47,21.1\nNorth,Sep,423.0,1375,75.4,0.43,17.5\nSouth,Oct,417.8,1350,80.2,0.41,12.3\nEast,Nov,416.2,1333,85.7,0.46,7.6\nWest,Dec,414.9,1340,90.0,0.44,3.4\n", "model_steps": ["Load the CSV data into a DataFrame", "Identify and replace missing values in 'co2_level', 'solar_radiation', and 'vegetation_index' with column medians", "Standardize numeric features: 'co2_level', 'solar_radiation', 'precipitation', 'vegetation_index'", "Normalize 'region' categorical variable by fixing inconsistent capitalization and then one-hot encode it", "Split data into training (80%) and testing (20%) sets", "Train a Gradient Boosting Regressor model on the training set", "Tune hyperparameters 'n_estimators' and 'max_depth' using 5-fold cross-validation", "Evaluate model performance on the test set using RMSE, MAE, and R2 metrics", "Analyze feature importances from the trained model"], "model_results": {"rmse": 1.45, "mae": 1.12, "r2": 0.87, "top_feature_importances": {"solar_radiation": 0.38, "vegetation_index": 0.27, "co2_level": 0.21, "precipitation": 0.14}, "best_hyperparameters": {"n_estimators": 150, "max_depth": 4}}}
{"purpose": "Predict whether a customer will make a repeat purchase within 30 days after their first order.", "raw_table": "CustomerID,Age,Gender,FirstPurchaseCategory,OrderValueUSD,DaysToRepeatPurchase,RepeatPurchase\n1001,34,Male,Electronics,120.50,25,Yes\n1002,28,Female,home & kitchen,80.00,,No\n1003,45,Male,Books,15.99,40,No\n1004,23,female,Clothing,45.00,15,Yes\n1005,55,Female,Sports,NaN,5,Yes\n1006,38,Male,ELECTRONICS,200,33,No\n1007,29,,Books,12.50,20,Yes\n1008,41,Female,Toys,60.00,NaN,No\n1009,31,Male,Home & Kitchen,100.00,18,Yes\n1010,27,Female,clothing,35.75,28,No\n1011,36,Male,Sports,150.00,12,Yes\n1012,50,Female,Books,23.45,14,Yes\n1013,30,male,Toys,NaN,NaN,No", "model_steps": ["Load the dataset from CSV and parse missing values.", "Normalize inconsistent capitalization in categorical columns (Gender, FirstPurchaseCategory).", "Impute missing numeric values (OrderValueUSD) using median imputation.", "Impute missing categorical values (Gender) with mode.", "Create binary target variable 'RepeatPurchase' encoded as 1 for Yes and 0 for No.", "Split data into train (80%) and test (20%) sets with stratification on the target variable.", "One-hot encode categorical variables: Gender and FirstPurchaseCategory.", "Standardize numeric features: Age, OrderValueUSD, DaysToRepeatPurchase (imputing missing DaysToRepeatPurchase with median).", "Train a RandomForestClassifier with 100 trees and max_depth=5.", "Evaluate model performance on test set with accuracy, precision, recall, and F1 score.", "Generate and analyze the confusion matrix.", "Extract and report feature importances from the trained model."], "model_results": {"accuracy": 0.83, "precision": 0.81, "recall": 0.85, "f1": 0.83, "confusion_matrix": [[8, 2], [3, 12]], "top_feature_importances": {"OrderValueUSD": 0.25, "DaysToRepeatPurchase": 0.22, "FirstPurchaseCategory_Electronics": 0.15, "Age": 0.12, "Gender_Female": 0.1}, "hyperparameters": {"n_estimators": 100, "max_depth": 5, "random_state": 42}}}
{"purpose": "Predict whether a loan applicant will default within 12 months based on their financial and demographic information.", "raw_table": "Applicant_ID,Income,Employment_Status,Credit_Score,Loan_Amount,Loan_Purpose,Defaulted\n1,55000,Full-Time,720,15000,Home Improvement,No\n2,43000,Part-Time,680,8000,Debt Consolidation,yes\n3,NaN,full-time,610,12000,Car,No\n4,67000,Self-Employed,NaN,20000,Home Improvement,No\n5,39000,Unemployed,590,5000,Car,Yes\n6,48000,Full-Time,700,NaN,Debt consolidation,No\n7,52000,Part-time,690,7000,Home Improvement,No\n8,62000,Full-time,710,14000,Car,No\n9,58000,Self-employed,650,11000,Debt Consolidation,Yes\n10,46000,Full-Time,640,6000,Car,No\n11,45000,Unemployed,N/A,4000,Home Improvement,Yes\n12,53000,Full-Time,705,13000,Debt Consolidation,No\n13,49000,Part-Time,675,9000,Car,Yes\n14,60000,Full-Time,730,16000,Home Improvement,No", "model_steps": ["Load dataset and parse CSV string into a DataFrame", "Standardize capitalization in categorical columns and correct inconsistent entries (e.g., 'full-time' to 'Full-Time')", "Impute missing numeric values using median imputation", "Impute missing categorical values by filling with the mode", "Encode categorical variables using one-hot encoding", "Split data into train and test sets with 80% for training and 20% for testing", "Standardize numeric features to zero mean and unit variance", "Train a RandomForestClassifier with 100 trees on the training data", "Perform grid search tuning max_depth parameter over [5, 10, 15]", "Evaluate model performance using accuracy, precision, recall, and F1 score on the test set", "Generate and analyze the confusion matrix to understand prediction errors", "Identify and report top 3 most important features influencing default prediction"], "model_results": {"accuracy": 0.85, "precision": 0.78, "recall": 0.72, "f1": 0.75, "confusion_matrix": {"true_positive": 18, "true_negative": 63, "false_positive": 5, "false_negative": 7}, "top_feature_importances": {"Credit_Score": 0.35, "Loan_Amount": 0.25, "Employment_Status_Full-Time": 0.12}, "best_hyperparameters": {"max_depth": 10, "n_estimators": 100}}}
{"purpose": "Predict whether a field's soil moisture level is suitable for optimal crop yield.", "raw_table": "FieldID,SoilType,AvgTemp,CropType,Rainfall_mm,SoilMoisture,SoilMoistureSuitable\nF001,Loam,22.5,Corn,120,0.32,Yes\nF002,SAND,25.1,Wheat,85,0.20,No\nF003,Clay,,Soybean,100,0.28,Yes\nF004,Loam,21.3,Corn,NaN,0.35,Yes\nF005,silt,19.8,Wheat,90,0.18,No\nF006,Loam,23.0,Corn,110,0.30,Yes\nF007,Clay,20.5,Soybean,95,missing,Yes\nF008,Sand,24.0,Wheat,88,0.22,No\nF009,Silt,21.7,Corn,105,0.33,Yes\nF010,Clay,22.2,Soybean,98,0.27,Yes\nF011,loam,23.5,Corn,115,0.31,Yes\nF012,sand,19.5,Wheat,82,0.19,No\nF013,Clay,21.0,Soybean,NaN,0.25,Yes\nF014,Loam,22.8,Corn,112,0.34,Yes", "model_steps": ["Load the dataset and identify missing and inconsistent values", "Replace 'missing' and empty values with the median for numeric columns", "Standardize capitalization in categorical columns (e.g., SoilType)", "One-hot encode categorical variables: SoilType and CropType", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features: AvgTemp and Rainfall_mm", "Train a RandomForestClassifier to predict SoilMoistureSuitable", "Perform grid search to tune max_depth parameter of the RandomForest", "Evaluate classification metrics: accuracy, F1-score, precision, recall on test set", "Generate confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.92, "f1": 0.9, "precision": 0.89, "recall": 0.92, "confusion_matrix": {"TruePositive": 7, "TrueNegative": 5, "FalsePositive": 1, "FalseNegative": 1}, "top_feature_importances": {"SoilMoisture": 0.38, "Rainfall_mm": 0.22, "SoilType_Loam": 0.15, "AvgTemp": 0.1, "CropType_Corn": 0.08}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a student will pass the final exam based on study habits and demographic factors.", "raw_table": "StudentID,Age,Gender,Hours_Studied_per_Week,Attendance_Rate,Major,Previous_Grade,Pass_Final\n1,20,Male,12,95,Engineering,B,Yes\n2,21,Female,8,88,Biology,C,No\n3,19,Female,15,92,engineering,a,Yes\n4,22,Male,5,75,Physics,B,No\n5,20,,10,85,Math,C,Yes\n6,23,Female,abc,90,Math,B,Yes\n7,21,Male,9,80,Biology,B,No\n8,20,Female,11,,Engineering,C,Yes\n9,22,Male,7,70,Physics,c,No\n10,21,Female,13,98,math,A,Yes\n11,19,Male,6,65,Biology,b,No\n12,20,Female,14,90,Engineering,A,Yes\n13,22,Male,4,60,Physics,B,No\n14,21,Female,10,85,Math,C,Yes", "model_steps": ["Load the CSV data into a DataFrame", "Clean data by correcting inconsistent capitalization in 'Major' and 'Previous_Grade' columns", "Handle missing values: impute missing 'Gender' with the mode, 'Attendance_Rate' with median, and drop rows with non-numeric 'Hours_Studied_per_Week'", "Encode categorical variables: One-hot encode 'Gender' and 'Major'; label encode 'Previous_Grade'", "Convert target variable 'Pass_Final' to binary (Yes=1, No=0)", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features: 'Age', 'Hours_Studied_per_Week', and 'Attendance_Rate'", "Train a RandomForestClassifier with 100 trees", "Evaluate the model on test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix to analyze classification performance", "Identify and report top 3 feature importances from the RandomForest model"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.89, "f1": 0.86, "confusion_matrix": {"true_positive": 8, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Hours_Studied_per_Week": 0.35, "Attendance_Rate": 0.28, "Previous_Grade": 0.22}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict hourly electricity consumption category (Low, Medium, High) for residential households based on weather and time features.", "raw_table": "Hour,TemperatureC,HumidityPercent,DayType,Holiday,PreviousHourConsumption,ConsumptionCategory\n0,15.2,82,weekday,no,0.35,Low\n1,14.8,85,Weekday,No,0.30,Low\n2,14.5,83,weekday,No,0.28,Low\n3,14.3,80,Weekday,no,0.25,Low\n4,14.0,79,weekday,No,0.27,Low\n5,14.5,MISSING,weekday,No,0.35,Medium\n6,16.0,75,weekday,No,0.50,Medium\n7,18.5,70,Weekday,No,0.75,Medium\n8,21.0,65,Weekday,No,1.10,High\n9,22.5,60,Weekday,No,1.25,High\n10,23.0,58,weekend,No,1.30,High\n11,24.0,55,Weekend,Yes,1.40,High\n12,25.5,50,weekend,Yes,1.35,High\n13,26.0,48,Weekend,Yes,1.20,High\n14,25.8,47,Weekend,Yes,1.10,Medium", "model_steps": ["Load the CSV data into a DataFrame.", "Clean the 'HumidityPercent' column by replacing 'MISSING' values with the median humidity.", "Normalize text in categorical columns 'DayType' and 'Holiday' to lowercase.", "Encode categorical variables 'DayType' and 'Holiday' using one-hot encoding.", "Split the dataset into training (80%) and testing (20%) sets, stratified by 'ConsumptionCategory'.", "Standardize numeric features: 'Hour', 'TemperatureC', 'HumidityPercent', and 'PreviousHourConsumption'.", "Train a RandomForestClassifier to predict 'ConsumptionCategory' using the processed features.", "Perform a grid search to tune the number of trees (n_estimators) and max_depth hyperparameters.", "Evaluate the model on the test set and compute accuracy, precision, recall, and F1 score.", "Generate and analyze the confusion matrix to understand misclassifications.", "Identify top 3 most important features from the trained RandomForest model."], "model_results": {"accuracy": 0.85, "precision": {"Low": 0.88, "Medium": 0.83, "High": 0.86}, "recall": {"Low": 0.9, "Medium": 0.78, "High": 0.87}, "f1": {"Low": 0.89, "Medium": 0.8, "High": 0.86}, "confusion_matrix": {"Low": {"Low": 8, "Medium": 1, "High": 0}, "Medium": {"Low": 2, "Medium": 7, "High": 2}, "High": {"Low": 0, "Medium": 1, "High": 10}}, "top_features": ["PreviousHourConsumption", "Hour", "TemperatureC"], "best_hyperparameters": {"n_estimators": 100, "max_depth": 6}}}
{"purpose": "Predict the likelihood of hospital readmission within 30 days for diabetic patients based on their clinical and demographic data.", "raw_table": "PatientID,Age,Gender,HbA1c,MedicationType,BMI,PreviousAdmissions,Smoker,Readmitted\n1,54,Male,7.8,Insulin,28.5,2,Yes,Yes\n2,62,Female,NaN,Metformin,31.2,1,No,No\n3,45,Male,6.5,Metformin,27.0,0,no,No\n4,70,Female,9.1,insulin,33.1,3,Yes,Yes\n5,38,Male,5.9,Metformin,24.4,0,No,No\n6,50,Female,7.3,Insulin,29.8,1,YEs,No\n7,55,Female,8.0,Metformin,NaN,2,No,Yes\n8,60,Male,7.0,Metformin,30.0,1,No,No\n9,48,Male,7.4,Insulin,26.1,1,no,Yes\n10,59,female,8.3,Insulin,32.7,2,Yes,Yes\n11,65,Male,7.5,Metformin,28.0,2,No,No\n12,52,Female,6.8,Metformin,27.5,1,No,No\n13,43,Female,7.9,Insulin,29.3,0,Yes,Yes\n14,57,Male,7.2,Metformin,31.5,,No,No\n15,49,Male,7.0,Insulin,28.7,1,No,No", "model_steps": ["Load the dataset and inspect for missing values and inconsistent entries", "Correct inconsistent capitalization in 'MedicationType' and 'Smoker' columns", "Impute missing numeric values with column medians (HbA1c and BMI)", "Fill missing categorical value in 'PreviousAdmissions' with median (1)", "Encode 'Gender', 'MedicationType', and 'Smoker' as one-hot vectors", "Convert target variable 'Readmitted' to binary labels (Yes=1, No=0)", "Split the data into training (80%) and test (20%) sets stratified by target", "Standardize numeric features: Age, HbA1c, BMI, PreviousAdmissions", "Train a RandomForestClassifier with 100 trees on the training set", "Perform hyperparameter tuning on max_depth using 5-fold cross-validation", "Evaluate model performance on test set with accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix to assess false positives and negatives"], "model_results": {"accuracy": 0.8, "precision": 0.78, "recall": 0.75, "f1": 0.76, "confusion_matrix": {"true_positive": 6, "true_negative": 6, "false_positive": 2, "false_negative": 2}, "top_feature_importances": {"HbA1c": 0.3, "PreviousAdmissions": 0.22, "MedicationType_Insulin": 0.15, "BMI": 0.1, "Smoker_Yes": 0.08, "Age": 0.07, "Gender_Female": 0.08}, "best_max_depth": 7}}
{"purpose": "Predict whether a social media post will go viral based on its attributes.", "raw_table": "post_id,user_followers,post_length,post_type,day_posted,has_image,engagement_level\n1,1500,120,Text,Monday,yes,high\n2,3000,250,Image,Tuesday,YES,medium\n3,500,80,video,Wednesday,no,low\n4,NaN,200,Text,thursday,yes,medium\n5,1200,150,Image,Friday,No,high\n6,800,100,Text,Saturday,yes,low\n7,4000,300,Video,Sunday,Yes,high\n8,3500,NaN,image,Monday,No,medium\n9,700,110,Text,Tuesday,yes,low\n10,2000,220,VIDEO,Wednesday,no,medium\n11,1000,130,Text,Friday,YES,medium\n12,NaN,90,Image,Sunday,yes,low\n13,2500,180,Text,Monday,No,high\n14,1800,210,Video,Tuesday,yes,medium", "model_steps": ["Handle missing numeric values in user_followers and post_length by imputing median values", "Normalize capitalization inconsistencies in post_type and day_posted columns", "Convert 'has_image' column to binary indicator (yes=1, no=0)", "One-hot encode categorical variables: post_type and day_posted", "Encode target variable engagement_level into numeric classes (low=0, medium=1, high=2)", "Split data into training (80%) and test sets (20%) randomly", "Train a Gradient Boosting Classifier to predict engagement_level", "Tune hyperparameters max_depth and learning_rate using grid search with 5-fold cross-validation", "Evaluate final model on test set computing accuracy, F1 score, precision, and recall", "Generate confusion matrix to analyze misclassification patterns"], "model_results": {"accuracy": 0.79, "f1": 0.77, "precision": 0.76, "recall": 0.75, "best_hyperparameters": {"max_depth": 3, "learning_rate": 0.1}, "top_feature_importances": {"user_followers": 0.35, "post_length": 0.2, "has_image": 0.15, "post_type_Video": 0.12, "day_posted_Tuesday": 0.08}, "confusion_matrix": {"low": {"low": 3, "medium": 1, "high": 0}, "medium": {"low": 1, "medium": 4, "high": 1}, "high": {"low": 0, "medium": 1, "high": 4}}}}
{"purpose": "Predict whether a retail customer will make a purchase during a promotional campaign.", "raw_table": "CustomerID,Age,Gender,AnnualIncome,MembershipLevel,LastPurchaseDaysAgo,PreferredStore,MadePurchase\n1001,34,M,55000,Gold,12,Downtown,Yes\n1002,28,F,NaN,Silver,30,Suburb,No\n1003,45,M,72000,Platinum,5,downtown,Yes\n1004,23,F,31000,Bronze,,Suburb,No\n1005,37,F,48000,Silver,20,Downtown,Yes\n1006,50,M,83000,,15,suburb,Yes\n1007,29,F,40000,Bronze,40,Downtown,No\n1008,41,m,67000,Gold,7,Suburb,Yes\n1009,36,F,52000,Silver,NaN,Downtown,No\n1010,33,,59000,Gold,25,Suburb,No\n1011,47,M,75000,Platinum,3,Downtown,Yes\n1012,39,F,60000,Silver,18,Suburb,No\n1013,31,F,45000,bronze,22,Downtown,No\n1014,44,M,70000,Gold,8,Suburb,Yes", "model_steps": ["Load CSV data into a DataFrame", "Clean 'Gender' column by standardizing capitalization and imputing missing values with mode", "Fill missing 'AnnualIncome' and 'LastPurchaseDaysAgo' with median values", "Standardize the capitalization of 'PreferredStore' and 'MembershipLevel' to ensure consistency", "Encode categorical variables ('Gender', 'MembershipLevel', 'PreferredStore') using one-hot encoding", "Split the dataset into training (80%) and test (20%) sets", "Standardize numeric features ('Age', 'AnnualIncome', 'LastPurchaseDaysAgo') using StandardScaler", "Train a RandomForestClassifier to predict 'MadePurchase'", "Perform grid search over 'n_estimators' and 'max_depth' hyperparameters", "Evaluate the final model on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix and analyze feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": [[10, 3], [2, 15]], "top_feature_importances": {"MembershipLevel_Platinum": 0.22, "LastPurchaseDaysAgo": 0.18, "MembershipLevel_Gold": 0.15, "AnnualIncome": 0.12, "PreferredStore_Downtown": 0.1}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 8}}}
{"purpose": "Predict wheat crop yield (tons per hectare) based on environmental and soil factors.", "raw_table": "Region,Soil_Type,Rainfall_mm,Temperature_C,Seed_Variety,Fertilizer_Used,Yield_tph\nNorth,clay,450,22,VarA,Yes,3.2\nSouth,Sandy,380,26,VArB,No,2.7\nEast,Loam,500,21,VarA,Yes,3.8\nWest,CLAY,420,23,VarC,yes,3.0\nNorth,Loam,NaN,20,VarB,No,3.1\nSouth,Sandy,400,25,VarA,No,2.9\nEast,Loam,480,21,VArB,Yes,3.7\nWest,Clay,430,22,VarC,No,3.1\nNorth,Sandy,410,23,VarA,Yes,3.0\nSouth,Loam,390,24,VarB,No,2.8\nEast,loam,510,20,VarA,Yes,3.9\nWest,Sandy,425,22,VarC,No,3.0\nNorth,Clay,440,23,VarB,Yes,3.3\nSouth,Sandy,395,25,VarA,No,2.6\nEast,Loam,485,21,VArC,Yes,3.5", "model_steps": ["Load CSV data and inspect for missing or inconsistent values", "Standardize capitalization and fix inconsistent entries in Soil_Type and Seed_Variety columns", "Impute missing Rainfall_mm value using the median of the column", "Convert categorical variables Soil_Type, Seed_Variety, and Fertilizer_Used into one-hot encoded features", "Split data into training (80%) and test (20%) sets randomly", "Standardize numeric features Rainfall_mm and Temperature_C using training set statistics", "Train a RandomForestRegressor model to predict Yield_tph on the training set", "Tune max_depth hyperparameter using 5-fold cross-validation on the training set", "Evaluate model performance on the test set calculating RMSE, MAE, and R2", "Identify top 3 important features from the trained model"], "model_results": {"rmse": 0.15, "mae": 0.12, "r2": 0.87, "top_feature_importances": {"Rainfall_mm": 0.38, "Seed_Variety_VarA": 0.25, "Soil_Type_Loam": 0.18}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a citizen will comply with tax filing requirements based on demographic and financial data.", "raw_table": "Age,Income,Employment_Status,Marital_Status,Number_of_Dependents,Previous_Compliance,Region,Filed_Taxes\n34,55000,Employed,married,2,Yes,North,Yes\n45,NaN,Self-Employed,Single,0,No,south,No\n28,42000,employed,Married,1,Yes,East,Yes\n52,72000,Unemployed,Divorced,3,No,West,No\n37,61000,Employed,,2,Yes,North,Yes\n23,39000,Employed,Single,0,Yes,East,Yes\n47,80000,Self-Employed,Married,4,NO,South,No\n31,48000,employed,Single,1,Yes,west,Yes\n55,67000,Unemployed,Married,3,No,North,No\n29,50000,Employed,Single,0,Yes,East,Yes\n42,62000,Employed,married,2,Yes,South,Yes\n38,NaN,Self-employed,Divorced,1,No,West,No\n50,70000,Employed,Married,3,No,North,No\n27,46000,Employed,Single,0,Yes,East,Yes\n44,68000,Self-Employed,Married,3,Yes,South,Yes", "model_steps": ["Load raw CSV data into a dataframe", "Clean and standardize categorical values in Employment_Status and Previous_Compliance (e.g., unify capitalization)", "Impute missing numeric values in Income with median income", "Impute missing categorical values in Marital_Status with the mode", "Encode categorical variables (Employment_Status, Marital_Status, Previous_Compliance, Region) using one-hot encoding", "Split data into training (80%) and testing (20%) sets", "Scale numeric features (Age, Income, Number_of_Dependents) using standardization", "Train a RandomForestClassifier with default hyperparameters to predict Filed_Taxes", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix to analyze prediction errors", "Identify top 3 most important features based on feature importances from the trained model"], "model_results": {"accuracy": 0.87, "precision": 0.85, "recall": 0.9, "f1": 0.87, "confusion_matrix": {"true_positive": 8, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Previous_Compliance_Yes": 0.32, "Income": 0.25, "Employment_Status_Employed": 0.18}}}
{"purpose": "Predict whether a telecommunications customer will churn in the next month based on usage patterns and service attributes.", "raw_table": "customer_id,age,contract_type,monthly_charge,data_usage_gb,customer_support_calls,region,churned\n1,34,Month-to-month,75.5,15.2,2,North,Yes\n2,45,One year,56.0,8.7,0,south,No\n3,29,Month-to-month,NaN,20.5,3,East,yes\n4,53,Two Year,89.3,30.1,1,West,No\n5,40,Month-to-month,65.2,NaN,5,North,Yes\n6,62,One Year,50.1,12.0,0,East,No\n7,37,Month-to-month,70.0,18.3,2,South,Yes\n8,27,,62.5,22.0,4,West,No\n9,48,Two year,88.0,25.5,1,north,No\n10,55,Month-to-Month,72.3,NaN,3,East,yes\n11,33,One year,55.0,10.2,1,South,No\n12,60,Two Year,NaN,28.7,0,West,No\n13,42,Month-to-month,68.7,19.4,3,East,Yes\n14,30,One Year,58.8,15.0,NaN,North,No\n15,50,Month-to-month,73.2,17.8,2,South,Yes", "model_steps": ["Load data and identify missing values and inconsistent capitalization", "Standardize categorical values (e.g., unify 'Month-to-month', 'Month-to-Month', and fix 'One year' vs 'One Year')", "Impute missing numeric values using median values per column", "Fill missing categorical entries with the mode of the respective column", "Convert target variable 'churned' to binary (Yes=1, No=0), handling different capitalizations", "One-hot encode categorical variables: contract_type and region", "Split data into train and test sets (80% train, 20% test) with stratification on target", "Standardize numeric features: age, monthly_charge, data_usage_gb, customer_support_calls", "Train a RandomForestClassifier with 100 trees and max_depth=5", "Evaluate model performance on test set computing accuracy, F1-score, precision, and recall", "Extract top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.87, "f1": 0.82, "precision": 0.8, "recall": 0.85, "top_feature_importances": {"contract_type_Month-to-month": 0.28, "customer_support_calls": 0.25, "monthly_charge": 0.2}}}
{"purpose": "Build a regression model to predict hourly energy consumption of industrial facilities based on environmental and operational factors.", "raw_table": "Facility_ID,Day_of_Week,Temperature_C,Humidity_percent,Shift,Production_Level,Energy_Consumption_kWh\nF001,Monday,22.5,45,Day,High,3500\nF002,Tuesday,19.8,50,Night,Medium,2800\nF003,Wednesday,NaN,55,day,Low,2100\nF004,Thursday,25.1,missing,Day,HIGH,4000\nF005,Friday,23.0,48,Night,Medium,3300\nf006,Saturday,20.5,52,Day,Low,2300\nF007,Sunday,21.7,47,NIGHT,High,3700\nF008,Monday,18.9,50,day,Medium,3100\nF009,Tuesday,22.0,53,Day,,3400\nF010,Wednesday,24.3,49,Night,Medium,3600\nF011,Thursday,21.5,NaN,Day,low,2200\nF012,Friday,20.0,51,Night,Medium,3000\nF013,Saturday,23.7,48,Day,High,3900\nF014,Sunday,19.5,50,Night,Medium,3200", "model_steps": ["Load dataset and inspect for missing and inconsistent values", "Normalize inconsistent capitalization in categorical columns (Shift, Production_Level)", "Impute missing numeric values for Temperature_C and Humidity_percent using median values", "Impute missing categorical values in Production_Level with mode", "One-hot encode categorical variables: Day_of_Week, Shift, Production_Level", "Split data into training (80%) and test (20%) sets randomly", "Standardize numeric features Temperature_C and Humidity_percent", "Train a GradientBoostingRegressor on the training set with default parameters", "Evaluate model performance on the test set using RMSE, MAE, and R2 score", "Extract and report top 3 feature importances from the trained model"], "model_results": {"rmse": 210.5, "mae": 165.3, "r2": 0.89, "top_feature_importances": {"Production_Level_High": 0.32, "Temperature_C": 0.25, "Shift_Day": 0.18}}}
{"purpose": "Predict whether a given day will experience extreme heat based on weather and environmental factors.", "raw_table": "Date,TemperatureMaxC,HumidityPercent,WindSpeedKmH,PrecipitationMm,WeatherCondition,UrbanArea,ExtremeHeat\n2024-06-01,35,45,15,0.0,Sunny,Yes,Yes\n2024-06-02,28,50,10,0.5,rainy,No,No\n2024-06-03,32,55,5,,Cloudy,yes,Yes\n2024-06-04,29,60,12,0.0,SUNNY,No,No\n2024-06-05,41,40,20,0.0,Sunny,Yes,Yes\n2024-06-06,26,65,8,2.0,Rainy,No,No\n2024-06-07,30,70,15,0.1,cloudy,No,No\n2024-06-08,38,35,18,0.0,Sunny,YES,Yes\n2024-06-09,25,80,7,5.5,Rainy,No,No\n2024-06-10,33,50,10,0.0,Cloudy,No,Yes\n2024-06-11,27,55,12,,Sunny,No,No\n2024-06-12,39,42,22,0.0,sunny,Yes,Yes\n2024-06-13,31,60,15,1.0,Cloudy,No,No\n2024-06-14,34,48,20,0.0,Sunny,Yes,Yes", "model_steps": ["Parse the CSV data into a DataFrame and identify columns", "Convert Date column to datetime but exclude from model features", "Fix inconsistent capitalization in WeatherCondition and UrbanArea columns", "Impute missing values in PrecipitationMm with median value", "Encode WeatherCondition and UrbanArea as categorical variables using one-hot encoding", "Define target variable as ExtremeHeat (binary classification)", "Split data into train and test sets with 75% training, 25% testing", "Standardize numeric features TemperatureMaxC, HumidityPercent, WindSpeedKmH, PrecipitationMm", "Train a RandomForestClassifier with 100 trees on the training data", "Evaluate the model on test data with accuracy, precision, recall, and F1 score", "Extract feature importances from the trained RandomForest model", "Generate and display confusion matrix for test predictions"], "model_results": {"accuracy": 0.85, "precision": 0.88, "recall": 0.8, "f1": 0.84, "feature_importances": {"TemperatureMaxC": 0.45, "HumidityPercent": 0.15, "WindSpeedKmH": 0.1, "PrecipitationMm": 0.05, "WeatherCondition_Cloudy": 0.07, "WeatherCondition_Rainy": 0.05, "WeatherCondition_Sunny": 0.08, "UrbanArea_Yes": 0.05}, "confusion_matrix": {"true_positive": 8, "true_negative": 5, "false_positive": 1, "false_negative": 2}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict customer churn probability in a telecommunications company.", "raw_table": "CustomerID,Tenure,MonthlyCharge,ContractType,PaymentMethod,InternetService,Churn\n001,12,70.35,Month-to-month,Electronic check,Fiber optic,Yes\n002,24,50.10,One year,Mailed check,DSL,No\n003,3,85.5,month-to-month,Electronic Check,Fiber Optic,yes\n004,48,45.0,Two year,Bank transfer (automatic),DSL,No\n005,5,95.25,Month-to-month,Credit card (automatic),Fiber optic,Yes\n006,,60.0,One year,Electronic check,DSL,No\n007,36,55.5,TWO YEAR,Mailed check,Fiber optic,No\n008,18,75.0,Month-to-Month,Credit Card (Automatic),Fiber optic,Yes\n009,9,NaN,Month-to-month,Electronic check,DSL,Yes\n010,60,40.0,Two Year,Bank Transfer (Automatic),DSL,No\n011,15,80.0,Month-to-month,Electronic Check,Fiber Optic,Yes\n012,30,55.0,One Year,Mailed Check,DSL,No\n013,6,88.0,Month-to-month,Credit card (automatic),Fiber Optic,Yes\n014,21,49.5,One Year,Bank transfer (automatic),DSL,No", "model_steps": ["Load the dataset and identify missing values in 'Tenure' and 'MonthlyCharge' columns", "Impute missing numeric values using median imputation", "Standardize numeric features 'Tenure' and 'MonthlyCharge'", "Normalize inconsistent capitalization in categorical columns: 'ContractType', 'PaymentMethod', 'InternetService', and 'Churn'", "Encode categorical variables using one-hot encoding", "Split data into train and test sets with a ratio of 80/20", "Train a RandomForestClassifier on the training data", "Perform grid search to tune 'max_depth' parameter with values [5, 10, 15]", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix for the test set predictions"], "model_results": {"accuracy": 0.85, "precision": 0.82, "recall": 0.79, "f1": 0.805, "confusion_matrix": [[7, 2], [2, 8]], "top_feature_importances": {"ContractType_Month-to-month": 0.35, "InternetService_Fiber optic": 0.25, "MonthlyCharge": 0.2, "Tenure": 0.15, "PaymentMethod_Electronic check": 0.05}, "best_hyperparameters": {"max_depth": 10}}}
{"purpose": "Predict crop yield category based on soil and weather conditions.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Avg_Temp_C,NDVI,Previous_Crop,Crop_Yield_Category\nF001,Loam,120,22.5,0.75,Wheat,High\nF002,sandy,95,25.1,0.60,Corn,Medium\nF003,Clay,80,,0.55,Soybean,Low\nF004,Loam,110,21.0,0.70,Wheat,High\nF005,SANDY,NaN,24.3,0.65,Corn,Medium\nF006,Clay,85,20.5,0.50,Soybean,Low\nF007,Loam,130,23.0,,Wheat,High\nF008,Silt,105,22.8,0.68,Corn,Medium\nF009,Clay,90,21.5,0.52,Soybean,Low\nF010,loam,115,22.2,0.72,Wheat,High\nF011,Silt,100,23.4,abc,Corn,Medium\nF012,SANDY,98,24.0,0.63,Corn,Medium\nF013,Clay,82,20.8,0.48,Soybean,Low\nF014,Loam,125,23.5,0.74,Wheat,High", "model_steps": ["Load the dataset from CSV string into a DataFrame", "Handle missing values in 'Avg_Temp_C' and 'NDVI' columns by imputing with median values", "Correct inconsistent capitalization in 'Soil_Type' and 'Previous_Crop' columns by converting to lowercase", "Remove or correct non-numeric 'NDVI' value (e.g., replace 'abc' with median)", "One-hot encode categorical variables 'Soil_Type' and 'Previous_Crop'", "Split data into train and test sets with 80% training and 20% testing", "Standardize numeric features: 'Rainfall_mm', 'Avg_Temp_C', and 'NDVI'", "Train a RandomForestClassifier to predict 'Crop_Yield_Category'", "Perform hyperparameter tuning on 'max_depth' and 'n_estimators' using grid search with cross-validation", "Evaluate model performance on the test set with accuracy, precision, recall, and F1 score metrics", "Generate and analyze the confusion matrix", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.84, "recall": 0.82, "confusion_matrix": {"High": {"High": 5, "Medium": 0, "Low": 0}, "Medium": {"High": 1, "Medium": 4, "Low": 0}, "Low": {"High": 0, "Medium": 1, "Low": 4}}, "top_feature_importances": {"NDVI": 0.32, "Rainfall_mm": 0.28, "Soil_Type_loam": 0.15}, "best_hyperparameters": {"max_depth": 6, "n_estimators": 100}}}
{"purpose": "Predict whether a customer will make a purchase during a promotional campaign.", "raw_table": "CustomerID,Age,Gender,Income,PreviousPurchases,PreferredChannel,DaysSinceLastPurchase,MadePurchase\n1,34,Male,55000,3,Online,5,Yes\n2,27,female,48000,1,In-Store,NaN,No\n3,45,Male,72000,5,online,12,Yes\n4,31,Female,NaN,2,Online,3,Yes\n5,22,M,31000,0,In-Store,45,No\n6,38,FEMALE,60000,4,Online,7,Yes\n7,29,Female,58000,2,IN-STORE,15,No\n8,41,Male,65000,3,Online,NaN,Yes\n9,35,Male,62000,3,Online,10,Yes\n10,28,Female,52000,1,In-Store,20,No\n11,30,Male,50000,2,online,25,No\n12,33,Female,53000,3,IN-STORE,2,Yes\n13,40,Female,61000,4,Online,8,Yes\n14,26,Male,45000,1,In-store,30,No\n15,37,Female,59000,3,Online,6,Yes", "model_steps": ["Load the data and inspect for missing or inconsistent values", "Normalize inconsistent capitalization in categorical fields such as Gender and PreferredChannel", "Impute missing numeric values in Income and DaysSinceLastPurchase using median values", "Encode categorical variables Gender and PreferredChannel using one-hot encoding", "Convert target variable MadePurchase to binary format (Yes=1, No=0)", "Split the dataset into training and testing sets with 80% training and 20% testing", "Standardize numeric features Age, Income, PreviousPurchases, DaysSinceLastPurchase", "Train a RandomForestClassifier on the training data", "Perform grid search to tune max_depth and number of estimators", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix for test predictions"], "model_results": {"accuracy": 0.87, "precision": 0.89, "recall": 0.85, "f1": 0.87, "confusion_matrix": [[12, 3], [2, 18]], "top_feature_importances": {"PreviousPurchases": 0.32, "DaysSinceLastPurchase": 0.25, "Income": 0.18, "PreferredChannel_Online": 0.15, "Age": 0.1}, "best_hyperparameters": {"max_depth": 6, "n_estimators": 100}}}
{"purpose": "Predict whether a social media post will go viral based on its characteristics.", "raw_table": "post_id,post_length,post_type,hashtags_count,user_followers,user_verified,post_time,target_viral\n1,120,Image,5,1500,Yes,Evening,Yes\n2,45,Text,0,300,No,Morning,No\n3,200,Video,10,5000,Yes,NIGHT,Yes\n4,,Text,2,800,No,Afternoon,No\n5,90,Image,Three,1200,No,Morning,No\n6,300,Video,15,10000,Yes,evening,Yes\n7,75,Text,1,400,No,Morning,No\n8,60,Image,0,2000,No,Afternoon,No\n9,180,Video,8,7000,YES,Night,Yes\n10,100,Text,4,1000,No,Morning,No\n11,250,Video,12,8000,Yes,Evening,Yes\n12,85,Image,2,1500,No,Afternoon,No\n13,110,Text,,600,No,Morning,No\n14,130,Image,4,1700,yes,Night,Yes", "model_steps": ["Load data and inspect for missing or inconsistent values", "Clean 'hashtags_count' column by converting non-numeric entries and missing values to 0", "Standardize capitalization of categorical columns 'post_type', 'post_time', and 'user_verified'", "Impute missing numeric values in 'post_length' with median", "Encode categorical variables: one-hot encode 'post_type' and 'post_time', label encode 'user_verified'", "Split data into train and test sets using an 80/20 ratio", "Standardize numeric features 'post_length', 'hashtags_count', and 'user_followers'", "Train a RandomForestClassifier to predict 'target_viral'", "Evaluate model performance using accuracy, F1 score, precision, and recall on the test set", "Extract and report top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.8, "recall": 0.87, "top_feature_importances": {"user_followers": 0.35, "post_type_Video": 0.25, "hashtags_count": 0.15}}}
{"purpose": "Predict whether a student will pass the final exam based on demographic and academic features.", "raw_table": "StudentID,Age,Gender,StudyHours,PreviousGrade,SchoolType,Participation,Passed\n1,17,Male,15,88,Public,Yes,Yes\n2,18,Female,8,,Private,No,No\n3,17,Female,12,75,public,Yes,Yes\n4,16,Male,10,65,Private,yes,No\n5,17,Male,5,55,Public,No,No\n6,18,Female,20,92,Private,Yes,Yes\n7,17,Female,7,70,public,No,No\n8,16,Male,,60,Public,Yes,No\n9,16,Female,14,85,Private,YES,Yes\n10,17,Male,13,80,Public,no,Yes\n11,18,Female,9,77,Private,Yes,No\n12,17,Male,11,68,Public,Yes,No\n13,16,Female,6,50,Public,No,No\n14,18,Male,16,90,private,Yes,Yes\n15,17,Female,10,72,Public,Yes,Yes", "model_steps": ["Load dataset from CSV string", "Identify target variable as 'Passed' and features as remaining columns excluding StudentID", "Handle missing values: impute missing 'StudyHours' with median and 'PreviousGrade' with mean", "Standardize capitalization in categorical columns 'SchoolType' and 'Participation' to ensure consistency", "Encode categorical variables: one-hot encode 'Gender', 'SchoolType', and 'Participation'", "Split data into training (80%) and test (20%) sets with stratification on target variable", "Standardize numeric features 'Age', 'StudyHours', and 'PreviousGrade' using training data statistics", "Train a RandomForestClassifier with 100 trees on the training set", "Tune max_depth hyperparameter by grid search over [3,5,7] using 5-fold cross-validation", "Evaluate model on test set calculating accuracy, precision, recall, and F1 score", "Compute confusion matrix for test set predictions", "Identify top 3 feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.87, "precision": 0.89, "recall": 0.85, "f1": 0.87, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 1, "false_negative": 2}, "top_features": {"StudyHours": 0.35, "PreviousGrade": 0.3, "Participation_Yes": 0.12}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Build a classification model to predict whether a tomato plant will develop late blight disease based on environmental and plant conditions.", "raw_table": "PlantID,SoilPH,AvgTempC,HumidityPercent,RainfallMM,SunlightHours,PlantAgeWeeks,Variety,Disease\n1,6.5,22.1,80,15.2,8,10,roma,Yes\n2,5.8,19.5,75,NA,7,12,Heirloom,No\n3,6.2,23.4,82,20.1,9,8,Roma,Yes\n4,7.0,21.0,77,14.3,8,11,HEIRLOOM,No\n5,6.1,20.0,70,13.8,6,9,roma,No\n6,,22.5,85,18.0,8,10,Heirloom,Yes\n7,6.4,21.3,78,15.0,7,7,Roma,No\n8,5.9,19.8,74,14.7,6,12,heirloom,No\n9,6.3,22.8,83,19.5,9,10,Roma,Yes\n10,6.0,20.5,76,15.1,7,11,Heirloom,No\n11,6.7,21.7,80,16.0,8,9,ROMA,Yes\n12,5.7,19.0,72,13.5,6,13,heirloom,No\n13,6.2,23.1,79,17.2,8,8,Roma,Yes\n14,6.5,21.4,75,14.9,7,10,Heirloom,No", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing or inconsistent values", "Standardize categorical values in 'Variety' column to lowercase for consistency", "Impute missing SoilPH value with median SoilPH of the dataset", "Convert 'Disease' target variable to binary label (Yes=1, No=0)", "Split data into training (80%) and testing (20%) sets with stratification on target", "One-hot encode the 'Variety' categorical variable", "Standardize numeric features: SoilPH, AvgTempC, HumidityPercent, RainfallMM, SunlightHours, PlantAgeWeeks", "Train a RandomForestClassifier with default parameters on the training data", "Perform grid search cross-validation over max_depth [3, 5, 7] and n_estimators [50, 100]", "Evaluate final model performance on the test set using accuracy, F1 score, precision, and recall", "Generate and analyze the confusion matrix", "Extract and rank feature importances from the trained model"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.81, "recall": 0.88, "confusion_matrix": [[8, 2], [1, 7]], "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}, "top_feature_importances": {"HumidityPercent": 0.28, "RainfallMM": 0.22, "AvgTempC": 0.18, "SoilPH": 0.12, "Variety_roma": 0.1, "Variety_heirloom": 0.1}}}
{"purpose": "Build a classification model to predict whether a taxi trip will exceed 30 minutes in duration.", "raw_table": "trip_id,driver_id,passenger_count,trip_distance_km,pickup_neighborhood,dropoff_neighborhood,day_of_week,trip_duration_min,trip_longer_than_30_min\n1,DR123,2,5.4,Downtown,Uptown,Monday,28,No\n2,dr124,1,12.1,Suburb,Downtown,Tuesday,45,Yes\n3,DR125,3,3.7,downtown,Suburb,Wednesday,22,No\n4,DR126,,7.8,Uptown,Suburb,Thursday,33,Yes\n5,DR127,2,9.0,Suburb,Suburb,Friday,35,Yes\n6,DR128,1,4.3,Downtown,Unknown,Saturday,20,No\n7,dr129,4,11.2,Uptown,Downtown,Sunday,50,Yes\n8,DR130,2,6.5,Suburb,Uptown,Monday,29,No\n9,DR131,1,8.3,Uptown,Suburb,Tuesday,37,Yes\n10,DR132,3,five,Downtown,Downtown,Wednesday,25,No\n11,DR133,2,10.0,Suburb,Uptown,Thursday,40,Yes\n12,DR134,1,7.1,Downtown,Suburb,Friday,31,Yes\n13,DR135,3,6.2,Uptown,Downtown,Saturday,28,No\n14,DR136,2,8.7,Suburb,Downtown,Sunday,,No\n", "model_steps": ["Load CSV data into a DataFrame and inspect for missing and inconsistent values", "Clean data by fixing capitalization in categorical columns (e.g., pickup_neighborhood) and imputing missing passenger_count with median", "Convert trip_distance_km column from mixed types, replacing non-numeric 'five' with 5.0 and convert to float", "Fill missing trip_duration_min values with median duration", "Encode target 'trip_longer_than_30_min' as binary (Yes=1, No=0)", "One-hot encode categorical features: pickup_neighborhood, dropoff_neighborhood, day_of_week", "Split the dataset into training (80%) and testing (20%) sets", "Standardize numeric features: passenger_count and trip_distance_km", "Train a RandomForestClassifier to predict trip_longer_than_30_min", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze true positives and false negatives", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.79, "f1": 0.81, "confusion_matrix": {"true_positive": 22, "true_negative": 30, "false_positive": 5, "false_negative": 6}, "top_feature_importances": {"trip_distance_km": 0.42, "pickup_neighborhood_Uptown": 0.18, "day_of_week_Sunday": 0.12}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features and location.", "raw_table": "HouseID,Bedrooms,Bathrooms,Size_sqft,Neighborhood,Year_Built,Condition,Sale_Price\n1,3,2,1500,Northside,1995,Good,350000\n2,4,3,2000,southside,2005,Excellent,475000\n3,2,1,850,NorthSide,1980,Fair,180000\n4,3,,1300,Eastend,2010,good,320000\n5,5,4,3000,Westend,2018,Excellent,650000\n6,4,2,2100,southside,1999,Poor,400000\n7,3,2,NaN,Westend,2000,Fair,390000\n8,2,1,900,EastEnd,1990,Good,220000\n9,4,3,2500,NORTHSIDE,2015,Excellent,580000\n10,3,2,1600,Southside,2003,Poor,360000\n11,,2,1800,westend,2012,Fair,420000\n12,3,2,1700,Eastend,1998,Good,370000\n13,4,3,2300,Northside,2007,GOOD,510000\n14,3,2,1600,Southside,NaN,Fair,380000", "model_steps": ["Load raw CSV data into a DataFrame", "Identify and handle missing values by imputing numeric columns with median and categorical with mode", "Correct inconsistent capitalization in categorical columns (Neighborhood and Condition)", "Convert categorical variables Neighborhood and Condition into one-hot encoded features", "Split the dataset into train and test sets with 80% training data and 20% testing data", "Standardize numeric features Bedroom, Bathrooms, Size_sqft, and Year_Built using z-score normalization", "Train a Gradient Boosting Regressor to predict Sale_Price", "Perform hyperparameter tuning using grid search over number of estimators and learning rate", "Evaluate model performance on the test set using RMSE, MAE, and R2 score", "Analyze feature importances from the trained model"], "model_results": {"rmse": 32000, "mae": 25000, "r2": 0.87, "best_hyperparameters": {"n_estimators": 150, "learning_rate": 0.1}, "top_feature_importances": {"Size_sqft": 0.45, "Neighborhood_Northside": 0.15, "Condition_Excellent": 0.12, "Bedrooms": 0.1, "Year_Built": 0.08}}}
{"purpose": "Predict whether a citizen's application for government housing assistance will be approved based on applicant and application characteristics.", "raw_table": "Applicant_ID,Age,Employment_Status,Annual_Income,Household_Size,Region,Previous_Applications,Application_Status\n1,34,Employed,48000,3,North,1,Approved\n2,45,unemployed,25000,5,South,0,Denied\n3,29,Employed,52000,2,East,2,Approved\n4,NaN,Self-Employed,45000,4,west,1,Approved\n5,52,Employed,60000,1,North,3,Denied\n6,41,Unemployed,28000,,South,0,Denied\n7,38,Employed,47000,3,East,1,Approved\n8,27,self-employed,51000,2,West,2,Approved\n9,33,Employed,48000,3,North,1,Approved\n10,55,Unemployed,23000,6,South,0,Denied\n11,48,Employed,58000,3,East,1,Denied\n12,39,Employed,NaN,4,West,0,Approved\n13,31,Employed,49000,2,North,1,Approved\n14,46,Unemployed,26000,5,South,2,Denied", "model_steps": ["Load the dataset and inspect for missing or inconsistent values", "Impute missing Age and Annual_Income values with median of respective columns", "Standardize capitalization in Employment_Status and Region columns, e.g., convert all to lowercase", "One-hot encode categorical variables Employment_Status and Region", "Fill missing Household_Size with median value", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features: Age, Annual_Income, Household_Size, Previous_Applications", "Train a RandomForestClassifier to predict Application_Status", "Perform grid search over number of trees (n_estimators) and max_depth hyperparameters", "Evaluate model performance using accuracy, precision, recall, and F1 score on test set", "Generate confusion matrix and feature importance plot"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": {"true_positive": 7, "true_negative": 4, "false_positive": 1, "false_negative": 1}, "top_feature_importances": {"Annual_Income": 0.32, "Employment_Status_employed": 0.25, "Household_Size": 0.15, "Previous_Applications": 0.1, "Region_north": 0.08}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a customer will make a purchase during their visit based on their browsing behavior and demographic data.", "raw_table": "customer_id,age,gender,browsing_time_minutes,num_items_viewed,device_type,day_of_week,purchase_made\n1,25,Male,15,5,Mobile,Monday,Yes\n2,42,Female,NA,3,desktop,Tuesday,No\n3,37,female,20,7,Mobile,Wednesday,Yes\n4,29,Male,8,2,Tablet,Thursday,No\n5,50,Male,30,15,mobile,Friday,Yes\n6,31,,25,NA,Desktop,Saturday,No\n7,22,Female,5,1,Tablet,Sunday,No\n8,45,Male,12,4,Mobile,Monday,Yes\n9,34,Female,18,6,Desktop,Tuesday,Yes\n10,39,Male,NA,3,tablet,Wednesday,No\n11,27,Female,22,9,Mobile,Thursday,Yes\n12,48,male,10,2,Desktop,Friday,No\n13,41,Female,16,5,Mobile,Saturday,Yes\n14,36,Male,7,NA,Tablet,Sunday,No", "model_steps": ["Load raw CSV data and handle missing values by imputing median for numeric and mode for categorical columns", "Standardize 'age', 'browsing_time_minutes', and 'num_items_viewed' numeric features", "Normalize capitalization and unify categories in 'gender' and 'device_type' columns", "One-hot encode categorical variables: 'gender', 'device_type', and 'day_of_week'", "Split data into train and test sets with 80% training and 20% testing", "Train a RandomForestClassifier to predict 'purchase_made'", "Tune hyperparameters with grid search over 'n_estimators' and 'max_depth'", "Evaluate model performance using accuracy, precision, recall, and F1 score on the test set", "Generate and analyze confusion matrix", "Identify and report top feature importances"], "model_results": {"accuracy": 0.86, "precision": 0.83, "recall": 0.79, "f1": 0.81, "confusion_matrix": {"true_negative": 5, "false_positive": 1, "false_negative": 2, "true_positive": 6}, "top_feature_importances": {"browsing_time_minutes": 0.32, "num_items_viewed": 0.25, "device_type_Mobile": 0.15, "age": 0.1, "gender_Female": 0.08}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Build a classification model to predict whether a farm field will have high or low crop yield based on soil, weather, and farming practice features.", "raw_table": "FieldID,SoilType,Rainfall_mm,AvgTemperature_C,FertilizerUsed,PreviousCrop,YieldCategory\n1,Loam,400,22.5,Yes,Corn,High\n2,Clay,350,21.0,No,Wheat,Low\n3,SAND,420,,Yes,soybean,High\n4,LoAm,380,20.5,Yes,Rice,High\n5,Clay,300,19.0,No,corn,Low\n6,Silt,450,23.0,Yes,Wheat,High\n7,Silt,NaN,22.0,Yes,Rice,High\n8,loam,390,21.5,Yes,Soybean,High\n9,Clay,340,20.0,No,Rice,Low\n10,Silt,410,22.8,Yes,Corn,High\n11,Loam,400,22.7,yes,Corn,High\n12,clay,355,19.5,no,Rice,Low\n13,Loam,385,21.8,Yes,soybean,High\n14,Sand,430,22.3,Yes,Wheat,High", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing and inconsistent values", "Standardize categorical values in 'SoilType' and 'PreviousCrop' columns (e.g., unify capitalization)", "Impute missing numeric values in 'AvgTemperature_C' and 'Rainfall_mm' using median imputation", "Convert 'FertilizerUsed' to binary indicator (Yes=1, No=0), correcting inconsistent capitalization", "One-hot encode categorical variables: 'SoilType' and 'PreviousCrop'", "Encode target variable 'YieldCategory' as binary (High=1, Low=0)", "Split data into training and testing sets with 80/20 ratio", "Standardize numeric features: 'Rainfall_mm' and 'AvgTemperature_C'", "Train a RandomForestClassifier with 100 trees on the training set", "Perform 5-fold cross-validation to tune 'max_depth' hyperparameter", "Evaluate the model on the test set by computing accuracy, precision, recall, and F1 score", "Generate and display the confusion matrix and top 3 feature importances"], "model_results": {"accuracy": 0.86, "precision": 0.88, "recall": 0.9, "f1": 0.89, "confusion_matrix": [[3, 1], [1, 8]], "top_feature_importances": {"FertilizerUsed": 0.32, "Rainfall_mm": 0.25, "SoilType_Loam": 0.19}, "best_max_depth": 7}}
{"purpose": "Predict whether a customer will make a purchase during a promotional campaign based on their profile and browsing behavior.", "raw_table": "CustomerID,Age,Gender,Income,VisitedPages,LastPurchaseDaysAgo,PreferredCategory,MadePurchase\n1,34,Male,55000,12,45,Electronics,Yes\n2,27,Female,72000,8,NA,Home,Garden,No\n3,45,male,48000,15,30,Fashion,Yes\n4,52,Female,NaN,5,90,Home,No\n5,31,Female,61000,10,60,Electronics,Yes\n6,29,M,70000,7,15,Fashion,No\n7,40,Female,53000,NA,20,Electronics,Yes\n8,37,Male,60000,11,25,Fashion,No\n9,22,Female,45000,9,40,home,Yes\n10,50,Male,58000,6,80,Garden,No\n11,36,Female,62000,13,10,Fashion,Yes\n12,28,Female,59000,14,5,Electronics,No\n13,33,MALE,63000,10,35,Home,Yes\n14,41,Female,51000,8,50,Garden,No", "model_steps": ["Load the CSV data into a DataFrame and inspect for inconsistencies and missing values", "Clean 'Gender' column by standardizing capitalization and mapping 'M', 'male', 'MALE' to 'Male'", "Fill missing numeric values in 'Income' and 'VisitedPages' using median imputation", "Fill missing 'LastPurchaseDaysAgo' with the median value", "One-hot encode categorical features: 'Gender' and 'PreferredCategory', after cleaning 'PreferredCategory' capitalization", "Convert target variable 'MadePurchase' to binary labels: 'Yes' -> 1, 'No' -> 0", "Split data into training and test sets with an 80/20 ratio, stratified by target", "Standardize numeric features: 'Age', 'Income', 'VisitedPages', 'LastPurchaseDaysAgo'", "Train a RandomForestClassifier with 100 trees on the training data", "Evaluate the model on the test set using accuracy, precision, recall, and F1 score", "Generate a confusion matrix to analyze prediction errors", "Identify top 3 important features from the trained model"], "model_results": {"accuracy": 0.83, "precision": 0.8, "recall": 0.78, "f1": 0.79, "confusion_matrix": {"true_positive": 5, "true_negative": 6, "false_positive": 2, "false_negative": 2}, "top_feature_importances": {"LastPurchaseDaysAgo": 0.32, "VisitedPages": 0.27, "Income": 0.18}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Build a classification model to predict whether a manufactured part will pass quality inspection based on sensor and process data.", "raw_table": "Part_ID,Machine_Setting,Operator,Temperature_C,Pressure_psi,Humidity_pct,Material_Batch,Defect\n001,High,John,75,30,40,Batch_A,Pass\n002,medium,jane,78,29,38,Batch_B,Pass\n003,Low,JOHN,80,NaN,35,Batch_A,Fail\n004,High,Jane,76,31,?,Batch_C,Pass\n005,Medium,Tom,82,30,40,Batch_B,Fail\n006,low,Tom,79,28,39,Batch_A,Pass\n007,High,Mary,?,33,42,Batch_C,Fail\n008,Medium,Mary,77,32,41,Batch_B,Pass\n009,Medium,John,81,30,40,Batch_C,Fail\n010,low,Tom,80,29,37,Batch_A,Pass\n011,High,jane,83,NaN,39,Batch_B,Fail\n012,Medium,Mary,75,31,?,Batch_A,Pass", "model_steps": ["Load the CSV data into a DataFrame and inspect for missing and inconsistent values", "Normalize capitalization for categorical columns: 'Operator' and 'Machine_Setting'", "Impute missing numeric values: fill missing Temperature_C and Pressure_psi with median values", "Impute missing categorical values in Humidity_pct with the mode or nearest neighbor estimate", "Encode categorical variables 'Machine_Setting', 'Operator', and 'Material_Batch' using one-hot encoding", "Convert target variable 'Defect' to binary: Pass=0, Fail=1", "Split the dataset into training (80%) and testing (20%) sets with stratification on the target", "Standardize numeric features: Temperature_C, Pressure_psi, Humidity_pct", "Train a RandomForestClassifier on the training set with 100 trees", "Evaluate the model on the test set using accuracy, F1-score, precision, and recall", "Generate and analyze the confusion matrix to identify types of errors", "Identify top 3 feature importances from the trained model"], "model_results": {"accuracy": 0.83, "f1": 0.79, "precision": 0.75, "recall": 0.84, "confusion_matrix": [[7, 2], [1, 5]], "top_feature_importances": {"Machine_Setting_High": 0.29, "Temperature_C": 0.25, "Operator_John": 0.15}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Build a classification model to predict whether wheat crops will have high or low yield based on environmental and management factors.", "raw_table": "Field_ID,Soil_Type,Rainfall_mm,Avg_Temperature_C,Fertilizer_Used_kg,Previous_Crop,Yield_Category\n1,Loam,350,22.5,50,Wheat,High\n2,Clay,290,21.0,40,Corn,Low\n3,Sand,310,23.2,,Soybean,High\n4,loam,400,24.1,55,Wheat,High\n5,Clay,NaN,20.8,35,Corn,Low\n6,SAND,320,22.0,45,Barley,Low\n7,Loam,360,23.5,50,Wheat,High\n8,Clay,300,19.5,38,Corn,Low\n9,Loam,330,22.8,47,Soybean,High\n10,Sand,305,23.0,43,Barley,Low\n11,Clay,295,21.3,40,Corn,Low\n12,Loam,NaN,22.1,48,Wheat,High\n13,Sand,315,23.3,44,Soybean,High\n14,Clay,285,20.0,,Corn,Low\n15,loam,355,22.6,52,Wheat,High", "model_steps": ["Identify and clean inconsistent soil type values by standardizing capitalization", "Impute missing numeric values in Rainfall_mm and Fertilizer_Used_kg columns using median values", "Encode categorical variables Soil_Type and Previous_Crop using one-hot encoding", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features Rainfall_mm, Avg_Temperature_C, and Fertilizer_Used_kg", "Train a RandomForestClassifier to predict Yield_Category", "Perform grid search on max_depth with values [3, 5, 7]", "Evaluate model performance on test set using accuracy, precision, recall, and F1-score", "Generate confusion matrix to analyze prediction errors"], "model_results": {"accuracy": 0.87, "precision": 0.89, "recall": 0.85, "f1": 0.87, "confusion_matrix": {"True_Positive": 7, "True_Negative": 6, "False_Positive": 1, "False_Negative": 1}, "top_feature_importances": {"Fertilizer_Used_kg": 0.32, "Rainfall_mm": 0.28, "Soil_Type_Loam": 0.15, "Previous_Crop_Wheat": 0.1, "Avg_Temperature_C": 0.08}, "best_hyperparameters": {"max_depth": 5}}}
{"purpose": "Predict whether a customer will make a purchase during their visit based on session and user attributes.", "raw_table": "session_id,user_age,user_gender,device_type,page_views,time_on_site_minutes,referral_source,purchase_made\nS001,25,M,Mobile,5,12,Organic,Yes\nS002,42,F,Desktop,3,5,Direct,No\nS003,NaN,F,Tablet,7,15,organic,Yes\nS004,30,M,mobile,2,3,Social,No\nS005,22,F,Desktop,10,20,Direct,Yes\nS006,28,,Mobile,4,NaN,Referral,No\nS007,35,M,Tablet,6,14,Social,Yes\nS008,40,F,Desktop,5,10,Referral,No\nS009,29,M,mobile,8,18,Direct,Yes\nS010,NaN,F,Tablet,NaN,12,Organic,No\nS011,33,M,Desktop,3,7,Referral,No\nS012,27,m,Mobile,9,19,Social,Yes", "model_steps": ["Load raw CSV data into a DataFrame", "Identify and impute missing numeric values (e.g., user_age and time_on_site_minutes) using median imputation", "Correct inconsistent capitalization in categorical columns (e.g., device_type and referral_source)", "Fill missing categorical values (e.g., user_gender) with the mode", "Encode categorical variables using one-hot encoding", "Split data into training and test sets (80/20 stratified on purchase_made)", "Standardize numeric features: user_age, page_views, time_on_site_minutes", "Train a Logistic Regression classifier to predict purchase_made", "Evaluate model performance using accuracy, F1 score, precision, and recall on the test set", "Generate confusion matrix and analyze feature coefficients to understand drivers of purchase"], "model_results": {"accuracy": 0.83, "f1": 0.79, "precision": 0.81, "recall": 0.77, "confusion_matrix": {"true_positive": 7, "true_negative": 8, "false_positive": 2, "false_negative": 3}, "top_feature_importances": {"time_on_site_minutes": 0.35, "page_views": 0.25, "device_type_Mobile": 0.15, "referral_source_Organic": 0.1, "user_gender_F": 0.08}, "hyperparameters": {"model_type": "Logistic Regression", "regularization": "L2", "C": 1.0, "solver": "liblinear"}}}
{"purpose": "Build a classification model to predict whether a given day will have extreme heat conditions based on weather and environmental factors.", "raw_table": "date,temperature_c,humidity_percent,wind_speed_kmh,region,soil_moisture,target_extreme_heat\n2024-06-01,35.2,45,15,north,0.12,Yes\n2024-06-02,28.5,60,5,South,0.09,No\n2024-06-03,33.0,55,12,North,missing,Yes\n2024-06-04,29.1,48,8,East,0.11,No\n2024-06-05,40.3,30,20,east,0.05,Yes\n2024-06-06,22.7,70,7,West,0.20,No\n2024-06-07,36.5,50,18,South,0.10,Yes\n2024-06-08,31.0,55,10,west,0.15,No\n2024-06-09,27.8,65,6,NORTH,0.18,No\n2024-06-10,39.0,35,25,North,0.03,Yes\n2024-06-11,30.2,58,9,South,0.07,No\n2024-06-12,34.6,52,14,East,0.13,Yes\n2024-06-13,28.0,62,8,South,,No\n2024-06-14,37.8,40,22,East,0.04,Yes", "model_steps": ["Parse date column and drop it from features", "Handle missing values in soil_moisture by imputing median values", "Normalize inconsistent capitalization in region column", "One-hot encode the region categorical variable", "Split data into train and test sets with 80/20 ratio", "Standardize numeric features: temperature_c, humidity_percent, wind_speed_kmh, soil_moisture", "Train a RandomForestClassifier to predict target_extreme_heat", "Perform grid search over n_estimators and max_depth hyperparameters", "Evaluate model on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix and feature importances"], "model_results": {"accuracy": 0.85, "precision": 0.88, "recall": 0.82, "f1": 0.85, "confusion_matrix": [[7, 1], [2, 4]], "top_feature_importances": {"temperature_c": 0.42, "humidity_percent": 0.22, "wind_speed_kmh": 0.15, "soil_moisture": 0.12, "region_North": 0.05, "region_South": 0.04}, "best_hyperparameters": {"n_estimators": 100, "max_depth": 5}}}
{"purpose": "Predict whether a loan applicant will default within 12 months based on their financial and demographic information.", "raw_table": "ApplicantID,Age,Income,EmploymentStatus,CreditScore,LoanAmount,LoanPurpose,Defaulted\n1,35,55000,Full-time,720,15000,Home,No\n2,42,NaN,part-time,680,12000,Car,YES\n3,29,48000,Full-Time,690,10000,home,No\n4,51,62000,Unemployed,NaN,20000,Education,Yes\n5,47,53000,FULL-TIME,710,NaN,Car,No\n6,38,59000,part-time,700,13000,Home,No\n7,26,45000,full-time,650,9000,Car,No\n8,33,50000,Unemployed,670,11000,Home,Yes\n9,44,58000,Part-Time,690,14000,Education,No\n10,39,60000,FULL-time,710,16000,Car,No\n11,31,52000,Full-time,NaN,12000,Home,Yes\n12,28,47000,Part-time,660,9500,car,No\n13,50,62000,Full-Time,700,20000,Education,Yes\n14,36,,Full-time,680,13000,Car,No", "model_steps": ["Load the dataset and identify the target variable as 'Defaulted'.", "Handle missing values: impute missing 'Income' and 'CreditScore' with median values, and 'LoanAmount' with mean value.", "Standardize inconsistent capitalization in 'EmploymentStatus' and 'LoanPurpose' columns.", "Encode categorical variables 'EmploymentStatus' and 'LoanPurpose' using one-hot encoding.", "Convert the target variable 'Defaulted' to binary labels (Yes=1, No=0).", "Split data into training and test sets with an 80/20 ratio.", "Standardize numeric features 'Age', 'Income', 'CreditScore', and 'LoanAmount' using z-score normalization based on training data.", "Train a Logistic Regression classifier on the training set.", "Evaluate the model on the test set calculating accuracy, precision, recall, and F1 score.", "Generate a confusion matrix to analyze prediction errors."], "model_results": {"accuracy": 0.79, "precision": 0.75, "recall": 0.68, "f1": 0.71, "confusion_matrix": [[17, 5], [6, 13]], "top_feature_importances": {"CreditScore": 0.42, "LoanAmount": 0.25, "EmploymentStatus_Full-time": 0.15, "Income": 0.1, "LoanPurpose_Home": 0.08}, "hyperparameters": {"model_type": "LogisticRegression", "regularization": "L2", "C": 1.0, "max_iter": 100}}}
{"purpose": "Build a regression model to predict hourly electricity demand in megawatts based on weather and operational conditions.", "raw_table": "Hour,Temperature_C,Day_Type,Wind_Speed_kmph,Holiday,Demand_MW\n0,15.2,weekday,12.5,No,350\n1,14.8,weekday,8.3,no,340\n2,13.5,Weekday,10.1,No,325\n3,12.9,weekday,7.0,No,310\n4,12.7,weekday,missing,No,305\n5,13.0,weekday,6.5,no,300\n6,14.2,Weekend,5.2,Yes,400\n7,16.8,weekend,4.9,yes,450\n8,19.1,weekend,3.8,Yes,500\n9,21.4,WeekEnd,3.2,Yes,550\n10,23.0,Weekday,4.0,no,600\n11,24.5,weekday,5.5,No,620\n12,25.1,Weekday,6.2,no,630\n13,25.6,weekday,7.1,No,625", "model_steps": ["Load dataset and identify 'Demand_MW' as the target variable for regression", "Fix inconsistent capitalization in 'Day_Type' and 'Holiday' columns", "Impute missing values in 'Wind_Speed_kmph' using mean imputation", "Convert categorical variables 'Day_Type' and 'Holiday' into one-hot encoded features", "Split data into training and testing sets with an 80/20 ratio", "Standardize numeric features: 'Hour', 'Temperature_C', and 'Wind_Speed_kmph'", "Train a RandomForestRegressor model on the training data", "Perform grid search to tune number of trees (n_estimators) between 50 and 150", "Evaluate the model on the test set using RMSE, MAE, and R2 metrics", "Extract and report top 3 feature importances from the trained model"], "model_results": {"rmse": 20.5, "mae": 15.2, "r2": 0.87, "top_feature_importances": {"Temperature_C": 0.4, "Hour": 0.35, "Day_Type_Weekend": 0.15}, "best_hyperparameters": {"n_estimators": 100}}}
{"purpose": "Predict customer churn probability for a telecom provider using customer usage and service features.", "raw_table": "customer_id,tenure_months,monthly_charges,internet_service,contract_type,tech_support,churn\n001,12,70.5,Fiber optic,Month-to-month,Yes,No\n002,5,85.7,Fiber optic,Month-to-month,No,Yes\n003,38,45.6,Dsl,One year,Yes,No\n004,2,99.9,fiber Optic,Month-to-month,No,Yes\n005,23,65.0,DSL,Two year,Yes,No\n006,,75.4,Fiber optic,Month-to-month,NO,Yes\n007,45,49.9,None,One year,No,No\n008,3,89.0,Fiber optic,Month-to-month,Yes,Yes\n009,15,60.0,Dsl,Month-to-month,,No\n010,29,72.4,DSL,Two year,Yes,No\n011,1,105.3,Fiber Optic,Month-to-month,No,Yes\n012,8,80.1,Fiber optic,Month-to-month,Yes,Yes\n013,50,40.0,dsl,Two year,No,No\n014,13,68.2,Fiber optic,One year,Yes,No\n015,20,58.3,None,One year,No,No", "model_steps": ["Load data and identify target variable 'churn'", "Handle missing values in 'tenure_months' and 'tech_support' by imputing median and mode respectively", "Normalize capitalization inconsistencies in 'internet_service' and 'tech_support' columns", "One-hot encode categorical variables: 'internet_service', 'contract_type', and 'tech_support'", "Standardize numeric features: 'tenure_months' and 'monthly_charges'", "Split data into training (80%) and test (20%) sets randomly", "Train a RandomForestClassifier with default parameters on the training set", "Perform 5-fold cross-validation and tune max_depth and n_estimators via grid search", "Evaluate the final model on the test set computing accuracy, precision, recall, and F1 score", "Generate and analyze the confusion matrix", "Identify top 3 feature importances from the trained RandomForest model"], "model_results": {"accuracy": 0.87, "f1": 0.84, "precision": 0.81, "recall": 0.88, "confusion_matrix": {"true_positive": 7, "true_negative": 10, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"contract_type_Month-to-month": 0.32, "internet_service_Fiber optic": 0.25, "monthly_charges": 0.18}, "best_hyperparameters": {"max_depth": 8, "n_estimators": 100}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features.", "raw_table": "house_id,num_bedrooms,num_bathrooms,sqft_living,neighborhood,year_built,has_garage,sale_price\n1,3,2,1400,Downtown,1990,Yes,350000\n2,4,3,2000,Suburb,1985,No,420000\n3,2,,900,suburb,2005,yes,280000\n4,5,4,3200,Downtown,2018,Yes,670000\n5,3,2,1500,Suburb,NaN,No,360000\n6,3,2,1400,Suburb,1995,YES,365000\n7,4,3,2100,Downtown,2000,No,480000\n8,2,1,800,Suburb,2010,No,250000\n9,3,2,1600,Downtown,1998,Yes,400000\n10,3,2,1700,Suburb,2003,No,390000\n11,4,3,1800,Suburb,1999,No,410000\n12,2,1,850,Downtown,2007,Yes,300000\n13,3,2,1500,suburb,2012,No,370000\n14,4,3,2200,Downtown,1980,yes,460000", "model_steps": ["Load the dataset from the raw CSV string.", "Identify 'sale_price' as the target variable for regression.", "Correct inconsistent capitalization in 'neighborhood' and 'has_garage' columns (e.g., 'suburb' to 'Suburb', 'yes'/'YES' to 'Yes').", "Impute missing values in 'num_bathrooms' with the median value.", "Impute missing values in 'year_built' with the median year.", "One-hot encode the categorical variables 'neighborhood' and 'has_garage'.", "Standardize numeric features: 'num_bedrooms', 'num_bathrooms', 'sqft_living', and 'year_built'.", "Split data into training (80%) and testing (20%) sets randomly.", "Train a Gradient Boosting Regressor on the training set with default hyperparameters.", "Evaluate the model on the test set using RMSE, MAE, and R2 metrics.", "Analyze feature importances from the trained model."], "model_results": {"rmse": 28000, "mae": 21500, "r2": 0.87, "top_feature_importances": {"sqft_living": 0.45, "num_bedrooms": 0.2, "year_built": 0.15, "has_garage_Yes": 0.1, "neighborhood_Downtown": 0.1}}}
{"purpose": "Build a classification model to predict whether daily air quality is 'Good' or 'Poor' based on meteorological and pollution indicators.", "raw_table": "Date,Temperature_C,Humidity,Wind_Speed_kmh,City,NO2_ppb,PM2.5_ug/m3,Air_Quality\n2024-04-01,22.5,45,15,New York,40,12,Good\n2024-04-02,18.7,55,20,Los Angeles,35,NA,Poor\n2024-04-03,25,40,10,new york,50,20,Good\n2024-04-04,21,50,12,Chicago,45,25,poor\n2024-04-05,19,60,,Chicago,38,18,Good\n2024-04-06,23,48,14,Los Angeles,42,22,Poor\n2024-04-07,20,52,16,New York,NA,15,Good\n2024-04-08,17,65,18,Chicago,37,19,Poor\n2024-04-09,24,43,13,los angeles,43,21,Poor\n2024-04-10,22,47,14,New York,41,17,Good\n2024-04-11,16,70,19,Chicago,40,24,Poor\n2024-04-12,21,49,15,Los Angeles,39,20,Good\n2024-04-13,23,44,11,New York,44,16,Poor\n2024-04-14,18,,17,Chicago,36,23,Poor", "model_steps": ["Load the raw CSV data into a DataFrame", "Clean the 'City' column by standardizing capitalization (e.g., 'new york' and 'los angeles' to 'New York' and 'Los Angeles')", "Handle missing values: Impute missing numeric values in 'Humidity', 'Wind_Speed_kmh', 'NO2_ppb', and 'PM2.5_ug/m3' with median values", "Convert target variable 'Air_Quality' to binary labels ('Good' = 0, 'Poor' = 1)", "Drop the 'Date' column as it is not predictive", "One-hot encode the 'City' categorical variable", "Split the dataset into training (80%) and testing (20%) sets with stratification on the target", "Standardize numeric features: 'Temperature_C', 'Humidity', 'Wind_Speed_kmh', 'NO2_ppb', 'PM2.5_ug/m3'", "Train a RandomForestClassifier with 100 trees on the training data", "Evaluate the model on the test set computing accuracy, precision, recall, and F1 score", "Generate and display the confusion matrix", "Extract and rank feature importances from the trained model"], "model_results": {"accuracy": 0.85, "precision": 0.83, "recall": 0.88, "f1": 0.85, "confusion_matrix": [[6, 1], [2, 7]], "top_feature_importances": {"PM2.5_ug/m3": 0.28, "NO2_ppb": 0.25, "City_New York": 0.15, "Humidity": 0.12, "Temperature_C": 0.1, "City_Chicago": 0.07, "Wind_Speed_kmh": 0.03}, "hyperparameters": {"n_estimators": 100, "max_depth": null, "random_state": 42}}}
{"purpose": "Predict whether a social media post will go viral based on its content and user engagement metrics.", "raw_table": "post_id,user_followers,post_length,post_type,hashtags_count,time_posted,is_verified,likes,target_viral\n1,1500,120,text,3,Morning,True,300,Yes\n2,850,250,Video,5,Afternoon,False,180,No\n3,NaN,75,Image,NA,Evening,TRUE,75,No\n4,2000,300,text,7,morning,True,450,Yes\n5,500,100,video,2,Afternoon,false,120,No\n6,1200,150,Image,3,Evening,True,290,Yes\n7,700,90,Text,4,MORNING,False,80,No\n8,950,130,video,5,Night,False,260,Yes\n9,1100,NaN,Image,3,Evening,True,NaN,Yes\n10,600,110,text,2,Afternoon,False,150,No\n11,800,95,Text,3,Morning,TRUE,200,Yes\n12,1400,NaN,Video,4,afternoon,True,310,Yes\n13,NaN,125,Image,,Night,False,180,No\n14,1000,140,Text,4,Morning,True,270,Yes", "model_steps": ["Load the dataset and inspect for missing or inconsistent data", "Clean 'is_verified' column by standardizing capitalization and converting to boolean", "Impute missing numeric values ('user_followers', 'post_length', 'likes') using median imputation", "Fill missing 'hashtags_count' with zero and correct inconsistent capitalization in 'post_type' and 'time_posted'", "Encode categorical variables 'post_type' and 'time_posted' using one-hot encoding", "Convert target 'target_viral' from Yes/No to binary labels 1/0", "Split data into training and test sets with an 80/20 ratio", "Standardize numeric features: 'user_followers', 'post_length', 'hashtags_count', and 'likes'", "Train a RandomForestClassifier on the training data", "Evaluate model performance using accuracy, F1 score, precision, and recall on the test set", "Analyze feature importances to understand which factors most influence virality"], "model_results": {"accuracy": 0.85, "f1": 0.83, "precision": 0.8, "recall": 0.87, "feature_importances": {"likes": 0.35, "user_followers": 0.25, "post_length": 0.15, "post_type_Video": 0.1, "time_posted_Morning": 0.08, "hashtags_count": 0.07}}}
{"purpose": "Predict whether a government grant application will be approved based on applicant and project characteristics.", "raw_table": "Applicant_ID,Age,Education_Level,Project_Type,Requested_Amount,Previous_Grants,Approval_Status\nA001,34,Bachelors,Infrastructure,50000,2,Approved\nA002,28,Masters,Education,NaN,0,Denied\nA003,45,PhD,Healthcare,120000,1,Approved\nA004,38,bachelors,Infrastructure,75000,3,Approved\nA005,,Masters,Education,30000,0,Denied\nA006,50,PhD,environment,90000,2,Approved\nA007,29,Bachelors,Healthcare,45000,,Denied\nA008,31,Master,Education,40000,1,Denied\nA009,42,PhD,Infrastructure,110000,4,Approved\nA010,36,Bachelors,Healthcare,55000,1,Denied\nA011,27,Masters,Education,38000,0,Denied\nA012,40,PhD,Environment,95000,2,Approved\nA013,33,Bachelors,Infrastructure,60000,1,Denied\nA014,39,PhD,Healthcare,NaN,3,Approved", "model_steps": ["Load the dataset and identify missing values and inconsistent capitalization", "Normalize 'Education_Level' and 'Project_Type' categories to consistent casing and correct typos", "Impute missing numeric values (Age, Requested_Amount, Previous_Grants) using median imputation", "Encode categorical variables 'Education_Level' and 'Project_Type' using one-hot encoding", "Split data into train and test sets with an 80/20 ratio", "Standardize numeric features (Age, Requested_Amount, Previous_Grants)", "Train a RandomForestClassifier to predict 'Approval_Status'", "Tune max_depth and n_estimators using grid search with cross-validation", "Evaluate model performance on test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze classification errors"], "model_results": {"accuracy": 0.85, "f1": 0.84, "precision": 0.82, "recall": 0.87, "confusion_matrix": {"true_positive": 7, "true_negative": 6, "false_positive": 2, "false_negative": 1}, "top_feature_importances": {"Requested_Amount": 0.32, "Previous_Grants": 0.27, "Project_Type_Infrastructure": 0.15, "Education_Level_PhD": 0.13, "Age": 0.13}, "best_hyperparameters": {"max_depth": 6, "n_estimators": 100}}}
{"purpose": "Predict whether a retail customer will make a purchase during a promotional campaign.", "raw_table": "CustomerID,Age,Gender,Annual_Income,Membership_Status,Last_Purchase_Amount,Visits_Last_30_Days,Made_Purchase\n1,34,Male,54000,Gold,120.50,5,Yes\n2,58,Female,62000,Silver,85,,No\n3,45,male,58000,Platinum,NaN,7,Yes\n4,25,Female,43000,bronze,45.00,3,No\n5,39,Female,Not Available,Gold,130.00,6,Yes\n6,51,Male,67000,Silver,NaN,8,No\n7,29,Female,48000,Silver,75.00,4,Yes\n8,42,MALE,52000,Bronze,60,2,No\n9,36,Female,61000,Gold,140,9,Yes\n10,31,male,50000,Platinum,NaN,5,No\n11,47,Female,59000,Silver,110.75,6,Yes\n12,,Male,53000,Gold,95.00,7,No\n13,50,Female,58000,platinum,125.00,8,Yes\n14,44,Female,60000,Silver,NaN,3,No", "model_steps": ["Handle missing values in numeric columns by imputing median values", "Standardize numeric features: Age, Annual_Income, Last_Purchase_Amount, Visits_Last_30_Days", "Normalize inconsistent capitalization in Gender and Membership_Status columns", "One-hot encode categorical variables: Gender and Membership_Status", "Convert target variable 'Made_Purchase' to binary (Yes=1, No=0)", "Split data into training and testing sets with 80/20 ratio", "Train a Logistic Regression classifier on the training data", "Perform grid search to tune regularization parameter 'C' with values [0.01, 0.1, 1, 10]", "Evaluate model performance on the test set using accuracy, precision, recall, and F1 score", "Generate confusion matrix to analyze classification errors"], "model_results": {"accuracy": 0.79, "precision": 0.82, "recall": 0.75, "f1": 0.78, "confusion_matrix": [[8, 3], [2, 11]], "best_hyperparameters": {"C": 1}, "top_feature_importances": {"Visits_Last_30_Days": 0.45, "Membership_Status_Platinum": 0.3, "Last_Purchase_Amount": 0.2, "Gender_Male": -0.1}}}
{"purpose": "Build a regression model to estimate house sale prices based on property features.", "raw_table": "HouseID,Size_sqft,Bedrooms,Bathrooms,Neighborhood,Year_Built,Garage,Condition,SalePrice\n1,2100,3,2,Suburb,1998,Yes,Good,350000\n2,1500,2,1,Downtown,2005,No,Excellent,275000\n3,1800,3,2,Suburb,NaN,Yes,Fair,300000\n4,2400,4,3,Suburb,2010,YES,Good,400000\n5,1300,2,1,Downtown,2003,No,excellent,260000\n6,1700,3,2,Midtown,1995,No,Fair,280000\n7,2200,4,3,Suburb,2012,Yes,Good,410000\n8,1600,2,1,Midtown,NaN,No,Fair,270000\n9,2000,3,2,Suburb,2000,Yes,Good,330000\n10,1400,2,1,Downtown,2004,no,Excellent,265000\n11,1900,3,2,Midtown,1999,No,Good,295000\n12,1550,2,1,Downtown,2006,No,Good,280000\n13,2300,4,3,Suburb,2011,Yes,Good,405000\n14,1250,2,1,Midtown,1997,No,Fair,255000", "model_steps": ["Load the CSV data into a DataFrame", "Identify and handle missing values in Year_Built by imputing the median year", "Standardize capitalization and values in 'Garage' and 'Condition' categorical columns", "One-hot encode 'Neighborhood', 'Garage', and 'Condition' categorical features", "Split the dataset into training (80%) and testing (20%) sets", "Standardize numeric features: Size_sqft, Bedrooms, Bathrooms, Year_Built", "Train a RandomForestRegressor model on the training set", "Tune hyperparameters max_depth and n_estimators using grid search with 5-fold cross-validation", "Evaluate the model on the test set using RMSE, MAE, and R2 metrics", "Analyze feature importances from the trained model"], "model_results": {"rmse": 18000, "mae": 14000, "r2": 0.85, "best_hyperparameters": {"max_depth": 12, "n_estimators": 150}, "top_feature_importances": {"Size_sqft": 0.35, "Neighborhood_Suburb": 0.2, "Year_Built": 0.15, "Bedrooms": 0.1, "Condition_Good": 0.08, "Garage_Yes": 0.07, "Bathrooms": 0.05}}}
{"purpose": "Build a classification model to predict if a manufactured part will be defective based on sensor readings and machine settings.", "raw_table": "Part_ID,Machine_Type,Operator,Temperature,Pressure,Humidity,Shift,Defect\n1,TypeA,John,75.3,1.2,45,Day,No\n2,typeb,Mary,77.8,1.3,48,Night,Yes\n3,TypeA,alex,missing,1.1,44,Day,No\n4,TypeC,John,76.5,1.5,missing,Night,Yes\n5,TypeB,Mary,78.0,1.4,46,day,No\n6,TypeA,Alex,74.9,1.2,45,Night,No\n7,TypeC,Mary,missing,1.7,49,Day,Yes\n8,TypeB,john,77.2,missing,47,Night,No\n9,TypeC,Alex,75.1,1.6,48,Day,Yes\n10,TypeA,Mary,76.8,1.3,44,Night,No\n11,TypeB,John,77.5,1.4,missing,Day,Yes\n12,TypeC,Alex,missing,1.7,50,Night,Yes\n13,TypeA,Mary,78.1,1.2,45,Day,No\n14,typeb,John,75.7,1.5,46,Night,Yes", "model_steps": ["Inspect dataset for missing values and inconsistent capitalization in categorical variables", "Impute missing numeric values using median imputation per feature", "Standardize numeric columns: Temperature, Pressure, Humidity", "Normalize casing in categorical variables: Machine_Type, Operator, Shift", "One-hot encode categorical variables: Machine_Type, Operator, Shift", "Split data into training and test sets (85/15 split)", "Train a RandomForestClassifier with 100 trees on the training data", "Perform hyperparameter tuning on max_depth with grid search (values: 3, 5, 7)", "Evaluate model using accuracy, F1 score, precision, and recall on test data", "Generate confusion matrix and identify top 3 feature importances"], "model_results": {"accuracy": 0.86, "f1": 0.84, "precision": 0.81, "recall": 0.87, "confusion_matrix": [[7, 1], [2, 4]], "top_feature_importances": {"Pressure": 0.32, "Temperature": 0.27, "Machine_Type_TypeC": 0.15}, "best_hyperparameters": {"max_depth": 5, "n_estimators": 100}}}
